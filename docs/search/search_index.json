{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\/\\s\\-\\.]+"},"docs":[{"location":"","text":"","title":"\u4e3b\u9875"},{"location":"about-analytics-cookies/","text":"By using this website, you are confirming that you accept our use of cookies for the purposes of collecting anonymous usage data to improve the user experience and content on the website. Continue to sections below for details about knative.dev, or use the following resources to learn about cookies in general: Learn about basic site analytics usuage at: https://www.cookiechoices.org/ You can also watch a video about how Google uses cookies . What are cookies? \u00b6 Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior. What cookies are used on knative.dev \u00b6 We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement . Options for opting out \u00b6 Use the following options to prevent your data from being shared with websites. Opt-out Browser Add-on \u00b6 You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on . Disabling cookies \u00b6 You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Learn about Google Analytics cookies"},{"location":"about-analytics-cookies/#what-are-cookies","text":"Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior.","title":"What are cookies?"},{"location":"about-analytics-cookies/#what-cookies-are-used-on-knativedev","text":"We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement .","title":"What cookies are used on knative.dev"},{"location":"about-analytics-cookies/#options-for-opting-out","text":"Use the following options to prevent your data from being shared with websites.","title":"Options for opting out"},{"location":"about-analytics-cookies/#opt-out-browser-add-on","text":"You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on .","title":"Opt-out Browser Add-on"},{"location":"about-analytics-cookies/#disabling-cookies","text":"You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Disabling cookies"},{"location":"about/testimonials/","text":"Enterprise-grade Serverless on your own terms. Understanding Knative \"If Kubernetes is an electrical grid, then Knative is its light switch.\" \u2014Kelsey Hightower, Google Cloud Platform Knative is an automated system that helps development teams manage and maintain processes in Kubernetes. Its purpose is to simplify, automate, and monitor deployments of Kubernetes so teams spend less time on maintenance and more time on app development and projects. Knative takes over repetitive and time-intensive tasks while removing obstacles and delays. Knative does this through two features. The first is Knative Eventing. Eventing allows developers to set up detailed actions triggered by specific events within a broader environment. The second is Knative Serving, which automatically manages the creation and scaling of services through Kubernetes, including scaling down to zero. Each of these features aims to free up resources that teams would otherwise spend managing systems. They also save businesses money by reacting to conditions in real time. Meaning, companies only pay for the resources they are using, not the ones they might use. Scale to Zero is a feature of Knative Serving that automatically turns off services running in containers when there is no demand for them. Instead of running programs on standby, they can be turned off and turned back on when needed again. Scale to zero reduces costs over time and helps manage technical resources. The core idea behind Knative is to allow teams to harness the power of serverless application deployment. Serverless refers to managing cloud-based servers and virtual machines, often hosted on platforms like AWS, Google Cloud, Microsoft Azure, and more. Serverless is a great option for companies looking to move away from the costly endeavor of managing their own servers and infrastructure. \"I often think of Knative as part of 'Serverless 2.0.' It combines the good things about serverless with a loosening of constraints around execution time and availability of resources.\" -Michael Behrendt, Distinguished Engineer and Chief Architect of Serverless and Cloud Functions for IBM. IBM is a committed sponsor of Knative Knative in the broader ecosystem To understand Knative more fully, it is important to know that it exists in a larger ecosystem of services that work together. For example, Knative acts as a framework on top of Kubernetes that helps build a serverless platform. Kubernetes itself is a system that orchestrates the creation and running of containers used in app deployment, scaling, and more. Those containers can run anything, from simple tools written in python to complex Al systems. Containers were developed to help tackle the problem of complexity. As development teams build software products, they create massive codebases. Left unorganized, those codebases can become gigantic and confusing-even for those who make them. Containers solve this problem by breaking codebases into small, self-contained processes that can interact to do work. They also help developers manage complex webs of dependencies like APIs and databases. These containers are easier to maintain for teams looking to work fast while maintaining best practices. Knative's value in DevOps DevOps promises effective application development processes with faster deployments and fewer bugs. While Kubernetes helps facilitate this, it can produce significant complexity. Achieving value at scale with Kubernetes traditionally involves teams developing specialized knowledge. Knative cuts down on that by providing a serverless experience that removes the need for all development team members to know or understand the ins and outs of Kubernetes. \"What we are doing with Knative is to provide a developer experience that makes it easier to focus on code. Cloud developers focus on the business problems they are solving without having to coordinate or wait for approvals from platform teams to scale their apps. Knative is a framework that helps automate platform capabilities so your apps can scale as if they were running on Serverless compute.\" -Aparna Sinha, Director of Product Management, Google Tangible benefits of Knative for teams It has always been true that organizations need to develop and innovate faster than their competition while deploying products with fewer flaws. However, being bogged down by configuring networks and operating systems harms developer productivity and morale. Developers want to create things, and Knative helps them do that. \"The amount of internal work needed to use Knative is minuscule.\" -Tilen Kav\u010di\u010d, Backend Developer for Outfit7, which uses Knative for key backend system The advantage of Open Source Open source has been a powerful resource for creating business solutions for decades. Kubernetes and Knative are now paving the way for that relationship to become stronger. Each project has significant support from some of the biggest names in tech including IBM, Google, Red Hat, and VMware. The Kubernetes and Knative ecosystem consists of widely adopted projects that are proven across many installations for a multitude of uses. The open-source foundation of Knative means that anyone using the platform can participate in the community to get help, solve problems, and influence the direction of deployment for future versions. Find out more > Case Studies: Read about organizations using Knative, from platform developers to proven companies to innovative startups > Check the getting started guide to get up and running with Knative in an afternoon > Join the Knative Slack to talk to the community","title":"\u5956\u72b6"},{"location":"about/case-studies/deepc/","text":"deepc Case Study \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d -- Andrew Webber, Senior Software Engineer for deepc AI Startup deepc Connects Researchers to Radiologists with Knative Eventing deepc is a startup at the cutting edge, bringing advanced data techniques and artificial intelligence to healthcare. The German company helps radiologists access better diagnostics and resources through cloud-based technology. Their goal is to elevate treatment quality across the board while creating more efficiency and improvement in medical settings. This revolution in technology comes with hefty challenges. Care systems are highly regulated, making patient privacy and safety a top priority for deepc. Doctors and medical staff also demand that new technologies be reliable and stable when lives are on the line. Rising to the challenge deepc has risen to meet these challenges through carefully architected solutions, using tools like Knative Eventing to their full potential. Their product helps radiologists access a wide selection of artificial intelligence (AI) programs that analyze imaging, like x-rays and MRIs. The data generated from these AI programs help radiologists make more accurate diagnoses. The deepc workflow The radiologist uploads the image into deepcOS, initially to a virtual machine within the hospital IT infrastructure containing the deepcOS client application. After making a series of selections, deepcOS identifies the proper AI to use. It then removes the patient information from the scans before encrypting the data. deepcOS sends that data to the cloud-based deepc AI platform. This platform does the heavy lifting in providing the computing power the AI algorithms need to do their work. After the program finishes, the results are sent back. Finally, the data is reassociated with the patient, and the radiologist can take action based on the results. Critically, patient information always remains on-premises in the hospital and is not transmitted to deepc servers. A Knative-powered process The deepcOS workflow builds on a sophisticated implementation of Knative Eventing. Knative Eventing allows teams to deploy event-driven architecture with serverless applications quickly. In conjunction with Knative Serving, deepc resources and programs scale up and down automatically based on specific event triggers laid out by the developers. Knative takes care of the management, so the process does not need to wait for a person to take action. When data is sent to deepc's cloud-based platform, Knative emits an event that triggers a specific AI. After one is selected, Knative starts a container environment for the program to run. Some AI programs may only need one container. Others may require multiple, running parallel or in sequence. In the case of multiple containers, the deepc team created workflows using Knative Eventing to coordinate the complex processes. After the process finishes and provides the output for the radiologist, Knative triggers stop the active containers. \"Knative gives us a foundation of consistency,\" said Andrew Webber, Senior Software Engineer. Bridging between legacy and advanced systems The platform makes available AIs developed by leading global companies and researchers. Knative has also allowed integration with the work of independent researchers through an SDK implementation for radiologists. They don\u2019t need to be Kubernetes experts or take days to bring their work to patients through deepc\u2019s platform. \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d said Webber. Some implementations are more complex. They use legacy technology that does not fit into a standard container or they have unique architectures that require OS-level configuration. deepc has built out APIs and virtual machines that connect those technologies to their own cloud-based platform and still integrate with the Knative Eventing workflow. This approach ensures those programs work flawlessly within the system. The case for business The choice to develop their platform around Knative has had several business benefits for the startup. One of the most complicated aspects of growing a company is scaling. Many technology companies find their developers start scrambling when more customers are onboarded, uncovering new bugs and other issues. However, because of the nature of Knative, this is less of a problem for deepc. Knative's combination of automation and serverless methods means as more customers are onboarded, deepc will not need to build out more resources - it will all happen automatically. Knative has also allowed the startup to add real value to customers using their technology. For example, because many applications used by radiologists are built by different companies, medical professionals have had to interact with disparate systems and procedures. deepc provides access to the work of many researchers on one platform, ending complicated processes for professionals on the ground. Healthcare systems get simple, unified billing. Knative has helped deepc create a robust case for customers to use their platform. Looking forward deepc has already done amazing things as a company, with many more features planned. The company is a model for how Knative can help any organization build an impressive technical architecture capable of addressing some of today's most complex problems. Using features provided by Knative has enabled them to pioneer what is possible. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"deepc"},{"location":"about/case-studies/outfit7/","text":"Outfit7 Case Study \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Game maker Outfit7 automates high performance ad bidding with Knative Serving Since its founding in 2009, the mobile gaming company Outfit7 has seen astronomical growth\u2014garnering over 17 billion downloads and over 85 billion video views last year. Outfit7 was in the Top 5 game publishers list on iOS and Google Play worldwide by the number of game downloads for 6 consecutive years (2015 - 2020). With their latest launch, My Talking Angela 2, they were number 1 by global games downloads in July, August, and September (more than 120 million downloads). The success of the well-known game developer behind massive hits like the Talking Tom & Friends franchise and stand-alone titles like Mythic Legends created large-scale challenges. With up to 470 million monthly active users, 20 thousand server requests per second, and terabytes of data generated daily, they needed a stable, high performance solution. They turned to a Knative and Kubernetes solution to optimize real-time bidding ad sales in a way that can automatically scale up and down as needed. They were able to develop a system so easy to maintain that it freed up two software engineers who are now able to work on more important tasks, like optimizing backend costs and adding new game features. High performance in-app bidding Ad sales are an important revenue stream of Outfit7. The team needed to walk a careful balance: sell the space for the highest bid, use technical resources efficiently, and make sure ads are served to players quickly. To achieve this they decided to adopt an in-app bidding approach. The Outfit7 user base generates around 8,000 ad-related requests per second. With so many users spread worldwide, the amount of these requests can drop or surge depending on all sorts of factors. Not just predictable things like the time of day, but current events can suddenly create traffic. The pandemic saw their usage soar, for instance. To manage the process in-house, the team needed to be able to test and deploy very efficiently. \"There were two specific use cases we wanted to cover,\" explained Luka Draksler, backend engineer in the ad technology department at Outfit7. \"One was to have the ability to do zero downtime canary deployments using an automatic gradual rollout process. This works in a way that the new version of the software is deployed using a continuous deployment pipeline with a small amount of traffic first. If everything checks out, all production traffic is migrated to the new version. In the worst-case scenario (if requests start failing) traffic can be quickly migrated to the old version. The second use-case was the ability to have developers deploy versions to specific groups of users for instances of A/B testing and other use cases.\" The team decided to adopt Knative Serving as the backbone of their solution. Knative allowed Outfit7 to streamline deployments and cut down on development time. After being surprised at how easily they generated an internal proof of concept, the team saw that it could craft custom solutions tuned for their internal workflows\u2014without consuming valuable developer time. In addition, they could quickly configure A/B testing and deploy multiple versions of code simultaneously. Serverless solution Knative Serving gave Outfit7 access to a robust set of tools and features that allows their team to automate and monitor the deployment of applications to handle ad requests. When more requests are coming in, their system automatically spins up more containers that house the workers and tools. When these requests drop, unneeded containers shut down. Outfit7 only pays for the resources they require for the current load. Knative works as a layer installed on top of Kubernetes. It brings the power of serverless workloads to the scalable capabilities of Kubernetes. Teams quickly spin up container-based applications without needing to consider the details of Kubernetes. Knative also simplifies project deployments to Kubernetes. Mitja Bezen\u0161ek, the Lead Developer on Outfit7's backend team, estimated that the traditional development that Knative replaced would have required three full-time engineers to maintain. Their new platform operates with minimal work and allows the developers to deploy updates at will. The open source community Outfit7's team was blown away by the supportive and helpful community around Knative. After discovering a problem with network scaling, the team was surprised by how easy it was to find answers and solutions. \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Sharing their story The great experience with Knative encouraged their team to share their experience with fellow companies and engineers at a local meetup. The presentation which included several live demos was a success, helping spawn another meet-up focused on the technology. \"Tilen showed them the demo and what it's all about,\" said Bezen\u0161ek. \"I hope we got them engaged going forward.\" Looking forward Outfit7 shows no signs of slowing down. \u201cAs we want to support our vision in expanding our games portfolio, we are always looking for new strategic partners who can accompany us on this path,\u201d added Helder Lopes, Head of R&D in Cyprus headquarters. The company plans to incorporate and adopt Knative into other back-end systems \u2013 taking advantage of the easier workflows that Knative offers. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"Outfit7"},{"location":"about/case-studies/puppet/","text":"Puppet Case Study \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional\" -- Noah Fontes, Senior Principal Software Engineer for Puppet Relay by Puppet Brings Workflows to Everything using Knative Puppet is a software company specializing in infrastructure automation. The company was founded in 2009 to solve some of the most complex problems for those who work in operations. Around 2019, the team noticed that cloud operations teams weren\u2019t able to effectively manage modern cloud-native applications because they were relying on manual workflow processes. The group saw an opportunity here to build out a platform to connect events triggered by modern architectures to ensure cloud environments remained secure, compliant, and cost-effective. This is the story of how Relay , a cloud-native workflow automation platform, was created, and how Knative and Kubernetes modernize and super-charge business process automation. The glue for DevOps When Puppet first began exploring the power and flexibility of Knative to trigger their Tekton-based workflow execution engine, they weren't quite sure where their journey was going to take them. Knative offered an attractive feature set, so they began building and experimenting. They wanted to build an event-driven DevOps tool; they weren't interested in building just another continuous integration product. In addition, as they continued to explore, they realized that they wanted something flexible and not tied to just one vertical. Whatever they were building, it was not going to focus on just one market. As their target came into focus, they realized that the serverless applications and functions enabled by Knative Serving would be perfect for a cloud-based business process automation service. Out of this realization, they built Relay , a cloud workflow automation product that helps Cloud Ops teams solve the integration and eventing issues that arise as organizations adopt multiple clouds and SaaS products alongside legacy solutions. Containers and webhooks Containers and webhooks are key elements in the Relay architecture. Containers allow Puppet to offer a cloud-based solution where businesses can configure and deploy workflows as discrete business units. Since the containers provide self-contained environments, even legacy services and packages can be included. This proved to be an essential feature for business customers. Anything that can be contained in a Docker image, for example, can be part of a Relay workflow. \"We focused on containers because they provide isolation,\" explains Noah Fontes, Senior Principal Software Engineer for Puppet, \"Containers provide discrete units of execution, where users can decrease the maintenance burden of complex systems.\" Allowing fully-configurable webhooks gives users the flexibility needed to incorporate business processes of all kinds. With webhooks, Relay can interact with nearly any web-based API to trigger rich, fully featured workflows across third party SaaS products, cloud services, web applications, and even system utilities. Knative Serving provides important infrastructure for Relay. It allows webhooks and services to scale automatically, even down to zero . This allows Relay to support pretty much any integration, including those used by only a small number of users. With autoscaling, those services don't consume resources while they are not being used. What is Knative Serving? Modern cloud-based applications deal with massive scaling challenges through several approaches. At the core of most of these is the use of containers: discrete computing units that run single applications, single services, or even just single functions. This approach is incredibly powerful, allowing services to scale the number of resources they consume as demand dictates. However, while all of this sounds amazing, it can be difficult to manage and configure. One of the most successful solutions for delivering this advanced architecture is Knative Serving. This framework builds on top of Kubernetes to support the deployment and management of serverless applications, services, and functions. In particular, Knative Services focuses on being easy to configure, deploy, and manage. Workflow integrations The open architecture allows Relay to integrate dozens of different services and platforms into workflows. A look at the Relay integrations GitHub page provides a list of these integrations and demonstrates their commitment to the open source community. \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional,\" says Fontes. Results: automated infrastructure management While Relay's infrastructure runs on the Google Cloud Platform, it is a library of workflows, integrations, steps, and triggers that includes services across all major cloud service providers. Relay customers can integrate across Microsoft Azure, AWS, and Oracle Cloud Infrastructure among others. By combining these integrations with SaaS offerings, it truly is becoming the Zapier of infrastructure management. \u201cOur customers have diverse needs for managing their workloads that are often best implemented as web APIs. Our product provides a serverless microservice environment powered by Knative that allows them to build this complex tooling without the management and maintenance overhead of traditional deployment architectures. We pass the cost savings on to them, and everyone is happier,\" said Fontes. Building and deploying Relay would not have been possible without the existing infrastructure offered by systems such as Knative and Tekton . Remarkably, Fontes' team never grew above eight engineers. Once they solidified their plan for Relay, they were able to bring it to production in just three months, says Fontes. \"Thanks to Knative, getting Relay out the door was easier than we thought it would be.\" said Noah Fontes, Senior Principal Software Engineer. Knative aims to make scalable, secure, stateless architectures available quickly by abstracting away the complex details of a Kubernetes installation and enabling developers to focus on what matters. Find out more Getting started with Knative Knative Serving Knative Eventing A Basic Introduction to Webhooks","title":"Puppet"},{"location":"client/","text":"CLI \u5de5\u5177 \u00b6 kubectl \u00b6 \u53ef\u4ee5\u4f7f\u7528 kubectl \u5e94\u7528\u5b89\u88c5Knative\u7ec4\u4ef6\u6240\u9700\u7684YAML\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528YAML\u521b\u5efaKnative\u8d44\u6e90\uff0c\u4f8b\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\u3002 \u53c2\u89c1 \u5b89\u88c5\u548c\u8bbe\u7f6e kubectl . kn \u00b6 kn \u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u7b80\u5355\u7684\u63a5\u53e3\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn \u8fd8\u7b80\u5316\u4e86\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u7a0b\u5e8f\u7684\u5b8c\u6210\u3002 Note kn \u4e0d\u80fd\u7528\u4e8e\u5b89\u88c5Knative\u7ec4\u4ef6\uff0c\u5982\u670d\u52a1\u6216\u4e8b\u4ef6\u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u67e5\u770b \u5b89\u88c5 kn . \u53c2\u89c1Github\u4e2d\u7684 kn \u6587\u6863 \u3002 func \u00b6 func CLI\u4f7f\u60a8\u80fd\u591f\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72Knative\u51fd\u6570\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u53c2\u89c1 \u5b89\u88c5Knative\u51fd\u6570 . \u53c2\u89c1Github\u4e2d\u7684 func \u6587\u6863 \u3002 \u5c06CLI\u5de5\u5177\u8fde\u63a5\u5230\u96c6\u7fa4 \u00b6 \u5b89\u88c5\u4e86 kubectl \u6216 kn \u540e\uff0c\u8fd9\u4e9b\u5de5\u5177\u5c06\u5728\u9ed8\u8ba4\u4f4d\u7f6e $HOME/.kube/config \u4e2d\u641c\u7d22\u96c6\u7fa4\u7684 kubeconfig \u6587\u4ef6\uff0c\u5e76\u4f7f\u7528\u8be5\u6587\u4ef6\u8fde\u63a5\u5230\u96c6\u7fa4\u3002 \u5728\u521b\u5efaKubernetes\u96c6\u7fa4\u65f6\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u521b\u5efa\u4e00\u4e2a kubeconfig \u6587\u4ef6\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 --kubeconfig : \u4f7f\u7528\u6b64\u9009\u9879\u6307\u5411 kubeconfig \u6587\u4ef6\u3002\u8fd9\u76f8\u5f53\u4e8e\u8bbe\u7f6e $KUBECONFIG \u73af\u5883\u53d8\u91cf\u3002 --context : \u4f7f\u7528\u6b64\u9009\u9879\u53ef\u4ece\u73b0\u6709\u7684 kubeconfig \u6587\u4ef6\u4e2d\u6307\u5b9a\u4e0a\u4e0b\u6587\u7684\u540d\u79f0\u3002\u4f7f\u7528 kubectl \u8f93\u51fa\u4e2d\u7684\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6: \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 \u4f7f\u7528 kn CLI --config \u9009\u9879\uff0c\u4f8b\u5982\uff0c kn service list --config path/to/config.yaml \u3002 \u9ed8\u8ba4\u7684\u914d\u7f6e\u662f ~/.config/kn/config.yaml \u3002 \u6709\u5173 kubeconfig \u6587\u4ef6\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528kubeconfig\u6587\u4ef6\u7ec4\u7ec7\u96c6\u7fa4\u8bbf\u95ee . \u5728\u5e73\u53f0\u4e0a\u4f7f\u7528kubeconfig\u6587\u4ef6 \u00b6 \u4f7f\u7528 kubeconfig \u6587\u4ef6\u7684\u8bf4\u660e\u53ef\u7528\u4e8e\u4ee5\u4e0b\u5e73\u53f0: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform \u542f\u52a8 minikube \u4f1a\u81ea\u52a8\u5199\u5165\u8be5\u6587\u4ef6\uff0c\u6216\u8005\u5728\u73b0\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u63d0\u4f9b\u9002\u5f53\u7684\u4e0a\u4e0b\u6587\u3002","title":"kn \u6982\u8ff0"},{"location":"client/#cli","text":"","title":"CLI \u5de5\u5177"},{"location":"client/#kubectl","text":"\u53ef\u4ee5\u4f7f\u7528 kubectl \u5e94\u7528\u5b89\u88c5Knative\u7ec4\u4ef6\u6240\u9700\u7684YAML\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528YAML\u521b\u5efaKnative\u8d44\u6e90\uff0c\u4f8b\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\u3002 \u53c2\u89c1 \u5b89\u88c5\u548c\u8bbe\u7f6e kubectl .","title":"kubectl"},{"location":"client/#kn","text":"kn \u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u7b80\u5355\u7684\u63a5\u53e3\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn \u8fd8\u7b80\u5316\u4e86\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u7a0b\u5e8f\u7684\u5b8c\u6210\u3002 Note kn \u4e0d\u80fd\u7528\u4e8e\u5b89\u88c5Knative\u7ec4\u4ef6\uff0c\u5982\u670d\u52a1\u6216\u4e8b\u4ef6\u3002","title":"kn"},{"location":"client/#_1","text":"\u67e5\u770b \u5b89\u88c5 kn . \u53c2\u89c1Github\u4e2d\u7684 kn \u6587\u6863 \u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"client/#func","text":"func CLI\u4f7f\u60a8\u80fd\u591f\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72Knative\u51fd\u6570\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002","title":"func"},{"location":"client/#_2","text":"\u53c2\u89c1 \u5b89\u88c5Knative\u51fd\u6570 . \u53c2\u89c1Github\u4e2d\u7684 func \u6587\u6863 \u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"client/#cli_1","text":"\u5b89\u88c5\u4e86 kubectl \u6216 kn \u540e\uff0c\u8fd9\u4e9b\u5de5\u5177\u5c06\u5728\u9ed8\u8ba4\u4f4d\u7f6e $HOME/.kube/config \u4e2d\u641c\u7d22\u96c6\u7fa4\u7684 kubeconfig \u6587\u4ef6\uff0c\u5e76\u4f7f\u7528\u8be5\u6587\u4ef6\u8fde\u63a5\u5230\u96c6\u7fa4\u3002 \u5728\u521b\u5efaKubernetes\u96c6\u7fa4\u65f6\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u521b\u5efa\u4e00\u4e2a kubeconfig \u6587\u4ef6\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 --kubeconfig : \u4f7f\u7528\u6b64\u9009\u9879\u6307\u5411 kubeconfig \u6587\u4ef6\u3002\u8fd9\u76f8\u5f53\u4e8e\u8bbe\u7f6e $KUBECONFIG \u73af\u5883\u53d8\u91cf\u3002 --context : \u4f7f\u7528\u6b64\u9009\u9879\u53ef\u4ece\u73b0\u6709\u7684 kubeconfig \u6587\u4ef6\u4e2d\u6307\u5b9a\u4e0a\u4e0b\u6587\u7684\u540d\u79f0\u3002\u4f7f\u7528 kubectl \u8f93\u51fa\u4e2d\u7684\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6: \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 \u4f7f\u7528 kn CLI --config \u9009\u9879\uff0c\u4f8b\u5982\uff0c kn service list --config path/to/config.yaml \u3002 \u9ed8\u8ba4\u7684\u914d\u7f6e\u662f ~/.config/kn/config.yaml \u3002 \u6709\u5173 kubeconfig \u6587\u4ef6\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528kubeconfig\u6587\u4ef6\u7ec4\u7ec7\u96c6\u7fa4\u8bbf\u95ee .","title":"\u5c06CLI\u5de5\u5177\u8fde\u63a5\u5230\u96c6\u7fa4"},{"location":"client/#kubeconfig","text":"\u4f7f\u7528 kubeconfig \u6587\u4ef6\u7684\u8bf4\u660e\u53ef\u7528\u4e8e\u4ee5\u4e0b\u5e73\u53f0: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform \u542f\u52a8 minikube \u4f1a\u81ea\u52a8\u5199\u5165\u8be5\u6587\u4ef6\uff0c\u6216\u8005\u5728\u73b0\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u63d0\u4f9b\u9002\u5f53\u7684\u4e0a\u4e0b\u6587\u3002","title":"\u5728\u5e73\u53f0\u4e0a\u4f7f\u7528kubeconfig\u6587\u4ef6"},{"location":"client/configure-kn/","text":"\u5b9a\u5236 kn \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a config.yaml \u914d\u7f6e\u6587\u4ef6\u6765\u5b9a\u5236\u60a8\u7684 kn \u547d\u4ee4\u884c\u8bbe\u7f6e\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u2014\u2014config \u6807\u5fd7\u6765\u63d0\u4f9b\u6b64\u914d\u7f6e\uff0c\u5426\u5219\u914d\u7f6e\u5c06\u4ece\u9ed8\u8ba4\u4f4d\u7f6e\u83b7\u53d6\u3002 \u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u7b26\u5408 XDG\u57fa\u672c\u76ee\u5f55\u89c4\u8303 \uff0c\u5bf9\u4e8eUnix\u7cfb\u7edf\u548cWindows\u7cfb\u7edf\u662f\u4e0d\u540c\u7684\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 XDG_CONFIG_HOME \u73af\u5883\u53d8\u91cf\uff0c kn \u5bfb\u627e\u7684\u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u662f $XDG_CONFIG_HOME/kn \u3002 \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e XDG_CONFIG_HOME \u73af\u5883\u53d8\u91cf\uff0c kn \u5728\u7528\u6237\u7684\u4e3b\u76ee\u5f55 $HOME/.config/kn/config.yaml \u4e2d\u67e5\u627e\u914d\u7f6e\u3002 \u5bf9\u4e8eWindows\u7cfb\u7edf\uff0c\u9ed8\u8ba4\u7684 kn \u914d\u7f6e\u4f4d\u7f6e\u662f %APPDATA%\\kn \u3002 \u793a\u4f8b\u914d\u7f6e\u6587\u4ef6 \u00b6 plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services \u54ea\u91cc path-lookup \u6307\u5b9a kn \u662f\u5426\u5e94\u8be5\u5728 PATH \u73af\u5883\u53d8\u91cf\u4e2d\u67e5\u627e plugins \u3002 \u8fd9\u662f\u4e00\u4e2a\u5e03\u5c14\u914d\u7f6e\u9009\u9879(\u9ed8\u8ba4\u503c: true )\u3002 \u6ce8\u610f: path-lookup \u9009\u9879\u5df2\u5f03\u7528\uff0c\u5c06\u5728\u5c06\u6765\u65e0\u6761\u4ef6\u542f\u7528\u8def\u5f84\u67e5\u627e\u7684\u7248\u672c\u4e2d\u5220\u9664\u3002 directory \u6307\u5b9a kn \u67e5\u627e\u63d2\u4ef6\u7684\u76ee\u5f55\u3002 \u5982\u524d\u6240\u8ff0\uff0c\u9ed8\u8ba4\u8def\u5f84\u53d6\u51b3\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u3002 \u8fd9\u53ef\u4ee5\u662f\u7528\u6237\u53ef\u89c1\u7684\u4efb\u4f55\u76ee\u5f55(\u9ed8\u8ba4: $base_dir/plugins \uff0c\u5176\u4e2d $base_dir \u662f\u5b58\u50a8\u914d\u7f6e\u6587\u4ef6\u7684\u76ee\u5f55)\u3002 sink-mappings \u5b9a\u4e49\u4e86Kubernetes\u53ef\u5bfb\u5740\u8d44\u6e90\uff0c\u5f53\u60a8\u5728 kn CLI\u547d\u4ee4\u4e2d\u4f7f\u7528 --sink \u6807\u5fd7\u65f6\u4f7f\u7528\u8be5\u8d44\u6e90\u3002 prefix : \u4f60\u60f3\u7528\u6765\u63cf\u8ff0\u4f60\u7684\u63a5\u6536\u5668\u7684\u524d\u7f00\u3002Service\u3001 svc , channel , \u548c broker \u662f kn \u4e2d\u7684\u9884\u5b9a\u4e49\u524d\u7f00\u3002 group : Kubernetes\u8d44\u6e90\u7684API\u7ec4\u3002 version : Kubernetes\u8d44\u6e90\u7684\u7248\u672c\u3002 resource : Kubernetes\u8d44\u6e90\u7c7b\u578b\u7684\u5c0f\u5199\u590d\u6570\u540d\u79f0\u3002\u4f8b\u5982\uff0c services \u6216 brokers \u3002","title":"kn \u5b9a\u5236"},{"location":"client/configure-kn/#kn","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a config.yaml \u914d\u7f6e\u6587\u4ef6\u6765\u5b9a\u5236\u60a8\u7684 kn \u547d\u4ee4\u884c\u8bbe\u7f6e\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u2014\u2014config \u6807\u5fd7\u6765\u63d0\u4f9b\u6b64\u914d\u7f6e\uff0c\u5426\u5219\u914d\u7f6e\u5c06\u4ece\u9ed8\u8ba4\u4f4d\u7f6e\u83b7\u53d6\u3002 \u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u7b26\u5408 XDG\u57fa\u672c\u76ee\u5f55\u89c4\u8303 \uff0c\u5bf9\u4e8eUnix\u7cfb\u7edf\u548cWindows\u7cfb\u7edf\u662f\u4e0d\u540c\u7684\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 XDG_CONFIG_HOME \u73af\u5883\u53d8\u91cf\uff0c kn \u5bfb\u627e\u7684\u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u662f $XDG_CONFIG_HOME/kn \u3002 \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e XDG_CONFIG_HOME \u73af\u5883\u53d8\u91cf\uff0c kn \u5728\u7528\u6237\u7684\u4e3b\u76ee\u5f55 $HOME/.config/kn/config.yaml \u4e2d\u67e5\u627e\u914d\u7f6e\u3002 \u5bf9\u4e8eWindows\u7cfb\u7edf\uff0c\u9ed8\u8ba4\u7684 kn \u914d\u7f6e\u4f4d\u7f6e\u662f %APPDATA%\\kn \u3002","title":"\u5b9a\u5236 kn"},{"location":"client/configure-kn/#_1","text":"plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services \u54ea\u91cc path-lookup \u6307\u5b9a kn \u662f\u5426\u5e94\u8be5\u5728 PATH \u73af\u5883\u53d8\u91cf\u4e2d\u67e5\u627e plugins \u3002 \u8fd9\u662f\u4e00\u4e2a\u5e03\u5c14\u914d\u7f6e\u9009\u9879(\u9ed8\u8ba4\u503c: true )\u3002 \u6ce8\u610f: path-lookup \u9009\u9879\u5df2\u5f03\u7528\uff0c\u5c06\u5728\u5c06\u6765\u65e0\u6761\u4ef6\u542f\u7528\u8def\u5f84\u67e5\u627e\u7684\u7248\u672c\u4e2d\u5220\u9664\u3002 directory \u6307\u5b9a kn \u67e5\u627e\u63d2\u4ef6\u7684\u76ee\u5f55\u3002 \u5982\u524d\u6240\u8ff0\uff0c\u9ed8\u8ba4\u8def\u5f84\u53d6\u51b3\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u3002 \u8fd9\u53ef\u4ee5\u662f\u7528\u6237\u53ef\u89c1\u7684\u4efb\u4f55\u76ee\u5f55(\u9ed8\u8ba4: $base_dir/plugins \uff0c\u5176\u4e2d $base_dir \u662f\u5b58\u50a8\u914d\u7f6e\u6587\u4ef6\u7684\u76ee\u5f55)\u3002 sink-mappings \u5b9a\u4e49\u4e86Kubernetes\u53ef\u5bfb\u5740\u8d44\u6e90\uff0c\u5f53\u60a8\u5728 kn CLI\u547d\u4ee4\u4e2d\u4f7f\u7528 --sink \u6807\u5fd7\u65f6\u4f7f\u7528\u8be5\u8d44\u6e90\u3002 prefix : \u4f60\u60f3\u7528\u6765\u63cf\u8ff0\u4f60\u7684\u63a5\u6536\u5668\u7684\u524d\u7f00\u3002Service\u3001 svc , channel , \u548c broker \u662f kn \u4e2d\u7684\u9884\u5b9a\u4e49\u524d\u7f00\u3002 group : Kubernetes\u8d44\u6e90\u7684API\u7ec4\u3002 version : Kubernetes\u8d44\u6e90\u7684\u7248\u672c\u3002 resource : Kubernetes\u8d44\u6e90\u7c7b\u578b\u7684\u5c0f\u5199\u590d\u6570\u540d\u79f0\u3002\u4f8b\u5982\uff0c services \u6216 brokers \u3002","title":"\u793a\u4f8b\u914d\u7f6e\u6587\u4ef6"},{"location":"client/install-kn/","text":"\u5b89\u88c5 Knative CLI \u00b6 \u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5Knative kn \u547d\u4ee4\u884c\u3002 \u5b89\u88c5 Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 \u4f7f\u7528\u591c\u95f4\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5b89\u88c5kn \u00b6 Warning \u6bcf\u665a\u5bb9\u5668\u6620\u50cf\u5305\u62ec\u53ef\u80fd\u4e0d\u5305\u542b\u5728\u6700\u65b0Knative\u7248\u672c\u4e2d\u7684\u7279\u6027\uff0c\u5e76\u4e14\u88ab\u8ba4\u4e3a\u662f\u4e0d\u7a33\u5b9a\u7684\u3002 \u60f3\u8981\u5b89\u88c5 kn \u6700\u65b0\u9884\u53d1\u5e03\u7248\u672c\u7684\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u5230\u6700\u65b0\u7684\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u94fe\u63a5\u5728\u8fd9\u91cc: macOS Linux Windows Tekton\u4f7f\u7528 kn \u00b6 \u53c2\u89c1 Tekton\u6587\u6863 .","title":"kn \u5b89\u88c5"},{"location":"client/install-kn/#knative-cli","text":"\u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5Knative kn \u547d\u4ee4\u884c\u3002","title":"\u5b89\u88c5 Knative CLI"},{"location":"client/install-kn/#knative-cli_1","text":"Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"\u5b89\u88c5 Knative CLI"},{"location":"client/install-kn/#kn","text":"Warning \u6bcf\u665a\u5bb9\u5668\u6620\u50cf\u5305\u62ec\u53ef\u80fd\u4e0d\u5305\u542b\u5728\u6700\u65b0Knative\u7248\u672c\u4e2d\u7684\u7279\u6027\uff0c\u5e76\u4e14\u88ab\u8ba4\u4e3a\u662f\u4e0d\u7a33\u5b9a\u7684\u3002 \u60f3\u8981\u5b89\u88c5 kn \u6700\u65b0\u9884\u53d1\u5e03\u7248\u672c\u7684\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u5230\u6700\u65b0\u7684\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u94fe\u63a5\u5728\u8fd9\u91cc: macOS Linux Windows","title":"\u4f7f\u7528\u591c\u95f4\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5b89\u88c5kn"},{"location":"client/install-kn/#tekton-kn","text":"\u53c2\u89c1 Tekton\u6587\u6863 .","title":"Tekton\u4f7f\u7528 kn"},{"location":"client/kn-plugins/","text":"kn \u63d2\u4ef6 \u00b6 kn \u547d\u4ee4\u884c\u652f\u6301\u4f7f\u7528\u63d2\u4ef6\u3002 \u63d2\u4ef6\u5141\u8bb8\u60a8\u901a\u8fc7\u6dfb\u52a0\u81ea\u5b9a\u4e49\u547d\u4ee4\u548c\u5176\u4ed6\u4e0d\u5c5e\u4e8e kn \u6838\u5fc3\u53d1\u884c\u7248\u7684\u5171\u4eab\u547d\u4ee4\u6765\u6269\u5c55 kn \u5b89\u88c5\u7684\u529f\u80fd\u3002 Warning \u63d2\u4ef6\u5fc5\u987b\u4ee5\u524d\u7f00 kn- \u547d\u540d\uff0c\u4ee5\u4fbf\u7531 kn \u68c0\u6d4b\u3002 \u4f8b\u5982\uff0c kn-func \u4f1a\u88ab\u68c0\u6d4b\u5230\uff0c\u4f46 func \u4e0d\u4f1a\u88ab\u68c0\u6d4b\u5230\u3002 kn \u6e90\u7684\u63d2\u4ef6 \u00b6 \u4e8b\u4ef6\u6e90\u63d2\u4ef6\u5177\u6709\u4ee5\u4e0b\u7279\u5f81: \u5b83\u7684\u540d\u79f0\u5c5e\u4e8e kn source \u7ec4\u7684\u4e00\u90e8\u5206\u3002 \u5b83\u63d0\u4f9b CRUD \u5b50\u547d\u4ee4; create , update , delete , describe , \u6709\u65f6 apply . \u5f53\u4f7f\u7528 create \u547d\u4ee4\u65f6\uff0c\u5b83\u8981\u6c42\u4f20\u9012\u4e00\u4e2a\u5f3a\u5236\u7684 --sink \u6807\u5fd7\u3002 Knative \u63d2\u4ef6\u5217\u8868 \u00b6 \u60a8\u53ef\u4ee5\u5728 Knative Sandbox \u5e93 \u4e2d\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684 kn \u63d2\u4ef6. \u63d2\u4ef6 \u63cf\u8ff0 \u53ef\u4ee5\u901a\u8fc7 Homebrew? kn-plugin-admin kn plugin \u7528\u4e8e\u7ba1\u7406\u57fa\u4e8e Kubernetes \u7684 Knative \u5b89\u88c5 Y kn-plugin-diag kn plugin \u7528\u4e8e\u901a\u8fc7\u516c\u5f00 Knative \u5bf9\u8c61\u7684\u4e0d\u540c\u5c42\u7684\u8be6\u7ec6\u4fe1\u606f\u6765\u8bca\u65ad\u95ee\u9898 N kn-plugin-event kn plugin \u7528\u4e8e\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230 Knative \u63a5\u6536\u5668 Y kn-plugin-func kn plugin \u7528\u6237\u51fd\u6570 Y kn-plugin-migration kn plugin \u7528\u4e8e\u5c06 Knative \u670d\u52a1\u4ece\u4e00\u4e2a\u96c6\u7fa4\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u96c6\u7fa4 N kn-plugin-operator kn plugin \u4f7f\u7528 Knative Operator \u7ba1\u7406 Knative N kn-plugin-quickstart kn plugin \u4e3a\u5f00\u53d1\u4eba\u5458\u5b89\u88c5\u4e00\u4e2a quickstart \u7684 Knative \u96c6\u7fa4\uff0c\u4ee5\u8fdb\u884c\u5b9e\u9a8c Y kn-plugin-service-log kn plugin \u7528\u4e8e\u663e\u793a Knative \u670d\u52a1\u7684\u6807\u51c6\u8f93\u51fa N kn-plugin-source-kafka kn plugin \u7528\u4e8e\u7ba1\u7406 Kafka \u4e8b\u4ef6\u6e90 Y kn-plugin-source-kamelet kn plugin \u7528\u4e8e\u7ba1\u7406 Kamelets \u548c KameletBindings Y \u624b\u52a8\u5b89\u88c5\u63d2\u4ef6 \u00b6 \u60a8\u53ef\u4ee5\u624b\u52a8\u5b89\u88c5\u6240\u6709\u63d2\u4ef6\u3002\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6: \u4ece GitHub \u4e0b\u8f7d\u63d2\u4ef6\u7684\u5f53\u524d\u7248\u672c\u3002\u4f60\u53ef\u4ee5\u4e0b\u8f7d Knative \u63d2\u4ef6\u5217\u8868 \u3002 \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-admin-darwin-amd64 \u91cd\u547d\u540d\u4e3a kn-admin \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-admin \u3002 \u5c06\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\u4e2d\u3002\u4f8b\u5982, /usr/local/bin \u3002 \u4f7f\u7528 Homebrew \u5b89\u88c5\u63d2\u4ef6 \u00b6 \u53ef\u4ee5\u4f7f\u7528 Knative plugins Homebrew Tap \u5b89\u88c5\u4e00\u4e9b\u63d2\u4ef6\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c brew install knative-sandbox/kn-plugins/admin \u6765\u5b89\u88c5 kn-admin \u63d2\u4ef6\u3002 \u53ef\u7528\u63d2\u4ef6\u5217\u8868 \u00b6 \u4f60\u53ef\u4ee5\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u6240\u6709\u53ef\u7528\u7684(\u5df2\u5b89\u88c5\u7684)\u63d2\u4ef6: kn plugin list","title":"kn \u63d2\u4ef6"},{"location":"client/kn-plugins/#kn","text":"kn \u547d\u4ee4\u884c\u652f\u6301\u4f7f\u7528\u63d2\u4ef6\u3002 \u63d2\u4ef6\u5141\u8bb8\u60a8\u901a\u8fc7\u6dfb\u52a0\u81ea\u5b9a\u4e49\u547d\u4ee4\u548c\u5176\u4ed6\u4e0d\u5c5e\u4e8e kn \u6838\u5fc3\u53d1\u884c\u7248\u7684\u5171\u4eab\u547d\u4ee4\u6765\u6269\u5c55 kn \u5b89\u88c5\u7684\u529f\u80fd\u3002 Warning \u63d2\u4ef6\u5fc5\u987b\u4ee5\u524d\u7f00 kn- \u547d\u540d\uff0c\u4ee5\u4fbf\u7531 kn \u68c0\u6d4b\u3002 \u4f8b\u5982\uff0c kn-func \u4f1a\u88ab\u68c0\u6d4b\u5230\uff0c\u4f46 func \u4e0d\u4f1a\u88ab\u68c0\u6d4b\u5230\u3002","title":"kn \u63d2\u4ef6"},{"location":"client/kn-plugins/#kn_1","text":"\u4e8b\u4ef6\u6e90\u63d2\u4ef6\u5177\u6709\u4ee5\u4e0b\u7279\u5f81: \u5b83\u7684\u540d\u79f0\u5c5e\u4e8e kn source \u7ec4\u7684\u4e00\u90e8\u5206\u3002 \u5b83\u63d0\u4f9b CRUD \u5b50\u547d\u4ee4; create , update , delete , describe , \u6709\u65f6 apply . \u5f53\u4f7f\u7528 create \u547d\u4ee4\u65f6\uff0c\u5b83\u8981\u6c42\u4f20\u9012\u4e00\u4e2a\u5f3a\u5236\u7684 --sink \u6807\u5fd7\u3002","title":"kn \u6e90\u7684\u63d2\u4ef6"},{"location":"client/kn-plugins/#knative","text":"\u60a8\u53ef\u4ee5\u5728 Knative Sandbox \u5e93 \u4e2d\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684 kn \u63d2\u4ef6. \u63d2\u4ef6 \u63cf\u8ff0 \u53ef\u4ee5\u901a\u8fc7 Homebrew? kn-plugin-admin kn plugin \u7528\u4e8e\u7ba1\u7406\u57fa\u4e8e Kubernetes \u7684 Knative \u5b89\u88c5 Y kn-plugin-diag kn plugin \u7528\u4e8e\u901a\u8fc7\u516c\u5f00 Knative \u5bf9\u8c61\u7684\u4e0d\u540c\u5c42\u7684\u8be6\u7ec6\u4fe1\u606f\u6765\u8bca\u65ad\u95ee\u9898 N kn-plugin-event kn plugin \u7528\u4e8e\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230 Knative \u63a5\u6536\u5668 Y kn-plugin-func kn plugin \u7528\u6237\u51fd\u6570 Y kn-plugin-migration kn plugin \u7528\u4e8e\u5c06 Knative \u670d\u52a1\u4ece\u4e00\u4e2a\u96c6\u7fa4\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u96c6\u7fa4 N kn-plugin-operator kn plugin \u4f7f\u7528 Knative Operator \u7ba1\u7406 Knative N kn-plugin-quickstart kn plugin \u4e3a\u5f00\u53d1\u4eba\u5458\u5b89\u88c5\u4e00\u4e2a quickstart \u7684 Knative \u96c6\u7fa4\uff0c\u4ee5\u8fdb\u884c\u5b9e\u9a8c Y kn-plugin-service-log kn plugin \u7528\u4e8e\u663e\u793a Knative \u670d\u52a1\u7684\u6807\u51c6\u8f93\u51fa N kn-plugin-source-kafka kn plugin \u7528\u4e8e\u7ba1\u7406 Kafka \u4e8b\u4ef6\u6e90 Y kn-plugin-source-kamelet kn plugin \u7528\u4e8e\u7ba1\u7406 Kamelets \u548c KameletBindings Y","title":"Knative \u63d2\u4ef6\u5217\u8868"},{"location":"client/kn-plugins/#_1","text":"\u60a8\u53ef\u4ee5\u624b\u52a8\u5b89\u88c5\u6240\u6709\u63d2\u4ef6\u3002\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6: \u4ece GitHub \u4e0b\u8f7d\u63d2\u4ef6\u7684\u5f53\u524d\u7248\u672c\u3002\u4f60\u53ef\u4ee5\u4e0b\u8f7d Knative \u63d2\u4ef6\u5217\u8868 \u3002 \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-admin-darwin-amd64 \u91cd\u547d\u540d\u4e3a kn-admin \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-admin \u3002 \u5c06\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\u4e2d\u3002\u4f8b\u5982, /usr/local/bin \u3002","title":"\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6"},{"location":"client/kn-plugins/#homebrew","text":"\u53ef\u4ee5\u4f7f\u7528 Knative plugins Homebrew Tap \u5b89\u88c5\u4e00\u4e9b\u63d2\u4ef6\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c brew install knative-sandbox/kn-plugins/admin \u6765\u5b89\u88c5 kn-admin \u63d2\u4ef6\u3002","title":"\u4f7f\u7528 Homebrew \u5b89\u88c5\u63d2\u4ef6"},{"location":"client/kn-plugins/#_2","text":"\u4f60\u53ef\u4ee5\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u6240\u6709\u53ef\u7528\u7684(\u5df2\u5b89\u88c5\u7684)\u63d2\u4ef6: kn plugin list","title":"\u53ef\u7528\u63d2\u4ef6\u5217\u8868"},{"location":"concepts/","text":"\u6982\u5ff5 \u00b6 \u672c\u8282\u4e2d\u7684\u6587\u6863\u89e3\u91ca\u4e86\u5e38\u7528\u7684Knative\u6982\u5ff5\u548c\u62bd\u8c61\uff0c\u5e76\u5e2e\u52a9\u60a8\u66f4\u597d\u5730\u7406\u89e3Knative\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 \u4ec0\u4e48\u662f Knative? \u00b6 Knative\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u8fd0\u884c \u65e0\u670d\u52a1\u5668 \u90e8\u7f72\u3002 Knative \u670d\u52a1 \u00b6 Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002 Knative \u4e8b\u4ef6 \u00b6 Knative \u4e8b\u4ef6\u662f\u4e00\u4e2a API \u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b API \u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94 HTTP \u8bf7\u6c42\u3002 Knative \u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684 HTTP POST \u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents \u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative \u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002","title":"\u6982\u8ff0"},{"location":"concepts/#_1","text":"\u672c\u8282\u4e2d\u7684\u6587\u6863\u89e3\u91ca\u4e86\u5e38\u7528\u7684Knative\u6982\u5ff5\u548c\u62bd\u8c61\uff0c\u5e76\u5e2e\u52a9\u60a8\u66f4\u597d\u5730\u7406\u89e3Knative\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002","title":"\u6982\u5ff5"},{"location":"concepts/#knative","text":"Knative\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u8fd0\u884c \u65e0\u670d\u52a1\u5668 \u90e8\u7f72\u3002","title":"\u4ec0\u4e48\u662f Knative?"},{"location":"concepts/#knative_1","text":"Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002","title":"Knative \u670d\u52a1"},{"location":"concepts/#knative_2","text":"Knative \u4e8b\u4ef6\u662f\u4e00\u4e2a API \u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b API \u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94 HTTP \u8bf7\u6c42\u3002 Knative \u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684 HTTP POST \u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents \u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative \u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002","title":"Knative \u4e8b\u4ef6"},{"location":"concepts/duck-typing/","text":"\u9e2d\u5b50\u7c7b\u578b \u00b6 Knative\u901a\u8fc7\u4f7f\u7528 \u9e2d\u5b50\u7c7b\u578b \u652f\u6301\u5176\u7ec4\u4ef6\u7684 \u677e\u6563\u8026\u5408 \u3002 \u9e2d\u5b50\u7c7b\u578b\u610f\u5473\u7740\u5728Knative\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u8d44\u6e90\u7684\u517c\u5bb9\u6027\u7531\u7528\u4e8e\u8bc6\u522b\u8d44\u6e90\u63a7\u5236\u5e73\u9762\u5f62\u72b6\u548c\u884c\u4e3a\u7684\u67d0\u4e9b\u5c5e\u6027\u51b3\u5b9a\u3002 \u8fd9\u4e9b\u5c5e\u6027\u57fa\u4e8e\u4e00\u7ec4\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u8d44\u6e90\u7684\u516c\u5171\u5b9a\u4e49\uff0c\u79f0\u4e3a\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u53ef\u4ee5\u50cf\u4f7f\u7528\u6cdb\u578b\u9e2d\u7c7b\u578b\u4e00\u6837\u4f7f\u7528\u8d44\u6e90\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u8d44\u6e90\u7c7b\u578b\u6709\u5177\u4f53\u7684\u4e86\u89e3\uff0c\u5982\u679c: \u8d44\u6e90\u5728\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u6a21\u5f0f\u4f4d\u7f6e\u4e2d\u5177\u6709\u76f8\u540c\u7684\u5b57\u6bb5 \u4e0e\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u7684\u63a7\u4ef6\u6216\u6570\u636e\u5e73\u9762\u884c\u4e3a \u6709\u4e9b\u8d44\u6e90\u53ef\u4ee5\u9009\u62e9\u52a0\u5165\u591a\u79cd\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u4e2d\u9e2d\u5b50\u7c7b\u578b\u7684\u4e00\u4e2a\u57fa\u672c\u7528\u9014\u662f\u5728\u8d44\u6e90 \u89c4\u8303 \u4e2d\u4f7f\u7528\u5bf9\u8c61\u5f15\u7528\u6765\u6307\u5411\u5176\u4ed6\u8d44\u6e90\u3002 \u5305\u542b\u5f15\u7528\u7684\u5bf9\u8c61\u7684\u5b9a\u4e49\u89c4\u5b9a\u4e86\u88ab\u5f15\u7528\u8d44\u6e90\u7684\u9884\u671f\u9e2d\u5b50\u7c7b\u578b\u3002 \u4f8b\u5b50 \u00b6 \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e00\u4e2a\u540d\u4e3a pointer \u7684Knative \u793a\u4f8b \u8d44\u6e90\u5728\u5176\u89c4\u8303\u4e2d\u5f15\u7528\u4e86\u4e00\u4e2a\u540d\u4e3a pointee \u7684 Dog \u8d44\u6e90: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee \u5982\u679c\u4e00\u4e2a\u53ef\u89c2\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u671f\u671b\u5f62\u72b6\u662f\uff0c\u5728 status \u4e2d\uff0c\u6a21\u5f0f\u5f62\u72b6\u5982\u4e0b: status : height : <in centimetres> weight : <in kilograms> \u73b0\u5728 pointee \u7684\u5b9e\u4f8b\u770b\u8d77\u6765\u50cf\u8fd9\u6837: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 \u5f53 \u793a\u4f8b \u8d44\u6e90\u8d77\u4f5c\u7528\u65f6\uff0c\u5b83\u53ea\u4f5c\u7528\u4e8e\u5927\u5c3a\u5bf8\u9e2d\u5b50\u7c7b\u578b\u5f62\u72b6\u7684\u4fe1\u606f\uff0c\u800c Dog \u5b9e\u73b0\u53ef\u4ee5\u81ea\u7531\u5730\u62e5\u6709\u5bf9\u8be5\u8d44\u6e90\u6700\u6709\u610f\u4e49\u7684\u4fe1\u606f\u3002 \u5f53\u6211\u4eec\u7528\u4e00\u79cd\u65b0\u7c7b\u578b\u6269\u5c55\u7cfb\u7edf\u65f6\uff0c\u9e2d\u5b50\u7c7b\u578b\u7684\u5a01\u529b\u662f\u663e\u800c\u6613\u89c1\u7684\uff0c\u4f8b\u5982\uff0c Human , \u5982\u679c\u65b0\u8d44\u6e90\u7b26\u5408\u5927\u516c\u53f8\u8bbe\u5b9a\u7684\u5408\u540c\u3002 apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 \u793a\u4f8b \u8d44\u6e90\u80fd\u591f\u5e94\u7528\u4e3a\u5176\u914d\u7f6e\u7684\u903b\u8f91\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u5730\u4e86\u89e3 Human \u6216 Dog \u3002 Knative \u9e2d\u5b50\u7c7b\u578b \u00b6 Knative\u5b9a\u4e49\u4e86\u51e0\u4e2a\u5728\u6574\u4e2a\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u5951\u7ea6: \u9e2d\u5b50\u7c7b\u578b \u4f8b\u5b50 Knative \u9e2d\u5b50\u7c7b\u578b Addressable(\u53ef\u5bfb\u5740) Binding(\u7ed1\u5b9a) Source(\u6e90) Addressable(\u53ef\u5bfb\u5740) \u00b6 \u53ef\u5bfb\u5740\u7684\u5f62\u72b6\u9884\u671f\u5982\u4e0b: apiVersion : group/version kind : Kind status : address : url : http://host/path?query Binding(\u7ed1\u5b9a) \u00b6 \u6709\u4e86\u76f4\u63a5\u7684 subject , Binding \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name \u4f7f\u7528\u95f4\u63a5 subject \u65f6, Binding \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value Source(\u6e90) \u00b6 \u4f7f\u7528 ref \u63a5\u6536\u5668\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host \u4f7f\u7528 uri \u63a5\u6536\u5668\u65f6\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query \u5bf9\u4e8e ref \u548c uri \u63a5\u6536\u5668\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"\u9e2d\u5b50\u7c7b\u578b"},{"location":"concepts/duck-typing/#_1","text":"Knative\u901a\u8fc7\u4f7f\u7528 \u9e2d\u5b50\u7c7b\u578b \u652f\u6301\u5176\u7ec4\u4ef6\u7684 \u677e\u6563\u8026\u5408 \u3002 \u9e2d\u5b50\u7c7b\u578b\u610f\u5473\u7740\u5728Knative\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u8d44\u6e90\u7684\u517c\u5bb9\u6027\u7531\u7528\u4e8e\u8bc6\u522b\u8d44\u6e90\u63a7\u5236\u5e73\u9762\u5f62\u72b6\u548c\u884c\u4e3a\u7684\u67d0\u4e9b\u5c5e\u6027\u51b3\u5b9a\u3002 \u8fd9\u4e9b\u5c5e\u6027\u57fa\u4e8e\u4e00\u7ec4\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u8d44\u6e90\u7684\u516c\u5171\u5b9a\u4e49\uff0c\u79f0\u4e3a\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u53ef\u4ee5\u50cf\u4f7f\u7528\u6cdb\u578b\u9e2d\u7c7b\u578b\u4e00\u6837\u4f7f\u7528\u8d44\u6e90\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u8d44\u6e90\u7c7b\u578b\u6709\u5177\u4f53\u7684\u4e86\u89e3\uff0c\u5982\u679c: \u8d44\u6e90\u5728\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u6a21\u5f0f\u4f4d\u7f6e\u4e2d\u5177\u6709\u76f8\u540c\u7684\u5b57\u6bb5 \u4e0e\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u7684\u63a7\u4ef6\u6216\u6570\u636e\u5e73\u9762\u884c\u4e3a \u6709\u4e9b\u8d44\u6e90\u53ef\u4ee5\u9009\u62e9\u52a0\u5165\u591a\u79cd\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u4e2d\u9e2d\u5b50\u7c7b\u578b\u7684\u4e00\u4e2a\u57fa\u672c\u7528\u9014\u662f\u5728\u8d44\u6e90 \u89c4\u8303 \u4e2d\u4f7f\u7528\u5bf9\u8c61\u5f15\u7528\u6765\u6307\u5411\u5176\u4ed6\u8d44\u6e90\u3002 \u5305\u542b\u5f15\u7528\u7684\u5bf9\u8c61\u7684\u5b9a\u4e49\u89c4\u5b9a\u4e86\u88ab\u5f15\u7528\u8d44\u6e90\u7684\u9884\u671f\u9e2d\u5b50\u7c7b\u578b\u3002","title":"\u9e2d\u5b50\u7c7b\u578b"},{"location":"concepts/duck-typing/#_2","text":"\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e00\u4e2a\u540d\u4e3a pointer \u7684Knative \u793a\u4f8b \u8d44\u6e90\u5728\u5176\u89c4\u8303\u4e2d\u5f15\u7528\u4e86\u4e00\u4e2a\u540d\u4e3a pointee \u7684 Dog \u8d44\u6e90: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee \u5982\u679c\u4e00\u4e2a\u53ef\u89c2\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u671f\u671b\u5f62\u72b6\u662f\uff0c\u5728 status \u4e2d\uff0c\u6a21\u5f0f\u5f62\u72b6\u5982\u4e0b: status : height : <in centimetres> weight : <in kilograms> \u73b0\u5728 pointee \u7684\u5b9e\u4f8b\u770b\u8d77\u6765\u50cf\u8fd9\u6837: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 \u5f53 \u793a\u4f8b \u8d44\u6e90\u8d77\u4f5c\u7528\u65f6\uff0c\u5b83\u53ea\u4f5c\u7528\u4e8e\u5927\u5c3a\u5bf8\u9e2d\u5b50\u7c7b\u578b\u5f62\u72b6\u7684\u4fe1\u606f\uff0c\u800c Dog \u5b9e\u73b0\u53ef\u4ee5\u81ea\u7531\u5730\u62e5\u6709\u5bf9\u8be5\u8d44\u6e90\u6700\u6709\u610f\u4e49\u7684\u4fe1\u606f\u3002 \u5f53\u6211\u4eec\u7528\u4e00\u79cd\u65b0\u7c7b\u578b\u6269\u5c55\u7cfb\u7edf\u65f6\uff0c\u9e2d\u5b50\u7c7b\u578b\u7684\u5a01\u529b\u662f\u663e\u800c\u6613\u89c1\u7684\uff0c\u4f8b\u5982\uff0c Human , \u5982\u679c\u65b0\u8d44\u6e90\u7b26\u5408\u5927\u516c\u53f8\u8bbe\u5b9a\u7684\u5408\u540c\u3002 apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 \u793a\u4f8b \u8d44\u6e90\u80fd\u591f\u5e94\u7528\u4e3a\u5176\u914d\u7f6e\u7684\u903b\u8f91\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u5730\u4e86\u89e3 Human \u6216 Dog \u3002","title":"\u4f8b\u5b50"},{"location":"concepts/duck-typing/#knative","text":"Knative\u5b9a\u4e49\u4e86\u51e0\u4e2a\u5728\u6574\u4e2a\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u5951\u7ea6: \u9e2d\u5b50\u7c7b\u578b \u4f8b\u5b50 Knative \u9e2d\u5b50\u7c7b\u578b Addressable(\u53ef\u5bfb\u5740) Binding(\u7ed1\u5b9a) Source(\u6e90)","title":"Knative \u9e2d\u5b50\u7c7b\u578b"},{"location":"concepts/duck-typing/#addressable","text":"\u53ef\u5bfb\u5740\u7684\u5f62\u72b6\u9884\u671f\u5982\u4e0b: apiVersion : group/version kind : Kind status : address : url : http://host/path?query","title":"Addressable(\u53ef\u5bfb\u5740)"},{"location":"concepts/duck-typing/#binding","text":"\u6709\u4e86\u76f4\u63a5\u7684 subject , Binding \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name \u4f7f\u7528\u95f4\u63a5 subject \u65f6, Binding \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value","title":"Binding(\u7ed1\u5b9a)"},{"location":"concepts/duck-typing/#source","text":"\u4f7f\u7528 ref \u63a5\u6536\u5668\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host \u4f7f\u7528 uri \u63a5\u6536\u5668\u65f6\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query \u5bf9\u4e8e ref \u548c uri \u63a5\u6536\u5668\uff0cSource \u5e94\u8be5\u662f\u4ee5\u4e0b\u5f62\u72b6: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"Source(\u6e90)"},{"location":"concepts/eventing-resources/brokers/","text":"\u4ee3\u7406 \u00b6 \u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002 \u76f8\u5173\u7684\u6982\u5ff5 \u00b6 \u89e6\u53d1\u5668 \u00b6 \u4e8b\u4ef6\u8fdb\u5165\u4ee3\u7406\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u5c06\u5176\u8f6c\u53d1\u7ed9\u8ba2\u9605\u8005\u3002 \u89e6\u53d1\u5668\u5141\u8bb8\u901a\u8fc7\u5c5e\u6027\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u8fd9\u6837\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u4e8b\u4ef6\u5c31\u53ef\u4ee5\u53d1\u9001\u5230\u6ce8\u518c\u4e86\u5bf9\u5177\u6709\u8fd9\u4e9b\u5c5e\u6027\u7684\u4e8b\u4ef6\u611f\u5174\u8da3\u7684\u8ba2\u9605\u670d\u52a1\u5668\u3002 \u8ba2\u9605\u8005 \u00b6 \u8ba2\u9605\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u4efb\u4f55URL\u6216\u53ef\u5bfb\u5740\u8d44\u6e90\u3002 \u8ba2\u9605\u8005\u8fd8\u53ef\u4ee5\u54cd\u5e94\u6765\u81ea\u4ee3\u7406\u7684\u6d3b\u52a8\u8bf7\u6c42\uff0c\u5e76\u4f7f\u7528\u53d1\u9001\u56de\u4ee3\u7406\u7684\u65b0\u4e8b\u4ef6\u8fdb\u884c\u54cd\u5e94\u3002","title":"\u4ee3\u7406"},{"location":"concepts/eventing-resources/brokers/#_1","text":"\u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002","title":"\u4ee3\u7406"},{"location":"concepts/eventing-resources/brokers/#_2","text":"","title":"\u76f8\u5173\u7684\u6982\u5ff5"},{"location":"concepts/eventing-resources/brokers/#_3","text":"\u4e8b\u4ef6\u8fdb\u5165\u4ee3\u7406\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u5c06\u5176\u8f6c\u53d1\u7ed9\u8ba2\u9605\u8005\u3002 \u89e6\u53d1\u5668\u5141\u8bb8\u901a\u8fc7\u5c5e\u6027\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u8fd9\u6837\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u4e8b\u4ef6\u5c31\u53ef\u4ee5\u53d1\u9001\u5230\u6ce8\u518c\u4e86\u5bf9\u5177\u6709\u8fd9\u4e9b\u5c5e\u6027\u7684\u4e8b\u4ef6\u611f\u5174\u8da3\u7684\u8ba2\u9605\u670d\u52a1\u5668\u3002","title":"\u89e6\u53d1\u5668"},{"location":"concepts/eventing-resources/brokers/#_4","text":"\u8ba2\u9605\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u4efb\u4f55URL\u6216\u53ef\u5bfb\u5740\u8d44\u6e90\u3002 \u8ba2\u9605\u8005\u8fd8\u53ef\u4ee5\u54cd\u5e94\u6765\u81ea\u4ee3\u7406\u7684\u6d3b\u52a8\u8bf7\u6c42\uff0c\u5e76\u4f7f\u7528\u53d1\u9001\u56de\u4ee3\u7406\u7684\u65b0\u4e8b\u4ef6\u8fdb\u884c\u54cd\u5e94\u3002","title":"\u8ba2\u9605\u8005"},{"location":"concepts/serving-resources/revisions/","text":"\u4fee\u8ba2 \u00b6 \u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002 \u76f8\u5173\u7684\u6982\u5ff5 \u00b6 \u81ea\u52a8\u7f29\u653e \u00b6 \u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u653e\u5927\u6216\u7f29\u5c0f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u81ea\u52a8\u7f29\u653e \u3002 \u9010\u6b65\u63a8\u51fa\u4fee\u8ba2\u6d41\u91cf \u00b6 \u4fee\u8ba2\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u66f4\u6539\u7684\u9010\u6b65\u8f6c\u51fa\u548c\u56de\u6eda\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7248\u672c\u7684\u914d\u7f6e\u9009\u9879 . \u4fee\u8ba2\u7684\u914d\u7f6e\u9009\u9879 \u00b6 \u96c6\u7fa4\u7ba1\u7406\u5458(\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684)\u914d\u7f6e\u9009\u9879 \u5f00\u53d1\u4eba\u5458(\u6bcf\u4fee\u8ba2)\u914d\u7f6e\u9009\u9879 \u989d\u5916\u8d44\u6e90 \u00b6 \u4fee\u8ba2API\u89c4\u8303 \u4e0b\u4e00\u6b65 \u00b6 \u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u5220\u9664\u3001\u63cf\u8ff0\u548c\u5217\u51fa\u4fee\u8ba2 \u68c0\u67e5\u4fee\u8ba2\u7684\u72b6\u6001 \u5728\u670d\u52a1\u7684\u4fee\u8ba2\u4e4b\u95f4\u8def\u7531\u901a\u4fe1","title":"\u4fee\u6b63"},{"location":"concepts/serving-resources/revisions/#_1","text":"\u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002","title":"\u4fee\u8ba2"},{"location":"concepts/serving-resources/revisions/#_2","text":"","title":"\u76f8\u5173\u7684\u6982\u5ff5"},{"location":"concepts/serving-resources/revisions/#_3","text":"\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u653e\u5927\u6216\u7f29\u5c0f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u81ea\u52a8\u7f29\u653e \u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"concepts/serving-resources/revisions/#_4","text":"\u4fee\u8ba2\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u66f4\u6539\u7684\u9010\u6b65\u8f6c\u51fa\u548c\u56de\u6eda\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u3002","title":"\u9010\u6b65\u63a8\u51fa\u4fee\u8ba2\u6d41\u91cf"},{"location":"concepts/serving-resources/revisions/#_5","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7248\u672c\u7684\u914d\u7f6e\u9009\u9879 .","title":"\u5783\u573e\u6536\u96c6"},{"location":"concepts/serving-resources/revisions/#_6","text":"\u96c6\u7fa4\u7ba1\u7406\u5458(\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684)\u914d\u7f6e\u9009\u9879 \u5f00\u53d1\u4eba\u5458(\u6bcf\u4fee\u8ba2)\u914d\u7f6e\u9009\u9879","title":"\u4fee\u8ba2\u7684\u914d\u7f6e\u9009\u9879"},{"location":"concepts/serving-resources/revisions/#_7","text":"\u4fee\u8ba2API\u89c4\u8303","title":"\u989d\u5916\u8d44\u6e90"},{"location":"concepts/serving-resources/revisions/#_8","text":"\u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u5220\u9664\u3001\u63cf\u8ff0\u548c\u5217\u51fa\u4fee\u8ba2 \u68c0\u67e5\u4fee\u8ba2\u7684\u72b6\u6001 \u5728\u670d\u52a1\u7684\u4fee\u8ba2\u4e4b\u95f4\u8def\u7531\u901a\u4fe1","title":"\u4e0b\u4e00\u6b65"},{"location":"contributing/","text":"Welcome to the Knative community \u00b6 Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"\u6b22\u8fce\u6765\u5230Knative\u793e\u533a"},{"location":"contributing/#welcome-to-the-knative-community","text":"Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"Welcome to the Knative community"},{"location":"contributing/about/","text":"About the community \u00b6 This page provides links to documents for common Knative community practices and a description of Knative's audience. Community values \u00b6 This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team. Governance \u00b6 This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports. Processes \u00b6 This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain. Community calendar \u00b6 The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace. Knative's audience \u00b6 Knative is designed for different personas: Developers \u00b6 Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group. Operators \u00b6 Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers. Contributors \u00b6 With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components: Knative authors \u00b6 Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"\u5173\u4e8e\u793e\u533a"},{"location":"contributing/about/#about-the-community","text":"This page provides links to documents for common Knative community practices and a description of Knative's audience.","title":"About the community"},{"location":"contributing/about/#community-values","text":"This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team.","title":"Community values"},{"location":"contributing/about/#governance","text":"This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports.","title":"Governance"},{"location":"contributing/about/#processes","text":"This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain.","title":"Processes"},{"location":"contributing/about/#community-calendar","text":"The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace.","title":"Community calendar"},{"location":"contributing/about/#knatives-audience","text":"Knative is designed for different personas:","title":"Knative's audience"},{"location":"contributing/about/#developers","text":"Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group.","title":"Developers"},{"location":"contributing/about/#operators","text":"Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers.","title":"Operators"},{"location":"contributing/about/#contributors","text":"With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components:","title":"Contributors"},{"location":"contributing/about/#knative-authors","text":"Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"Knative authors"},{"location":"contributing/contributing/","text":"Contribute to Knative \u00b6 This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved. Prerequisites \u00b6 If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must sign the CNCF EasyCLA using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community . Contribute to the code \u00b6 Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: To get started with contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: To get started with contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): To get started with contributing, see the Client development workflow . For good starter issues, see Client issues . Functions: To get started with contributing, see the Functions development workflow . For good starter issues, see Functions issues . Documentation: To get started with contributing, see the Docs contributor guide . For good starter issues, see Documentation issues . Contribute code samples to the community \u00b6 Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site. Learn and connect \u00b6 Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers. Community Meetup \u00b6 This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter . More ways to get involved \u00b6 Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"\u8d21\u732e Knative"},{"location":"contributing/contributing/#contribute-to-knative","text":"This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved.","title":"Contribute to Knative"},{"location":"contributing/contributing/#prerequisites","text":"If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must sign the CNCF EasyCLA using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community .","title":"Prerequisites"},{"location":"contributing/contributing/#contribute-to-the-code","text":"Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: To get started with contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: To get started with contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): To get started with contributing, see the Client development workflow . For good starter issues, see Client issues . Functions: To get started with contributing, see the Functions development workflow . For good starter issues, see Functions issues . Documentation: To get started with contributing, see the Docs contributor guide . For good starter issues, see Documentation issues .","title":"Contribute to the code"},{"location":"contributing/contributing/#contribute-code-samples-to-the-community","text":"Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site.","title":"Contribute code samples to the community"},{"location":"contributing/contributing/#learn-and-connect","text":"Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers.","title":"Learn and connect"},{"location":"contributing/contributing/#community-meetup","text":"This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter .","title":"Community Meetup"},{"location":"contributing/contributing/#more-ways-to-get-involved","text":"Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"More ways to get involved"},{"location":"eventing/","text":"Knative \u4e8b\u4ef6 \u00b6 Knative \u4e8b\u4ef6\u662f\u4e00\u4e2a API \u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b API \u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94 HTTP \u8bf7\u6c42\u3002 Knative \u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684 HTTP POST \u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents \u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative \u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002 \u652f\u6301\u7684 Knative \u4e8b\u4ef6\u7528\u4f8b\u793a\u4f8b: \u5728\u4e0d\u521b\u5efa\u6d88\u8d39\u8005\u7684\u60c5\u51b5\u4e0b\u53d1\u5e03\u4e8b\u4ef6\u3002\u60a8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u4f5c\u4e3a HTTP POST \u53d1\u9001\u5230\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u7ed1\u5b9a\u5c06\u76ee\u6807\u914d\u7f6e\u4e0e\u751f\u6210\u4e8b\u4ef6\u7684\u5e94\u7528\u7a0b\u5e8f\u5206\u79bb\u5f00\u6765\u3002 \u5728\u4e0d\u521b\u5efa\u53d1\u5e03\u8005\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u4e8b\u4ef6\u3002\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u6839\u636e\u4e8b\u4ef6\u5c5e\u6027\u4f7f\u7528\u6765\u81ea\u4ee3\u7406\u7684\u4e8b\u4ef6\u3002\u5e94\u7528\u7a0b\u5e8f\u4ee5 HTTP POST \u7684\u5f62\u5f0f\u63a5\u6536\u4e8b\u4ef6\u3002 Tip \u591a\u4e2a\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u53ef\u4ee5\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u521b\u5efa\u66f4\u9ad8\u7ea7\u7684 Knative \u4e8b\u4ef6 \u6d41\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7528\u4f8b\u3002 \u4e8b\u4ef6\u793a\u4f8b \u00b6 \u521b\u5efa\u5e76\u54cd\u5e94 Kubernetes API \u4e8b\u4ef6 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u521b\u5efa\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u7ba1\u9053 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u5728\u5927\u89c4\u6a21\u3001\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u519c\u4e1a\u9879\u76ee\u4e2d\u4fc3\u8fdb AI \u5de5\u4f5c \u4e0b\u4e00\u6b65 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406.","title":"\u4e8b\u4ef6\u6982\u8ff0"},{"location":"eventing/#knative","text":"Knative \u4e8b\u4ef6\u662f\u4e00\u4e2a API \u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b API \u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94 HTTP \u8bf7\u6c42\u3002 Knative \u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684 HTTP POST \u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents \u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative \u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002 \u652f\u6301\u7684 Knative \u4e8b\u4ef6\u7528\u4f8b\u793a\u4f8b: \u5728\u4e0d\u521b\u5efa\u6d88\u8d39\u8005\u7684\u60c5\u51b5\u4e0b\u53d1\u5e03\u4e8b\u4ef6\u3002\u60a8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u4f5c\u4e3a HTTP POST \u53d1\u9001\u5230\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u7ed1\u5b9a\u5c06\u76ee\u6807\u914d\u7f6e\u4e0e\u751f\u6210\u4e8b\u4ef6\u7684\u5e94\u7528\u7a0b\u5e8f\u5206\u79bb\u5f00\u6765\u3002 \u5728\u4e0d\u521b\u5efa\u53d1\u5e03\u8005\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u4e8b\u4ef6\u3002\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u6839\u636e\u4e8b\u4ef6\u5c5e\u6027\u4f7f\u7528\u6765\u81ea\u4ee3\u7406\u7684\u4e8b\u4ef6\u3002\u5e94\u7528\u7a0b\u5e8f\u4ee5 HTTP POST \u7684\u5f62\u5f0f\u63a5\u6536\u4e8b\u4ef6\u3002 Tip \u591a\u4e2a\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u53ef\u4ee5\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u521b\u5efa\u66f4\u9ad8\u7ea7\u7684 Knative \u4e8b\u4ef6 \u6d41\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7528\u4f8b\u3002","title":"Knative \u4e8b\u4ef6"},{"location":"eventing/#_1","text":"\u521b\u5efa\u5e76\u54cd\u5e94 Kubernetes API \u4e8b\u4ef6 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u521b\u5efa\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u7ba1\u9053 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u5728\u5927\u89c4\u6a21\u3001\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u519c\u4e1a\u9879\u76ee\u4e2d\u4fc3\u8fdb AI \u5de5\u4f5c","title":"\u4e8b\u4ef6\u793a\u4f8b"},{"location":"eventing/#_2","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406.","title":"\u4e0b\u4e00\u6b65"},{"location":"eventing/accessing-traces/","text":"\u8bbf\u95eeCloudEvent\u8ddf\u8e2a \u00b6 \u6839\u636e\u60a8\u5728Knative\u4e8b\u4ef6\u96c6\u7fa4\u4e0a\u5b89\u88c5\u7684\u8bf7\u6c42\u8ddf\u8e2a\u5de5\u5177\uff0c\u6709\u5173\u5982\u4f55\u53ef\u89c6\u5316\u548c\u8ddf\u8e2a\u8bf7\u6c42\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u76f8\u5e94\u7684\u90e8\u5206\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u60a8\u5fc5\u987b\u6709\u4e00\u4e2a\u5b89\u88c5\u4e86\u4e8b\u4ef6\u5904\u7406\u7ec4\u4ef6\u5e76\u8fd0\u884c\u7684Knative\u96c6\u7fa4\u3002 \u4e86\u89e3\u66f4\u591a . \u914d\u7f6e\u8ddf\u8e2a \u00b6 \u9664\u4e86\u5bfc\u5165\u5668\u4e4b\u5916\uff0cKnative\u4e8b\u4ef6\u8ddf\u8e2a\u662f\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-tracing ConfigMap\u914d\u7f6e\u7684\u3002 \u5927\u591a\u6570\u5bfc\u5165\u5668 \u4e0d \u4f7f\u7528ConfigMap\uff0c\u800c\u662f\u4f7f\u7528\u9759\u6001\u76841%\u62bd\u6837\u7387\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 config-tracing ConfigMap\u914d\u7f6e\u4ee5\u4e0b\u4e8b\u4ef6\u7ec4\u4ef6: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource \u4e3e\u4f8b: \u4e0b\u9762\u7684\u793a\u4f8b config-tracing ConfigMap\u5bf9\u6240\u6709CloudEvents\u768410%\u8fdb\u884c\u91c7\u6837: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\" \u914d\u7f6e\u9009\u9879 \u00b6 \u4f60\u53ef\u4ee5\u7528\u4ee5\u4e0b\u9009\u9879\u914d\u7f6e\u4f60\u7684 config-tracing : backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server. \u67e5\u770b\u4f60\u7684 config-tracing ConfigMap \u00b6 To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml \u7f16\u8f91\u548c\u90e8\u7f72 config-tracing ConfigMap \u00b6 To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing \u5728\u4e8b\u4ef6\u5904\u7406\u4e2d\u8bbf\u95ee\u8ddf\u8e2a \u00b6 To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger \u4e3e\u4f8b \u00b6 The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"\u8bbf\u95eeCloudEvent\u75d5\u8ff9"},{"location":"eventing/accessing-traces/#cloudevent","text":"\u6839\u636e\u60a8\u5728Knative\u4e8b\u4ef6\u96c6\u7fa4\u4e0a\u5b89\u88c5\u7684\u8bf7\u6c42\u8ddf\u8e2a\u5de5\u5177\uff0c\u6709\u5173\u5982\u4f55\u53ef\u89c6\u5316\u548c\u8ddf\u8e2a\u8bf7\u6c42\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u76f8\u5e94\u7684\u90e8\u5206\u3002","title":"\u8bbf\u95eeCloudEvent\u8ddf\u8e2a"},{"location":"eventing/accessing-traces/#_1","text":"\u60a8\u5fc5\u987b\u6709\u4e00\u4e2a\u5b89\u88c5\u4e86\u4e8b\u4ef6\u5904\u7406\u7ec4\u4ef6\u5e76\u8fd0\u884c\u7684Knative\u96c6\u7fa4\u3002 \u4e86\u89e3\u66f4\u591a .","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/accessing-traces/#_2","text":"\u9664\u4e86\u5bfc\u5165\u5668\u4e4b\u5916\uff0cKnative\u4e8b\u4ef6\u8ddf\u8e2a\u662f\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-tracing ConfigMap\u914d\u7f6e\u7684\u3002 \u5927\u591a\u6570\u5bfc\u5165\u5668 \u4e0d \u4f7f\u7528ConfigMap\uff0c\u800c\u662f\u4f7f\u7528\u9759\u6001\u76841%\u62bd\u6837\u7387\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 config-tracing ConfigMap\u914d\u7f6e\u4ee5\u4e0b\u4e8b\u4ef6\u7ec4\u4ef6: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource \u4e3e\u4f8b: \u4e0b\u9762\u7684\u793a\u4f8b config-tracing ConfigMap\u5bf9\u6240\u6709CloudEvents\u768410%\u8fdb\u884c\u91c7\u6837: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\"","title":"\u914d\u7f6e\u8ddf\u8e2a"},{"location":"eventing/accessing-traces/#_3","text":"\u4f60\u53ef\u4ee5\u7528\u4ee5\u4e0b\u9009\u9879\u914d\u7f6e\u4f60\u7684 config-tracing : backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server.","title":"\u914d\u7f6e\u9009\u9879"},{"location":"eventing/accessing-traces/#config-tracing-configmap","text":"To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml","title":"\u67e5\u770b\u4f60\u7684 config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#config-tracing-configmap_1","text":"To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing","title":"\u7f16\u8f91\u548c\u90e8\u7f72 config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#_4","text":"To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger","title":"\u5728\u4e8b\u4ef6\u5904\u7406\u4e2d\u8bbf\u95ee\u8ddf\u8e2a"},{"location":"eventing/accessing-traces/#_5","text":"The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"\u4e3e\u4f8b"},{"location":"eventing/event-delivery/","text":"\u6295\u9012\u5931\u8d25\u5904\u7406 \u00b6 \u60a8\u53ef\u4ee5\u4e3aKnative\u4e8b\u4ef6\u7ec4\u4ef6\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u53c2\u6570\uff0c\u5f53\u4e8b\u4ef6\u4f20\u9012\u5931\u8d25\u65f6\u5e94\u7528\u8fd9\u4e9b\u53c2\u6570 \u914d\u7f6e\u8ba2\u9605\u4e8b\u4ef6\u4f20\u9012 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u5411 Subscription \u5bf9\u8c61\u6dfb\u52a0 delivery \u89c4\u8303\u6765\u914d\u7f6e\u4e8b\u4ef6\u5982\u4f55\u4e3a\u6bcf\u4e2aSubscription\u4f20\u9012\uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where deadLetterSink \u89c4\u8303\u5305\u542b\u542f\u7528\u4f7f\u7528\u6b7b\u4fe1\u63a5\u6536\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u8fd9\u5c06\u544a\u8bc9\u8ba2\u9605\u65e0\u6cd5\u4f20\u9012\u7ed9\u8ba2\u9605\u670d\u52a1\u5668\u7684\u4e8b\u4ef6\u53d1\u751f\u4e86\u4ec0\u4e48\u3002 \u914d\u7f6e\u6b64\u53c2\u6570\u540e\uff0c\u53d1\u9001\u5931\u8d25\u7684\u4e8b\u4ef6\u5c06\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u76ee\u7684\u5730\u3002 \u76ee\u7684\u5730\u53ef\u4ee5\u662fKnative Service\u6216URI\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c\u76ee\u7684\u5730\u662f\u4e00\u4e2a\u540d\u4e3a example-sink \u7684 Service \u5bf9\u8c61\u6216Knative Service\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u6307\u5b9a\u5931\u8d25\u540e\u91cd\u8bd5\u5c1d\u8bd5\u4e8b\u4ef6\u4f20\u9012\u4e4b\u524d\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002 backoffDelay \u53c2\u6570\u7684\u6301\u7eed\u65f6\u95f4\u4f7f\u7528ISO 8601\u683c\u5f0f\u6307\u5b9a\u3002 \u4f8b\u5982\uff0c PT1S \u6307\u5b9a1\u79d2\u5ef6\u8fdf\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u53ef\u7528\u4e8e\u6307\u5b9a\u91cd\u8bd5\u56de\u9000\u7b56\u7565\u3002 \u8be5\u7b56\u7565\u53ef\u4ee5\u6307\u5b9a\u4e3a\u201c\u7ebf\u6027\u201d\u6216\u201c\u6307\u6570\u201d\u3002 \u5f53\u4f7f\u7528\u201c\u7ebf\u6027\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u662f\u91cd\u8bd5\u4e4b\u95f4\u6307\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u3002 \u5f53\u4f7f\u7528'\u6307\u6570'\u56de\u9000\u7b56\u7565\u65f6\uff0c\u56de\u9000\u5ef6\u8fdf\u7b49\u4e8e backoffDelay*2^<numberOfRetries> \u3002 retry \u6307\u5b9a\u5728\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u4e4b\u524d\u91cd\u8bd5\u4e8b\u4ef6\u4f20\u9012\u7684\u6b21\u6570\u3002 \u914d\u7f6e\u4ee3\u7406\u4e8b\u4ef6\u4f20\u9012 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a delivery \u89c4\u8303\u6765\u914d\u7f6e\u4e8b\u4ef6\u5982\u4f55\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u4ea4\u4ed8\uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where deadLetterSink \u89c4\u8303\u5305\u542b\u542f\u7528\u4f7f\u7528\u6b7b\u4fe1\u63a5\u6536\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u8fd9\u5c06\u544a\u8bc9\u8ba2\u9605\u65e0\u6cd5\u4f20\u9012\u7ed9\u8ba2\u9605\u670d\u52a1\u5668\u7684\u4e8b\u4ef6\u53d1\u751f\u4e86\u4ec0\u4e48\u3002 \u914d\u7f6e\u6b64\u53c2\u6570\u540e\uff0c\u53d1\u9001\u5931\u8d25\u7684\u4e8b\u4ef6\u5c06\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u76ee\u7684\u5730\u3002 \u76ee\u7684\u5730\u53ef\u4ee5\u662f\u7b26\u5408Knative\u4e8b\u4ef6\u63a5\u6536\u5668\u5951\u7ea6\u7684\u4efb\u4f55\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982Knative\u670d\u52a1\u3001Kubernetes\u670d\u52a1\u6216URI\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c\u76ee\u7684\u5730\u662f\u4e00\u4e2a\u540d\u4e3a example-sink \u7684 Service \u5bf9\u8c61\u6216Knative Service\u3002 backoffDelay \u4f20\u9012\u53c2\u6570\u6307\u5b9a\u5931\u8d25\u540e\u91cd\u8bd5\u5c1d\u8bd5\u4e8b\u4ef6\u4f20\u9012\u4e4b\u524d\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002 backoffDelay \u53c2\u6570\u7684\u6301\u7eed\u65f6\u95f4\u4f7f\u7528ISO 8601\u683c\u5f0f\u6307\u5b9a\u3002 \u4f8b\u5982\uff0c PT1S \u6307\u5b9a1\u79d2\u5ef6\u8fdf\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u53ef\u7528\u4e8e\u6307\u5b9a\u91cd\u8bd5\u56de\u9000\u7b56\u7565\u3002 \u8be5\u7b56\u7565\u53ef\u4ee5\u6307\u5b9a\u4e3a\u201c\u7ebf\u6027\u201d\u6216\u201c\u6307\u6570\u201d\u3002 \u5f53\u4f7f\u7528\u201c\u7ebf\u6027\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u662f\u91cd\u8bd5\u4e4b\u95f4\u6307\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u3002 \u8fd9\u662f\u4e00\u4e2a\u7ebf\u6027\u589e\u52a0\u7684\u5ef6\u8fdf\uff0c\u8fd9\u610f\u5473\u7740\u540e\u9000\u5ef6\u8fdf\u6309\u6bcf\u6b21\u91cd\u8bd5\u7684\u7ed9\u5b9a\u95f4\u9694\u589e\u52a0\u3002 \u5f53\u4f7f\u7528\u201c\u6307\u6570\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u4e3a\u6bcf\u6b21\u91cd\u8bd5\u589e\u52a0\u7ed9\u5b9a\u95f4\u9694\u7684\u4e58\u6570\u3002 retry \u6307\u5b9a\u5728\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u4e4b\u524d\u91cd\u8bd5\u4e8b\u4ef6\u4f20\u9012\u7684\u6b21\u6570\u3002 \u521d\u59cb\u7684\u4ea4\u4ed8\u5c1d\u8bd5\u4e0d\u5305\u62ec\u5728\u91cd\u8bd5\u8ba1\u6570\u4e2d\uff0c\u56e0\u6b64\u4ea4\u4ed8\u5c1d\u8bd5\u7684\u603b\u6570\u7b49\u4e8e\u201c\u91cd\u8bd5\u201d\u503c+1\u3002 \u4ee3\u7406\u652f\u6301 \u00b6 \u4e0b\u8868\u603b\u7ed3\u4e86\u6bcf\u79cd\u4ee3\u7406\u5b9e\u73b0\u7c7b\u578b\u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570: \u4ee3\u7406\u7c7b \u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570 googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker \u53d6\u51b3\u4e8e\u5e95\u5c42\u7684\u901a\u9053 RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink \u5fc5\u987b\u662f\u4e00\u4e2aGCP Pub/Sub\u4e3b\u9898URI\u3002 googlecloud \u4ee3\u7406\u53ea\u652f\u6301\u201c\u6307\u6570\u201d\u56de\u9000\u7b56\u7565\u3002 \u914d\u7f6e\u901a\u9053\u4e8b\u4ef6\u4f20\u9012 \u00b6 \u5931\u8d25\u7684\u4e8b\u4ef6\u53ef\u4ee5\u5728\u8f6c\u53d1\u5230 deadLetterSink \u4e4b\u524d\u901a\u8fc7\u6269\u5c55\u5c5e\u6027\u8fdb\u884c\u589e\u5f3a\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u7279\u5b9a\u901a\u9053\u5b9e\u73b0\u3002 \u8fd9\u4e9b\u6269\u5c55\u5c5e\u6027\u5982\u4e0b: knativeerrordest Type: String Description: \u5c06\u5931\u8d25\u4e8b\u4ef6\u53d1\u9001\u5230\u7684\u539f\u59cb\u76ee\u6807URL\u3002 \u6839\u636e\u9047\u5230\u5931\u8d25\u4e8b\u4ef6\u7684\u64cd\u4f5c\uff0c\u8fd9\u53ef\u4ee5\u662f\u4e00\u4e2a\u201c\u4ea4\u4ed8\u201d\u6216\u201c\u56de\u590d\u201dURL\u3002 Constraints: \u603b\u662f\u5b58\u5728\uff0c\u56e0\u4e3a\u6bcf\u4e2aHTTP\u8bf7\u6c42\u90fd\u6709\u4e00\u4e2a\u76ee\u6807URL\u3002 Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: \u6765\u81ea\u6700\u540e\u4e8b\u4ef6\u5206\u6d3e\u5c1d\u8bd5\u7684HTTP\u54cd\u5e94 StatusCode \u3002 Constraints: \u603b\u662f\u5b58\u5728\uff0c\u56e0\u4e3a\u6bcf\u4e2aHTTP\u54cd\u5e94\u90fd\u5305\u542b\u4e00\u4e2a StatusCode \u3002 Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: \u6765\u81ea\u6700\u540e\u4e8b\u4ef6\u5206\u6d3e\u5c1d\u8bd5\u7684HTTP\u54cd\u5e94 Body \u3002 Constraints: \u5982\u679cHTTP Response Body \u4e3a\u7a7a\uff0c\u5219\u4e3a\u7a7a;\u5982\u679c\u957f\u5ea6\u8fc7\u5927\uff0c\u5219\u53ef\u80fd\u88ab\u622a\u65ad\u3002 Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body... \u901a\u9053\u652f\u6301 \u00b6 \u4e0b\u8868\u603b\u7ed3\u4e86\u6bcf\u4e2a\u901a\u9053\u5b9e\u73b0\u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570\u3002 \u901a\u9053\u7c7b\u578b \u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570 GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"\u6295\u9012\u5931\u8d25\u5904\u7406"},{"location":"eventing/event-delivery/#_1","text":"\u60a8\u53ef\u4ee5\u4e3aKnative\u4e8b\u4ef6\u7ec4\u4ef6\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u53c2\u6570\uff0c\u5f53\u4e8b\u4ef6\u4f20\u9012\u5931\u8d25\u65f6\u5e94\u7528\u8fd9\u4e9b\u53c2\u6570","title":"\u6295\u9012\u5931\u8d25\u5904\u7406"},{"location":"eventing/event-delivery/#_2","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u5411 Subscription \u5bf9\u8c61\u6dfb\u52a0 delivery \u89c4\u8303\u6765\u914d\u7f6e\u4e8b\u4ef6\u5982\u4f55\u4e3a\u6bcf\u4e2aSubscription\u4f20\u9012\uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where deadLetterSink \u89c4\u8303\u5305\u542b\u542f\u7528\u4f7f\u7528\u6b7b\u4fe1\u63a5\u6536\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u8fd9\u5c06\u544a\u8bc9\u8ba2\u9605\u65e0\u6cd5\u4f20\u9012\u7ed9\u8ba2\u9605\u670d\u52a1\u5668\u7684\u4e8b\u4ef6\u53d1\u751f\u4e86\u4ec0\u4e48\u3002 \u914d\u7f6e\u6b64\u53c2\u6570\u540e\uff0c\u53d1\u9001\u5931\u8d25\u7684\u4e8b\u4ef6\u5c06\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u76ee\u7684\u5730\u3002 \u76ee\u7684\u5730\u53ef\u4ee5\u662fKnative Service\u6216URI\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c\u76ee\u7684\u5730\u662f\u4e00\u4e2a\u540d\u4e3a example-sink \u7684 Service \u5bf9\u8c61\u6216Knative Service\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u6307\u5b9a\u5931\u8d25\u540e\u91cd\u8bd5\u5c1d\u8bd5\u4e8b\u4ef6\u4f20\u9012\u4e4b\u524d\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002 backoffDelay \u53c2\u6570\u7684\u6301\u7eed\u65f6\u95f4\u4f7f\u7528ISO 8601\u683c\u5f0f\u6307\u5b9a\u3002 \u4f8b\u5982\uff0c PT1S \u6307\u5b9a1\u79d2\u5ef6\u8fdf\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u53ef\u7528\u4e8e\u6307\u5b9a\u91cd\u8bd5\u56de\u9000\u7b56\u7565\u3002 \u8be5\u7b56\u7565\u53ef\u4ee5\u6307\u5b9a\u4e3a\u201c\u7ebf\u6027\u201d\u6216\u201c\u6307\u6570\u201d\u3002 \u5f53\u4f7f\u7528\u201c\u7ebf\u6027\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u662f\u91cd\u8bd5\u4e4b\u95f4\u6307\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u3002 \u5f53\u4f7f\u7528'\u6307\u6570'\u56de\u9000\u7b56\u7565\u65f6\uff0c\u56de\u9000\u5ef6\u8fdf\u7b49\u4e8e backoffDelay*2^<numberOfRetries> \u3002 retry \u6307\u5b9a\u5728\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u4e4b\u524d\u91cd\u8bd5\u4e8b\u4ef6\u4f20\u9012\u7684\u6b21\u6570\u3002","title":"\u914d\u7f6e\u8ba2\u9605\u4e8b\u4ef6\u4f20\u9012"},{"location":"eventing/event-delivery/#_3","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2a delivery \u89c4\u8303\u6765\u914d\u7f6e\u4e8b\u4ef6\u5982\u4f55\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u4ea4\u4ed8\uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where deadLetterSink \u89c4\u8303\u5305\u542b\u542f\u7528\u4f7f\u7528\u6b7b\u4fe1\u63a5\u6536\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u8fd9\u5c06\u544a\u8bc9\u8ba2\u9605\u65e0\u6cd5\u4f20\u9012\u7ed9\u8ba2\u9605\u670d\u52a1\u5668\u7684\u4e8b\u4ef6\u53d1\u751f\u4e86\u4ec0\u4e48\u3002 \u914d\u7f6e\u6b64\u53c2\u6570\u540e\uff0c\u53d1\u9001\u5931\u8d25\u7684\u4e8b\u4ef6\u5c06\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u76ee\u7684\u5730\u3002 \u76ee\u7684\u5730\u53ef\u4ee5\u662f\u7b26\u5408Knative\u4e8b\u4ef6\u63a5\u6536\u5668\u5951\u7ea6\u7684\u4efb\u4f55\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982Knative\u670d\u52a1\u3001Kubernetes\u670d\u52a1\u6216URI\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c\u76ee\u7684\u5730\u662f\u4e00\u4e2a\u540d\u4e3a example-sink \u7684 Service \u5bf9\u8c61\u6216Knative Service\u3002 backoffDelay \u4f20\u9012\u53c2\u6570\u6307\u5b9a\u5931\u8d25\u540e\u91cd\u8bd5\u5c1d\u8bd5\u4e8b\u4ef6\u4f20\u9012\u4e4b\u524d\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002 backoffDelay \u53c2\u6570\u7684\u6301\u7eed\u65f6\u95f4\u4f7f\u7528ISO 8601\u683c\u5f0f\u6307\u5b9a\u3002 \u4f8b\u5982\uff0c PT1S \u6307\u5b9a1\u79d2\u5ef6\u8fdf\u3002 backoffPolicy \u4f20\u9012\u53c2\u6570\u53ef\u7528\u4e8e\u6307\u5b9a\u91cd\u8bd5\u56de\u9000\u7b56\u7565\u3002 \u8be5\u7b56\u7565\u53ef\u4ee5\u6307\u5b9a\u4e3a\u201c\u7ebf\u6027\u201d\u6216\u201c\u6307\u6570\u201d\u3002 \u5f53\u4f7f\u7528\u201c\u7ebf\u6027\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u662f\u91cd\u8bd5\u4e4b\u95f4\u6307\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u3002 \u8fd9\u662f\u4e00\u4e2a\u7ebf\u6027\u589e\u52a0\u7684\u5ef6\u8fdf\uff0c\u8fd9\u610f\u5473\u7740\u540e\u9000\u5ef6\u8fdf\u6309\u6bcf\u6b21\u91cd\u8bd5\u7684\u7ed9\u5b9a\u95f4\u9694\u589e\u52a0\u3002 \u5f53\u4f7f\u7528\u201c\u6307\u6570\u201d\u540e\u9000\u7b56\u7565\u65f6\uff0c\u540e\u9000\u5ef6\u8fdf\u4e3a\u6bcf\u6b21\u91cd\u8bd5\u589e\u52a0\u7ed9\u5b9a\u95f4\u9694\u7684\u4e58\u6570\u3002 retry \u6307\u5b9a\u5728\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u6b7b\u4fe1\u63a5\u6536\u4e4b\u524d\u91cd\u8bd5\u4e8b\u4ef6\u4f20\u9012\u7684\u6b21\u6570\u3002 \u521d\u59cb\u7684\u4ea4\u4ed8\u5c1d\u8bd5\u4e0d\u5305\u62ec\u5728\u91cd\u8bd5\u8ba1\u6570\u4e2d\uff0c\u56e0\u6b64\u4ea4\u4ed8\u5c1d\u8bd5\u7684\u603b\u6570\u7b49\u4e8e\u201c\u91cd\u8bd5\u201d\u503c+1\u3002","title":"\u914d\u7f6e\u4ee3\u7406\u4e8b\u4ef6\u4f20\u9012"},{"location":"eventing/event-delivery/#_4","text":"\u4e0b\u8868\u603b\u7ed3\u4e86\u6bcf\u79cd\u4ee3\u7406\u5b9e\u73b0\u7c7b\u578b\u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570: \u4ee3\u7406\u7c7b \u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570 googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker \u53d6\u51b3\u4e8e\u5e95\u5c42\u7684\u901a\u9053 RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink \u5fc5\u987b\u662f\u4e00\u4e2aGCP Pub/Sub\u4e3b\u9898URI\u3002 googlecloud \u4ee3\u7406\u53ea\u652f\u6301\u201c\u6307\u6570\u201d\u56de\u9000\u7b56\u7565\u3002","title":"\u4ee3\u7406\u652f\u6301"},{"location":"eventing/event-delivery/#_5","text":"\u5931\u8d25\u7684\u4e8b\u4ef6\u53ef\u4ee5\u5728\u8f6c\u53d1\u5230 deadLetterSink \u4e4b\u524d\u901a\u8fc7\u6269\u5c55\u5c5e\u6027\u8fdb\u884c\u589e\u5f3a\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u7279\u5b9a\u901a\u9053\u5b9e\u73b0\u3002 \u8fd9\u4e9b\u6269\u5c55\u5c5e\u6027\u5982\u4e0b: knativeerrordest Type: String Description: \u5c06\u5931\u8d25\u4e8b\u4ef6\u53d1\u9001\u5230\u7684\u539f\u59cb\u76ee\u6807URL\u3002 \u6839\u636e\u9047\u5230\u5931\u8d25\u4e8b\u4ef6\u7684\u64cd\u4f5c\uff0c\u8fd9\u53ef\u4ee5\u662f\u4e00\u4e2a\u201c\u4ea4\u4ed8\u201d\u6216\u201c\u56de\u590d\u201dURL\u3002 Constraints: \u603b\u662f\u5b58\u5728\uff0c\u56e0\u4e3a\u6bcf\u4e2aHTTP\u8bf7\u6c42\u90fd\u6709\u4e00\u4e2a\u76ee\u6807URL\u3002 Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: \u6765\u81ea\u6700\u540e\u4e8b\u4ef6\u5206\u6d3e\u5c1d\u8bd5\u7684HTTP\u54cd\u5e94 StatusCode \u3002 Constraints: \u603b\u662f\u5b58\u5728\uff0c\u56e0\u4e3a\u6bcf\u4e2aHTTP\u54cd\u5e94\u90fd\u5305\u542b\u4e00\u4e2a StatusCode \u3002 Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: \u6765\u81ea\u6700\u540e\u4e8b\u4ef6\u5206\u6d3e\u5c1d\u8bd5\u7684HTTP\u54cd\u5e94 Body \u3002 Constraints: \u5982\u679cHTTP Response Body \u4e3a\u7a7a\uff0c\u5219\u4e3a\u7a7a;\u5982\u679c\u957f\u5ea6\u8fc7\u5927\uff0c\u5219\u53ef\u80fd\u88ab\u622a\u65ad\u3002 Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body...","title":"\u914d\u7f6e\u901a\u9053\u4e8b\u4ef6\u4f20\u9012"},{"location":"eventing/event-delivery/#_6","text":"\u4e0b\u8868\u603b\u7ed3\u4e86\u6bcf\u4e2a\u901a\u9053\u5b9e\u73b0\u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570\u3002 \u901a\u9053\u7c7b\u578b \u652f\u6301\u7684\u4ea4\u4ed8\u53c2\u6570 GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"\u901a\u9053\u652f\u6301"},{"location":"eventing/event-registry/","text":"\u4e8b\u4ef6\u6ce8\u518c\u8868 \u00b6 Knative\u4e8b\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2a EventType \u5bf9\u8c61\uff0c\u4ee5\u4f7f\u6d88\u8d39\u8005\u66f4\u5bb9\u6613\u53d1\u73b0\u4ed6\u4eec\u53ef\u4ee5\u4ece\u4ee3\u7406\u4f7f\u7528\u7684\u4e8b\u4ef6\u7c7b\u578b\u3002 The event registry maintains a catalog of event types that each Broker can consume. The event types stored in the registry contain all required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. This topic provides information about how you can populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest. Note Before using the event registry, it is recommended that you have a basic understanding of Brokers, Triggers, Event Sources, and the CloudEvents spec (particularly the Context Attributes section). \u5173\u4e8eEventType\u5bf9\u8c61 \u00b6 EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional \u7528\u4e8b\u4ef6\u586b\u5145\u6ce8\u518c\u8868 \u00b6 You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources. \u624b\u52a8\u6ce8\u518c \u00b6 For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml> \u81ea\u52a8\u767b\u8bb0 \u00b6 Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated. \u652f\u6301\u81ea\u52a8\u6ce8\u518c \u00b6 Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink. \u81ea\u52a8\u767b\u8bb0\u7a0b\u5e8f \u00b6 To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry. \u793a\u4f8b:\u4f7f\u7528KafkaSource\u81ea\u52a8\u6ce8\u518c \u00b6 Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics. \u4f7f\u7528\u6ce8\u518c\u8868\u53d1\u73b0\u4e8b\u4ef6 \u00b6 Using the registry, you can discover the different types of events that Broker event meshes can consume. \u67e5\u770b\u60a8\u53ef\u4ee5\u8ba2\u9605\u7684\u6240\u6709\u4e8b\u4ef6\u7c7b\u578b \u00b6 To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink. \u67e5\u770bEventType\u5bf9\u8c61\u7684YAML \u00b6 To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page. \u5173\u4e8e\u8ba2\u9605\u4e8b\u4ef6 \u00b6 After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready. \u4e0b\u4e00\u6b65 \u00b6 Knative\u4ee3\u7801\u793a\u4f8b \u662f\u4e00\u4e2a\u6709\u7528\u7684\u8d44\u6e90\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4e00\u4e9b\u4e8b\u4ef6\u6e90\u3002 \u8bb0\u4f4f\uff0c\u5982\u679c\u60a8\u60f3\u5728\u6ce8\u518c\u8868\u4e2d\u81ea\u52a8\u6ce8\u518ceventtype\uff0c\u5219\u5fc5\u987b\u5c06\u6e90\u6307\u5411\u4ee3\u7406\u3002","title":"\u4e8b\u4ef6\u6ce8\u518c\u8868"},{"location":"eventing/event-registry/#_1","text":"Knative\u4e8b\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2a EventType \u5bf9\u8c61\uff0c\u4ee5\u4f7f\u6d88\u8d39\u8005\u66f4\u5bb9\u6613\u53d1\u73b0\u4ed6\u4eec\u53ef\u4ee5\u4ece\u4ee3\u7406\u4f7f\u7528\u7684\u4e8b\u4ef6\u7c7b\u578b\u3002 The event registry maintains a catalog of event types that each Broker can consume. The event types stored in the registry contain all required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. This topic provides information about how you can populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest. Note Before using the event registry, it is recommended that you have a basic understanding of Brokers, Triggers, Event Sources, and the CloudEvents spec (particularly the Context Attributes section).","title":"\u4e8b\u4ef6\u6ce8\u518c\u8868"},{"location":"eventing/event-registry/#eventtype","text":"EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional","title":"\u5173\u4e8eEventType\u5bf9\u8c61"},{"location":"eventing/event-registry/#_2","text":"You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources.","title":"\u7528\u4e8b\u4ef6\u586b\u5145\u6ce8\u518c\u8868"},{"location":"eventing/event-registry/#_3","text":"For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml>","title":"\u624b\u52a8\u6ce8\u518c"},{"location":"eventing/event-registry/#_4","text":"Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated.","title":"\u81ea\u52a8\u767b\u8bb0"},{"location":"eventing/event-registry/#_5","text":"Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink.","title":"\u652f\u6301\u81ea\u52a8\u6ce8\u518c"},{"location":"eventing/event-registry/#_6","text":"To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry.","title":"\u81ea\u52a8\u767b\u8bb0\u7a0b\u5e8f"},{"location":"eventing/event-registry/#kafkasource","text":"Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics.","title":"\u793a\u4f8b:\u4f7f\u7528KafkaSource\u81ea\u52a8\u6ce8\u518c"},{"location":"eventing/event-registry/#_7","text":"Using the registry, you can discover the different types of events that Broker event meshes can consume.","title":"\u4f7f\u7528\u6ce8\u518c\u8868\u53d1\u73b0\u4e8b\u4ef6"},{"location":"eventing/event-registry/#_8","text":"To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink.","title":"\u67e5\u770b\u60a8\u53ef\u4ee5\u8ba2\u9605\u7684\u6240\u6709\u4e8b\u4ef6\u7c7b\u578b"},{"location":"eventing/event-registry/#eventtypeyaml","text":"To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page.","title":"\u67e5\u770bEventType\u5bf9\u8c61\u7684YAML"},{"location":"eventing/event-registry/#_9","text":"After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready.","title":"\u5173\u4e8e\u8ba2\u9605\u4e8b\u4ef6"},{"location":"eventing/event-registry/#_10","text":"Knative\u4ee3\u7801\u793a\u4f8b \u662f\u4e00\u4e2a\u6709\u7528\u7684\u8d44\u6e90\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4e00\u4e9b\u4e8b\u4ef6\u6e90\u3002 \u8bb0\u4f4f\uff0c\u5982\u679c\u60a8\u60f3\u5728\u6ce8\u518c\u8868\u4e2d\u81ea\u52a8\u6ce8\u518ceventtype\uff0c\u5219\u5fc5\u987b\u5c06\u6e90\u6307\u5411\u4ee3\u7406\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"eventing/brokers/","text":"\u5173\u4e8e\u4ee3\u7406\u5668 \u00b6 \u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002 \u4e8b\u4ef6\u4ea4\u4ed8 \u00b6 \u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u662f\u4f9d\u8d56\u4e8e\u914d\u7f6e\u7684\u4ee3\u7406\u7c7b\u7684\u5b9e\u73b0\u7ec6\u8282\u3002 \u4f7f\u7528\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u53ef\u4ee5\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u4e8b\u4ef6\u4f7f\u7528\u8005\u62bd\u8c61\u4e8b\u4ef6\u8def\u7531\u7684\u7ec6\u8282\u3002 \u9ad8\u7ea7\u7528\u4f8b \u00b6 \u5bf9\u4e8e\u5927\u591a\u6570\u7528\u4f8b\u6765\u8bf4\uff0c\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u4e00\u4e2a\u4ee3\u7406\u5c31\u8db3\u591f\u4e86\uff0c\u4f46\u662f\u5728\u4e00\u4e9b\u7528\u4f8b\u4e2d\uff0c\u591a\u4e2a\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u4f53\u7cfb\u7ed3\u6784\u3002 \u4f8b\u5982\uff0c\u9488\u5bf9\u5305\u542b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u548c\u975e PII \u4e8b\u4ef6\u7684\u72ec\u7acb\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u5ba1\u8ba1\u548c\u8bbf\u95ee\u63a7\u5236\u89c4\u5219\u3002 \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u521b\u5efa\u4e00\u4e2a \u57fa\u4e8e MT \u901a\u9053\u7684\u4ee3\u7406 . \u914d\u7f6e \u9ed8\u8ba4\u4ee3\u7406 ConfigMap \u8bbe\u7f6e . \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u4ee3\u7406\u6982\u5ff5\u6587\u6863 \u4ee3\u7406\u7684\u89c4\u8303","title":"\u5173\u4e8e\u4ee3\u7406\u5668"},{"location":"eventing/brokers/#_1","text":"\u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002","title":"\u5173\u4e8e\u4ee3\u7406\u5668"},{"location":"eventing/brokers/#_2","text":"\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u662f\u4f9d\u8d56\u4e8e\u914d\u7f6e\u7684\u4ee3\u7406\u7c7b\u7684\u5b9e\u73b0\u7ec6\u8282\u3002 \u4f7f\u7528\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u53ef\u4ee5\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u4e8b\u4ef6\u4f7f\u7528\u8005\u62bd\u8c61\u4e8b\u4ef6\u8def\u7531\u7684\u7ec6\u8282\u3002","title":"\u4e8b\u4ef6\u4ea4\u4ed8"},{"location":"eventing/brokers/#_3","text":"\u5bf9\u4e8e\u5927\u591a\u6570\u7528\u4f8b\u6765\u8bf4\uff0c\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u4e00\u4e2a\u4ee3\u7406\u5c31\u8db3\u591f\u4e86\uff0c\u4f46\u662f\u5728\u4e00\u4e9b\u7528\u4f8b\u4e2d\uff0c\u591a\u4e2a\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u4f53\u7cfb\u7ed3\u6784\u3002 \u4f8b\u5982\uff0c\u9488\u5bf9\u5305\u542b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u548c\u975e PII \u4e8b\u4ef6\u7684\u72ec\u7acb\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u5ba1\u8ba1\u548c\u8bbf\u95ee\u63a7\u5236\u89c4\u5219\u3002","title":"\u9ad8\u7ea7\u7528\u4f8b"},{"location":"eventing/brokers/#_4","text":"\u521b\u5efa\u4e00\u4e2a \u57fa\u4e8e MT \u901a\u9053\u7684\u4ee3\u7406 . \u914d\u7f6e \u9ed8\u8ba4\u4ee3\u7406 ConfigMap \u8bbe\u7f6e .","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"eventing/brokers/#_5","text":"\u4ee3\u7406\u6982\u5ff5\u6587\u6863 \u4ee3\u7406\u7684\u89c4\u8303","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"eventing/brokers/broker-admin-config-options/","text":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a\u4ee3\u7406\u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002 Knative \u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a config-br-defaults ConfigMap\uff0c\u5176\u4e2d\u5305\u542b\u7ba1\u7406\u9ed8\u8ba4\u4ee3\u7406\u521b\u5efa\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u9ed8\u8ba4\u7684 config-br-defaults ConfigMap \u5982\u4e0b: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing \u901a\u9053\u5b9e\u73b0\u9009\u9879 \u00b6 \u4e0b\u9762\u7684\u793a\u4f8b\u663e\u793a\u4e86\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u5176\u4e2d spec.config \u914d\u7f6e\u5728 config-br-default-channel ConfigMap \u4e2d\u6307\u5b9a: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation. \u66f4\u6539\u540d\u79f0\u7a7a\u95f4\u7684\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0 \u00b6 You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing \u914d\u7f6e\u4ea4\u4ed8\u89c4\u8303\u9ed8\u8ba4\u503c \u00b6 You can configure default event delivery parameters for Brokers that are applied in cases where an event fails to be delivered: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing delivery: retry: 10 backoffDelay: PT0.2S backoffPolicy: exponential namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing delivery: deadLetterSink: ref: kind: Service namespace: example-namespace name: example-service apiVersion: v1 uri: example-uri retry: 10 backoffPolicy: exponential backoffDelay: \"PT0.2S\" \u6b7b\u4fe1\u69fd \u00b6 You can configure the deadLetterSink delivery parameter so that if an event fails to be delivered it is sent to the specified event sink. \u91cd\u8bd5 \u00b6 You can set a minimum number of times that the delivery must be retried before the event is sent to the dead letter sink, by configuring the retry delivery parameter with an integer value. \u540e\u9000\u5ef6\u8fdf \u00b6 You can set the backoffDelay delivery parameter to specify the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. \u9000\u51fa\u653f\u7b56 \u00b6 The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential. When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential backoff policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . \u4ee3\u7406\u7c7b\u9009\u9879 \u00b6 When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u914d\u7f6e\u4ee3\u7406\u7c7b \u00b6 To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: ```yaml apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: default namespace: default ``` Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: ```yaml apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: default namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing ``` \u4e3a\u96c6\u7fa4\u914d\u7f6e\u9ed8\u8ba4\u7684 BrokerClass \u00b6 You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class. Example \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker \u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u9ed8\u8ba4\u7684 BrokerClass \u00b6 You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker \u5c06 Istio \u4e0e Knative \u4ee3\u7406\u96c6\u6210 \u00b6 \u901a\u8fc7\u4f7f\u7528 JSON Web Token (JWT)\u548c Istio \u6765\u4fdd\u62a4 Knative \u4ee3\u7406 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 You have installed Knative Eventing. You have installed Istio. \u8fc7\u7a0b \u00b6 Label the knative-eventing namespace, so that Istio can handle JWT-based user authentication, by running the command: kubectl label namespace knative-eventing istio-injection = enabled Restart the broker ingress pod, so that the istio-proxy container can be injected as a sidecar, by running the command: kubectl delete pod <broker-ingress-pod-name> -n knative-eventing Where <broker-ingress-pod-name> is the name of your broker ingress pod. The pod now has two containers: knative-eventing <broker-ingress-pod-name> 2 /2 Running 1 175m Create a broker, then use get the URL of your broker by running the command: kubectl get broker <broker-name> Example output: NAMESPACE NAME URL AGE READY REASON default my-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker 6s True Start a curl pod: kubectl -n default run curl --image = radial/busyboxplus:curl -i --tty Send a CloudEvent with an HTTP POST against the broker URL: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> Where <broker-URL> is the URL of your broker. For example: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker You will receive a 202 HTTP response code, that the broker did accept the request: ... * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 13 :37:57 GMT < content-length: 0 < x-envoy-upstream-service-time: 79 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* Apply a AuthorizationPolicy object in the knative-eventing namespace to describe that the path to the Broker is restricted to a given user: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : require-jwt namespace : knative-eventing spec : action : ALLOW rules : - from : - source : requestPrincipals : [ \"testing@secure.istio.io/testing@secure.istio.io\" ] to : - operation : methods : [ \"POST\" ] paths : [ \"/default/my-broker\" ] Create a RequestAuthentication object for the user requestPrincipal in the istio-system namespace: apiVersion : security.istio.io/v1beta1 kind : RequestAuthentication metadata : name : \"jwt-example\" namespace : istio-system spec : jwtRules : - issuer : \"testing@secure.istio.io\" jwksUri : \"https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/jwks.json\" Now retrying the curl command results in a 403 - Forbidden response code from the server: ... * Mark bundle as not supporting multiuse < HTTP/1.1 403 Forbidden < content-length: 19 < content-type: text/plain < date: Tue, 15 Mar 2022 13 :47:53 GMT < server: istio-envoy < connection: close < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* To access the Broker, add the Bearer JSON Web Token as part of the request: TOKEN = $( curl https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/demo.jwt -s ) curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> The server now responds with a 202 response code, indicating that it has accepted the HTTP request: * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 14 :05:09 GMT < content-length: 0 < x-envoy-upstream-service-time: 40 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_1","text":"\u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a\u4ee3\u7406\u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002 Knative \u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a config-br-defaults ConfigMap\uff0c\u5176\u4e2d\u5305\u542b\u7ba1\u7406\u9ed8\u8ba4\u4ee3\u7406\u521b\u5efa\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u9ed8\u8ba4\u7684 config-br-defaults ConfigMap \u5982\u4e0b: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_2","text":"\u4e0b\u9762\u7684\u793a\u4f8b\u663e\u793a\u4e86\u4e00\u4e2a\u4ee3\u7406\u5bf9\u8c61\uff0c\u5176\u4e2d spec.config \u914d\u7f6e\u5728 config-br-default-channel ConfigMap \u4e2d\u6307\u5b9a: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation.","title":"\u901a\u9053\u5b9e\u73b0\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_3","text":"You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"\u66f4\u6539\u540d\u79f0\u7a7a\u95f4\u7684\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0"},{"location":"eventing/brokers/broker-admin-config-options/#_4","text":"You can configure default event delivery parameters for Brokers that are applied in cases where an event fails to be delivered: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing delivery: retry: 10 backoffDelay: PT0.2S backoffPolicy: exponential namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing delivery: deadLetterSink: ref: kind: Service namespace: example-namespace name: example-service apiVersion: v1 uri: example-uri retry: 10 backoffPolicy: exponential backoffDelay: \"PT0.2S\"","title":"\u914d\u7f6e\u4ea4\u4ed8\u89c4\u8303\u9ed8\u8ba4\u503c"},{"location":"eventing/brokers/broker-admin-config-options/#_5","text":"You can configure the deadLetterSink delivery parameter so that if an event fails to be delivered it is sent to the specified event sink.","title":"\u6b7b\u4fe1\u69fd"},{"location":"eventing/brokers/broker-admin-config-options/#_6","text":"You can set a minimum number of times that the delivery must be retried before the event is sent to the dead letter sink, by configuring the retry delivery parameter with an integer value.","title":"\u91cd\u8bd5"},{"location":"eventing/brokers/broker-admin-config-options/#_7","text":"You can set the backoffDelay delivery parameter to specify the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format.","title":"\u540e\u9000\u5ef6\u8fdf"},{"location":"eventing/brokers/broker-admin-config-options/#_8","text":"The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential. When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential backoff policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> .","title":"\u9000\u51fa\u653f\u7b56"},{"location":"eventing/brokers/broker-admin-config-options/#_9","text":"When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u4ee3\u7406\u7c7b\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_10","text":"To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: ```yaml apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: default namespace: default ``` Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: ```yaml apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: default namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing ```","title":"\u914d\u7f6e\u4ee3\u7406\u7c7b"},{"location":"eventing/brokers/broker-admin-config-options/#brokerclass","text":"You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class.","title":"\u4e3a\u96c6\u7fa4\u914d\u7f6e\u9ed8\u8ba4\u7684 BrokerClass"},{"location":"eventing/brokers/broker-admin-config-options/#example","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker","title":"Example"},{"location":"eventing/brokers/broker-admin-config-options/#brokerclass_1","text":"You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"\u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u9ed8\u8ba4\u7684 BrokerClass"},{"location":"eventing/brokers/broker-admin-config-options/#istio-knative","text":"","title":"\u5c06 Istio \u4e0e Knative \u4ee3\u7406\u96c6\u6210"},{"location":"eventing/brokers/broker-admin-config-options/#json-web-token-jwt-istio-knative","text":"","title":"\u901a\u8fc7\u4f7f\u7528 JSON Web Token (JWT)\u548c Istio \u6765\u4fdd\u62a4 Knative \u4ee3\u7406"},{"location":"eventing/brokers/broker-admin-config-options/#_11","text":"You have installed Knative Eventing. You have installed Istio.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/brokers/broker-admin-config-options/#_12","text":"Label the knative-eventing namespace, so that Istio can handle JWT-based user authentication, by running the command: kubectl label namespace knative-eventing istio-injection = enabled Restart the broker ingress pod, so that the istio-proxy container can be injected as a sidecar, by running the command: kubectl delete pod <broker-ingress-pod-name> -n knative-eventing Where <broker-ingress-pod-name> is the name of your broker ingress pod. The pod now has two containers: knative-eventing <broker-ingress-pod-name> 2 /2 Running 1 175m Create a broker, then use get the URL of your broker by running the command: kubectl get broker <broker-name> Example output: NAMESPACE NAME URL AGE READY REASON default my-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker 6s True Start a curl pod: kubectl -n default run curl --image = radial/busyboxplus:curl -i --tty Send a CloudEvent with an HTTP POST against the broker URL: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> Where <broker-URL> is the URL of your broker. For example: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker You will receive a 202 HTTP response code, that the broker did accept the request: ... * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 13 :37:57 GMT < content-length: 0 < x-envoy-upstream-service-time: 79 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* Apply a AuthorizationPolicy object in the knative-eventing namespace to describe that the path to the Broker is restricted to a given user: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : require-jwt namespace : knative-eventing spec : action : ALLOW rules : - from : - source : requestPrincipals : [ \"testing@secure.istio.io/testing@secure.istio.io\" ] to : - operation : methods : [ \"POST\" ] paths : [ \"/default/my-broker\" ] Create a RequestAuthentication object for the user requestPrincipal in the istio-system namespace: apiVersion : security.istio.io/v1beta1 kind : RequestAuthentication metadata : name : \"jwt-example\" namespace : istio-system spec : jwtRules : - issuer : \"testing@secure.istio.io\" jwksUri : \"https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/jwks.json\" Now retrying the curl command results in a 403 - Forbidden response code from the server: ... * Mark bundle as not supporting multiuse < HTTP/1.1 403 Forbidden < content-length: 19 < content-type: text/plain < date: Tue, 15 Mar 2022 13 :47:53 GMT < server: istio-envoy < connection: close < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* To access the Broker, add the Bearer JSON Web Token as part of the request: TOKEN = $( curl https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/demo.jwt -s ) curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> The server now responds with a 202 response code, indicating that it has accepted the HTTP request: * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 14 :05:09 GMT < content-length: 0 < x-envoy-upstream-service-time: 40 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*","title":"\u8fc7\u7a0b"},{"location":"eventing/brokers/broker-developer-config-options/","text":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u4ee3\u7406\u914d\u7f6e\u793a\u4f8b \u00b6 \u4e0b\u9762\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5bf9\u8c61\u7684\u5b8c\u6574\u793a\u4f8b\uff0c\u5176\u4e2d\u663e\u793a\u4e86\u60a8\u53ef\u4ee5\u4fee\u6539\u7684\u53ef\u80fd\u914d\u7f6e\u9009\u9879: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" \u60a8\u53ef\u4ee5\u4e3a\u4ee3\u7406\u6307\u5b9a\u4efb\u4f55\u6709\u6548\u7684 name \u3002\u4f7f\u7528 default \u5c06\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 namespace \u5fc5\u987b\u662f\u96c6\u7fa4\u4e2d\u5df2\u5b58\u5728\u7684\u547d\u540d\u7a7a\u95f4\u3002\u4f7f\u7528 default \u5c06\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406\u3002 \u60a8\u53ef\u4ee5\u8bbe\u7f6e event.knative.dev/broker.class \u6ce8\u91ca\u6765\u66f4\u6539\u4ee3\u7406\u7684\u7c7b\u3002 \u9ed8\u8ba4\u7684\u4ee3\u7406\u7c7b\u662f MTChannelBasedBroker \uff0c\u4f46 Knative \u4e5f\u652f\u6301\u4f7f\u7528 Kafka \u548c RabbitMQBroker \u4ee3\u7406\u7c7b\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Apache Kafka Broker \u6216 RabbitMQ Broker \u6587\u6863\u3002 spec.config \u7528\u4e8e\u4e3a\u57fa\u4e8e MT \u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\u6307\u5b9a\u9ed8\u8ba4\u7684\u540e\u5907\u901a\u9053\u914d\u7f6e\u3002 \u6709\u5173\u914d\u7f6e\u9ed8\u8ba4\u901a\u9053\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u6709\u5173 \u914d\u7f6e\u4ee3\u7406\u9ed8\u8ba4\u503c \u7684\u6587\u6863. spec.delivery \u7528\u4e8e\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u9009\u9879\u3002 \u4e8b\u4ef6\u4f20\u9012\u9009\u9879\u6307\u5b9a\u672a\u80fd\u4f20\u9012\u5230\u4e8b\u4ef6\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u4e8b\u4ef6\u4f20\u9012 \u7684\u6587\u6863\u3002","title":"\u5f00\u53d1\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-developer-config-options/#_1","text":"","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-developer-config-options/#_2","text":"\u4e0b\u9762\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5bf9\u8c61\u7684\u5b8c\u6574\u793a\u4f8b\uff0c\u5176\u4e2d\u663e\u793a\u4e86\u60a8\u53ef\u4ee5\u4fee\u6539\u7684\u53ef\u80fd\u914d\u7f6e\u9009\u9879: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" \u60a8\u53ef\u4ee5\u4e3a\u4ee3\u7406\u6307\u5b9a\u4efb\u4f55\u6709\u6548\u7684 name \u3002\u4f7f\u7528 default \u5c06\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 namespace \u5fc5\u987b\u662f\u96c6\u7fa4\u4e2d\u5df2\u5b58\u5728\u7684\u547d\u540d\u7a7a\u95f4\u3002\u4f7f\u7528 default \u5c06\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406\u3002 \u60a8\u53ef\u4ee5\u8bbe\u7f6e event.knative.dev/broker.class \u6ce8\u91ca\u6765\u66f4\u6539\u4ee3\u7406\u7684\u7c7b\u3002 \u9ed8\u8ba4\u7684\u4ee3\u7406\u7c7b\u662f MTChannelBasedBroker \uff0c\u4f46 Knative \u4e5f\u652f\u6301\u4f7f\u7528 Kafka \u548c RabbitMQBroker \u4ee3\u7406\u7c7b\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Apache Kafka Broker \u6216 RabbitMQ Broker \u6587\u6863\u3002 spec.config \u7528\u4e8e\u4e3a\u57fa\u4e8e MT \u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\u6307\u5b9a\u9ed8\u8ba4\u7684\u540e\u5907\u901a\u9053\u914d\u7f6e\u3002 \u6709\u5173\u914d\u7f6e\u9ed8\u8ba4\u901a\u9053\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u6709\u5173 \u914d\u7f6e\u4ee3\u7406\u9ed8\u8ba4\u503c \u7684\u6587\u6863. spec.delivery \u7528\u4e8e\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u9009\u9879\u3002 \u4e8b\u4ef6\u4f20\u9012\u9009\u9879\u6307\u5b9a\u672a\u80fd\u4f20\u9012\u5230\u4e8b\u4ef6\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u4e8b\u4ef6\u4f20\u9012 \u7684\u6587\u6863\u3002","title":"\u4ee3\u7406\u914d\u7f6e\u793a\u4f8b"},{"location":"eventing/brokers/create-mtbroker/","text":"\u521b\u5efa\u4ee3\u7406 \u00b6 \u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u521b\u5efa\u9ed8\u8ba4\u63d0\u4f9b\u7684\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u4f8b\u3002 \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406\u7684\u9ed8\u8ba4\u540e\u5907\u901a\u9053\u7c7b\u578b\u662fInMemoryChannel\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 kn CLI\u6216\u4f7f\u7528 kubectl \u5e94\u7528YAML\u6587\u4ef6\u6765\u521b\u5efa\u4ee3\u7406\u3002 kn kubectl \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: kn broker create <broker-name> -n <namespace> Note \u5982\u679c\u9009\u62e9\u4e0d\u6307\u5b9a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u4ee3\u7406\u5c06\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u53ef\u9009:\u9a8c\u8bc1\u662f\u5426\u901a\u8fc7\u5217\u51fa\u73b0\u6709\u7684\u4ee3\u7406\u521b\u5efa\u4e86\u4ee3\u7406\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker list \u53ef\u9009:\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u60a8\u521b\u5efa\u7684\u4ee3\u7406\u6765\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5b58\u5728\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker describe <broker-name> \u4e0b\u9762\u793a\u4f8b\u4e2d\u7684YAML\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efaYAML\u6587\u4ef6\uff0c\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u53ef\u9009:\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u6b63\u5e38\u5de5\u4f5c: kubectl -n <namespace> get broker <broker-name> \u8fd9\u5c06\u663e\u793a\u6709\u5173\u60a8\u7684\u4ee3\u7406\u7684\u4fe1\u606f\u3002\u5982\u679c\u4ee3\u7406\u6b63\u5e38\u5de5\u4f5c\uff0c\u5b83\u4f1a\u663e\u793a True \u7684 READY \u72b6\u6001: NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m \u5982\u679c READY \u72b6\u6001\u4e3a False \uff0c\u7b49\u5f85\u51e0\u5206\u949f\uff0c\u7136\u540e\u518d\u6b21\u8fd0\u884c\u547d\u4ee4\u3002","title":"\u521b\u5efa\u4ee3\u7406\u5668"},{"location":"eventing/brokers/create-mtbroker/#_1","text":"\u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u521b\u5efa\u9ed8\u8ba4\u63d0\u4f9b\u7684\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u4f8b\u3002 \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406\u7684\u9ed8\u8ba4\u540e\u5907\u901a\u9053\u7c7b\u578b\u662fInMemoryChannel\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 kn CLI\u6216\u4f7f\u7528 kubectl \u5e94\u7528YAML\u6587\u4ef6\u6765\u521b\u5efa\u4ee3\u7406\u3002 kn kubectl \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: kn broker create <broker-name> -n <namespace> Note \u5982\u679c\u9009\u62e9\u4e0d\u6307\u5b9a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u4ee3\u7406\u5c06\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u53ef\u9009:\u9a8c\u8bc1\u662f\u5426\u901a\u8fc7\u5217\u51fa\u73b0\u6709\u7684\u4ee3\u7406\u521b\u5efa\u4e86\u4ee3\u7406\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker list \u53ef\u9009:\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u60a8\u521b\u5efa\u7684\u4ee3\u7406\u6765\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5b58\u5728\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker describe <broker-name> \u4e0b\u9762\u793a\u4f8b\u4e2d\u7684YAML\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efaYAML\u6587\u4ef6\uff0c\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u53ef\u9009:\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u6b63\u5e38\u5de5\u4f5c: kubectl -n <namespace> get broker <broker-name> \u8fd9\u5c06\u663e\u793a\u6709\u5173\u60a8\u7684\u4ee3\u7406\u7684\u4fe1\u606f\u3002\u5982\u679c\u4ee3\u7406\u6b63\u5e38\u5de5\u4f5c\uff0c\u5b83\u4f1a\u663e\u793a True \u7684 READY \u72b6\u6001: NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m \u5982\u679c READY \u72b6\u6001\u4e3a False \uff0c\u7b49\u5f85\u51e0\u5206\u949f\uff0c\u7136\u540e\u518d\u6b21\u8fd0\u884c\u547d\u4ee4\u3002","title":"\u521b\u5efa\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/","text":"\u53ef\u7528\u7684\u4ee3\u7406\u7c7b\u578b \u00b6 \u4ee5\u4e0b\u4ee3\u7406\u7c7b\u578b\u53ef\u4e0eKnative\u4e8b\u4ef6\u4e00\u8d77\u4f7f\u7528\u3002 \u57fa\u4e8e\u591a\u79df\u6237\u901a\u9053\u7684\u4ee3\u7406 \u00b6 Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u4f7f\u7528\u901a\u9053\u8fdb\u884c\u4e8b\u4ef6\u8def\u7531\u3002 \u5728\u4f7f\u7528MT\u57fa\u4e8e\u901a\u9053\u7684\u4ee3\u7406\u4e4b\u524d\uff0c\u5fc5\u987b\u5b89\u88c5\u901a\u9053\u5b9e\u73b0\u3002 \u53ef\u9009\u7684\u4ee3\u7406\u5b9e\u73b0 \u00b6 \u5728Knative\u4e8b\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u53ea\u8981\u9075\u5b88 \u4ee3\u7406\u89c4\u8303 \uff0c\u5176\u4ed6\u4ee3\u7406\u5b9e\u73b0\u90fd\u662f\u53d7\u6b22\u8fce\u7684\u3002 \u4ee5\u4e0b\u662f\u793e\u533a\u6216\u4f9b\u5e94\u5546\u63d0\u4f9b\u7684\u4ee3\u7406\u5217\u8868: Apache Kafka\u4ee3\u7406 \u00b6 \u66f4\u591a\u4fe1\u606f\uff0c\u53c2\u89c1 Apache Kafka Broker . RabbitMQ \u4ee3\u7406 \u00b6 RabbitMQ\u4ee3\u7406\u4f7f\u7528 RabbitMQ \u4f5c\u4e3a\u5e95\u5c42\u5b9e\u73b0\u3002 \u8981\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u4ee3\u7406 \u6216 GitHub\u4e0a\u53ef\u7528\u7684\u6587\u6863 .","title":"\u4ee3\u7406\u7c7b\u578b"},{"location":"eventing/brokers/broker-types/#_1","text":"\u4ee5\u4e0b\u4ee3\u7406\u7c7b\u578b\u53ef\u4e0eKnative\u4e8b\u4ef6\u4e00\u8d77\u4f7f\u7528\u3002","title":"\u53ef\u7528\u7684\u4ee3\u7406\u7c7b\u578b"},{"location":"eventing/brokers/broker-types/#_2","text":"Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u4f7f\u7528\u901a\u9053\u8fdb\u884c\u4e8b\u4ef6\u8def\u7531\u3002 \u5728\u4f7f\u7528MT\u57fa\u4e8e\u901a\u9053\u7684\u4ee3\u7406\u4e4b\u524d\uff0c\u5fc5\u987b\u5b89\u88c5\u901a\u9053\u5b9e\u73b0\u3002","title":"\u57fa\u4e8e\u591a\u79df\u6237\u901a\u9053\u7684\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/#_3","text":"\u5728Knative\u4e8b\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u53ea\u8981\u9075\u5b88 \u4ee3\u7406\u89c4\u8303 \uff0c\u5176\u4ed6\u4ee3\u7406\u5b9e\u73b0\u90fd\u662f\u53d7\u6b22\u8fce\u7684\u3002 \u4ee5\u4e0b\u662f\u793e\u533a\u6216\u4f9b\u5e94\u5546\u63d0\u4f9b\u7684\u4ee3\u7406\u5217\u8868:","title":"\u53ef\u9009\u7684\u4ee3\u7406\u5b9e\u73b0"},{"location":"eventing/brokers/broker-types/#apache-kafka","text":"\u66f4\u591a\u4fe1\u606f\uff0c\u53c2\u89c1 Apache Kafka Broker .","title":"Apache Kafka\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/#rabbitmq","text":"RabbitMQ\u4ee3\u7406\u4f7f\u7528 RabbitMQ \u4f5c\u4e3a\u5e95\u5c42\u5b9e\u73b0\u3002 \u8981\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u4ee3\u7406 \u6216 GitHub\u4e0a\u53ef\u7528\u7684\u6587\u6863 .","title":"RabbitMQ \u4ee3\u7406"},{"location":"eventing/brokers/broker-types/kafka-broker/","text":"Knative Kafka Broker \u00b6 The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix The Knative Kafka Broker stores incoming CloudEvents as Kafka records, using the binary content mode . This means all CloudEvent attributes and extensions are mapped as headers on the Kafka record , while the data of the CloudEvent corresponds to the value of the Kafka record. Prerequisites \u00b6 You have installed Knative Eventing. You have access to an Apache Kafka cluster. Tip If you need to set up a Kafka cluster, you can do this by following the instructions on the Strimzi Quickstart page . Installation \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s Create a Kafka Broker \u00b6 A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka # Optional annotation to point to an externally managed kafka topic: # kafka.eventing.knative.dev/external.topic: <topic-name> name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service Configure a Kafka Broker \u00b6 The spec.config should reference any ConfigMap in any namespace that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"3\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the Knative Eventing SYSTEM_NAMESPACE in the cluster. You can edit the global configuration depending on your needs. You can also override these settings on a per broker base, by referencing a different ConfigMap on a different namespace or with a different name on your Kafka Broker's spec.config field. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 . Set as default broker implementation \u00b6 To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set. Bring your own topic \u00b6 By default the Knative Kafka Broker creates its own internal topic, however it is possible to point to an externally managed topic, using the kafka.eventing.knative.dev/external.topic annotation: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka kafka.eventing.knative.dev/external.topic : <my-topic-name> name : default namespace : default spec : # other spec fields ... Note When using an external topic, the Knative Kafka Broker does not own the topic and is not responsible for managing the topic. This includes the topic lifecycle or its general validity. Other restrictions for general access to the topic may apply. See the documentation about using Access Control Lists (ACLs) . Consumer Offsets Commit Interval \u00b6 Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset. Kafka Producer and Consumer configurations \u00b6 Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations . Enable debug logging for data plane components \u00b6 The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher Configuring the order of delivered events \u00b6 When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee. Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Kafka\u4ee3\u7406\u5668"},{"location":"eventing/brokers/broker-types/kafka-broker/#knative-kafka-broker","text":"The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix The Knative Kafka Broker stores incoming CloudEvents as Kafka records, using the binary content mode . This means all CloudEvent attributes and extensions are mapped as headers on the Kafka record , while the data of the CloudEvent corresponds to the value of the Kafka record.","title":"Knative Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#prerequisites","text":"You have installed Knative Eventing. You have access to an Apache Kafka cluster. Tip If you need to set up a Kafka cluster, you can do this by following the instructions on the Strimzi Quickstart page .","title":"Prerequisites"},{"location":"eventing/brokers/broker-types/kafka-broker/#installation","text":"Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/brokers/broker-types/kafka-broker/#create-a-kafka-broker","text":"A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka # Optional annotation to point to an externally managed kafka topic: # kafka.eventing.knative.dev/external.topic: <topic-name> name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service","title":"Create a Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#configure-a-kafka-broker","text":"The spec.config should reference any ConfigMap in any namespace that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"3\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the Knative Eventing SYSTEM_NAMESPACE in the cluster. You can edit the global configuration depending on your needs. You can also override these settings on a per broker base, by referencing a different ConfigMap on a different namespace or with a different name on your Kafka Broker's spec.config field. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 .","title":"Configure a Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#set-as-default-broker-implementation","text":"To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing","title":"Set as default broker implementation"},{"location":"eventing/brokers/broker-types/kafka-broker/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/brokers/broker-types/kafka-broker/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/brokers/broker-types/kafka-broker/#bring-your-own-topic","text":"By default the Knative Kafka Broker creates its own internal topic, however it is possible to point to an externally managed topic, using the kafka.eventing.knative.dev/external.topic annotation: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka kafka.eventing.knative.dev/external.topic : <my-topic-name> name : default namespace : default spec : # other spec fields ... Note When using an external topic, the Knative Kafka Broker does not own the topic and is not responsible for managing the topic. This includes the topic lifecycle or its general validity. Other restrictions for general access to the topic may apply. See the documentation about using Access Control Lists (ACLs) .","title":"Bring your own topic"},{"location":"eventing/brokers/broker-types/kafka-broker/#consumer-offsets-commit-interval","text":"Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset.","title":"Consumer Offsets Commit Interval"},{"location":"eventing/brokers/broker-types/kafka-broker/#kafka-producer-and-consumer-configurations","text":"Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations .","title":"Kafka Producer and Consumer configurations"},{"location":"eventing/brokers/broker-types/kafka-broker/#enable-debug-logging-for-data-plane-components","text":"The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher","title":"Enable debug logging for data plane components"},{"location":"eventing/brokers/broker-types/kafka-broker/#configuring-the-order-of-delivered-events","text":"When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee.","title":"Configuring the order of delivered events"},{"location":"eventing/brokers/broker-types/kafka-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/","text":"\u521b\u5efa RabbitMQ Broker \u00b6 \u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa RabbitMQ Broker\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 knative \u4e8b\u4ef6 \u5df2\u5b89\u88c5 CertManager v1.5.4 - \u4e0e RabbitMQ \u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u7b26\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5 RabbitMQ \u6d88\u606f\u4f20\u9012\u62d3\u6251\u64cd\u4f5c\u7b26 - \u6211\u4eec\u7684\u5efa\u8bae\u662f\u4f7f\u7528 CertManager \u7684 \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u8bbf\u95ee\u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684 RabbitMQ \u5b9e\u4f8b\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 RabbitMQ \u96c6\u7fa4 Kubernetes \u64cd\u4f5c\u7b26 \u6765\u521b\u5efa\u4e00\u4e2a RabbitMQ \u5b9e\u4f8b\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 RabbitMQ \u7f51\u7ad9 . \u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668 \u00b6 \u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml \u9a8c\u8bc1 rabbitmq-broker-controller \u548c rabbitmq-broker-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s \u521b\u5efa\u4e00\u4e2a RabbitMQBrokerConfig \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1alpha1 kind : RabbitmqBrokerConfig metadata : name : <rabbitmq-broker-config-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials queueType : quorum \u5728\u54ea\u91cc: <rabbitmq-broker-config-name> \u662f\u4f60\u60f3\u8981\u7684 RabbitMQBrokerConfig \u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4e4b\u524d\u521b\u5efa\u7684 RabbitMQ \u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u4f60\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c \u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eBroker\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c \u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl create -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2a RabbitMQBroker \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <broker-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqBrokerConfig name : <rabbitmq-broker-config-name> \u5176\u4e2d <rabbitmq-broker-config-name> \u662f\u4f60\u5728\u4e0a\u9762\u6b65\u9aa4\u4e2d\u7ed9\u4f60\u7684 RabbitMQBrokerConfig \u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u914d\u7f6e\u6d88\u606f\u6392\u5e8f \u00b6 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u89e6\u53d1\u5668\u6bcf\u6b21\u4f7f\u7528\u4e00\u6761\u6d88\u606f\u4ee5\u4fdd\u6301\u987a\u5e8f\u3002 \u5982\u679c\u4e8b\u4ef6\u7684\u987a\u5e8f\u5e76\u4e0d\u91cd\u8981\uff0c\u5e76\u4e14\u5e0c\u671b\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 parallelism \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5c06 parallelism \u8bbe\u7f6e\u4e3a n \u4e3a\u89e6\u53d1\u5668\u521b\u5efa n \u4e2a worker\uff0c\u8fd9\u4e9b worker \u90fd\u5c06\u5e76\u884c\u4f7f\u7528\u6d88\u606f\u3002 \u4e0b\u9762\u7684 YAML \u663e\u793a\u4e86\u4e00\u4e2a\u5e76\u884c\u5ea6\u8bbe\u7f6e\u4e3a 10 \u7684\u89e6\u53d1\u5668\u793a\u4f8b: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : high-throughput-trigger annotations : rabbitmq.eventing.knative.dev/parallelism : \"10\" \u989d\u5916\u7684\u4fe1\u606f \u00b6 \u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github \u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2a bug \u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github \u5b58\u50a8\u5e93 \u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898.","title":"RabbitMQ\u4ee3\u7406\u5668"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmq-broker","text":"\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa RabbitMQ Broker\u3002","title":"\u521b\u5efa RabbitMQ Broker"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_1","text":"\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 knative \u4e8b\u4ef6 \u5df2\u5b89\u88c5 CertManager v1.5.4 - \u4e0e RabbitMQ \u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u7b26\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5 RabbitMQ \u6d88\u606f\u4f20\u9012\u62d3\u6251\u64cd\u4f5c\u7b26 - \u6211\u4eec\u7684\u5efa\u8bae\u662f\u4f7f\u7528 CertManager \u7684 \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u8bbf\u95ee\u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684 RabbitMQ \u5b9e\u4f8b\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 RabbitMQ \u96c6\u7fa4 Kubernetes \u64cd\u4f5c\u7b26 \u6765\u521b\u5efa\u4e00\u4e2a RabbitMQ \u5b9e\u4f8b\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 RabbitMQ \u7f51\u7ad9 .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmq","text":"\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml \u9a8c\u8bc1 rabbitmq-broker-controller \u548c rabbitmq-broker-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s","title":"\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmqbrokerconfig","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1alpha1 kind : RabbitmqBrokerConfig metadata : name : <rabbitmq-broker-config-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials queueType : quorum \u5728\u54ea\u91cc: <rabbitmq-broker-config-name> \u662f\u4f60\u60f3\u8981\u7684 RabbitMQBrokerConfig \u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4e4b\u524d\u521b\u5efa\u7684 RabbitMQ \u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u4f60\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c \u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eBroker\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c \u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl create -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a RabbitMQBrokerConfig \u5bf9\u8c61"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmqbroker","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <broker-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqBrokerConfig name : <rabbitmq-broker-config-name> \u5176\u4e2d <rabbitmq-broker-config-name> \u662f\u4f60\u5728\u4e0a\u9762\u6b65\u9aa4\u4e2d\u7ed9\u4f60\u7684 RabbitMQBrokerConfig \u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a RabbitMQBroker \u5bf9\u8c61"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_2","text":"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u89e6\u53d1\u5668\u6bcf\u6b21\u4f7f\u7528\u4e00\u6761\u6d88\u606f\u4ee5\u4fdd\u6301\u987a\u5e8f\u3002 \u5982\u679c\u4e8b\u4ef6\u7684\u987a\u5e8f\u5e76\u4e0d\u91cd\u8981\uff0c\u5e76\u4e14\u5e0c\u671b\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 parallelism \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5c06 parallelism \u8bbe\u7f6e\u4e3a n \u4e3a\u89e6\u53d1\u5668\u521b\u5efa n \u4e2a worker\uff0c\u8fd9\u4e9b worker \u90fd\u5c06\u5e76\u884c\u4f7f\u7528\u6d88\u606f\u3002 \u4e0b\u9762\u7684 YAML \u663e\u793a\u4e86\u4e00\u4e2a\u5e76\u884c\u5ea6\u8bbe\u7f6e\u4e3a 10 \u7684\u89e6\u53d1\u5668\u793a\u4f8b: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : high-throughput-trigger annotations : rabbitmq.eventing.knative.dev/parallelism : \"10\"","title":"\u914d\u7f6e\u6d88\u606f\u6392\u5e8f"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_3","text":"\u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github \u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2a bug \u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github \u5b58\u50a8\u5e93 \u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898.","title":"\u989d\u5916\u7684\u4fe1\u606f"},{"location":"eventing/channels/","text":"\u901a\u9053 \u00b6 \u901a\u9053\u662fKubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90 \uff0c\u5b83\u5b9a\u4e49\u4e86\u5355\u4e2a\u4e8b\u4ef6\u8f6c\u53d1\u548c\u6301\u4e45\u5c42\u3002 \u901a\u9053\u63d0\u4f9b\u4e00\u79cd\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7\u8ba2\u9605\u5c06\u63a5\u6536\u5230\u7684\u4e8b\u4ef6\u6247\u5411\u591a\u4e2a\u76ee\u7684\u5730\u6216\u63a5\u6536\u5668\u3002\u63a5\u6536\u5668\u7684\u4f8b\u5b50\u5305\u62ec\u4ee3\u7406\u548cKnative\u670d\u52a1\u3002 \u4e0b\u4e00\u6b65 \u00b6 \u4e86\u89e3 \u9ed8\u8ba4\u53ef\u7528\u901a\u9053\u7c7b\u578b \u521b\u5efa \u901a\u9053 \u521b\u5efa \u8ba2\u9605","title":"\u5173\u4e8e\u901a\u9053"},{"location":"eventing/channels/#_1","text":"\u901a\u9053\u662fKubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90 \uff0c\u5b83\u5b9a\u4e49\u4e86\u5355\u4e2a\u4e8b\u4ef6\u8f6c\u53d1\u548c\u6301\u4e45\u5c42\u3002 \u901a\u9053\u63d0\u4f9b\u4e00\u79cd\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7\u8ba2\u9605\u5c06\u63a5\u6536\u5230\u7684\u4e8b\u4ef6\u6247\u5411\u591a\u4e2a\u76ee\u7684\u5730\u6216\u63a5\u6536\u5668\u3002\u63a5\u6536\u5668\u7684\u4f8b\u5b50\u5305\u62ec\u4ee3\u7406\u548cKnative\u670d\u52a1\u3002","title":"\u901a\u9053"},{"location":"eventing/channels/#_2","text":"\u4e86\u89e3 \u9ed8\u8ba4\u53ef\u7528\u901a\u9053\u7c7b\u578b \u521b\u5efa \u901a\u9053 \u521b\u5efa \u8ba2\u9605","title":"\u4e0b\u4e00\u6b65"},{"location":"eventing/channels/channel-types-defaults/","text":"\u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c \u00b6 Knative\u4f7f\u7528\u4e24\u79cd\u7c7b\u578b\u7684\u901a\u9053: \u4e00\u4e2a\u901a\u7528Channel\u5bf9\u8c61\u3002 \u6bcf\u4e2a\u901a\u9053\u5b9e\u73b0\u90fd\u6709\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\uff0c\u4f8b\u5982InMemoryChannel\u548cKafkaChannel\u3002 KafkaChannel\u652f\u6301\u6709\u5e8f\u7684\u6d88\u8d39\u8005\u4f20\u9012\u4fdd\u8bc1\uff0c\u8fd9\u662f\u4e00\u4e2a\u6bcf\u4e2a\u5206\u533a\u7684\u963b\u585e\u6d88\u8d39\u8005\uff0c\u5b83\u7b49\u5f85\u6765\u81eaCloudEvent\u8ba2\u9605\u8005\u7684\u6210\u529f\u54cd\u5e94\uff0c\u7136\u540e\u518d\u4f20\u9012\u8be5\u5206\u533a\u7684\u4e0b\u4e00\u4e2a\u6d88\u606f\u3002 \u6bcf\u4e2a\u81ea\u5b9a\u4e49\u901a\u9053\u5b9e\u73b0\u90fd\u6709\u81ea\u5df1\u7684\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\uff0c\u6bd4\u5982\u57fa\u4e8e\u5185\u5b58\u6216\u57fa\u4e8e\u4ee3\u7406\u7684\u673a\u5236\u3002 \u4ee3\u7406\u7684\u4f8b\u5b50\u5305\u62ecKafkaBroker\u548cGCP Pub/Sub Broker\u3002 Knative\u9ed8\u8ba4\u63d0\u4f9bInMemoryChannel\u901a\u9053\u5b9e\u73b0\u3002 \u8fd9\u4e2a\u9ed8\u8ba4\u5b9e\u73b0\u5bf9\u4e8e\u4e0d\u5e0c\u671b\u914d\u7f6e\u7279\u5b9a\u5b9e\u73b0\u7c7b\u578b(\u5982Apache Kafka\u6216NATSS Channels)\u7684\u5f00\u53d1\u4eba\u5458\u975e\u5e38\u6709\u7528\u3002 \u5982\u679c\u5e0c\u671b\u521b\u5efa\u901a\u9053\u800c\u4e0d\u6307\u5b9a\u4f7f\u7528\u54ea\u4e2a\u901a\u9053\u5b9e\u73b0CRD\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u901a\u7528\u901a\u9053\u5bf9\u8c61\u3002 \u5982\u679c\u60a8\u4e0d\u5173\u5fc3\u7279\u5b9a\u901a\u9053\u5b9e\u73b0\u63d0\u4f9b\u7684\u5c5e\u6027(\u6bd4\u5982\u6392\u5e8f\u548c\u6301\u4e45\u6027)\uff0c\u5e76\u4e14\u5e0c\u671b\u4f7f\u7528\u96c6\u7fa4\u7ba1\u7406\u5458\u9009\u62e9\u7684\u5b9e\u73b0\uff0c\u90a3\u4e48\u8fd9\u5c06\u975e\u5e38\u6709\u7528\u3002 \u96c6\u7fa4\u7ba1\u7406\u5458\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 default-ch-webhook ConfigMap\u6765\u4fee\u6539\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u8bbe\u7f6e\u3002 \u6709\u5173\u4fee\u6539ConfigMaps\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u4e8b\u4ef6\u64cd\u4f5c\u7b26\u81ea\u5b9a\u4e49\u8d44\u6e90 . \u53ef\u4ee5\u4e3a\u96c6\u7fa4\u3001\u96c6\u7fa4\u4e0a\u7684\u547d\u540d\u7a7a\u95f4\u6216\u4e24\u8005\u914d\u7f6e\u9ed8\u8ba4\u901a\u9053\u3002 Note \u5982\u679c\u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u4e86\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\uff0c\u5219\u8fd9\u5c06\u8986\u76d6\u96c6\u7fa4\u7684\u914d\u7f6e\u3002 \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u96c6\u7fa4\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u662fInMemoryChannel\uff0c\u800c example-namespace \u7684\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u662fKafkaChannel\u3002 apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4e0d\u80fd\u4f7f\u7528\u901a\u9053\u3002 \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u521b\u5efa InMemoryChannel","title":"\u901a\u9053\u7c7b\u578b"},{"location":"eventing/channels/channel-types-defaults/#_1","text":"Knative\u4f7f\u7528\u4e24\u79cd\u7c7b\u578b\u7684\u901a\u9053: \u4e00\u4e2a\u901a\u7528Channel\u5bf9\u8c61\u3002 \u6bcf\u4e2a\u901a\u9053\u5b9e\u73b0\u90fd\u6709\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\uff0c\u4f8b\u5982InMemoryChannel\u548cKafkaChannel\u3002 KafkaChannel\u652f\u6301\u6709\u5e8f\u7684\u6d88\u8d39\u8005\u4f20\u9012\u4fdd\u8bc1\uff0c\u8fd9\u662f\u4e00\u4e2a\u6bcf\u4e2a\u5206\u533a\u7684\u963b\u585e\u6d88\u8d39\u8005\uff0c\u5b83\u7b49\u5f85\u6765\u81eaCloudEvent\u8ba2\u9605\u8005\u7684\u6210\u529f\u54cd\u5e94\uff0c\u7136\u540e\u518d\u4f20\u9012\u8be5\u5206\u533a\u7684\u4e0b\u4e00\u4e2a\u6d88\u606f\u3002 \u6bcf\u4e2a\u81ea\u5b9a\u4e49\u901a\u9053\u5b9e\u73b0\u90fd\u6709\u81ea\u5df1\u7684\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\uff0c\u6bd4\u5982\u57fa\u4e8e\u5185\u5b58\u6216\u57fa\u4e8e\u4ee3\u7406\u7684\u673a\u5236\u3002 \u4ee3\u7406\u7684\u4f8b\u5b50\u5305\u62ecKafkaBroker\u548cGCP Pub/Sub Broker\u3002 Knative\u9ed8\u8ba4\u63d0\u4f9bInMemoryChannel\u901a\u9053\u5b9e\u73b0\u3002 \u8fd9\u4e2a\u9ed8\u8ba4\u5b9e\u73b0\u5bf9\u4e8e\u4e0d\u5e0c\u671b\u914d\u7f6e\u7279\u5b9a\u5b9e\u73b0\u7c7b\u578b(\u5982Apache Kafka\u6216NATSS Channels)\u7684\u5f00\u53d1\u4eba\u5458\u975e\u5e38\u6709\u7528\u3002 \u5982\u679c\u5e0c\u671b\u521b\u5efa\u901a\u9053\u800c\u4e0d\u6307\u5b9a\u4f7f\u7528\u54ea\u4e2a\u901a\u9053\u5b9e\u73b0CRD\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u901a\u7528\u901a\u9053\u5bf9\u8c61\u3002 \u5982\u679c\u60a8\u4e0d\u5173\u5fc3\u7279\u5b9a\u901a\u9053\u5b9e\u73b0\u63d0\u4f9b\u7684\u5c5e\u6027(\u6bd4\u5982\u6392\u5e8f\u548c\u6301\u4e45\u6027)\uff0c\u5e76\u4e14\u5e0c\u671b\u4f7f\u7528\u96c6\u7fa4\u7ba1\u7406\u5458\u9009\u62e9\u7684\u5b9e\u73b0\uff0c\u90a3\u4e48\u8fd9\u5c06\u975e\u5e38\u6709\u7528\u3002 \u96c6\u7fa4\u7ba1\u7406\u5458\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 default-ch-webhook ConfigMap\u6765\u4fee\u6539\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u8bbe\u7f6e\u3002 \u6709\u5173\u4fee\u6539ConfigMaps\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u4e8b\u4ef6\u64cd\u4f5c\u7b26\u81ea\u5b9a\u4e49\u8d44\u6e90 . \u53ef\u4ee5\u4e3a\u96c6\u7fa4\u3001\u96c6\u7fa4\u4e0a\u7684\u547d\u540d\u7a7a\u95f4\u6216\u4e24\u8005\u914d\u7f6e\u9ed8\u8ba4\u901a\u9053\u3002 Note \u5982\u679c\u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u4e86\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\uff0c\u5219\u8fd9\u5c06\u8986\u76d6\u96c6\u7fa4\u7684\u914d\u7f6e\u3002 \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u96c6\u7fa4\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u662fInMemoryChannel\uff0c\u800c example-namespace \u7684\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u662fKafkaChannel\u3002 apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4e0d\u80fd\u4f7f\u7528\u901a\u9053\u3002","title":"\u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c"},{"location":"eventing/channels/channel-types-defaults/#_2","text":"\u521b\u5efa InMemoryChannel","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"eventing/channels/channels-crds/","text":"\u8fd9\u662fKnative\u4e8b\u4ef6\u53ef\u7528\u901a\u9053\u7684\u4e00\u4e2a\u4e0d\u8be6\u5c3d\u7684\u5217\u8868\u3002 Note \u5217\u5165\u8fd9\u4e2a\u540d\u5355\u5e76\u4e0d\u4ee3\u8868\u80cc\u4e66\uff0c\u4e5f\u4e0d\u610f\u5473\u7740\u4efb\u4f55\u7a0b\u5ea6\u7684\u652f\u6301\u3002 \u540d\u5b57 \u72b6\u6001 \u7ef4\u62a4\u4eba\u5458 \u63cf\u8ff0 InMemoryChannel Stable Knative \u5185\u5b58\u901a\u9053\u662f\u4e00\u79cd\u6700\u597d\u7684\u901a\u9053\u3002\u5b83\u4eec\u4e0d\u5e94\u8be5\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u3002\u5b83\u4eec\u5bf9\u5f00\u53d1\u662f\u6709\u7528\u7684\u3002 KafkaChannel Beta Knative \u901a\u9053\u7531 Apache Kafka \u4e3b\u9898\u652f\u6301\u3002 NatssChannel Alpha Knative \u9891\u9053\u7531 NATS\u6d41\u5a92\u4f53 \u652f\u6301.","title":"\u53ef\u7528\u901a\u9053"},{"location":"eventing/channels/create-default-channel/","text":"\u4f7f\u7528\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u503c\u521b\u5efa\u901a\u9053 \u00b6 \u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u901a\u9053\u5bf9\u8c61\u7684\u5b9e\u4f8b\u6765\u521b\u5efa\u4efb\u4f55\u652f\u6301\u7684\u5b9e\u73b0\u7c7b\u578b\u7684\u901a\u9053\u3002 \u521b\u5efa\u4e00\u4e2a\u901a\u9053: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a \u901a\u9053\u5bf9\u8c61 \u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u901a\u9053\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u76ee\u6807\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5982\u679c\u60a8\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u8be5\u5bf9\u8c61\uff0c\u6839\u636e \u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c \u4e2d\u7684\u9ed8\u8ba4ConfigMap\u793a\u4f8b\uff0c\u5b83\u662f\u4e00\u4e2aInMemoryChannel\u901a\u9053\u5b9e\u73b0\u3002 \u901a\u9053\u5bf9\u8c61\u521b\u5efa\u540e\uff0c\u4e00\u4e2a\u7a81\u53d8\u7684\u51c6\u5165webhook\u6839\u636e\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u8bbe\u7f6e spec.channelTemplate : apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u901a\u9053\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u76ee\u6807\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <channel-template-kind> \u5b83\u662f\u4e00\u79cd\u57fa\u4e8e\u9ed8\u8ba4ConfigMap\u7684\u901a\u9053\uff0c\u4f8b\u5982InMemoryChannel\u6216KafkaChannel\u3002\u53c2\u89c1 \u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c \u4e2d\u7684\u793a\u4f8b. Note spec.channelTemplate \u5c5e\u6027\u5728\u521b\u5efa\u540e\u4e0d\u80fd\u66f4\u6539\uff0c\u56e0\u4e3a\u5b83\u662f\u7531\u9ed8\u8ba4\u901a\u9053\u673a\u5236\u800c\u4e0d\u662f\u7528\u6237\u8bbe\u7f6e\u7684\u3002 \u901a\u9053\u63a7\u5236\u5668\u57fa\u4e8e spec.channelTemplate \u521b\u5efa\u4e00\u4e2a\u540e\u5907\u901a\u9053\u5b9e\u4f8b\u3002 \u5f53\u4f7f\u7528\u8fd9\u4e2a\u673a\u5236\u65f6\uff0c\u4f1a\u521b\u5efa\u4e24\u4e2a\u5bf9\u8c61;\u4e00\u4e2a\u901a\u7528\u901a\u9053\u5bf9\u8c61\u548c\u4e00\u4e2aInMemoryChannel\u5bf9\u8c61\u3002 \u901a\u8fc7\u5c06InMemoryChannel\u5bf9\u8c61\u7684\u8ba2\u9605\u590d\u5236\u5230InMemoryChannel\u5bf9\u8c61\uff0c\u5e76\u5c06\u5176\u72b6\u6001\u8bbe\u7f6e\u4e3aInMemoryChannel\u5bf9\u8c61\u7684\u8ba2\u9605\uff0c\u6cdb\u578b\u5bf9\u8c61\u5145\u5f53InMemoryChannel\u5bf9\u8c61\u7684\u4ee3\u7406\u3002 Note \u9ed8\u8ba4\u503c\u53ea\u5728\u6700\u521d\u521b\u5efa\u901a\u9053\u6216\u5e8f\u5217\u65f6\u7531webhook\u5e94\u7528\u3002 \u5982\u679c\u66f4\u6539\u4e86\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u5219\u65b0\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u5c06\u53ea\u5e94\u7528\u4e8e\u65b0\u521b\u5efa\u7684\u901a\u9053\u3001\u4ee3\u7406\u6216\u5e8f\u5217\u3002 \u73b0\u6709\u8d44\u6e90\u4e0d\u4f1a\u81ea\u52a8\u66f4\u65b0\u4ee5\u4f7f\u7528\u65b0\u914d\u7f6e\u3002","title":"\u521b\u5efa\u9ed8\u8ba4\u901a\u9053"},{"location":"eventing/channels/create-default-channel/#_1","text":"\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u901a\u9053\u5bf9\u8c61\u7684\u5b9e\u4f8b\u6765\u521b\u5efa\u4efb\u4f55\u652f\u6301\u7684\u5b9e\u73b0\u7c7b\u578b\u7684\u901a\u9053\u3002 \u521b\u5efa\u4e00\u4e2a\u901a\u9053: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a \u901a\u9053\u5bf9\u8c61 \u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u901a\u9053\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u76ee\u6807\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5982\u679c\u60a8\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u8be5\u5bf9\u8c61\uff0c\u6839\u636e \u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c \u4e2d\u7684\u9ed8\u8ba4ConfigMap\u793a\u4f8b\uff0c\u5b83\u662f\u4e00\u4e2aInMemoryChannel\u901a\u9053\u5b9e\u73b0\u3002 \u901a\u9053\u5bf9\u8c61\u521b\u5efa\u540e\uff0c\u4e00\u4e2a\u7a81\u53d8\u7684\u51c6\u5165webhook\u6839\u636e\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0\u8bbe\u7f6e spec.channelTemplate : apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u901a\u9053\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u76ee\u6807\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <channel-template-kind> \u5b83\u662f\u4e00\u79cd\u57fa\u4e8e\u9ed8\u8ba4ConfigMap\u7684\u901a\u9053\uff0c\u4f8b\u5982InMemoryChannel\u6216KafkaChannel\u3002\u53c2\u89c1 \u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c \u4e2d\u7684\u793a\u4f8b. Note spec.channelTemplate \u5c5e\u6027\u5728\u521b\u5efa\u540e\u4e0d\u80fd\u66f4\u6539\uff0c\u56e0\u4e3a\u5b83\u662f\u7531\u9ed8\u8ba4\u901a\u9053\u673a\u5236\u800c\u4e0d\u662f\u7528\u6237\u8bbe\u7f6e\u7684\u3002 \u901a\u9053\u63a7\u5236\u5668\u57fa\u4e8e spec.channelTemplate \u521b\u5efa\u4e00\u4e2a\u540e\u5907\u901a\u9053\u5b9e\u4f8b\u3002 \u5f53\u4f7f\u7528\u8fd9\u4e2a\u673a\u5236\u65f6\uff0c\u4f1a\u521b\u5efa\u4e24\u4e2a\u5bf9\u8c61;\u4e00\u4e2a\u901a\u7528\u901a\u9053\u5bf9\u8c61\u548c\u4e00\u4e2aInMemoryChannel\u5bf9\u8c61\u3002 \u901a\u8fc7\u5c06InMemoryChannel\u5bf9\u8c61\u7684\u8ba2\u9605\u590d\u5236\u5230InMemoryChannel\u5bf9\u8c61\uff0c\u5e76\u5c06\u5176\u72b6\u6001\u8bbe\u7f6e\u4e3aInMemoryChannel\u5bf9\u8c61\u7684\u8ba2\u9605\uff0c\u6cdb\u578b\u5bf9\u8c61\u5145\u5f53InMemoryChannel\u5bf9\u8c61\u7684\u4ee3\u7406\u3002 Note \u9ed8\u8ba4\u503c\u53ea\u5728\u6700\u521d\u521b\u5efa\u901a\u9053\u6216\u5e8f\u5217\u65f6\u7531webhook\u5e94\u7528\u3002 \u5982\u679c\u66f4\u6539\u4e86\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u5219\u65b0\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u5c06\u53ea\u5e94\u7528\u4e8e\u65b0\u521b\u5efa\u7684\u901a\u9053\u3001\u4ee3\u7406\u6216\u5e8f\u5217\u3002 \u73b0\u6709\u8d44\u6e90\u4e0d\u4f1a\u81ea\u52a8\u66f4\u65b0\u4ee5\u4f7f\u7528\u65b0\u914d\u7f6e\u3002","title":"\u4f7f\u7528\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u503c\u521b\u5efa\u901a\u9053"},{"location":"eventing/channels/subscriptions/","text":"\u8ba2\u9605 \u00b6 \u5728\u521b\u5efa\u901a\u9053\u548c\u63a5\u6536\u5668\u4e4b\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u8ba2\u9605\u4ee5\u542f\u7528\u4e8b\u4ef6\u4f20\u9012\u3002 \u8ba2\u9605\u7531\u4e00\u4e2a\u8ba2\u9605\u5bf9\u8c61\u7ec4\u6210\uff0c\u8be5\u5bf9\u8c61\u6307\u5b9a\u8981\u5411\u5176\u4f20\u9012\u4e8b\u4ef6\u7684\u901a\u9053\u548c\u63a5\u6536\u5668(\u4e5f\u79f0\u4e3a\u8ba2\u9605\u670d\u52a1\u5668)\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e9b\u7279\u5b9a\u4e8e\u63a5\u6536\u5668\u7684\u9009\u9879\uff0c\u4f8b\u5982\u5982\u4f55\u5904\u7406\u5931\u8d25\u3002 \u6709\u5173\u8ba2\u9605\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u8ba2\u9605 . \u521b\u5efa\u8ba2\u9605 \u00b6 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u901a\u9053\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u521b\u5efa\u8ba2\u9605: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel \u6307\u5b9a\u5e94\u8be5\u5904\u7406\u7684\u4e91\u4e8b\u4ef6\u7684\u6e90\u3002 \u60a8\u5fc5\u987b\u63d0\u4f9b\u901a\u9053\u540d\u79f0\u3002 \u5982\u679c\u60a8\u4e0d\u4f7f\u7528\u7531\u901a\u9053\u8d44\u6e90\u652f\u6301\u7684\u9ed8\u8ba4\u901a\u9053\uff0c\u5219\u5fc5\u987b\u4e3a\u6307\u5b9a\u7684\u901a\u9053\u7c7b\u578b\u5728\u901a\u9053\u540d\u79f0\u524d\u9762\u52a0\u4e0a <Group:Version:Kind> \u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8ekafka\u652f\u6301\u7684\u901a\u9053\uff0c\u8fd9\u662f messaging.knative.dev:v1beta1:KafkaChannel \u3002 --sink \u6307\u5b9a\u5e94\u5c06\u4e8b\u4ef6\u4f20\u9012\u5230\u7684\u76ee\u6807\u76ee\u7684\u5730\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c <sink-name> \u88ab\u89e3\u91ca\u4e3a\u8be5\u540d\u79f0\u7684Knative\u670d\u52a1\uff0c\u4f4d\u4e8e\u4e0e\u8ba2\u9605\u76f8\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u524d\u7f00\u4e4b\u4e00\u6307\u5b9a\u63a5\u6536\u5668\u7684\u7c7b\u578b: ksvc : Knative\u670d\u52a1\u3002 svc : Kubernetes\u670d\u52a1\u3002 channel : \u5e94\u8be5\u7528\u4f5c\u76ee\u7684\u5730\u7684\u901a\u9053\u3002\u6b64\u5904\u53ea\u80fd\u5f15\u7528\u9ed8\u8ba4\u7684\u901a\u9053\u7c7b\u578b\u3002 broker : \u4e8b\u4ef6\u4ee3\u7406\u3002 --sink-reply \u5b83\u662f\u4e00\u4e2a\u53ef\u9009\u53c2\u6570\uff0c\u53ef\u7528\u4e8e\u6307\u5b9a\u53d1\u9001\u63a5\u6536\u5668\u5e94\u7b54\u7684\u4f4d\u7f6e\u3002 \u5b83\u4f7f\u7528\u4e0e --sink \u6807\u5fd7\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u6765\u6307\u5b9a\u63a5\u6536\u5668\u3002 --sink-dead-letter \u5b83\u662f\u4e00\u4e2a\u53ef\u9009\u53c2\u6570\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6307\u5b9a\u5411\u4f55\u5904\u53d1\u9001 CloudEvent\uff0c\u4ee5\u9632\u51fa\u73b0\u6545\u969c\u3002 \u5b83\u4f7f\u7528\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u5c06\u63a5\u6536\u5668\u6307\u5b9a\u4e3a --sink \u6807\u5fd7\u3002 ksvc : Knative\u670d\u52a1\u3002 svc : Kubernetes\u670d\u52a1\u3002 channel : \u5e94\u8be5\u7528\u4f5c\u76ee\u7684\u5730\u7684\u901a\u9053\u3002\u8fd9\u91cc\u53ea\u80fd\u5f15\u7528\u9ed8\u8ba4\u901a\u9053\u7c7b\u578b\u3002 broker : \u4e8b\u4ef6\u4ee3\u7406 --sink-reply \u548c --sink-dead-letter \u662f\u53ef\u9009\u53c2\u6570. \u5b83\u4eec\u53ef\u4ee5\u5206\u522b\u7528\u4e8e\u6307\u5b9a\u53d1\u9001Sink\u56de\u590d\u7684\u4f4d\u7f6e\uff0c\u4ee5\u53ca\u5728\u53d1\u751f\u6545\u969c\u65f6\u53d1\u9001CloudEvent\u7684\u4f4d\u7f6e\u3002 \u4e24\u8005\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u5c06Sink\u6307\u5b9a\u4e3a --sink \u6807\u5fd7\u3002 \u8fd9\u4e2a\u793a\u4f8b\u547d\u4ee4\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a mysubscription \u7684\u8ba2\u9605\uff0c\u5b83\u5c06\u4e8b\u4ef6\u4ece\u540d\u4e3a mychannel \u7684\u901a\u9053\u8def\u7531\u5230\u540d\u4e3a myservice \u7684Knative\u670d\u52a1\u3002 Note Sink\u524d\u7f00\u662f\u53ef\u9009\u7684\u3002\u4f60\u4e5f\u53ef\u4ee5\u5c06 --sink \u7684\u670d\u52a1\u6307\u5b9a\u4e3a --sink <service-name> \uff0c\u5e76\u7701\u7565 ksvc \u524d\u7f00\u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u4e3a\u8ba2\u9605\u5bf9\u8c61\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Name of the Channel that the Subscription connects to. delivery : # Optional delivery configuration settings for events. deadLetterSink : # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> reply : # Optional configuration settings for the reply event. # This is the event Sink that events replied from the subscriber are delivered to. ref : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : <service-name> subscriber : # Required configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5217\u51fa\u8ba2\u9605 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 kn CLI\u5de5\u5177\u5217\u51fa\u6240\u6709\u73b0\u6709\u7684\u8ba2\u9605\u3002 \u5217\u51fa\u6240\u6709\u8ba2\u9605: kn subscription list YAML\u683c\u5f0f\u7684\u8ba2\u9605\u5217\u8868: kn subscription list -o yaml \u63cf\u8ff0\u8ba2\u9605 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 kn CLI\u5de5\u5177\u6253\u5370\u8ba2\u9605\u7684\u8be6\u7ec6\u4fe1\u606f: kn subscription describe <subscription-name> \u5220\u9664\u8ba2\u9605 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 kn or kubectl CLI\u5de5\u5177\u5220\u9664\u8ba2\u9605\u3002 kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name> \u4e0b\u4e00\u90e8 \u00b6 \u4f7f\u7528\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u503c\u521b\u5efa\u901a\u9053","title":"\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_1","text":"\u5728\u521b\u5efa\u901a\u9053\u548c\u63a5\u6536\u5668\u4e4b\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u8ba2\u9605\u4ee5\u542f\u7528\u4e8b\u4ef6\u4f20\u9012\u3002 \u8ba2\u9605\u7531\u4e00\u4e2a\u8ba2\u9605\u5bf9\u8c61\u7ec4\u6210\uff0c\u8be5\u5bf9\u8c61\u6307\u5b9a\u8981\u5411\u5176\u4f20\u9012\u4e8b\u4ef6\u7684\u901a\u9053\u548c\u63a5\u6536\u5668(\u4e5f\u79f0\u4e3a\u8ba2\u9605\u670d\u52a1\u5668)\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e9b\u7279\u5b9a\u4e8e\u63a5\u6536\u5668\u7684\u9009\u9879\uff0c\u4f8b\u5982\u5982\u4f55\u5904\u7406\u5931\u8d25\u3002 \u6709\u5173\u8ba2\u9605\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u8ba2\u9605 .","title":"\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_2","text":"kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u901a\u9053\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u521b\u5efa\u8ba2\u9605: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel \u6307\u5b9a\u5e94\u8be5\u5904\u7406\u7684\u4e91\u4e8b\u4ef6\u7684\u6e90\u3002 \u60a8\u5fc5\u987b\u63d0\u4f9b\u901a\u9053\u540d\u79f0\u3002 \u5982\u679c\u60a8\u4e0d\u4f7f\u7528\u7531\u901a\u9053\u8d44\u6e90\u652f\u6301\u7684\u9ed8\u8ba4\u901a\u9053\uff0c\u5219\u5fc5\u987b\u4e3a\u6307\u5b9a\u7684\u901a\u9053\u7c7b\u578b\u5728\u901a\u9053\u540d\u79f0\u524d\u9762\u52a0\u4e0a <Group:Version:Kind> \u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8ekafka\u652f\u6301\u7684\u901a\u9053\uff0c\u8fd9\u662f messaging.knative.dev:v1beta1:KafkaChannel \u3002 --sink \u6307\u5b9a\u5e94\u5c06\u4e8b\u4ef6\u4f20\u9012\u5230\u7684\u76ee\u6807\u76ee\u7684\u5730\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c <sink-name> \u88ab\u89e3\u91ca\u4e3a\u8be5\u540d\u79f0\u7684Knative\u670d\u52a1\uff0c\u4f4d\u4e8e\u4e0e\u8ba2\u9605\u76f8\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u524d\u7f00\u4e4b\u4e00\u6307\u5b9a\u63a5\u6536\u5668\u7684\u7c7b\u578b: ksvc : Knative\u670d\u52a1\u3002 svc : Kubernetes\u670d\u52a1\u3002 channel : \u5e94\u8be5\u7528\u4f5c\u76ee\u7684\u5730\u7684\u901a\u9053\u3002\u6b64\u5904\u53ea\u80fd\u5f15\u7528\u9ed8\u8ba4\u7684\u901a\u9053\u7c7b\u578b\u3002 broker : \u4e8b\u4ef6\u4ee3\u7406\u3002 --sink-reply \u5b83\u662f\u4e00\u4e2a\u53ef\u9009\u53c2\u6570\uff0c\u53ef\u7528\u4e8e\u6307\u5b9a\u53d1\u9001\u63a5\u6536\u5668\u5e94\u7b54\u7684\u4f4d\u7f6e\u3002 \u5b83\u4f7f\u7528\u4e0e --sink \u6807\u5fd7\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u6765\u6307\u5b9a\u63a5\u6536\u5668\u3002 --sink-dead-letter \u5b83\u662f\u4e00\u4e2a\u53ef\u9009\u53c2\u6570\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6307\u5b9a\u5411\u4f55\u5904\u53d1\u9001 CloudEvent\uff0c\u4ee5\u9632\u51fa\u73b0\u6545\u969c\u3002 \u5b83\u4f7f\u7528\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u5c06\u63a5\u6536\u5668\u6307\u5b9a\u4e3a --sink \u6807\u5fd7\u3002 ksvc : Knative\u670d\u52a1\u3002 svc : Kubernetes\u670d\u52a1\u3002 channel : \u5e94\u8be5\u7528\u4f5c\u76ee\u7684\u5730\u7684\u901a\u9053\u3002\u8fd9\u91cc\u53ea\u80fd\u5f15\u7528\u9ed8\u8ba4\u901a\u9053\u7c7b\u578b\u3002 broker : \u4e8b\u4ef6\u4ee3\u7406 --sink-reply \u548c --sink-dead-letter \u662f\u53ef\u9009\u53c2\u6570. \u5b83\u4eec\u53ef\u4ee5\u5206\u522b\u7528\u4e8e\u6307\u5b9a\u53d1\u9001Sink\u56de\u590d\u7684\u4f4d\u7f6e\uff0c\u4ee5\u53ca\u5728\u53d1\u751f\u6545\u969c\u65f6\u53d1\u9001CloudEvent\u7684\u4f4d\u7f6e\u3002 \u4e24\u8005\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u547d\u540d\u7ea6\u5b9a\u5c06Sink\u6307\u5b9a\u4e3a --sink \u6807\u5fd7\u3002 \u8fd9\u4e2a\u793a\u4f8b\u547d\u4ee4\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a mysubscription \u7684\u8ba2\u9605\uff0c\u5b83\u5c06\u4e8b\u4ef6\u4ece\u540d\u4e3a mychannel \u7684\u901a\u9053\u8def\u7531\u5230\u540d\u4e3a myservice \u7684Knative\u670d\u52a1\u3002 Note Sink\u524d\u7f00\u662f\u53ef\u9009\u7684\u3002\u4f60\u4e5f\u53ef\u4ee5\u5c06 --sink \u7684\u670d\u52a1\u6307\u5b9a\u4e3a --sink <service-name> \uff0c\u5e76\u7701\u7565 ksvc \u524d\u7f00\u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u4e3a\u8ba2\u9605\u5bf9\u8c61\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Name of the Channel that the Subscription connects to. delivery : # Optional delivery configuration settings for events. deadLetterSink : # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> reply : # Optional configuration settings for the reply event. # This is the event Sink that events replied from the subscriber are delivered to. ref : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : <service-name> subscriber : # Required configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_3","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 kn CLI\u5de5\u5177\u5217\u51fa\u6240\u6709\u73b0\u6709\u7684\u8ba2\u9605\u3002 \u5217\u51fa\u6240\u6709\u8ba2\u9605: kn subscription list YAML\u683c\u5f0f\u7684\u8ba2\u9605\u5217\u8868: kn subscription list -o yaml","title":"\u5217\u51fa\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_4","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 kn CLI\u5de5\u5177\u6253\u5370\u8ba2\u9605\u7684\u8be6\u7ec6\u4fe1\u606f: kn subscription describe <subscription-name>","title":"\u63cf\u8ff0\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_5","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 kn or kubectl CLI\u5de5\u5177\u5220\u9664\u8ba2\u9605\u3002 kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name>","title":"\u5220\u9664\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#_6","text":"\u4f7f\u7528\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u503c\u521b\u5efa\u901a\u9053","title":"\u4e0b\u4e00\u90e8"},{"location":"eventing/configuration/channel-configuration/","text":"\u914d\u7f6e\u901a\u9053\u9ed8\u8ba4\u503c \u00b6 Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a default-ch-webhook ConfigMap\uff0c\u5176\u4e2d\u5305\u542b\u7ba1\u7406\u9ed8\u8ba4\u901a\u9053\u521b\u5efa\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u9ed8\u8ba4\u7684 default-ch-webhook ConfigMap\u5982\u4e0b: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel \u901a\u8fc7\u66f4\u6539 data.default-ch-config \u5c5e\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49clusterDefaults\u548c\u6bcf\u4e2a\u547d\u540d\u7a7a\u95f4\u7684\u9ed8\u8ba4\u503c\u3002 \u901a\u9053\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRD)\u4f7f\u7528\u6b64\u914d\u7f6e\u521b\u5efa\u7279\u5b9a\u4e8e\u5e73\u53f0\u7684\u5b9e\u73b0\u3002 Note clusterDefault \u8bbe\u7f6e\u51b3\u5b9a\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u9ed8\u8ba4Channel\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 namespaceDefaults \u8bbe\u7f6e\u4e3a\u5404\u4e2a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u901a\u9053\u9ed8\u8ba4\u503c\u3002","title":"\u914d\u7f6e\u901a\u9053\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/channel-configuration/#_1","text":"Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a default-ch-webhook ConfigMap\uff0c\u5176\u4e2d\u5305\u542b\u7ba1\u7406\u9ed8\u8ba4\u901a\u9053\u521b\u5efa\u7684\u914d\u7f6e\u8bbe\u7f6e\u3002 \u9ed8\u8ba4\u7684 default-ch-webhook ConfigMap\u5982\u4e0b: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel \u901a\u8fc7\u66f4\u6539 data.default-ch-config \u5c5e\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49clusterDefaults\u548c\u6bcf\u4e2a\u547d\u540d\u7a7a\u95f4\u7684\u9ed8\u8ba4\u503c\u3002 \u901a\u9053\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRD)\u4f7f\u7528\u6b64\u914d\u7f6e\u521b\u5efa\u7279\u5b9a\u4e8e\u5e73\u53f0\u7684\u5b9e\u73b0\u3002 Note clusterDefault \u8bbe\u7f6e\u51b3\u5b9a\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u9ed8\u8ba4Channel\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 namespaceDefaults \u8bbe\u7f6e\u4e3a\u5404\u4e2a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u901a\u9053\u9ed8\u8ba4\u503c\u3002","title":"\u914d\u7f6e\u901a\u9053\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/kafka-channel-configuration/","text":"\u914d\u7f6eKafka\u901a\u9053 \u00b6 Note \u672c\u6307\u5357\u5047\u8bbeKnative\u4e8b\u4ef6\u5904\u7406\u5b89\u88c5\u5728 knative-eventing \u540d\u79f0\u7a7a\u95f4\u4e2d\u3002 \u5982\u679c\u60a8\u5728\u4e0d\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\uff0c\u8bf7\u7528\u8be5\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u66ff\u6362 knative-eventing \u3002 \u8981\u4f7f\u7528Kafka\u901a\u9053\uff0c\u4f60\u5fc5\u987b: \u5b89\u88c5KafkaChannel\u5b9a\u5236\u8d44\u6e90\u5b9a\u4e49(CRD)\u3002 \u521b\u5efa\u4e00\u4e2aConfigMap\uff0c\u6307\u5b9a\u5982\u4f55\u521b\u5efaKafkaChannel\u5b9e\u4f8b\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002 \u521b\u5efa\u4e00\u4e2a kafka-channel ConfigMap \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a kafka-channel ConfigMap\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note \u8fd9\u4e2a\u4f8b\u5b50\u6307\u5b9a\u4e86\u4e24\u4e2a\u989d\u5916\u7684\u7279\u5b9a\u4e8eKafka\u901a\u9053\u7684\u53c2\u6570; numPartitions and replicationFactor \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u53ef\u9009\u7684\u3002\u8981\u521b\u5efa\u4f7f\u7528Kafka\u901a\u9053\u7684\u4ee3\u7406\uff0c\u8bf7\u5728\u4ee3\u7406\u89c4\u8303\u4e2d\u6307\u5b9a kafka-channel ConfigMap\u3002\u4f60\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u914d\u7f6eKafka\u901a\u9053\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/kafka-channel-configuration/#kafka","text":"Note \u672c\u6307\u5357\u5047\u8bbeKnative\u4e8b\u4ef6\u5904\u7406\u5b89\u88c5\u5728 knative-eventing \u540d\u79f0\u7a7a\u95f4\u4e2d\u3002 \u5982\u679c\u60a8\u5728\u4e0d\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\uff0c\u8bf7\u7528\u8be5\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u66ff\u6362 knative-eventing \u3002 \u8981\u4f7f\u7528Kafka\u901a\u9053\uff0c\u4f60\u5fc5\u987b: \u5b89\u88c5KafkaChannel\u5b9a\u5236\u8d44\u6e90\u5b9a\u4e49(CRD)\u3002 \u521b\u5efa\u4e00\u4e2aConfigMap\uff0c\u6307\u5b9a\u5982\u4f55\u521b\u5efaKafkaChannel\u5b9e\u4f8b\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002","title":"\u914d\u7f6eKafka\u901a\u9053"},{"location":"eventing/configuration/kafka-channel-configuration/#kafka-channelconfigmap","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a kafka-channel ConfigMap\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note \u8fd9\u4e2a\u4f8b\u5b50\u6307\u5b9a\u4e86\u4e24\u4e2a\u989d\u5916\u7684\u7279\u5b9a\u4e8eKafka\u901a\u9053\u7684\u53c2\u6570; numPartitions and replicationFactor \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u53ef\u9009\u7684\u3002\u8981\u521b\u5efa\u4f7f\u7528Kafka\u901a\u9053\u7684\u4ee3\u7406\uff0c\u8bf7\u5728\u4ee3\u7406\u89c4\u8303\u4e2d\u6307\u5b9a kafka-channel ConfigMap\u3002\u4f60\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2akafka-channelConfigMap"},{"location":"eventing/configuration/sources-configuration/","text":"\u914d\u7f6e\u4e8b\u4ef6\u6e90\u9ed8\u8ba4\u503c \u00b6 \u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u914d\u7f6eKnative\u4e8b\u4ef6\u6e90\u7684\u9ed8\u8ba4\u503c\u3002\u60a8\u53ef\u4ee5\u6839\u636e\u4e8b\u4ef6\u6e90\u751f\u6210\u4e8b\u4ef6\u7684\u65b9\u5f0f\u6765\u914d\u7f6e\u4e8b\u4ef6\u6e90\u3002 \u4e3aPingSource\u914d\u7f6e\u9ed8\u8ba4\u503c \u00b6 PingSource\u662f\u4e00\u4e2a\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684cron\u8ba1\u5212\u4e0a\u751f\u6210\u5177\u6709\u56fa\u5b9a\u6709\u6548\u8d1f\u8f7d\u7684\u4e8b\u4ef6\u3002 \u5173\u4e8e\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u65b0\u7684PingSource\uff0c\u8bf7\u53c2\u89c1 \u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61 \u3002 \u53ef\u7528\u7684\u53c2\u6570\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u9664\u4e86\u53ef\u4ee5\u5728PingSource\u8d44\u6e90\u4e2d\u914d\u7f6e\u7684\u53c2\u6570\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u5168\u5c40ConfigMap\u79f0\u4e3a config-ping-defaults \u3002 \u8fd9\u4e2aConfigMap\u5141\u8bb8\u60a8\u66f4\u6539PingSource\u6dfb\u52a0\u5230\u5b83\u751f\u6210\u7684CloudEvents\u4e2d\u7684\u6700\u5927\u6570\u636e\u91cf\u3002 data-max-size \u53c2\u6570\u5141\u8bb8\u60a8\u8bbe\u7f6e\u6d88\u606f\u5141\u8bb8\u53d1\u9001\u7684\u6700\u5927\u5b57\u8282\u6570\uff0c\u4e0d\u5305\u62ec\u4efb\u4f55base64\u89e3\u7801\u3002 \u9ed8\u8ba4\u503c -1 \u4e0d\u8bbe\u7f6e\u6570\u636e\u9650\u5236\u3002 apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 kubectl edit cm config-ping-defaults -n knative-eventing","title":"\u914d\u7f6e\u4e8b\u4ef6\u6e90\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/sources-configuration/#_1","text":"\u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u914d\u7f6eKnative\u4e8b\u4ef6\u6e90\u7684\u9ed8\u8ba4\u503c\u3002\u60a8\u53ef\u4ee5\u6839\u636e\u4e8b\u4ef6\u6e90\u751f\u6210\u4e8b\u4ef6\u7684\u65b9\u5f0f\u6765\u914d\u7f6e\u4e8b\u4ef6\u6e90\u3002","title":"\u914d\u7f6e\u4e8b\u4ef6\u6e90\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/sources-configuration/#pingsource","text":"PingSource\u662f\u4e00\u4e2a\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684cron\u8ba1\u5212\u4e0a\u751f\u6210\u5177\u6709\u56fa\u5b9a\u6709\u6548\u8d1f\u8f7d\u7684\u4e8b\u4ef6\u3002 \u5173\u4e8e\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u65b0\u7684PingSource\uff0c\u8bf7\u53c2\u89c1 \u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61 \u3002 \u53ef\u7528\u7684\u53c2\u6570\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u9664\u4e86\u53ef\u4ee5\u5728PingSource\u8d44\u6e90\u4e2d\u914d\u7f6e\u7684\u53c2\u6570\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u5168\u5c40ConfigMap\u79f0\u4e3a config-ping-defaults \u3002 \u8fd9\u4e2aConfigMap\u5141\u8bb8\u60a8\u66f4\u6539PingSource\u6dfb\u52a0\u5230\u5b83\u751f\u6210\u7684CloudEvents\u4e2d\u7684\u6700\u5927\u6570\u636e\u91cf\u3002 data-max-size \u53c2\u6570\u5141\u8bb8\u60a8\u8bbe\u7f6e\u6d88\u606f\u5141\u8bb8\u53d1\u9001\u7684\u6700\u5927\u5b57\u8282\u6570\uff0c\u4e0d\u5305\u62ec\u4efb\u4f55base64\u89e3\u7801\u3002 \u9ed8\u8ba4\u503c -1 \u4e0d\u8bbe\u7f6e\u6570\u636e\u9650\u5236\u3002 apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 kubectl edit cm config-ping-defaults -n knative-eventing","title":"\u4e3aPingSource\u914d\u7f6e\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/sugar-configuration/","text":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668 \u00b6 \u4ecb\u7ecd\u7cd6\u63a7\u5236\u5668\u7684\u914d\u7f6e\u65b9\u6cd5\u3002 \u60a8\u53ef\u4ee5\u914d\u7f6e\u7cd6\u63a7\u5236\u5668\uff0c\u4ee5\u4fbf\u5728\u4f7f\u7528\u914d\u7f6e\u7684\u6807\u7b7e\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u65f6\u521b\u5efa\u4ee3\u7406\u3002 \u53c2\u89c1 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5236\u5668 \u7684\u793a\u4f8b\u3002 \u9ed8\u8ba4\u7684 config-sugar ConfigMap \u901a\u8fc7\u5c06 namespace-selector \u548c trigger-selector \u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\u6765\u7981\u7528\u7cd6\u63a7\u5236\u5668\u3002 \u542f\u7528\u7cd6\u63a7\u5236\u5668 \u5bf9\u4e8e\u547d\u540d\u7a7a\u95f4\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector namespace-selector \u3002 \u5bf9\u4e8e\u89e6\u53d1\u5668\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector trigger-selector \u3002 \u5728\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u89e6\u53d1\u5668\u4e0a\u542f\u7528\u7cd6\u63a7\u5236\u5668\u7684\u793a\u4f8b\u914d\u7f6e apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : namespace-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] trigger-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] \u7cd6\u63a7\u5236\u5668\u53ea\u4f1a\u5728\u5e26\u6709 eventing.knative.dev/injection: enabled \u6807\u7b7e\u7684\u547d\u540d\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u4e0a\u64cd\u4f5c\u3002 \u8fd9\u4e5f\u6a21\u62df\u4e86\u547d\u540d\u7a7a\u95f4\u7684\u9057\u7559\u7cd6\u63a7\u5236\u5668\u884c\u4e3a\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u7f16\u8f91\u8fd9\u4e2a ConfigMap: kubectl edit cm config-sugar -n knative-eventing","title":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/configuration/sugar-configuration/#_1","text":"\u4ecb\u7ecd\u7cd6\u63a7\u5236\u5668\u7684\u914d\u7f6e\u65b9\u6cd5\u3002 \u60a8\u53ef\u4ee5\u914d\u7f6e\u7cd6\u63a7\u5236\u5668\uff0c\u4ee5\u4fbf\u5728\u4f7f\u7528\u914d\u7f6e\u7684\u6807\u7b7e\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u65f6\u521b\u5efa\u4ee3\u7406\u3002 \u53c2\u89c1 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5236\u5668 \u7684\u793a\u4f8b\u3002 \u9ed8\u8ba4\u7684 config-sugar ConfigMap \u901a\u8fc7\u5c06 namespace-selector \u548c trigger-selector \u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\u6765\u7981\u7528\u7cd6\u63a7\u5236\u5668\u3002 \u542f\u7528\u7cd6\u63a7\u5236\u5668 \u5bf9\u4e8e\u547d\u540d\u7a7a\u95f4\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector namespace-selector \u3002 \u5bf9\u4e8e\u89e6\u53d1\u5668\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector trigger-selector \u3002 \u5728\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u89e6\u53d1\u5668\u4e0a\u542f\u7528\u7cd6\u63a7\u5236\u5668\u7684\u793a\u4f8b\u914d\u7f6e apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : namespace-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] trigger-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] \u7cd6\u63a7\u5236\u5668\u53ea\u4f1a\u5728\u5e26\u6709 eventing.knative.dev/injection: enabled \u6807\u7b7e\u7684\u547d\u540d\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u4e0a\u64cd\u4f5c\u3002 \u8fd9\u4e5f\u6a21\u62df\u4e86\u547d\u540d\u7a7a\u95f4\u7684\u9057\u7559\u7cd6\u63a7\u5236\u5668\u884c\u4e3a\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u7f16\u8f91\u8fd9\u4e2a ConfigMap: kubectl edit cm config-sugar -n knative-eventing","title":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/custom-event-source/","text":"\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90 \u00b6 \u5982\u679c\u60a8\u9700\u8981\u4eceKnative\u4e2d\u4e0d\u5305\u542b\u7684\u4e8b\u4ef6\u751f\u6210\u5668\u8f93\u5165\u4e8b\u4ef6\uff0c\u6216\u8005\u4eceKnative\u4f7f\u7528\u7684CloudEvent\u683c\u5f0f\u4e4b\u5916\u7684\u4e8b\u4ef6\u751f\u6210\u5668\u8f93\u5165\u4e8b\u4ef6\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u4e4b\u4e00\u6765\u5b9e\u73b0\u6b64\u76ee\u7684: \u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49Knative\u4e8b\u4ef6\u6e90 . \u901a\u8fc7\u521b\u5efa SinkBinding \uff0c\u4f7f\u7528PodSpecable\u5bf9\u8c61\u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u3002 \u4f7f\u7528\u5bb9\u5668\u4f5c\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u901a\u8fc7\u521b\u5efa ContainerSource .","title":"\u81ea\u5b9a\u4e49\u6982\u8ff0"},{"location":"eventing/custom-event-source/#_1","text":"\u5982\u679c\u60a8\u9700\u8981\u4eceKnative\u4e2d\u4e0d\u5305\u542b\u7684\u4e8b\u4ef6\u751f\u6210\u5668\u8f93\u5165\u4e8b\u4ef6\uff0c\u6216\u8005\u4eceKnative\u4f7f\u7528\u7684CloudEvent\u683c\u5f0f\u4e4b\u5916\u7684\u4e8b\u4ef6\u751f\u6210\u5668\u8f93\u5165\u4e8b\u4ef6\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u4e4b\u4e00\u6765\u5b9e\u73b0\u6b64\u76ee\u7684: \u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49Knative\u4e8b\u4ef6\u6e90 . \u901a\u8fc7\u521b\u5efa SinkBinding \uff0c\u4f7f\u7528PodSpecable\u5bf9\u8c61\u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u3002 \u4f7f\u7528\u5bb9\u5668\u4f5c\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u901a\u8fc7\u521b\u5efa ContainerSource .","title":"\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90"},{"location":"eventing/custom-event-source/containersource/","text":"\u521b\u5efaContainerSource \u00b6 ContainerSource\u5bf9\u8c61\u542f\u52a8\u4e00\u4e2a\u5bb9\u5668\u6620\u50cf\uff0c\u8be5\u6620\u50cf\u751f\u6210\u4e8b\u4ef6\u5e76\u5c06\u6d88\u606f\u53d1\u9001\u5230\u63a5\u6536\u5668URI\u3002\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528ContainerSource\u5728Knative\u4e2d\u652f\u6301\u60a8\u81ea\u5df1\u7684\u4e8b\u4ef6\u6e90\u3002 \u8981\u4f7f\u7528ContainerSource\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5fc5\u987b\u521b\u5efa\u5bb9\u5668\u6620\u50cf\u548c\u4f7f\u7528\u6620\u50cfURI\u7684ContainerSource\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster. \u5f00\u53d1\u3001\u6784\u5efa\u548c\u53d1\u5e03\u5bb9\u5668\u6620\u50cf \u00b6 You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format . \u521b\u5efa\u4e00\u4e2aContainerSource\u5bf9\u8c61 \u00b6 Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container. \u9a8c\u8bc1ContainerSource\u5bf9\u8c61 \u00b6 View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } \u5220\u9664ContainerSource\u5bf9\u8c61 \u00b6 To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object. \u53c2\u8003\u6587\u6863 \u00b6 \u53c2\u89c1 ContainerSource\u53c2\u8003 .","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/custom-event-source/containersource/#containersource","text":"ContainerSource\u5bf9\u8c61\u542f\u52a8\u4e00\u4e2a\u5bb9\u5668\u6620\u50cf\uff0c\u8be5\u6620\u50cf\u751f\u6210\u4e8b\u4ef6\u5e76\u5c06\u6d88\u606f\u53d1\u9001\u5230\u63a5\u6536\u5668URI\u3002\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528ContainerSource\u5728Knative\u4e2d\u652f\u6301\u60a8\u81ea\u5df1\u7684\u4e8b\u4ef6\u6e90\u3002 \u8981\u4f7f\u7528ContainerSource\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5fc5\u987b\u521b\u5efa\u5bb9\u5668\u6620\u50cf\u548c\u4f7f\u7528\u6620\u50cfURI\u7684ContainerSource\u3002","title":"\u521b\u5efaContainerSource"},{"location":"eventing/custom-event-source/containersource/#_1","text":"Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster.","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/custom-event-source/containersource/#_2","text":"You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format .","title":"\u5f00\u53d1\u3001\u6784\u5efa\u548c\u53d1\u5e03\u5bb9\u5668\u6620\u50cf"},{"location":"eventing/custom-event-source/containersource/#containersource_1","text":"Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container.","title":"\u521b\u5efa\u4e00\u4e2aContainerSource\u5bf9\u8c61"},{"location":"eventing/custom-event-source/containersource/#containersource_2","text":"View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"\u9a8c\u8bc1ContainerSource\u5bf9\u8c61"},{"location":"eventing/custom-event-source/containersource/#containersource_3","text":"To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object.","title":"\u5220\u9664ContainerSource\u5bf9\u8c61"},{"location":"eventing/custom-event-source/containersource/#_3","text":"\u53c2\u89c1 ContainerSource\u53c2\u8003 .","title":"\u53c2\u8003\u6587\u6863"},{"location":"eventing/custom-event-source/containersource/reference/","text":"ContainerSource \u53c2\u8003 \u00b6 \u672c\u4e3b\u9898\u63d0\u4f9b\u6709\u5173ContainerSource\u5bf9\u8c61\u7684\u53ef\u914d\u7f6e\u5b57\u6bb5\u7684\u53c2\u8003\u4fe1\u606f\u3002 ContainerSource \u00b6 ContainerSource\u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b or \u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u6b64\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3aContainerSource\u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6ContainerSource\u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982, a name . \u5fc5\u987b spec \u6307\u5b9a\u6b64ContainerSource\u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.template \u4e00\u4e2a\u5f62\u72b6\u4e3a Deployment.spec.template \u7684 template \uff0c\u7528\u4e8e\u6b64ContainerSource\u3002 \u5fc5\u987b spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009 \u6a21\u677f\u53c2\u6570 \u00b6 This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation . \u793a\u4f8b:\u6a21\u677f\u53c2\u6570 \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ... CloudEvent Overrides \u00b6 CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 A ceOverrides definition supports the following fields: Field Description \u5fc5\u987b or \u53ef\u9009 extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. \u53ef\u9009 Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. \u4e3e\u4f8b: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u5bf9\u8c61\u53c2\u8003"},{"location":"eventing/custom-event-source/containersource/reference/#containersource","text":"\u672c\u4e3b\u9898\u63d0\u4f9b\u6709\u5173ContainerSource\u5bf9\u8c61\u7684\u53ef\u914d\u7f6e\u5b57\u6bb5\u7684\u53c2\u8003\u4fe1\u606f\u3002","title":"ContainerSource \u53c2\u8003"},{"location":"eventing/custom-event-source/containersource/reference/#containersource_1","text":"ContainerSource\u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b or \u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u6b64\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3aContainerSource\u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6ContainerSource\u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982, a name . \u5fc5\u987b spec \u6307\u5b9a\u6b64ContainerSource\u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.template \u4e00\u4e2a\u5f62\u72b6\u4e3a Deployment.spec.template \u7684 template \uff0c\u7528\u4e8e\u6b64ContainerSource\u3002 \u5fc5\u987b spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009","title":"ContainerSource"},{"location":"eventing/custom-event-source/containersource/reference/#_1","text":"This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation .","title":"\u6a21\u677f\u53c2\u6570"},{"location":"eventing/custom-event-source/containersource/reference/#_2","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ...","title":"\u793a\u4f8b:\u6a21\u677f\u53c2\u6570"},{"location":"eventing/custom-event-source/containersource/reference/#cloudevent-overrides","text":"CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 A ceOverrides definition supports the following fields: Field Description \u5fc5\u987b or \u53ef\u9009 extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. \u53ef\u9009 Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/custom-event-source/containersource/reference/#cloudevent-overrides_1","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u4e3e\u4f8b: CloudEvent Overrides"},{"location":"eventing/custom-event-source/custom-event-source/","text":"\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90 \u00b6 \u5982\u679c\u5e0c\u671b\u4e3a\u7279\u5b9a\u4e8b\u4ef6\u751f\u6210\u5668\u7c7b\u578b\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5219\u5fc5\u987b\u521b\u5efa\u652f\u6301\u5c06\u4e8b\u4ef6\u4ece\u8be5\u751f\u6210\u5668\u7c7b\u578b\u8f6c\u53d1\u5230Knative\u63a5\u6536\u5668\u7684\u7ec4\u4ef6\u3002 \u8fd9\u79cd\u7c7b\u578b\u7684\u96c6\u6210\u6bd4\u4f7f\u7528\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u96c6\u6210\u7c7b\u578b\u9700\u8981\u66f4\u591a\u7684\u5de5\u4f5c\uff0c\u4f8b\u5982 SinkBinding \u6216 ContainerSource ;\u4f46\u662f\uff0c\u8fd9\u63d0\u4f9b\u4e86\u6700\u5b8c\u5584\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u662f\u7528\u6237\u6700\u5bb9\u6613\u4f7f\u7528\u7684\u96c6\u6210\u7c7b\u578b\u3002 \u901a\u8fc7\u4e3a\u6e90\u63d0\u4f9b\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRD)\u800c\u4e0d\u662f\u901a\u7528\u5bb9\u5668\u5b9a\u4e49\uff0c\u66f4\u5bb9\u6613\u5411\u7528\u6237\u516c\u5f00\u6709\u610f\u4e49\u7684\u914d\u7f6e\u9009\u9879\u548c\u6587\u6863\uff0c\u5e76\u9690\u85cf\u5b9e\u73b0\u7ec6\u8282\u3002 Note \u5982\u679c\u4f60\u5df2\u7ecf\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e8b\u4ef6\u6e90\u7c7b\u578b\uff0c\u800c\u4e0d\u662f\u6838\u5fc3Knative\u9879\u76ee\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u53ef\u4ee5\u6253\u5f00\u4e00\u4e2a\u62c9\u8bf7\u6c42\u5c06\u5b83\u6dfb\u52a0\u5230 \u7b2c\u4e09\u65b9\u6e90 \u7684\u5217\u8868\u4e2d\uff0c\u5e76\u5728 Knative Slack \u7684\u4e00\u4e2a\u901a\u9053\u4e2d\u5ba3\u5e03\u65b0\u7684\u6e90\u3002 \u60a8\u8fd8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u6e90\u6dfb\u52a0\u5230 knative-sandbox \u7ec4\u7ec7\u4e2d\uff0c\u65b9\u6cd5\u662f\u9075\u5faa \u521b\u5efa\u6c99\u7bb1\u5b58\u50a8\u5e93 \u7684\u8bf4\u660e\u3002 \u6240\u9700\u7684\u7ec4\u4ef6 \u00b6 \u8981\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5fc5\u987b\u521b\u5efa\u4ee5\u4e0b\u7ec4\u4ef6: \u7ec4\u4ef6 \u63cf\u8ff0 \u6536\u5230\u9002\u914d\u5668 \u5305\u542b\u6307\u5b9a\u5982\u4f55\u4ece\u751f\u4ea7\u8005\u83b7\u53d6\u4e8b\u4ef6\u3001\u63a5\u6536\u5668URI\u662f\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u5c06\u4e8b\u4ef6\u8f6c\u6362\u4e3aCloudEvent\u683c\u5f0f\u7684\u903b\u8f91\u3002 Kubernetes\u63a7\u5236\u5668 \u7ba1\u7406\u4e8b\u4ef6\u6e90\u5e76\u534f\u8c03\u5e95\u5c42\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\u3002 \u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49 (CRD) \u63d0\u4f9b\u63a7\u5236\u5668\u7528\u4e8e\u7ba1\u7406\u63a5\u6536\u9002\u914d\u5668\u7684\u914d\u7f6e\u3002 \u4f7f\u7528\u793a\u4f8b\u6e90\u4ee3\u7801 \u00b6 Knative\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u5b58\u50a8\u5e93\uff0c\u5176\u4e2d\u5305\u542b\u7528\u4e8e\u57fa\u672c\u4e8b\u4ef6\u6e90\u63a7\u5236\u5668\u548c\u63a5\u6536\u9002\u914d\u5668\u7684\u6a21\u677f\u3002 \u6709\u5173\u4f7f\u7528\u793a\u4f8b\u6e90\u4ee3\u7801\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u6587\u6863 . \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u5b9e\u73b0CloudEvent\u7ed1\u5b9a\u63a5\u53e3\uff0c CloudEvent\u7684go sdk \u63d0\u4f9b\u6807\u51c6\u8bbf\u95ee\u5e93\uff0c\u6839\u636e\u9700\u8981\u914d\u7f6e\u63a5\u53e3\u3002 \u63a7\u5236\u5668\u8fd0\u884c\u65f6(\u8fd9\u662f\u6211\u4eec\u901a\u8fc7\u6ce8\u5165\u5171\u4eab\u7684)\u5c06\u7279\u5b9a\u4e8e\u534f\u8bae\u7684\u914d\u7f6e\u5408\u5e76\u5230\u201c\u901a\u7528\u63a7\u5236\u5668\u201dCRD\u4e2d\u3002","title":"\u521b\u5efa\u4e8b\u4ef6\u6e90\u6982\u8ff0"},{"location":"eventing/custom-event-source/custom-event-source/#_1","text":"\u5982\u679c\u5e0c\u671b\u4e3a\u7279\u5b9a\u4e8b\u4ef6\u751f\u6210\u5668\u7c7b\u578b\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5219\u5fc5\u987b\u521b\u5efa\u652f\u6301\u5c06\u4e8b\u4ef6\u4ece\u8be5\u751f\u6210\u5668\u7c7b\u578b\u8f6c\u53d1\u5230Knative\u63a5\u6536\u5668\u7684\u7ec4\u4ef6\u3002 \u8fd9\u79cd\u7c7b\u578b\u7684\u96c6\u6210\u6bd4\u4f7f\u7528\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u96c6\u6210\u7c7b\u578b\u9700\u8981\u66f4\u591a\u7684\u5de5\u4f5c\uff0c\u4f8b\u5982 SinkBinding \u6216 ContainerSource ;\u4f46\u662f\uff0c\u8fd9\u63d0\u4f9b\u4e86\u6700\u5b8c\u5584\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u662f\u7528\u6237\u6700\u5bb9\u6613\u4f7f\u7528\u7684\u96c6\u6210\u7c7b\u578b\u3002 \u901a\u8fc7\u4e3a\u6e90\u63d0\u4f9b\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRD)\u800c\u4e0d\u662f\u901a\u7528\u5bb9\u5668\u5b9a\u4e49\uff0c\u66f4\u5bb9\u6613\u5411\u7528\u6237\u516c\u5f00\u6709\u610f\u4e49\u7684\u914d\u7f6e\u9009\u9879\u548c\u6587\u6863\uff0c\u5e76\u9690\u85cf\u5b9e\u73b0\u7ec6\u8282\u3002 Note \u5982\u679c\u4f60\u5df2\u7ecf\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e8b\u4ef6\u6e90\u7c7b\u578b\uff0c\u800c\u4e0d\u662f\u6838\u5fc3Knative\u9879\u76ee\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u53ef\u4ee5\u6253\u5f00\u4e00\u4e2a\u62c9\u8bf7\u6c42\u5c06\u5b83\u6dfb\u52a0\u5230 \u7b2c\u4e09\u65b9\u6e90 \u7684\u5217\u8868\u4e2d\uff0c\u5e76\u5728 Knative Slack \u7684\u4e00\u4e2a\u901a\u9053\u4e2d\u5ba3\u5e03\u65b0\u7684\u6e90\u3002 \u60a8\u8fd8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u6e90\u6dfb\u52a0\u5230 knative-sandbox \u7ec4\u7ec7\u4e2d\uff0c\u65b9\u6cd5\u662f\u9075\u5faa \u521b\u5efa\u6c99\u7bb1\u5b58\u50a8\u5e93 \u7684\u8bf4\u660e\u3002","title":"\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90"},{"location":"eventing/custom-event-source/custom-event-source/#_2","text":"\u8981\u521b\u5efa\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\uff0c\u5fc5\u987b\u521b\u5efa\u4ee5\u4e0b\u7ec4\u4ef6: \u7ec4\u4ef6 \u63cf\u8ff0 \u6536\u5230\u9002\u914d\u5668 \u5305\u542b\u6307\u5b9a\u5982\u4f55\u4ece\u751f\u4ea7\u8005\u83b7\u53d6\u4e8b\u4ef6\u3001\u63a5\u6536\u5668URI\u662f\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u5c06\u4e8b\u4ef6\u8f6c\u6362\u4e3aCloudEvent\u683c\u5f0f\u7684\u903b\u8f91\u3002 Kubernetes\u63a7\u5236\u5668 \u7ba1\u7406\u4e8b\u4ef6\u6e90\u5e76\u534f\u8c03\u5e95\u5c42\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\u3002 \u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49 (CRD) \u63d0\u4f9b\u63a7\u5236\u5668\u7528\u4e8e\u7ba1\u7406\u63a5\u6536\u9002\u914d\u5668\u7684\u914d\u7f6e\u3002","title":"\u6240\u9700\u7684\u7ec4\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/#_3","text":"Knative\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u5b58\u50a8\u5e93\uff0c\u5176\u4e2d\u5305\u542b\u7528\u4e8e\u57fa\u672c\u4e8b\u4ef6\u6e90\u63a7\u5236\u5668\u548c\u63a5\u6536\u9002\u914d\u5668\u7684\u6a21\u677f\u3002 \u6709\u5173\u4f7f\u7528\u793a\u4f8b\u6e90\u4ee3\u7801\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u6587\u6863 .","title":"\u4f7f\u7528\u793a\u4f8b\u6e90\u4ee3\u7801"},{"location":"eventing/custom-event-source/custom-event-source/#_4","text":"\u5b9e\u73b0CloudEvent\u7ed1\u5b9a\u63a5\u53e3\uff0c CloudEvent\u7684go sdk \u63d0\u4f9b\u6807\u51c6\u8bbf\u95ee\u5e93\uff0c\u6839\u636e\u9700\u8981\u914d\u7f6e\u63a5\u53e3\u3002 \u63a7\u5236\u5668\u8fd0\u884c\u65f6(\u8fd9\u662f\u6211\u4eec\u901a\u8fc7\u6ce8\u5165\u5171\u4eab\u7684)\u5c06\u7279\u5b9a\u4e8e\u534f\u8bae\u7684\u914d\u7f6e\u5408\u5e76\u5230\u201c\u901a\u7528\u63a7\u5236\u5668\u201dCRD\u4e2d\u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"eventing/custom-event-source/custom-event-source/controller/","text":"\u521b\u5efa\u4e00\u4e2a\u63a7\u5236\u5668 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528\u793a\u4f8b\u5b58\u50a8\u5e93 update-codegen.sh \u811a\u672c\u6765\u751f\u6210\u6240\u9700\u7684\u7ec4\u4ef6( clientset , cache , informers , \u548c listers ) \u5e76\u5c06\u5176\u6ce8\u5165\u5230\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\u4e2d\u3002 \u63a7\u5236\u5668\u4f8b\u5b50: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... } \u8fc7\u7a0b \u00b6 \u8fd0\u884c\u547d\u4ee4\u751f\u6210\u7ec4\u4ef6: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt \u6ce8\u5165\u7ec4\u4ef6\u7684\u547d\u4ee4\u5982\u4e0b: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt \u5c06\u65b0\u7684\u63a7\u5236\u5668\u5b9e\u73b0\u4f20\u9012\u7ed9 sharedmain \u65b9\u6cd5: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } \u5b9a\u4e49 NewController \u5b9e\u73b0: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } \u4e00\u4e2a configmap.Watcher \u548c\u4e00\u4e2a\u4e0a\u4e0b\u6587(\u6ce8\u5165\u7684\u5217\u8868\u7528\u4e8e\u534f\u8c03\u5668\u7ed3\u6784\u53c2\u6570)\u88ab\u4f20\u9012\u7ed9\u8fd9\u4e2a\u5b9e\u73b0\u3002 \u4ece knative.dev/pkg \u4f9d\u8d56\u9879\u5bfc\u5165\u57fa\u672c\u534f\u8c03\u5668: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) \u786e\u4fdd\u4e8b\u4ef6\u5904\u7406\u7a0b\u5e8f\u88ab\u8fc7\u6ee4\u5230\u6b63\u786e\u7684\u4fe1\u606f\u63d0\u4f9b\u8005: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) \u786e\u4fdd\u4e3a\u793a\u4f8b\u6e90\u4f7f\u7528\u7684\u8f85\u52a9\u8d44\u6e90\u6b63\u786e\u914d\u7f6e\u4e86\u901a\u77e5\u5668\uff0c\u4ee5\u90e8\u7f72\u548c\u7ed1\u5b9a\u4e8b\u4ef6\u6e90\u548c\u63a5\u6536\u9002\u914d\u5668: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"\u521b\u5efa\u63a7\u5236\u5668"},{"location":"eventing/custom-event-source/custom-event-source/controller/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528\u793a\u4f8b\u5b58\u50a8\u5e93 update-codegen.sh \u811a\u672c\u6765\u751f\u6210\u6240\u9700\u7684\u7ec4\u4ef6( clientset , cache , informers , \u548c listers ) \u5e76\u5c06\u5176\u6ce8\u5165\u5230\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\u4e2d\u3002 \u63a7\u5236\u5668\u4f8b\u5b50: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... }","title":"\u521b\u5efa\u4e00\u4e2a\u63a7\u5236\u5668"},{"location":"eventing/custom-event-source/custom-event-source/controller/#_2","text":"\u8fd0\u884c\u547d\u4ee4\u751f\u6210\u7ec4\u4ef6: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt \u6ce8\u5165\u7ec4\u4ef6\u7684\u547d\u4ee4\u5982\u4e0b: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt \u5c06\u65b0\u7684\u63a7\u5236\u5668\u5b9e\u73b0\u4f20\u9012\u7ed9 sharedmain \u65b9\u6cd5: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } \u5b9a\u4e49 NewController \u5b9e\u73b0: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } \u4e00\u4e2a configmap.Watcher \u548c\u4e00\u4e2a\u4e0a\u4e0b\u6587(\u6ce8\u5165\u7684\u5217\u8868\u7528\u4e8e\u534f\u8c03\u5668\u7ed3\u6784\u53c2\u6570)\u88ab\u4f20\u9012\u7ed9\u8fd9\u4e2a\u5b9e\u73b0\u3002 \u4ece knative.dev/pkg \u4f9d\u8d56\u9879\u5bfc\u5165\u57fa\u672c\u534f\u8c03\u5668: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) \u786e\u4fdd\u4e8b\u4ef6\u5904\u7406\u7a0b\u5e8f\u88ab\u8fc7\u6ee4\u5230\u6b63\u786e\u7684\u4fe1\u606f\u63d0\u4f9b\u8005: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) \u786e\u4fdd\u4e3a\u793a\u4f8b\u6e90\u4f7f\u7528\u7684\u8f85\u52a9\u8d44\u6e90\u6b63\u786e\u914d\u7f6e\u4e86\u901a\u77e5\u5668\uff0c\u4ee5\u90e8\u7f72\u548c\u7ed1\u5b9a\u4e8b\u4ef6\u6e90\u548c\u63a5\u6536\u9002\u914d\u5668: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"\u8fc7\u7a0b"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/","text":"\u5c06\u4e8b\u4ef6\u6e90\u53d1\u5e03\u5230\u96c6\u7fa4 \u00b6 \u542f\u52a8\u4e00\u4e2aminikube\u96c6\u7fa4: minikube start \u8bbe\u7f6e ko \u6765\u4f7f\u7528minikube docker\u5b9e\u4f8b\u548c\u672c\u5730\u6ce8\u518c\u8868: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local \u5e94\u7528CRD\u5e76\u914d\u7f6eYAML: ko apply -f config \u4e00\u65e6 sample-source-controller-manager \u5728 knative-samples \u547d\u540d\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u5e94\u7528 example.yaml \u6bcf\u9694 10s \u5c06\u6211\u4eec\u7684 sample-source \u76f4\u63a5\u8fde\u63a5\u5230 ksvc \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml \u534f\u8c03\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u786e\u8ba4 ksvc \u6bcf 10s \u8f93\u51fa\u4e00\u6b21\u6709\u6548\u7684\u4e91\u4e8b\u4ef6\uff0c\u4ee5\u4e0e\u6211\u4eec\u6307\u5b9a\u7684\u95f4\u9694\u4fdd\u6301\u4e00\u81f4\u3002 % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"\u5c06\u4e8b\u4ef6\u6e90\u53d1\u5e03\u5230\u96c6\u7fa4"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/#_1","text":"\u542f\u52a8\u4e00\u4e2aminikube\u96c6\u7fa4: minikube start \u8bbe\u7f6e ko \u6765\u4f7f\u7528minikube docker\u5b9e\u4f8b\u548c\u672c\u5730\u6ce8\u518c\u8868: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local \u5e94\u7528CRD\u5e76\u914d\u7f6eYAML: ko apply -f config \u4e00\u65e6 sample-source-controller-manager \u5728 knative-samples \u547d\u540d\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u5e94\u7528 example.yaml \u6bcf\u9694 10s \u5c06\u6211\u4eec\u7684 sample-source \u76f4\u63a5\u8fde\u63a5\u5230 ksvc \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml \u534f\u8c03\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u786e\u8ba4 ksvc \u6bcf 10s \u8f93\u51fa\u4e00\u6b21\u6709\u6548\u7684\u4e91\u4e8b\u4ef6\uff0c\u4ee5\u4e0e\u6211\u4eec\u6307\u5b9a\u7684\u95f4\u9694\u4fdd\u6301\u4e00\u81f4\u3002 % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"\u5c06\u4e8b\u4ef6\u6e90\u53d1\u5e03\u5230\u96c6\u7fa4"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/","text":"\u521b\u5efa\u63a5\u6536\u9002\u914d\u5668 \u00b6 \u4f5c\u4e3a\u6e90\u534f\u8c03\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u60a8\u5fc5\u987b\u521b\u5efa\u548c\u90e8\u7f72\u5e95\u5c42\u63a5\u6536\u9002\u914d\u5668\u3002 \u63a5\u6536\u9002\u914d\u5668\u9700\u8981\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u5165\u7684 main \u65b9\u6cd5\uff0c\u5b83\u4f4d\u4e8e cmd/receiver_adapter/main.go \u4e2d: // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } \u63a5\u6536\u9002\u914d\u5668\u7684 pkg \u5b9e\u73b0\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u51fd\u6570: \u4e00\u4e2a NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} \u8c03\u7528\uff0c\u5b83\u901a\u8fc7 EnvConfigAccessor \u521b\u5efa\u5e26\u6709\u4f20\u9012\u53d8\u91cf\u7684\u65b0\u9002\u914d\u5668\u3002 \u521b\u5efa\u7684\u9002\u914d\u5668\u88ab\u4f20\u9012\u7ed9CloudEvents\u5ba2\u6237\u7aef(\u4e8b\u4ef6\u88ab\u8f6c\u53d1\u5230\u8be5\u5ba2\u6237\u7aef)\u3002 \u8fd9\u5728Knative\u751f\u6001\u7cfb\u7edf\u4e2d\u6709\u65f6\u88ab\u79f0\u4e3asink\u6216 ceClient \u3002 \u8fd4\u56de\u503c\u662f\u7531\u9002\u914d\u5668\u7684\u672c\u5730\u7ed3\u6784\u5b9a\u4e49\u7684\u5bf9\u9002\u914d\u5668\u7684\u5f15\u7528\u3002 \u5728\u793a\u4f8b\u6e90\u7684\u60c5\u51b5\u4e0b: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } \u4e00\u4e2a Start \u51fd\u6570\uff0c\u4f5c\u4e3a\u9002\u914d\u5668 struct \u7684\u63a5\u53e3\u5b9e\u73b0: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh \u662f\u505c\u6b62\u9002\u914d\u5668\u7684\u4fe1\u53f7\u3002\u5426\u5219\uff0c\u51fd\u6570\u7684\u4f5c\u7528\u662f\u5904\u7406\u4e0b\u4e00\u4e2a\u4e8b\u4ef6\u3002 \u5728 sample-source \u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u51fd\u6570\u521b\u5efa\u4e00\u4e2aCloudEvent\uff0c\u6bcf\u9694X\u65f6\u95f4\u8f6c\u53d1\u5230\u6307\u5b9a\u7684\u63a5\u6536\u5668\uff0c\u7531 EnvConfigAccessor \u53c2\u6570\u6307\u5b9a\uff0c\u8be5\u53c2\u6570\u7531\u8d44\u6e90YAML\u52a0\u8f7d: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } } \u7ba1\u7406\u63a7\u5236\u5668\u4e2d\u7684\u63a5\u6536\u9002\u914d\u5668 \u00b6 \u66f4\u65b0 ObservedGeneration \u5e76\u521d\u59cb\u5316 Status \u6761\u4ef6\uff0c\u5982\u5728 samplesource_lifecycle.go \u548c samplesource_types.go \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation \u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u9002\u914d\u5668\u3002 \u9a8c\u8bc1\u6307\u5b9a\u7684Kubernetes\u8d44\u6e90\u662f\u5426\u6709\u6548\uff0c\u5e76\u76f8\u5e94\u5730\u66f4\u65b0 Status \u3002 \u96c6\u5408 ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note \u786e\u5207\u7684\u53c2\u6570\u53ef\u80fd\u4f1a\u6839\u636e\u529f\u80fd\u9700\u6c42\u800c\u6539\u53d8\u3002 \u6839\u636e\u63d0\u4f9b\u7684\u53c2\u6570\u521b\u5efa\u5e95\u5c42\u90e8\u7f72\uff0c\u6839\u636e\u9700\u8981\u5339\u914dpod\u6a21\u677f\u3001\u6807\u7b7e\u3001\u6240\u6709\u8005\u5f15\u7528\u7b49\u6765\u586b\u5199\u90e8\u7f72\u3002 \u4f8b\u5b50: pkg/reconciler/sample/resources/receive_adapter.go \u83b7\u53d6\u73b0\u6709\u7684\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) \u5982\u679c\u6ca1\u6709\u73b0\u6709\u7684\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\uff0c\u521b\u5efa\u4e00\u4e2a: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) \u68c0\u67e5\u9884\u671f\u7684\u89c4\u8303\u662f\u5426\u4e0e\u73b0\u6709\u7684\u89c4\u8303\u4e0d\u540c\uff0c\u5e76\u5728\u9700\u8981\u65f6\u66f4\u65b0\u90e8\u7f72: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } \u5982\u679c\u6709\u66f4\u65b0\uff0c\u8bf7\u8bb0\u5f55\u4e8b\u4ef6: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) \u5982\u679c\u6210\u529f\uff0c\u66f4\u65b0 Status \u548c MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) \u521b\u5efa\u4e00\u4e2aSinkBinding\u5c06\u63a5\u6536\u9002\u914d\u5668\u4e0e\u63a5\u6536\u5668\u7ed1\u5b9a\u3002 \u4e3a\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\u521b\u5efa\u4e00\u4e2a Reference \u3002\u8fd9\u4e2a\u90e8\u7f72\u662fSinkBinding\u7684\u6e90\u4ee3\u7801: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } \u83b7\u53d6\u73b0\u6709\u7684SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) \u5982\u679c\u4e0d\u5b58\u5728SinkBinding\uff0c\u521b\u5efa\u4e00\u4e2a: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) \u68c0\u67e5\u9884\u671f\u7684\u89c4\u8303\u662f\u5426\u4e0e\u73b0\u6709\u7684\u89c4\u8303\u4e0d\u540c\uff0c\u5982\u679c\u9700\u8981\u7684\u8bdd\u66f4\u65b0SinkBinding: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } \u5982\u679c\u6709\u66f4\u65b0\uff0c\u8bf7\u8bb0\u5f55\u4e8b\u4ef6: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink \u7ed3\u679c:: src . Status . MarkSink ( sb . Status . SinkURI ) \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u534f\u8c03\u5668\u4e8b\u4ef6\uff0c\u58f0\u660e\u6d41\u7a0b\u5df2\u7ecf\u5b8c\u6210: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"\u521b\u5efa\u63a5\u6536\u9002\u914d\u5668"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#_1","text":"\u4f5c\u4e3a\u6e90\u534f\u8c03\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u60a8\u5fc5\u987b\u521b\u5efa\u548c\u90e8\u7f72\u5e95\u5c42\u63a5\u6536\u9002\u914d\u5668\u3002 \u63a5\u6536\u9002\u914d\u5668\u9700\u8981\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u5165\u7684 main \u65b9\u6cd5\uff0c\u5b83\u4f4d\u4e8e cmd/receiver_adapter/main.go \u4e2d: // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } \u63a5\u6536\u9002\u914d\u5668\u7684 pkg \u5b9e\u73b0\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u51fd\u6570: \u4e00\u4e2a NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} \u8c03\u7528\uff0c\u5b83\u901a\u8fc7 EnvConfigAccessor \u521b\u5efa\u5e26\u6709\u4f20\u9012\u53d8\u91cf\u7684\u65b0\u9002\u914d\u5668\u3002 \u521b\u5efa\u7684\u9002\u914d\u5668\u88ab\u4f20\u9012\u7ed9CloudEvents\u5ba2\u6237\u7aef(\u4e8b\u4ef6\u88ab\u8f6c\u53d1\u5230\u8be5\u5ba2\u6237\u7aef)\u3002 \u8fd9\u5728Knative\u751f\u6001\u7cfb\u7edf\u4e2d\u6709\u65f6\u88ab\u79f0\u4e3asink\u6216 ceClient \u3002 \u8fd4\u56de\u503c\u662f\u7531\u9002\u914d\u5668\u7684\u672c\u5730\u7ed3\u6784\u5b9a\u4e49\u7684\u5bf9\u9002\u914d\u5668\u7684\u5f15\u7528\u3002 \u5728\u793a\u4f8b\u6e90\u7684\u60c5\u51b5\u4e0b: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } \u4e00\u4e2a Start \u51fd\u6570\uff0c\u4f5c\u4e3a\u9002\u914d\u5668 struct \u7684\u63a5\u53e3\u5b9e\u73b0: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh \u662f\u505c\u6b62\u9002\u914d\u5668\u7684\u4fe1\u53f7\u3002\u5426\u5219\uff0c\u51fd\u6570\u7684\u4f5c\u7528\u662f\u5904\u7406\u4e0b\u4e00\u4e2a\u4e8b\u4ef6\u3002 \u5728 sample-source \u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u51fd\u6570\u521b\u5efa\u4e00\u4e2aCloudEvent\uff0c\u6bcf\u9694X\u65f6\u95f4\u8f6c\u53d1\u5230\u6307\u5b9a\u7684\u63a5\u6536\u5668\uff0c\u7531 EnvConfigAccessor \u53c2\u6570\u6307\u5b9a\uff0c\u8be5\u53c2\u6570\u7531\u8d44\u6e90YAML\u52a0\u8f7d: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"\u521b\u5efa\u63a5\u6536\u9002\u914d\u5668"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#_2","text":"\u66f4\u65b0 ObservedGeneration \u5e76\u521d\u59cb\u5316 Status \u6761\u4ef6\uff0c\u5982\u5728 samplesource_lifecycle.go \u548c samplesource_types.go \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation \u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u9002\u914d\u5668\u3002 \u9a8c\u8bc1\u6307\u5b9a\u7684Kubernetes\u8d44\u6e90\u662f\u5426\u6709\u6548\uff0c\u5e76\u76f8\u5e94\u5730\u66f4\u65b0 Status \u3002 \u96c6\u5408 ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note \u786e\u5207\u7684\u53c2\u6570\u53ef\u80fd\u4f1a\u6839\u636e\u529f\u80fd\u9700\u6c42\u800c\u6539\u53d8\u3002 \u6839\u636e\u63d0\u4f9b\u7684\u53c2\u6570\u521b\u5efa\u5e95\u5c42\u90e8\u7f72\uff0c\u6839\u636e\u9700\u8981\u5339\u914dpod\u6a21\u677f\u3001\u6807\u7b7e\u3001\u6240\u6709\u8005\u5f15\u7528\u7b49\u6765\u586b\u5199\u90e8\u7f72\u3002 \u4f8b\u5b50: pkg/reconciler/sample/resources/receive_adapter.go \u83b7\u53d6\u73b0\u6709\u7684\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) \u5982\u679c\u6ca1\u6709\u73b0\u6709\u7684\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\uff0c\u521b\u5efa\u4e00\u4e2a: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) \u68c0\u67e5\u9884\u671f\u7684\u89c4\u8303\u662f\u5426\u4e0e\u73b0\u6709\u7684\u89c4\u8303\u4e0d\u540c\uff0c\u5e76\u5728\u9700\u8981\u65f6\u66f4\u65b0\u90e8\u7f72: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } \u5982\u679c\u6709\u66f4\u65b0\uff0c\u8bf7\u8bb0\u5f55\u4e8b\u4ef6: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) \u5982\u679c\u6210\u529f\uff0c\u66f4\u65b0 Status \u548c MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) \u521b\u5efa\u4e00\u4e2aSinkBinding\u5c06\u63a5\u6536\u9002\u914d\u5668\u4e0e\u63a5\u6536\u5668\u7ed1\u5b9a\u3002 \u4e3a\u63a5\u6536\u9002\u914d\u5668\u90e8\u7f72\u521b\u5efa\u4e00\u4e2a Reference \u3002\u8fd9\u4e2a\u90e8\u7f72\u662fSinkBinding\u7684\u6e90\u4ee3\u7801: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } \u83b7\u53d6\u73b0\u6709\u7684SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) \u5982\u679c\u4e0d\u5b58\u5728SinkBinding\uff0c\u521b\u5efa\u4e00\u4e2a: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) \u68c0\u67e5\u9884\u671f\u7684\u89c4\u8303\u662f\u5426\u4e0e\u73b0\u6709\u7684\u89c4\u8303\u4e0d\u540c\uff0c\u5982\u679c\u9700\u8981\u7684\u8bdd\u66f4\u65b0SinkBinding: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } \u5982\u679c\u6709\u66f4\u65b0\uff0c\u8bf7\u8bb0\u5f55\u4e8b\u4ef6: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink \u7ed3\u679c:: src . Status . MarkSink ( sb . Status . SinkURI ) \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u534f\u8c03\u5668\u4e8b\u4ef6\uff0c\u58f0\u660e\u6d41\u7a0b\u5df2\u7ecf\u5b8c\u6210: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"\u7ba1\u7406\u63a7\u5236\u5668\u4e2d\u7684\u63a5\u6536\u9002\u914d\u5668"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/","text":"\u4f7f\u7528Knative\u793a\u4f8b\u5b58\u50a8\u5e93 \u00b6 Knative\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a \u793a\u4f8b\u5b58\u50a8\u5e93 \uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u57fa\u672c\u4e8b\u4ef6\u6e90\u63a7\u5236\u5668\u548c\u63a5\u6536\u9002\u914d\u5668\u7684\u6a21\u677f\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u719f\u6089Kubernetes\u548cGo\u5f00\u53d1\u3002 \u5df2\u5b89\u88c5Git\u3002 \u5df2\u7ecf\u5b89\u88c5\u4e86Go\u3002 \u60a8\u5df2\u7ecf\u514b\u9686\u4e86 sample-source \u5b58\u50a8\u5e93 . \u53ef\u9009: \u5b89\u88c5 ko CLI \u5de5\u5177. \u5b89\u88c5 kubectl CLI \u5de5\u5177. \u914d\u7f6e minikube \u6216 kind . \u793a\u4f8b\u6587\u4ef6\u6982\u8ff0 \u00b6 \u63a5\u6536\u5668\u9002\u914d\u5668\u6587\u4ef6 \u00b6 cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents. \u63a7\u5236\u5668\u6587\u4ef6 \u00b6 cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method. CRD \u6587\u4ef6 \u00b6 pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file. \u8c03\u89e3\u4eba\u7684\u6587\u4ef6 \u00b6 pkg/reconciler/sample/samplesource.go - \u63a5\u6536\u9002\u914d\u5668\u7684\u8c03\u8282\u51fd\u6570\u3002 pkg/apis/samples/VERSION/samplesource_lifecycle.go - \u5305\u542b\u4e8b\u4ef6\u6e90\u7684\u534f\u8c03\u7ec6\u8282\u7684\u72b6\u6001\u4fe1\u606f: \u6e90\u51c6\u5907\u597d\u4e86 \u63a5\u6536\u5668\u63d0\u4f9b \u90e8\u7f72 EventType\u63d0\u4f9b Kubernetes\u8d44\u6e90\u6b63\u786e \u8fc7\u7a0b \u00b6 \u5728\u5176\u4e2d\u5b9a\u4e49\u8d44\u6e90\u6a21\u5f0f\u4e2d\u6240\u9700\u7684\u7c7b\u578b pkg/apis/samples/v1alpha1/samplesource_types.go . \u8fd9\u5305\u62ec\u8d44\u6e90YAML\u4e2d\u6240\u9700\u7684\u5b57\u6bb5\uff0c\u4ee5\u53ca\u4f7f\u7528\u6e90\u7684\u5ba2\u6237\u7aef\u96c6\u548cAPI\u5728\u63a7\u5236\u5668\u4e2d\u5f15\u7528\u7684\u5b57\u6bb5: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } \u5b9a\u4e49\u5728 status \u548c SinkURI \u5b57\u6bb5\u4e2d\u53cd\u6620\u7684\u751f\u547d\u5468\u671f: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) \u901a\u8fc7\u5b9a\u4e49\u8981\u4ece\u534f\u8c03\u5668\u51fd\u6570\u8c03\u7528\u7684\u51fd\u6570\u6765\u8bbe\u7f6e\u751f\u547d\u5468\u671f\u6761\u4ef6\u3002\u8fd9\u901a\u5e38\u662f\u5728\u91cc\u9762\u5b8c\u6210\u7684 pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"\u914d\u7f6e\u793a\u4f8b"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#knative","text":"Knative\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a \u793a\u4f8b\u5b58\u50a8\u5e93 \uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u57fa\u672c\u4e8b\u4ef6\u6e90\u63a7\u5236\u5668\u548c\u63a5\u6536\u9002\u914d\u5668\u7684\u6a21\u677f\u3002","title":"\u4f7f\u7528Knative\u793a\u4f8b\u5b58\u50a8\u5e93"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_1","text":"\u60a8\u719f\u6089Kubernetes\u548cGo\u5f00\u53d1\u3002 \u5df2\u5b89\u88c5Git\u3002 \u5df2\u7ecf\u5b89\u88c5\u4e86Go\u3002 \u60a8\u5df2\u7ecf\u514b\u9686\u4e86 sample-source \u5b58\u50a8\u5e93 . \u53ef\u9009: \u5b89\u88c5 ko CLI \u5de5\u5177. \u5b89\u88c5 kubectl CLI \u5de5\u5177. \u914d\u7f6e minikube \u6216 kind .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_2","text":"","title":"\u793a\u4f8b\u6587\u4ef6\u6982\u8ff0"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_3","text":"cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents.","title":"\u63a5\u6536\u5668\u9002\u914d\u5668\u6587\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_4","text":"cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method.","title":"\u63a7\u5236\u5668\u6587\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#crd","text":"pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file.","title":"CRD \u6587\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_5","text":"pkg/reconciler/sample/samplesource.go - \u63a5\u6536\u9002\u914d\u5668\u7684\u8c03\u8282\u51fd\u6570\u3002 pkg/apis/samples/VERSION/samplesource_lifecycle.go - \u5305\u542b\u4e8b\u4ef6\u6e90\u7684\u534f\u8c03\u7ec6\u8282\u7684\u72b6\u6001\u4fe1\u606f: \u6e90\u51c6\u5907\u597d\u4e86 \u63a5\u6536\u5668\u63d0\u4f9b \u90e8\u7f72 EventType\u63d0\u4f9b Kubernetes\u8d44\u6e90\u6b63\u786e","title":"\u8c03\u89e3\u4eba\u7684\u6587\u4ef6"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#_6","text":"\u5728\u5176\u4e2d\u5b9a\u4e49\u8d44\u6e90\u6a21\u5f0f\u4e2d\u6240\u9700\u7684\u7c7b\u578b pkg/apis/samples/v1alpha1/samplesource_types.go . \u8fd9\u5305\u62ec\u8d44\u6e90YAML\u4e2d\u6240\u9700\u7684\u5b57\u6bb5\uff0c\u4ee5\u53ca\u4f7f\u7528\u6e90\u7684\u5ba2\u6237\u7aef\u96c6\u548cAPI\u5728\u63a7\u5236\u5668\u4e2d\u5f15\u7528\u7684\u5b57\u6bb5: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } \u5b9a\u4e49\u5728 status \u548c SinkURI \u5b57\u6bb5\u4e2d\u53cd\u6620\u7684\u751f\u547d\u5468\u671f: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) \u901a\u8fc7\u5b9a\u4e49\u8981\u4ece\u534f\u8c03\u5668\u51fd\u6570\u8c03\u7528\u7684\u51fd\u6570\u6765\u8bbe\u7f6e\u751f\u547d\u5468\u671f\u6761\u4ef6\u3002\u8fd9\u901a\u5e38\u662f\u5728\u91cc\u9762\u5b8c\u6210\u7684 pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"\u8fc7\u7a0b"},{"location":"eventing/custom-event-source/sinkbinding/","text":"SinkBinding \u00b6 SinkBinding\u5bf9\u8c61\u652f\u6301\u5c06\u4e8b\u4ef6\u4ea7\u751f\u4e0e\u4ea4\u4ed8\u5bfb\u5740\u5206\u79bb\u3002 \u53ef\u4ee5\u4f7f\u7528\u63a5\u6536\u5668\u7ed1\u5b9a\u5c06\u4e3b\u9898\u6307\u5411\u63a5\u6536\u5668\u3002 subject \u662f\u4e00\u4e2aKubernetes\u8d44\u6e90\uff0c\u5b83\u5d4c\u5165PodSpec\u6a21\u677f\u5e76\u4ea7\u751f\u4e8b\u4ef6\u3002 sink \u662f\u4e00\u4e2a\u53ef\u4ee5\u63a5\u6536\u4e8b\u4ef6\u7684\u53ef\u5bfb\u5740Kubernetes\u5bf9\u8c61\u3002 SinkBinding\u5bf9\u8c61\u5c06\u73af\u5883\u53d8\u91cf\u6ce8\u5165\u5230\u63a5\u6536\u5668\u7684PodTemplateSpec\u4e2d\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u4e0d\u9700\u8981\u76f4\u63a5\u4e0eKubernetes API\u4ea4\u4e92\u6765\u5b9a\u4f4d\u4e8b\u4ef6\u76ee\u7684\u5730\u3002 \u8fd9\u4e9b\u73af\u5883\u53d8\u91cf\u5982\u4e0b: K_SINK - \u89e3\u6790\u63a5\u6536\u5668\u7684URL\u3002 K_CE_OVERRIDES - \u6307\u5b9a\u5bf9\u51fa\u7ad9\u4e8b\u4ef6\u7684\u8986\u76d6\u7684JSON\u5bf9\u8c61\u3002","title":"\u5173\u4e8e\u5bf9\u8c61"},{"location":"eventing/custom-event-source/sinkbinding/#sinkbinding","text":"SinkBinding\u5bf9\u8c61\u652f\u6301\u5c06\u4e8b\u4ef6\u4ea7\u751f\u4e0e\u4ea4\u4ed8\u5bfb\u5740\u5206\u79bb\u3002 \u53ef\u4ee5\u4f7f\u7528\u63a5\u6536\u5668\u7ed1\u5b9a\u5c06\u4e3b\u9898\u6307\u5411\u63a5\u6536\u5668\u3002 subject \u662f\u4e00\u4e2aKubernetes\u8d44\u6e90\uff0c\u5b83\u5d4c\u5165PodSpec\u6a21\u677f\u5e76\u4ea7\u751f\u4e8b\u4ef6\u3002 sink \u662f\u4e00\u4e2a\u53ef\u4ee5\u63a5\u6536\u4e8b\u4ef6\u7684\u53ef\u5bfb\u5740Kubernetes\u5bf9\u8c61\u3002 SinkBinding\u5bf9\u8c61\u5c06\u73af\u5883\u53d8\u91cf\u6ce8\u5165\u5230\u63a5\u6536\u5668\u7684PodTemplateSpec\u4e2d\u3002 \u56e0\u6b64\uff0c\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u4e0d\u9700\u8981\u76f4\u63a5\u4e0eKubernetes API\u4ea4\u4e92\u6765\u5b9a\u4f4d\u4e8b\u4ef6\u76ee\u7684\u5730\u3002 \u8fd9\u4e9b\u73af\u5883\u53d8\u91cf\u5982\u4e0b: K_SINK - \u89e3\u6790\u63a5\u6536\u5668\u7684URL\u3002 K_CE_OVERRIDES - \u6307\u5b9a\u5bf9\u51fa\u7ad9\u4e8b\u4ef6\u7684\u8986\u76d6\u7684JSON\u5bf9\u8c61\u3002","title":"SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/","text":"\u521b\u5efa\u4e00\u4e2aSinkBinding \u00b6 \u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efaSinkBinding\u5bf9\u8c61\u3002 SinkBinding\u5c06\u63a5\u6536\u5668\u89e3\u6790\u4e3aURI\uff0c\u5728\u73af\u5883\u53d8\u91cf K_SINK \u4e2d\u8bbe\u7f6eURI\uff0c\u5e76\u4f7f\u7528 K_SINK \u5c06URI\u6dfb\u52a0\u5230\u4e3b\u9898\u4e2d\u3002 \u5982\u679cURI\u53d1\u751f\u53d8\u5316\uff0cSinkBinding\u4f1a\u66f4\u65b0 K_SINK \u7684\u503c\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u63a5\u6536\u5668\u662fKnative\u670d\u52a1\uff0c\u800c\u4e3b\u9898\u662fCronJob\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u4e3b\u9898\u548c\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u521b\u5efaSinkBinding\u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u4e8b\u4ef6\u5904\u7406\u3002 \u53ef\u9009:\u5982\u679c\u4f60\u60f3\u5728SinkBinding\u4e2d\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u547d\u4ee4\u884c\u3002 \u53ef\u9009:\u9009\u62e9SinkBinding\u547d\u540d\u7a7a\u95f4\u9009\u62e9\u884c\u4e3a \u00b6 SinkBinding\u5bf9\u8c61\u4ee5\u4e24\u79cd\u6a21\u5f0f\u4e4b\u4e00\u64cd\u4f5c: exclusion or inclusion \u3002 \u9ed8\u8ba4\u6a21\u5f0f\u4e3a exclusion \u3002 \u5728\u6392\u9664\u6a21\u5f0f\u4e0b\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e3a\u547d\u540d\u7a7a\u95f4\u542f\u7528\u4e86SinkBinding\u884c\u4e3a\u3002 \u4e3a\u4e86\u7981\u6b62\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u88ab\u8bc4\u4f30\u4e3a\u7a81\u53d8\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 bindings.knative.dev/exclude: true \u6807\u7b7e\u6765\u6392\u9664\u5b83\u3002 \u5728\u5305\u542b\u6a21\u5f0f\u4e2d\uff0c\u6ca1\u6709\u4e3a\u547d\u540d\u7a7a\u95f4\u542f\u7528SinkBinding\u884c\u4e3a\u3002 \u5728\u5bf9\u547d\u540d\u7a7a\u95f4\u8fdb\u884c\u53d8\u5f02\u8bc4\u4f30\u4e4b\u524d\uff0c\u5fc5\u987b\u4f7f\u7528\u6807\u7b7e bindings.knative.dev/include: true \u663e\u5f0f\u5305\u542b\u5b83\u3002 \u8bbe\u7f6eSinkBinding\u5bf9\u8c61\u4e3a\u5305\u542b\u6a21\u5f0f\u3002 \u5c06 SINK_BINDING_SELECTION_MODE \u7684\u503c\u4ece exclusion \u6539\u4e3a inclusion \uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion \u8981\u9a8c\u8bc1 SINK_BINDING_SELECTION_MODE \u6309\u9700\u8981\u8bbe\u7f6e\uff0c\u6267\u884c: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING \u521b\u5efa\u547d\u540d\u7a7a\u95f4 \u00b6 \u5982\u679c\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\uff0c\u8bf7\u4e3aSinkBinding\u5bf9\u8c61\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4: kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u5e0c\u671bSinkBinding\u4f7f\u7528\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u4f8b\u5982, sinkbinding-example . Note \u5982\u679c\u9009\u62e9\u4e86\u5305\u542b\u6a21\u5f0f\uff0c\u5219\u5fc5\u987b\u5c06 bindings.knative.dev/include: true \u6807\u7b7e\u6dfb\u52a0\u5230\u547d\u540d\u7a7a\u95f4\uff0c\u4ee5\u542f\u7528SinkBinding\u884c\u4e3a\u3002 \u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u5668 \u00b6 \u63a5\u6536\u5668\u53ef\u4ee5\u662f\u4efb\u4f55\u53ef\u4ee5\u63a5\u6536\u4e8b\u4ef6\u7684\u53ef\u5bfb\u5740Kubernetes\u5bf9\u8c61\u3002 \u5982\u679c\u6ca1\u6709\u60f3\u8981\u8fde\u63a5\u5230SinkBinding\u5bf9\u8c61\u7684\u73b0\u6709\u63a5\u6536\u5668\uff0c\u8bf7\u521b\u5efaKnative\u670d\u52a1\u3002 Note \u8981\u521b\u5efaKnative\u670d\u52a1\uff0c\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u670d\u52a1\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efaKnative\u670d\u52a1: kn service create <app-name> --image <image-url> Where: <app-name> \u5b83\u662f\u5e94\u7528\u7a0b\u5e8f\u7684\u540d\u79f0\u3002 <image-url> \u5b83\u662f\u56fe\u50cf\u5bb9\u5668\u7684URL\u3002 \u4f8b\u5982: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3aKnative\u670d\u52a1\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> \u5b83\u662f\u5e94\u7528\u7a0b\u5e8f\u7684\u540d\u79f0\u3002\u4f8b\u5982, event-display . <image-url> \u662f\u56fe\u50cf\u5bb9\u5668\u7684URL\u3002\u4f8b\u5982, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2a\u4e3b\u9898 \u00b6 \u4e3b\u9898\u5fc5\u987b\u662fPodSpecable\u8d44\u6e90\u3002 \u4f60\u53ef\u4ee5\u5728\u4f60\u7684\u96c6\u7fa4\u4e2d\u4f7f\u7528\u4efb\u4f55PodSpecable\u8d44\u6e90\uff0c\u4f8b\u5982: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev \u5982\u679c\u6ca1\u6709\u60f3\u8981\u4f7f\u7528\u7684\u73b0\u6709PodSpecable\u4e3b\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efaCronJob\u5bf9\u8c61\u4f5c\u4e3a\u4e3b\u9898\u3002 \u4e0b\u9762\u7684CronJob\u521b\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9 K_SINK \u7684\u4e91\u4e8b\u4ef6\uff0c\u5e76\u6dfb\u52a0\u4e86 CE_OVERRIDES \u7ed9\u51fa\u7684\u4efb\u4f55\u989d\u5916\u8986\u76d6\u3002 \u4e3aCronJob\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b: apiVersion : batch/v1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2aSinkBinding\u5bf9\u8c61 \u00b6 \u521b\u5efa\u4e00\u4e2a SinkBinding \u5bf9\u8c61\uff0c\u5c06\u4e8b\u4ef6\u4ece\u4e3b\u9898\u5f15\u5bfc\u5230\u63a5\u6536\u5668\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a SinkBinding \u5bf9\u8c61: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684SinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u60a8\u4e3a\u8981\u4f7f\u7528\u7684SinkBinding\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 <subject> \u5b83\u662f\u8fde\u63a5\u7684\u4e3b\u4f53\u3002\u4f8b\u5b50: Job:batch/v1:app=heartbeat-cron \u5b83\u5339\u914d\u547d\u540d\u7a7a\u95f4\u4e2d\u6240\u6709\u6807\u7b7e\u4e3a app=heartbeat-cron \u7684\u4f5c\u4e1a\u3002 Deployment:apps/v1:myapp \u5b83\u5339\u914d\u547d\u540d\u7a7a\u95f4\u4e2d\u540d\u4e3a myapp \u7684\u90e8\u7f72\u3002 Service:serving.knative.dev/v1:hello \u5b83\u5339\u914d\u540d\u4e3a hello \u7684\u670d\u52a1\u3002 <sink> \u5b83\u662f\u8fde\u63a5\u7684\u63a5\u6536\u5668\u3002\u4f8b\u5982 http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> \u5f62\u5f0f\u662f key=value . Cloud Event\u8986\u76d6\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\uff0c\u5e76\u5728\u53d1\u9001\u4e8b\u4ef6\u4e4b\u524d\u5e94\u7528\u3002 \u60a8\u53ef\u4ee5\u591a\u6b21\u63d0\u4f9b\u6b64\u6807\u5fd7\u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u4f8b\u5982: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a SinkBinding \u5bf9\u8c61\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684SinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u3002\u4f8b\u5982, bind-heartbeat . <api-version> \u5b83\u662f\u8be5\u4e3b\u9898\u7684API\u7248\u672c\u3002\u4f8b\u5982 batch/v1 . <kind> \u8fd9\u662f\u4f60\u7684\u4e3b\u9898\u3002\u4f8b\u5982 Job . <label-key>: <label-value> \u5b83\u662f\u952e\u503c\u5bf9\u7684\u6620\u5c04\uff0c\u7528\u4e8e\u9009\u62e9\u5177\u6709\u5339\u914d\u6807\u7b7e\u7684\u4e3b\u9898\u3002\u4f8b\u5982, app: heartbeat-cron \u5b83\u4f1a\u9009\u62e9\u4efb\u4f55\u5e26\u6709 app: heartbeat-cron \u6807\u7b7e\u7684\u4e3b\u9898. <sink> \u5b83\u662f\u8fde\u63a5\u7684\u6c34\u69fd\u3002\u4f8b\u5982 event-display . \u6709\u5173\u53ef\u4ee5\u4e3aSinkBinding\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Sink Binding Reference .\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u9a8c\u8bc1SinkBinding\u5bf9\u8c61 \u00b6 \u901a\u8fc7\u67e5\u770b\u60a8\u7684\u63a5\u6536\u5668\u7684\u670d\u52a1\u65e5\u5fd7\uff0c\u9a8c\u8bc1\u6d88\u606f\u88ab\u53d1\u9001\u5230Knative\u4e8b\u4ef6\u7cfb\u7edf: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m \u4ece\u8f93\u51fa\u4e2d\uff0c\u89c2\u5bdf\u663e\u793a\u7531\u6e90\u53d1\u9001\u5230\u663e\u793a\u51fd\u6570\u7684\u4e8b\u4ef6\u6d88\u606f\u7684\u8bf7\u6c42\u5934\u548c\u6d88\u606f\u4f53\u7684\u884c\u3002\u4f8b\u5982: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" } \u5220\u9664\u4e00\u4e2aSinkBinding \u00b6 \u8981\u5220\u9664\u547d\u540d\u7a7a\u95f4\u4e2d\u7684SinkBinding\u5bf9\u8c61\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u547d\u540d\u7a7a\u95f4: kubectl delete namespace <namespace> \u5176\u4e2d <namespace> \u662f\u5305\u542bSinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#sinkbinding","text":"\u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efaSinkBinding\u5bf9\u8c61\u3002 SinkBinding\u5c06\u63a5\u6536\u5668\u89e3\u6790\u4e3aURI\uff0c\u5728\u73af\u5883\u53d8\u91cf K_SINK \u4e2d\u8bbe\u7f6eURI\uff0c\u5e76\u4f7f\u7528 K_SINK \u5c06URI\u6dfb\u52a0\u5230\u4e3b\u9898\u4e2d\u3002 \u5982\u679cURI\u53d1\u751f\u53d8\u5316\uff0cSinkBinding\u4f1a\u66f4\u65b0 K_SINK \u7684\u503c\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u63a5\u6536\u5668\u662fKnative\u670d\u52a1\uff0c\u800c\u4e3b\u9898\u662fCronJob\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u4e3b\u9898\u548c\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002","title":"\u521b\u5efa\u4e00\u4e2aSinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#_1","text":"\u5728\u521b\u5efaSinkBinding\u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u4e8b\u4ef6\u5904\u7406\u3002 \u53ef\u9009:\u5982\u679c\u4f60\u60f3\u5728SinkBinding\u4e2d\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u547d\u4ee4\u884c\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#sinkbinding_1","text":"SinkBinding\u5bf9\u8c61\u4ee5\u4e24\u79cd\u6a21\u5f0f\u4e4b\u4e00\u64cd\u4f5c: exclusion or inclusion \u3002 \u9ed8\u8ba4\u6a21\u5f0f\u4e3a exclusion \u3002 \u5728\u6392\u9664\u6a21\u5f0f\u4e0b\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e3a\u547d\u540d\u7a7a\u95f4\u542f\u7528\u4e86SinkBinding\u884c\u4e3a\u3002 \u4e3a\u4e86\u7981\u6b62\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u88ab\u8bc4\u4f30\u4e3a\u7a81\u53d8\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 bindings.knative.dev/exclude: true \u6807\u7b7e\u6765\u6392\u9664\u5b83\u3002 \u5728\u5305\u542b\u6a21\u5f0f\u4e2d\uff0c\u6ca1\u6709\u4e3a\u547d\u540d\u7a7a\u95f4\u542f\u7528SinkBinding\u884c\u4e3a\u3002 \u5728\u5bf9\u547d\u540d\u7a7a\u95f4\u8fdb\u884c\u53d8\u5f02\u8bc4\u4f30\u4e4b\u524d\uff0c\u5fc5\u987b\u4f7f\u7528\u6807\u7b7e bindings.knative.dev/include: true \u663e\u5f0f\u5305\u542b\u5b83\u3002 \u8bbe\u7f6eSinkBinding\u5bf9\u8c61\u4e3a\u5305\u542b\u6a21\u5f0f\u3002 \u5c06 SINK_BINDING_SELECTION_MODE \u7684\u503c\u4ece exclusion \u6539\u4e3a inclusion \uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion \u8981\u9a8c\u8bc1 SINK_BINDING_SELECTION_MODE \u6309\u9700\u8981\u8bbe\u7f6e\uff0c\u6267\u884c: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING","title":"\u53ef\u9009:\u9009\u62e9SinkBinding\u547d\u540d\u7a7a\u95f4\u9009\u62e9\u884c\u4e3a"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#_2","text":"\u5982\u679c\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\uff0c\u8bf7\u4e3aSinkBinding\u5bf9\u8c61\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4: kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u5e0c\u671bSinkBinding\u4f7f\u7528\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u4f8b\u5982, sinkbinding-example . Note \u5982\u679c\u9009\u62e9\u4e86\u5305\u542b\u6a21\u5f0f\uff0c\u5219\u5fc5\u987b\u5c06 bindings.knative.dev/include: true \u6807\u7b7e\u6dfb\u52a0\u5230\u547d\u540d\u7a7a\u95f4\uff0c\u4ee5\u542f\u7528SinkBinding\u884c\u4e3a\u3002","title":"\u521b\u5efa\u547d\u540d\u7a7a\u95f4"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#_3","text":"\u63a5\u6536\u5668\u53ef\u4ee5\u662f\u4efb\u4f55\u53ef\u4ee5\u63a5\u6536\u4e8b\u4ef6\u7684\u53ef\u5bfb\u5740Kubernetes\u5bf9\u8c61\u3002 \u5982\u679c\u6ca1\u6709\u60f3\u8981\u8fde\u63a5\u5230SinkBinding\u5bf9\u8c61\u7684\u73b0\u6709\u63a5\u6536\u5668\uff0c\u8bf7\u521b\u5efaKnative\u670d\u52a1\u3002 Note \u8981\u521b\u5efaKnative\u670d\u52a1\uff0c\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u670d\u52a1\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efaKnative\u670d\u52a1: kn service create <app-name> --image <image-url> Where: <app-name> \u5b83\u662f\u5e94\u7528\u7a0b\u5e8f\u7684\u540d\u79f0\u3002 <image-url> \u5b83\u662f\u56fe\u50cf\u5bb9\u5668\u7684URL\u3002 \u4f8b\u5982: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3aKnative\u670d\u52a1\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> \u5b83\u662f\u5e94\u7528\u7a0b\u5e8f\u7684\u540d\u79f0\u3002\u4f8b\u5982, event-display . <image-url> \u662f\u56fe\u50cf\u5bb9\u5668\u7684URL\u3002\u4f8b\u5982, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u5668"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#_4","text":"\u4e3b\u9898\u5fc5\u987b\u662fPodSpecable\u8d44\u6e90\u3002 \u4f60\u53ef\u4ee5\u5728\u4f60\u7684\u96c6\u7fa4\u4e2d\u4f7f\u7528\u4efb\u4f55PodSpecable\u8d44\u6e90\uff0c\u4f8b\u5982: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev \u5982\u679c\u6ca1\u6709\u60f3\u8981\u4f7f\u7528\u7684\u73b0\u6709PodSpecable\u4e3b\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efaCronJob\u5bf9\u8c61\u4f5c\u4e3a\u4e3b\u9898\u3002 \u4e0b\u9762\u7684CronJob\u521b\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9 K_SINK \u7684\u4e91\u4e8b\u4ef6\uff0c\u5e76\u6dfb\u52a0\u4e86 CE_OVERRIDES \u7ed9\u51fa\u7684\u4efb\u4f55\u989d\u5916\u8986\u76d6\u3002 \u4e3aCronJob\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b: apiVersion : batch/v1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a\u4e3b\u9898"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#sinkbinding_2","text":"\u521b\u5efa\u4e00\u4e2a SinkBinding \u5bf9\u8c61\uff0c\u5c06\u4e8b\u4ef6\u4ece\u4e3b\u9898\u5f15\u5bfc\u5230\u63a5\u6536\u5668\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a SinkBinding \u5bf9\u8c61: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684SinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u3002 <namespace> \u5b83\u662f\u60a8\u4e3a\u8981\u4f7f\u7528\u7684SinkBinding\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 <subject> \u5b83\u662f\u8fde\u63a5\u7684\u4e3b\u4f53\u3002\u4f8b\u5b50: Job:batch/v1:app=heartbeat-cron \u5b83\u5339\u914d\u547d\u540d\u7a7a\u95f4\u4e2d\u6240\u6709\u6807\u7b7e\u4e3a app=heartbeat-cron \u7684\u4f5c\u4e1a\u3002 Deployment:apps/v1:myapp \u5b83\u5339\u914d\u547d\u540d\u7a7a\u95f4\u4e2d\u540d\u4e3a myapp \u7684\u90e8\u7f72\u3002 Service:serving.knative.dev/v1:hello \u5b83\u5339\u914d\u540d\u4e3a hello \u7684\u670d\u52a1\u3002 <sink> \u5b83\u662f\u8fde\u63a5\u7684\u63a5\u6536\u5668\u3002\u4f8b\u5982 http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> \u5f62\u5f0f\u662f key=value . Cloud Event\u8986\u76d6\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\uff0c\u5e76\u5728\u53d1\u9001\u4e8b\u4ef6\u4e4b\u524d\u5e94\u7528\u3002 \u60a8\u53ef\u4ee5\u591a\u6b21\u63d0\u4f9b\u6b64\u6807\u5fd7\u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u4f8b\u5982: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u4e3a SinkBinding \u5bf9\u8c61\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684SinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u3002\u4f8b\u5982, bind-heartbeat . <api-version> \u5b83\u662f\u8be5\u4e3b\u9898\u7684API\u7248\u672c\u3002\u4f8b\u5982 batch/v1 . <kind> \u8fd9\u662f\u4f60\u7684\u4e3b\u9898\u3002\u4f8b\u5982 Job . <label-key>: <label-value> \u5b83\u662f\u952e\u503c\u5bf9\u7684\u6620\u5c04\uff0c\u7528\u4e8e\u9009\u62e9\u5177\u6709\u5339\u914d\u6807\u7b7e\u7684\u4e3b\u9898\u3002\u4f8b\u5982, app: heartbeat-cron \u5b83\u4f1a\u9009\u62e9\u4efb\u4f55\u5e26\u6709 app: heartbeat-cron \u6807\u7b7e\u7684\u4e3b\u9898. <sink> \u5b83\u662f\u8fde\u63a5\u7684\u6c34\u69fd\u3002\u4f8b\u5982 event-display . \u6709\u5173\u53ef\u4ee5\u4e3aSinkBinding\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Sink Binding Reference .\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2aSinkBinding\u5bf9\u8c61"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#sinkbinding_3","text":"\u901a\u8fc7\u67e5\u770b\u60a8\u7684\u63a5\u6536\u5668\u7684\u670d\u52a1\u65e5\u5fd7\uff0c\u9a8c\u8bc1\u6d88\u606f\u88ab\u53d1\u9001\u5230Knative\u4e8b\u4ef6\u7cfb\u7edf: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m \u4ece\u8f93\u51fa\u4e2d\uff0c\u89c2\u5bdf\u663e\u793a\u7531\u6e90\u53d1\u9001\u5230\u663e\u793a\u51fd\u6570\u7684\u4e8b\u4ef6\u6d88\u606f\u7684\u8bf7\u6c42\u5934\u548c\u6d88\u606f\u4f53\u7684\u884c\u3002\u4f8b\u5982: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" }","title":"\u9a8c\u8bc1SinkBinding\u5bf9\u8c61"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#sinkbinding_4","text":"\u8981\u5220\u9664\u547d\u540d\u7a7a\u95f4\u4e2d\u7684SinkBinding\u5bf9\u8c61\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u547d\u540d\u7a7a\u95f4: kubectl delete namespace <namespace> \u5176\u4e2d <namespace> \u662f\u5305\u542bSinkBinding\u5bf9\u8c61\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002","title":"\u5220\u9664\u4e00\u4e2aSinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/reference/","text":"SinkBinding \u53c2\u8003 \u00b6 \u672c\u4e3b\u9898\u63d0\u4f9b\u5173\u4e8eSinkBinding\u5bf9\u8c61\u53ef\u914d\u7f6e\u53c2\u6570\u7684\u53c2\u8003\u4fe1\u606f\u3002 \u652f\u6301\u53c2\u6570 \u00b6 SinkBinding \u8d44\u6e90\u652f\u6301\u4ee5\u4e0b\u53c2\u6570: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b \u6216 \u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u8be5\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3a SinkBinding \u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6 SinkBinding \u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a name \u3002 \u5fc5\u987b spec \u6307\u5b9a\u6b64 SinkBinding \u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.subject \u5bf9\u201c\u8fd0\u884c\u65f6\u5951\u7ea6\u201d\u901a\u8fc7\u7ed1\u5b9a\u5b9e\u73b0\u589e\u5f3a\u7684\u8d44\u6e90\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009 \u4e3b\u9898\u53c2\u6570 \u00b6 Subject\u53c2\u6570\u5f15\u7528\u201c\u8fd0\u884c\u65f6\u5951\u7ea6\u201d\u901a\u8fc7\u7ed1\u5b9a\u5b9e\u73b0\u589e\u5f3a\u7684\u8d44\u6e90\u3002 subject \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b or \u53ef\u9009 apiVersion \u5f15\u7528\u7684API\u7248\u672c\u3002 \u5fc5\u987b kind Kind of the referent. \u5fc5\u987b namespace \u5f15\u7528\u5bf9\u8c61\u7684\u547d\u540d\u7a7a\u95f4\u3002\u5982\u679c\u7701\u7565\uff0c\u9ed8\u8ba4\u4e3a\u4fdd\u5b58\u5b83\u7684\u5bf9\u8c61\u3002 \u53ef\u9009 name \u63a8\u8350\u4eba\u7684\u540d\u5b57\u3002 \u5982\u679c\u4f60\u914d\u7f6e\u4e86 selector \uff0c\u8bf7\u4e0d\u8981\u4f7f\u7528\u3002 selector \u5f15\u7528\u5bf9\u8c61\u7684\u9009\u62e9\u5668\u3002 \u5982\u679c\u914d\u7f6e\u4e86 name \uff0c\u8bf7\u4e0d\u8981\u4f7f\u7528\u3002 selector.matchExpressions \u6807\u7b7e\u9009\u62e9\u5668\u8981\u6c42\u7684\u5217\u8868\u3002\u8981\u6c42\u662f ANDed\u3002 \u4f7f\u7528 matchExpressions or matchLabels \u4e2d\u7684\u4e00\u4e2a selector.matchExpressions.key \u9009\u62e9\u5668\u5e94\u7528\u7684\u6807\u7b7e\u952e\u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchExpressions.operator \u8868\u793a\u952e\u4e0e\u4e00\u7ec4\u503c\u7684\u5173\u7cfb\u3002\u6709\u6548\u7684\u64cd\u4f5c\u7b26\u662f In , NotIn , Exists and DoesNotExist \u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchExpressions.values \u5b57\u7b26\u4e32\u503c\u7684\u6570\u7ec4\u3002\u5982\u679c operator \u4e3a In or NotIn \uff0c\u503c\u6570\u7ec4\u5fc5\u987b\u975e\u7a7a\u3002\u5982\u679c operator \u662f Exists or DoesNotExist \uff0c\u503c\u6570\u7ec4\u5fc5\u987b\u4e3a\u7a7a\u3002\u5728\u7b56\u7565\u5408\u5e76\u8865\u4e01\u671f\u95f4\u66ff\u6362\u6b64\u6570\u7ec4\u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchLabels \u952e\u503c\u5bf9\u7684\u6620\u5c04\u3002 matchLabels \u6620\u5c04\u4e2d\u7684\u6bcf\u4e2a\u952e-\u503c\u5bf9\u76f8\u5f53\u4e8e matchExpressions \u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u5176\u4e2d\u952e\u5b57\u6bb5\u662f matchLabels.<key> \uff0c operator \u662f In \uff0c values \u6570\u7ec4\u53ea\u5305\u542b\"matchLabels. \"\u3002\u8981\u6c42\u662f ANDed\u3002 \u4f7f\u7528 matchExpressions or matchLabels \u4e2d\u7684\u4e00\u4e2a \u4e3b\u9898\u53c2\u6570\u793a\u4f8b \u00b6 \u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u547d\u540d\u4e3a mysubject \u7684 Deployment \u88ab\u9009\u4e2d: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... \u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u9009\u62e9\u4efb\u4f55\u5e26\u6709 working=example \u6807\u7b7e\u7684 Job : apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job namespace : default selector : matchLabels : working : example ... \u7ed9\u5b9a\u4ee5\u4e0bYAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u9009\u62e9\u4efb\u4f55\u5e26\u6709 working=example or working=sample \u6807\u7b7e\u7684 Pod : apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ... CloudEvent Overrides (\u8986\u76d6) \u00b6 CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 ceOverrides \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b or \u53ef\u9009 extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. \u53ef\u9009 Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. CloudEvent Overrides \u4e3e\u4f8b \u00b6 apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u5bf9\u8c61\u53c2\u8003"},{"location":"eventing/custom-event-source/sinkbinding/reference/#sinkbinding","text":"\u672c\u4e3b\u9898\u63d0\u4f9b\u5173\u4e8eSinkBinding\u5bf9\u8c61\u53ef\u914d\u7f6e\u53c2\u6570\u7684\u53c2\u8003\u4fe1\u606f\u3002","title":"SinkBinding \u53c2\u8003"},{"location":"eventing/custom-event-source/sinkbinding/reference/#_1","text":"SinkBinding \u8d44\u6e90\u652f\u6301\u4ee5\u4e0b\u53c2\u6570: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b \u6216 \u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u8be5\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3a SinkBinding \u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6 SinkBinding \u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a name \u3002 \u5fc5\u987b spec \u6307\u5b9a\u6b64 SinkBinding \u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.subject \u5bf9\u201c\u8fd0\u884c\u65f6\u5951\u7ea6\u201d\u901a\u8fc7\u7ed1\u5b9a\u5b9e\u73b0\u589e\u5f3a\u7684\u8d44\u6e90\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009","title":"\u652f\u6301\u53c2\u6570"},{"location":"eventing/custom-event-source/sinkbinding/reference/#_2","text":"Subject\u53c2\u6570\u5f15\u7528\u201c\u8fd0\u884c\u65f6\u5951\u7ea6\u201d\u901a\u8fc7\u7ed1\u5b9a\u5b9e\u73b0\u589e\u5f3a\u7684\u8d44\u6e90\u3002 subject \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b or \u53ef\u9009 apiVersion \u5f15\u7528\u7684API\u7248\u672c\u3002 \u5fc5\u987b kind Kind of the referent. \u5fc5\u987b namespace \u5f15\u7528\u5bf9\u8c61\u7684\u547d\u540d\u7a7a\u95f4\u3002\u5982\u679c\u7701\u7565\uff0c\u9ed8\u8ba4\u4e3a\u4fdd\u5b58\u5b83\u7684\u5bf9\u8c61\u3002 \u53ef\u9009 name \u63a8\u8350\u4eba\u7684\u540d\u5b57\u3002 \u5982\u679c\u4f60\u914d\u7f6e\u4e86 selector \uff0c\u8bf7\u4e0d\u8981\u4f7f\u7528\u3002 selector \u5f15\u7528\u5bf9\u8c61\u7684\u9009\u62e9\u5668\u3002 \u5982\u679c\u914d\u7f6e\u4e86 name \uff0c\u8bf7\u4e0d\u8981\u4f7f\u7528\u3002 selector.matchExpressions \u6807\u7b7e\u9009\u62e9\u5668\u8981\u6c42\u7684\u5217\u8868\u3002\u8981\u6c42\u662f ANDed\u3002 \u4f7f\u7528 matchExpressions or matchLabels \u4e2d\u7684\u4e00\u4e2a selector.matchExpressions.key \u9009\u62e9\u5668\u5e94\u7528\u7684\u6807\u7b7e\u952e\u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchExpressions.operator \u8868\u793a\u952e\u4e0e\u4e00\u7ec4\u503c\u7684\u5173\u7cfb\u3002\u6709\u6548\u7684\u64cd\u4f5c\u7b26\u662f In , NotIn , Exists and DoesNotExist \u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchExpressions.values \u5b57\u7b26\u4e32\u503c\u7684\u6570\u7ec4\u3002\u5982\u679c operator \u4e3a In or NotIn \uff0c\u503c\u6570\u7ec4\u5fc5\u987b\u975e\u7a7a\u3002\u5982\u679c operator \u662f Exists or DoesNotExist \uff0c\u503c\u6570\u7ec4\u5fc5\u987b\u4e3a\u7a7a\u3002\u5728\u7b56\u7565\u5408\u5e76\u8865\u4e01\u671f\u95f4\u66ff\u6362\u6b64\u6570\u7ec4\u3002 \u5982\u679c\u4f7f\u7528 matchExpressions \u5fc5\u987b selector.matchLabels \u952e\u503c\u5bf9\u7684\u6620\u5c04\u3002 matchLabels \u6620\u5c04\u4e2d\u7684\u6bcf\u4e2a\u952e-\u503c\u5bf9\u76f8\u5f53\u4e8e matchExpressions \u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u5176\u4e2d\u952e\u5b57\u6bb5\u662f matchLabels.<key> \uff0c operator \u662f In \uff0c values \u6570\u7ec4\u53ea\u5305\u542b\"matchLabels. \"\u3002\u8981\u6c42\u662f ANDed\u3002 \u4f7f\u7528 matchExpressions or matchLabels \u4e2d\u7684\u4e00\u4e2a","title":"\u4e3b\u9898\u53c2\u6570"},{"location":"eventing/custom-event-source/sinkbinding/reference/#_3","text":"\u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u547d\u540d\u4e3a mysubject \u7684 Deployment \u88ab\u9009\u4e2d: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... \u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u9009\u62e9\u4efb\u4f55\u5e26\u6709 working=example \u6807\u7b7e\u7684 Job : apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job namespace : default selector : matchLabels : working : example ... \u7ed9\u5b9a\u4ee5\u4e0bYAML\uff0c\u5728 default \u547d\u540d\u7a7a\u95f4\u4e2d\u9009\u62e9\u4efb\u4f55\u5e26\u6709 working=example or working=sample \u6807\u7b7e\u7684 Pod : apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ...","title":"\u4e3b\u9898\u53c2\u6570\u793a\u4f8b"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides","text":"CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 ceOverrides \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b or \u53ef\u9009 extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. \u53ef\u9009 Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides (\u8986\u76d6)"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides_1","text":"apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"CloudEvent Overrides \u4e3e\u4f8b"},{"location":"eventing/experimental-features/","text":"\u4e8b\u4ef6\u5b9e\u9a8c\u7279\u5f81 \u00b6 \u4e3a\u4e86\u4fdd\u6301Knative\u7684\u521b\u65b0\u6027\uff0c\u8fd9\u4e2a\u9879\u76ee\u7684\u7ef4\u62a4\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a \u5b9e\u9a8c\u7279\u6027\u8fc7\u7a0b \uff0c\u5141\u8bb8\u7528\u6237\u4ea4\u4ed8\u548c\u6d4b\u8bd5\u65b0\u7684\u5b9e\u9a8c\u7279\u6027\uff0c\u800c\u4e0d\u5f71\u54cd\u6838\u5fc3\u9879\u76ee\u7684\u7a33\u5b9a\u6027\u3002 Warning \u5b9e\u9a8c\u7279\u6027\u662f\u4e0d\u7a33\u5b9a\u7684\uff0c\u53ef\u80fd\u4f1a\u5728\u60a8\u7684Knative\u8bbe\u7f6e\u751a\u81f3\u96c6\u7fa4\u8bbe\u7f6e\u4e2d\u5bfc\u81f4\u95ee\u9898\u3002 \u8fd9\u4e9b\u7279\u6027\u5e94\u8be5\u8c28\u614e\u4f7f\u7528\uff0c\u6c38\u8fdc\u4e0d\u5e94\u8be5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002 \u6709\u5173\u4e0d\u540c\u5f00\u53d1\u9636\u6bb5\u7279\u6027\u7684\u8d28\u91cf\u4fdd\u8bc1\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7279\u6027\u9636\u6bb5\u5b9a\u4e49 \u6587\u6863\u3002 \u672c\u6587\u6863\u89e3\u91ca\u4e86\u5982\u4f55\u542f\u7528\u5b9e\u9a8c\u7279\u6027\u4ee5\u53ca\u54ea\u4e9b\u7279\u6027\u73b0\u5728\u53ef\u7528\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 You must have a Knative cluster running with Knative Eventing installed. \u5b9e\u9a8c\u7279\u6027\u914d\u7f6e \u00b6 When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled \u53ef\u7528\u7684\u5b9e\u9a8c\u7279\u6027 \u00b6 \u4e0b\u8868\u6982\u8ff0\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e2d\u53ef\u7528\u7684\u5b9e\u9a8c\u7279\u6027: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter \u5728\u8ba1\u7b97\u91cd\u8bd5 429 \u548c 503 \u54cd\u5e94\u7684\u56de\u9000\u65f6\u95f4\u65f6\uff0c\u6307\u5b9a\u8986\u76d6HTTP Retry-After \u62a5\u5934\u7684\u6700\u5927\u91cd\u8bd5\u6301\u7eed\u65f6\u95f4\u3002 Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout \u5f53\u4f7f\u7528' delivery '\u89c4\u8303\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u53c2\u6570\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528' timeout '\u5b57\u6bb5\u6307\u5b9a\u6bcf\u4e2a\u53d1\u9001\u7684HTTP\u8bf7\u6c42\u7684\u8d85\u65f6\u65f6\u95f4\u3002 Alpha, disabled by default KReference.Group field kreference-group \u6307\u5b9a\u4e0d\u5e26API\u7248\u672c\u7684 KReference \u8d44\u6e90\u7684API group \u3002 Alpha, disabled by default Knative\u53c2\u8003\u6620\u5c04 kreference-mapping \u63d0\u4f9b\u4ece Knative\u5f15\u7528 \u5230\u6a21\u677f\u5316URI\u7684\u6620\u5c04\u3002 Alpha, disabled by default \u65b0\u7684\u89e6\u53d1\u8fc7\u6ee4\u5668 new-trigger-filters \u542f\u7528\u4e00\u4e2a\u65b0\u7684\u89e6\u53d1\u5668 filters \u5b57\u6bb5\uff0c\u8be5\u5b57\u6bb5\u652f\u6301\u4e00\u7ec4\u529f\u80fd\u5f3a\u5927\u7684\u8fc7\u6ee4\u5668\u8868\u8fbe\u5f0f\u3002 Alpha, disabled by default \u4e25\u683c\u7684\u8ba2\u9605\u7528\u6237 strict-subscriber \u5982\u679c\u672a\u5b9a\u4e49\u5b57\u6bb5 spec.subscriber \uff0c\u5219\u4f7f\u8ba2\u9605\u65e0\u6548\u3002 Alpha, disabled by default","title":"\u5173\u4e8e\u4e8b\u4ef6\u5b9e\u9a8c\u529f\u80fd"},{"location":"eventing/experimental-features/#_1","text":"\u4e3a\u4e86\u4fdd\u6301Knative\u7684\u521b\u65b0\u6027\uff0c\u8fd9\u4e2a\u9879\u76ee\u7684\u7ef4\u62a4\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a \u5b9e\u9a8c\u7279\u6027\u8fc7\u7a0b \uff0c\u5141\u8bb8\u7528\u6237\u4ea4\u4ed8\u548c\u6d4b\u8bd5\u65b0\u7684\u5b9e\u9a8c\u7279\u6027\uff0c\u800c\u4e0d\u5f71\u54cd\u6838\u5fc3\u9879\u76ee\u7684\u7a33\u5b9a\u6027\u3002 Warning \u5b9e\u9a8c\u7279\u6027\u662f\u4e0d\u7a33\u5b9a\u7684\uff0c\u53ef\u80fd\u4f1a\u5728\u60a8\u7684Knative\u8bbe\u7f6e\u751a\u81f3\u96c6\u7fa4\u8bbe\u7f6e\u4e2d\u5bfc\u81f4\u95ee\u9898\u3002 \u8fd9\u4e9b\u7279\u6027\u5e94\u8be5\u8c28\u614e\u4f7f\u7528\uff0c\u6c38\u8fdc\u4e0d\u5e94\u8be5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002 \u6709\u5173\u4e0d\u540c\u5f00\u53d1\u9636\u6bb5\u7279\u6027\u7684\u8d28\u91cf\u4fdd\u8bc1\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7279\u6027\u9636\u6bb5\u5b9a\u4e49 \u6587\u6863\u3002 \u672c\u6587\u6863\u89e3\u91ca\u4e86\u5982\u4f55\u542f\u7528\u5b9e\u9a8c\u7279\u6027\u4ee5\u53ca\u54ea\u4e9b\u7279\u6027\u73b0\u5728\u53ef\u7528\u3002","title":"\u4e8b\u4ef6\u5b9e\u9a8c\u7279\u5f81"},{"location":"eventing/experimental-features/#_2","text":"You must have a Knative cluster running with Knative Eventing installed.","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/experimental-features/#_3","text":"When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled","title":"\u5b9e\u9a8c\u7279\u6027\u914d\u7f6e"},{"location":"eventing/experimental-features/#_4","text":"\u4e0b\u8868\u6982\u8ff0\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e2d\u53ef\u7528\u7684\u5b9e\u9a8c\u7279\u6027: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter \u5728\u8ba1\u7b97\u91cd\u8bd5 429 \u548c 503 \u54cd\u5e94\u7684\u56de\u9000\u65f6\u95f4\u65f6\uff0c\u6307\u5b9a\u8986\u76d6HTTP Retry-After \u62a5\u5934\u7684\u6700\u5927\u91cd\u8bd5\u6301\u7eed\u65f6\u95f4\u3002 Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout \u5f53\u4f7f\u7528' delivery '\u89c4\u8303\u914d\u7f6e\u4e8b\u4ef6\u4f20\u9012\u53c2\u6570\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528' timeout '\u5b57\u6bb5\u6307\u5b9a\u6bcf\u4e2a\u53d1\u9001\u7684HTTP\u8bf7\u6c42\u7684\u8d85\u65f6\u65f6\u95f4\u3002 Alpha, disabled by default KReference.Group field kreference-group \u6307\u5b9a\u4e0d\u5e26API\u7248\u672c\u7684 KReference \u8d44\u6e90\u7684API group \u3002 Alpha, disabled by default Knative\u53c2\u8003\u6620\u5c04 kreference-mapping \u63d0\u4f9b\u4ece Knative\u5f15\u7528 \u5230\u6a21\u677f\u5316URI\u7684\u6620\u5c04\u3002 Alpha, disabled by default \u65b0\u7684\u89e6\u53d1\u8fc7\u6ee4\u5668 new-trigger-filters \u542f\u7528\u4e00\u4e2a\u65b0\u7684\u89e6\u53d1\u5668 filters \u5b57\u6bb5\uff0c\u8be5\u5b57\u6bb5\u652f\u6301\u4e00\u7ec4\u529f\u80fd\u5f3a\u5927\u7684\u8fc7\u6ee4\u5668\u8868\u8fbe\u5f0f\u3002 Alpha, disabled by default \u4e25\u683c\u7684\u8ba2\u9605\u7528\u6237 strict-subscriber \u5982\u679c\u672a\u5b9a\u4e49\u5b57\u6bb5 spec.subscriber \uff0c\u5219\u4f7f\u8ba2\u9605\u65e0\u6548\u3002 Alpha, disabled by default","title":"\u53ef\u7528\u7684\u5b9e\u9a8c\u7279\u6027"},{"location":"eventing/experimental-features/delivery-retryafter/","text":"DeliverySpec.RetryAfterMax field \u00b6 Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-retryafter/#deliveryspecretryaftermax-field","text":"Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-timeout/","text":"DeliverySpec.Timeout field \u00b6 Flag name : delivery-timeout Stage : Beta, enabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/delivery-timeout/#deliveryspectimeout-field","text":"Flag name : delivery-timeout Stage : Beta, enabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/kreference-group/","text":"KReference.Group field \u00b6 Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-group/#kreferencegroup-field","text":"Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-mapping/","text":"Knative reference mapping \u00b6 Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative\u5f15\u7528\u6620\u5c04"},{"location":"eventing/experimental-features/kreference-mapping/#knative-reference-mapping","text":"Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative reference mapping"},{"location":"eventing/experimental-features/new-trigger-filters/","text":"New trigger filters \u00b6 Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204 Overview \u00b6 This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service About the filters field \u00b6 An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression. Supported filter dialects \u00b6 The filters field supports the following dialects: exact \u00b6 CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push prefix \u00b6 CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github. suffix \u00b6 CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created all \u00b6 All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec any \u00b6 At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec not \u00b6 The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push cesql \u00b6 The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" Conflict with the current filter field \u00b6 The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service FAQ \u00b6 Why add yet another field? Why not make the current filter field more robust? \u00b6 The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field. Why filters and not another name that wouldn't conflict with the filter field? \u00b6 We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"\u65b0\u89e6\u53d1\u8fc7\u6ee4\u5668"},{"location":"eventing/experimental-features/new-trigger-filters/#new-trigger-filters","text":"Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204","title":"New trigger filters"},{"location":"eventing/experimental-features/new-trigger-filters/#overview","text":"This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Overview"},{"location":"eventing/experimental-features/new-trigger-filters/#about-the-filters-field","text":"An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression.","title":"About the filters field"},{"location":"eventing/experimental-features/new-trigger-filters/#supported-filter-dialects","text":"The filters field supports the following dialects:","title":"Supported filter dialects"},{"location":"eventing/experimental-features/new-trigger-filters/#exact","text":"CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push","title":"exact"},{"location":"eventing/experimental-features/new-trigger-filters/#prefix","text":"CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github.","title":"prefix"},{"location":"eventing/experimental-features/new-trigger-filters/#suffix","text":"CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created","title":"suffix"},{"location":"eventing/experimental-features/new-trigger-filters/#all","text":"All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"all"},{"location":"eventing/experimental-features/new-trigger-filters/#any","text":"At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"any"},{"location":"eventing/experimental-features/new-trigger-filters/#not","text":"The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push","title":"not"},{"location":"eventing/experimental-features/new-trigger-filters/#cesql","text":"The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"","title":"cesql"},{"location":"eventing/experimental-features/new-trigger-filters/#conflict-with-the-current-filter-field","text":"The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Conflict with the current filter field"},{"location":"eventing/experimental-features/new-trigger-filters/#faq","text":"","title":"FAQ"},{"location":"eventing/experimental-features/new-trigger-filters/#why-add-yet-another-field-why-not-make-the-current-filter-field-more-robust","text":"The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field.","title":"Why add yet another field? Why not make the current filter field more robust?"},{"location":"eventing/experimental-features/new-trigger-filters/#why-filters-and-not-another-name-that-wouldnt-conflict-with-the-filter-field","text":"We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"Why filters and not another name that wouldn't conflict with the filter field?"},{"location":"eventing/experimental-features/strict-subscriber/","text":"Strict Subscriber \u00b6 Flag name : strict-subscriber Stage : Beta, enabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"\u4e25\u683c\u7684\u7528\u6237"},{"location":"eventing/experimental-features/strict-subscriber/#strict-subscriber","text":"Flag name : strict-subscriber Stage : Beta, enabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"Strict Subscriber"},{"location":"eventing/flows/","text":"\u4e8b\u4ef6\u6d41 \u00b6 Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u7ec4 \u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs) \uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u5b9a\u4e49\u4e8b\u4ef6\u6d41: Sequence \u7528\u4e8e\u5b9a\u4e49\u6309\u987a\u5e8f\u6392\u5217\u7684\u51fd\u6570\u5217\u8868\u3002 Parallel \u5b83\u7528\u4e8e\u5b9a\u4e49\u4e00\u4e2a\u5206\u652f\u5217\u8868\uff0c\u6bcf\u4e2a\u5206\u652f\u63a5\u6536\u76f8\u540c\u7684CloudEvent\u3002","title":"\u5173\u4e8e\u6d41"},{"location":"eventing/flows/#_1","text":"Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u7ec4 \u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs) \uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u5b9a\u4e49\u4e8b\u4ef6\u6d41: Sequence \u7528\u4e8e\u5b9a\u4e49\u6309\u987a\u5e8f\u6392\u5217\u7684\u51fd\u6570\u5217\u8868\u3002 Parallel \u5b83\u7528\u4e8e\u5b9a\u4e49\u4e00\u4e2a\u5206\u652f\u5217\u8868\uff0c\u6bcf\u4e2a\u5206\u652f\u63a5\u6536\u76f8\u540c\u7684CloudEvent\u3002","title":"\u4e8b\u4ef6\u6d41"},{"location":"eventing/flows/parallel/","text":"\u5e76\u884c \u00b6 \u5e76\u884cCRD\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bb9\u6613\u5b9a\u4e49\u5206\u652f\u5217\u8868\u7684\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5206\u652f\u63a5\u6536\u53d1\u9001\u5230\u5e76\u884c\u5165\u53e3\u901a\u9053\u7684\u76f8\u540c\u7684CloudEvent\u3002 \u901a\u5e38\uff0c\u6bcf\u4e2a\u5206\u652f\u90fd\u7531\u4e00\u4e2a\u8fc7\u6ee4\u5668\u51fd\u6570\u7ec4\u6210\uff0c\u4ee5\u4fdd\u62a4\u5206\u652f\u7684\u6267\u884c\u3002 \u5e76\u884c\u521b\u5efa Channel \u548c Subscription \u7684\u5e95\u5c42\u3002 \u4f7f\u7528 \u00b6 \u5e76\u884c\u7684\u89c4\u8303 \u00b6 \u5e76\u884c\u6709\u4e09\u4e2a\u90e8\u5206\u7684\u89c4\u683c: branches \u5b9a\u4e49\u4e86 filter \u548c subscriber \u5bf9\u7684\u5217\u8868\uff0c\u6bcf\u4e2a\u5206\u652f\u4e00\u4e2a\uff0c\u8fd8\u6709\u4e00\u4e2a\u53ef\u9009\u7684 reply \u5bf9\u8c61\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5206\u652f: (\u53ef\u9009) filter \u88ab\u6c42\u503c\uff0c\u5f53\u5b83\u8fd4\u56de\u4e00\u4e2a\u4e8b\u4ef6\u65f6 subscriber \u88ab\u6267\u884c\u3002 filter and subscriber \u90fd\u5fc5\u987b\u662f \u53ef\u5bfb\u5740 \u3002 subscriber \u8fd4\u56de\u7684\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u5206\u652f reply \u5bf9\u8c61\u3002 \u5f53 reply \u4e3a\u7a7a\u65f6\uff0c\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230 spec.reply \u5bf9\u8c61\u3002 (\u53ef\u9009) channelTemplate \u5b9a\u4e49\u4e86\u7528\u4e8e\u521b\u5efa Channel s\u7684\u6a21\u677f\u3002 (\u53ef\u9009) reply \u5b9a\u4e49\u4e86\u5f53\u5206\u652f\u6ca1\u6709\u81ea\u5df1\u7684 reply \u5bf9\u8c61\u65f6\uff0c\u6bcf\u4e2a\u5206\u652f\u7684\u7ed3\u679c\u88ab\u53d1\u9001\u5230\u54ea\u91cc\u3002 \u5e76\u884c\u72b6\u6001 \u00b6 \u5e76\u884c\u72b6\u6001\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206: conditions \uff0c\u8be6\u7ec6\u63cf\u8ff0\u5e76\u884c\u5bf9\u8c61\u7684\u6574\u4f53\u72b6\u6001 ingressChannelStatus and branchesStatuses \u4f20\u9012\u4f5c\u4e3a\u5e76\u884c\u7684\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42 Channel and Subscription \u8d44\u6e90\u7684\u72b6\u6001\u3002 address \u88ab\u516c\u5f00\uff0c\u4ee5\u4fbfparallel\u53ef\u4ee5\u5728\u53ef\u5bfb\u5740\u7684\u5730\u65b9\u4f7f\u7528\u3002 \u53d1\u9001\u5230\u6b64\u5730\u5740\u5c06\u9488\u5bf9\u8fd9\u4e2a\u5e76\u884c\u7684\u524d\u9762\u7684\u201c\u901a\u9053\u201d(\u4e0e ingressChannelStatus \u76f8\u540c)\u3002 \u4f8b\u5b50 \u00b6 \u6309\u7167 \u4ee3\u7801\u793a\u4f8b \u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u5e76\u884c.","title":"\u5e76\u884c"},{"location":"eventing/flows/parallel/#_1","text":"\u5e76\u884cCRD\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bb9\u6613\u5b9a\u4e49\u5206\u652f\u5217\u8868\u7684\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5206\u652f\u63a5\u6536\u53d1\u9001\u5230\u5e76\u884c\u5165\u53e3\u901a\u9053\u7684\u76f8\u540c\u7684CloudEvent\u3002 \u901a\u5e38\uff0c\u6bcf\u4e2a\u5206\u652f\u90fd\u7531\u4e00\u4e2a\u8fc7\u6ee4\u5668\u51fd\u6570\u7ec4\u6210\uff0c\u4ee5\u4fdd\u62a4\u5206\u652f\u7684\u6267\u884c\u3002 \u5e76\u884c\u521b\u5efa Channel \u548c Subscription \u7684\u5e95\u5c42\u3002","title":"\u5e76\u884c"},{"location":"eventing/flows/parallel/#_2","text":"","title":"\u4f7f\u7528"},{"location":"eventing/flows/parallel/#_3","text":"\u5e76\u884c\u6709\u4e09\u4e2a\u90e8\u5206\u7684\u89c4\u683c: branches \u5b9a\u4e49\u4e86 filter \u548c subscriber \u5bf9\u7684\u5217\u8868\uff0c\u6bcf\u4e2a\u5206\u652f\u4e00\u4e2a\uff0c\u8fd8\u6709\u4e00\u4e2a\u53ef\u9009\u7684 reply \u5bf9\u8c61\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5206\u652f: (\u53ef\u9009) filter \u88ab\u6c42\u503c\uff0c\u5f53\u5b83\u8fd4\u56de\u4e00\u4e2a\u4e8b\u4ef6\u65f6 subscriber \u88ab\u6267\u884c\u3002 filter and subscriber \u90fd\u5fc5\u987b\u662f \u53ef\u5bfb\u5740 \u3002 subscriber \u8fd4\u56de\u7684\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230\u5206\u652f reply \u5bf9\u8c61\u3002 \u5f53 reply \u4e3a\u7a7a\u65f6\uff0c\u4e8b\u4ef6\u88ab\u53d1\u9001\u5230 spec.reply \u5bf9\u8c61\u3002 (\u53ef\u9009) channelTemplate \u5b9a\u4e49\u4e86\u7528\u4e8e\u521b\u5efa Channel s\u7684\u6a21\u677f\u3002 (\u53ef\u9009) reply \u5b9a\u4e49\u4e86\u5f53\u5206\u652f\u6ca1\u6709\u81ea\u5df1\u7684 reply \u5bf9\u8c61\u65f6\uff0c\u6bcf\u4e2a\u5206\u652f\u7684\u7ed3\u679c\u88ab\u53d1\u9001\u5230\u54ea\u91cc\u3002","title":"\u5e76\u884c\u7684\u89c4\u8303"},{"location":"eventing/flows/parallel/#_4","text":"\u5e76\u884c\u72b6\u6001\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206: conditions \uff0c\u8be6\u7ec6\u63cf\u8ff0\u5e76\u884c\u5bf9\u8c61\u7684\u6574\u4f53\u72b6\u6001 ingressChannelStatus and branchesStatuses \u4f20\u9012\u4f5c\u4e3a\u5e76\u884c\u7684\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42 Channel and Subscription \u8d44\u6e90\u7684\u72b6\u6001\u3002 address \u88ab\u516c\u5f00\uff0c\u4ee5\u4fbfparallel\u53ef\u4ee5\u5728\u53ef\u5bfb\u5740\u7684\u5730\u65b9\u4f7f\u7528\u3002 \u53d1\u9001\u5230\u6b64\u5730\u5740\u5c06\u9488\u5bf9\u8fd9\u4e2a\u5e76\u884c\u7684\u524d\u9762\u7684\u201c\u901a\u9053\u201d(\u4e0e ingressChannelStatus \u76f8\u540c)\u3002","title":"\u5e76\u884c\u72b6\u6001"},{"location":"eventing/flows/parallel/#_5","text":"\u6309\u7167 \u4ee3\u7801\u793a\u4f8b \u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u5e76\u884c.","title":"\u4f8b\u5b50"},{"location":"eventing/flows/sequence/","text":"\u5e8f\u5217 \u00b6 \u5e8f\u5217CRD\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9a\u4e49\u5c06\u88ab\u8c03\u7528\u7684\u51fd\u6570\u7684\u6309\u987a\u5e8f\u5217\u8868\u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u53ef\u4ee5\u4fee\u6539\u3001\u7b5b\u9009\u6216\u521b\u5efa\u4e00\u79cd\u65b0\u7684\u4e8b\u4ef6\u7c7b\u578b\u3002 \u5e8f\u5217\u5728\u5e95\u5c42\u521b\u5efa\u201c\u9891\u9053\u201d\u548c\u201c\u8ba2\u9605\u201d\u3002 Info \u5e8f\u5217\u9700\u8981\"hairpin\"\u6d41\u91cf\u3002 \u8bf7\u9a8c\u8bc1\u60a8\u7684 Pod \u53ef\u4ee5\u901a\u8fc7\u670d\u52a1IP\u5230\u8fbe\u81ea\u5df1\u3002 \u5982\u679c\"hairpin\"\u6d41\u91cf\u4e0d\u53ef\u7528\uff0c\u60a8\u53ef\u4ee5\u8054\u7cfb\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406\u5458\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u96c6\u7fa4\u7ea7\u522b(\u901a\u5e38\u662fCNI)\u8bbe\u7f6e\u3002 \u4f7f\u7528 \u00b6 \u5e8f\u5217\u89c4\u8303 \u00b6 \u5e8f\u5217\u6709\u4e09\u4e2a\u90e8\u5206\u7684\u89c4\u8303: Steps \uff0c\u5b83\u5b9a\u4e49\u4e86 Subscriber \u7684\u6309\u987a\u5e8f\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u6309\u5217\u51fa\u7684\u987a\u5e8f\u6267\u884c\u7684\u51fd\u6570\u3002 \u8fd9\u4e9b\u662f\u4f7f\u7528 messaging.v1.SubscriberSpec \u6307\u5b9a\u7684\uff0c\u5c31\u50cf\u521b\u5efa Subscription \u65f6\u4e00\u6837\u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u5e94\u8be5\u662f Addressable \u3002 ChannelTemplate \u5b9a\u4e49\u4e86\u7528\u4e8e\u5728\u6b65\u9aa4\u4e4b\u95f4\u521b\u5efa Channel \u7684\u6a21\u677f\u3002 Reply (\u53ef\u9009)\u5f15\u7528\u5e8f\u5217\u4e2d\u6700\u540e\u4e00\u6b65\u7684\u7ed3\u679c\u88ab\u53d1\u9001\u5230\u7684\u4f4d\u7f6e\u3002 \u5e8f\u5217\u72b6\u6001 \u00b6 \u5e8f\u5217\u6709\u56db\u4e2a\u90e8\u5206\u7684\u72b6\u6001: \u8be6\u7ec6\u63cf\u8ff0\u5e8f\u5217\u5bf9\u8c61\u7684\u6574\u4f53\u72b6\u6001\u7684\u6761\u4ef6 ChannelStatuses \uff0c\u5b83\u4f20\u9012\u4f5c\u4e3a\u8be5\u5e8f\u5217\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42\u201c\u901a\u9053\u201d\u8d44\u6e90\u7684\u72b6\u6001 \u5b83\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u6bcf\u4e2aStatus\u5bf9\u5e94\u4e8eStep\u53f7\uff0c \u56e0\u6b64\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6761\u76ee\u662f\u7b2c\u4e00\u4e2aStep\u4e4b\u524d\u7684 Channel \u7684Status\u3002 SubscriptionStatuses \uff0c\u5b83\u4f20\u9012\u4f5c\u4e3a\u8be5\u5e8f\u5217\u7684\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42\u201c\u8ba2\u9605\u201d\u8d44\u6e90\u7684\u72b6\u6001\u3002 \u5b83\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u6bcf\u4e2aStatus\u5bf9\u5e94\u4e8eStep\u53f7\uff0c\u56e0\u6b64\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6761\u76ee\u662f\u201cSubscription\u201d\uff0c \u521b\u5efa\u5b83\u662f\u4e3a\u4e86\u5c06\u7b2c\u4e00\u4e2a\u901a\u9053\u8fde\u63a5\u5230\u201cSteps\u201d\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u3002 AddressStatus \u662f\u516c\u5f00\u7684\uff0c\u4ee5\u4fbfSequence\u53ef\u4ee5\u5728\u53ef\u4ee5\u4f7f\u7528Addressable\u7684\u5730\u65b9\u4f7f\u7528\u3002 \u53d1\u9001\u5230\u6b64\u5730\u5740\u5c06\u9488\u5bf9\u5e8f\u5217\u4e2d\u7b2c\u4e00\u6b65\u524d\u9762\u7684 Channel \u3002 \u4f8b\u5b50 \u00b6 \u5bf9\u4e8e\u4e0b\u9762\u7684\u6bcf\u4e2a\u793a\u4f8b\uff0c\u90fd\u4f7f\u7528 PingSource \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u3002 \u6211\u4eec\u8fd8\u4f7f\u7528\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684 \u8f6c\u6362\u5668 \uff0c\u5b83\u5bf9\u4f20\u5165\u7684\u4e8b\u4ef6\u6267\u884c\u975e\u5e38\u7b80\u5355\u7684\u8f6c\u6362\uff0c\u4ee5\u6f14\u793a\u5b83\u4eec\u5df2\u7ecf\u901a\u8fc7\u4e86\u6bcf\u4e2a\u9636\u6bb5\u3002 \u65e0\u5e94\u7b54\u5e8f\u5217 \u00b6 \u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u53c2\u89c1 \u65e0\u5e94\u7b54\u7684\u5e8f\u5217(\u7ec8\u7aef\u6700\u540e\u4e00\u6b65) . \u6709\u5e94\u7b54\u5e8f\u5217 \u00b6 \u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u7684\u76f8\u540c\u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u552f\u4e00\u7684\u533a\u522b\u662f\u6211\u4eec\u5c06\u4f7f\u7528 Subscriber.Spec.Reply \u5b57\u6bb5\u5c06\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa\u8fde\u63a5\u5230\u4e8b\u4ef6\u663e\u793aPod\u3002 \u53c2\u89c1 \u6709\u5e94\u7b54\u5e8f\u5217(\u6700\u540e\u4e00\u6b65\u4ea7\u751f\u8f93\u51fa) . \u5c06\u5e8f\u5217\u94fe\u63a5\u5728\u4e00\u8d77 \u00b6 \u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u7684\u76f8\u540c\u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u552f\u4e00\u7684\u533a\u522b\u662f\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 Subscriber.Spec.Reply \u5b57\u6bb5\u5c06\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa\u8fde\u63a5\u5230\u53e6\u4e00\u4e2a Sequence \uff0c\u8be5 Sequence \u6267\u884c\u4e0e\u7b2c\u4e00\u4e2a\u7ba1\u9053\u76f8\u540c\u7684\u6d88\u606f\u4fee\u6539(\u53ea\u662f\u6b65\u9aa4\u4e0d\u540c)\u3002 \u53c2\u89c1 \u5c06\u5e8f\u5217\u8fde\u63a5\u5728\u4e00\u8d77 . \u4f7f\u7528\u5e26\u6709\u4ee3\u7406/\u89e6\u53d1\u5668\u6a21\u578b\u7684\u5e8f\u5217 \u00b6 \u4f60\u4e5f\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u76ee\u6807\u4e3a Sequence \u7684\u89e6\u53d1\u5668\u3002 \u8fd9\u6b21\u6211\u4eec\u5c06\u8fde\u63a5 PingSource \u5c06\u4e8b\u4ef6\u53d1\u9001\u5230 Broker \uff0c\u7136\u540e\u6211\u4eec\u5c06\u8ba9 Sequence \u5c06\u4ea7\u751f\u7684\u4e8b\u4ef6\u53d1\u9001\u56deBroker\uff0c\u4ee5\u4fbf Sequence \u7684\u7ed3\u679c\u53ef\u4ee5\u88ab\u5176\u4ed6\u89e6\u53d1\u5668\u89c2\u5bdf\u5230\u3002 \u53c2\u89c1 \u4f7f\u7528\u5e26\u6709\u4ee3\u7406/\u89e6\u53d1\u5668\u6a21\u578b\u7684\u5e8f\u5217 .","title":"\u5173\u4e8e\u5e8f\u5217"},{"location":"eventing/flows/sequence/#_1","text":"\u5e8f\u5217CRD\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9a\u4e49\u5c06\u88ab\u8c03\u7528\u7684\u51fd\u6570\u7684\u6309\u987a\u5e8f\u5217\u8868\u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u53ef\u4ee5\u4fee\u6539\u3001\u7b5b\u9009\u6216\u521b\u5efa\u4e00\u79cd\u65b0\u7684\u4e8b\u4ef6\u7c7b\u578b\u3002 \u5e8f\u5217\u5728\u5e95\u5c42\u521b\u5efa\u201c\u9891\u9053\u201d\u548c\u201c\u8ba2\u9605\u201d\u3002 Info \u5e8f\u5217\u9700\u8981\"hairpin\"\u6d41\u91cf\u3002 \u8bf7\u9a8c\u8bc1\u60a8\u7684 Pod \u53ef\u4ee5\u901a\u8fc7\u670d\u52a1IP\u5230\u8fbe\u81ea\u5df1\u3002 \u5982\u679c\"hairpin\"\u6d41\u91cf\u4e0d\u53ef\u7528\uff0c\u60a8\u53ef\u4ee5\u8054\u7cfb\u60a8\u7684\u96c6\u7fa4\u7ba1\u7406\u5458\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e00\u4e2a\u96c6\u7fa4\u7ea7\u522b(\u901a\u5e38\u662fCNI)\u8bbe\u7f6e\u3002","title":"\u5e8f\u5217"},{"location":"eventing/flows/sequence/#_2","text":"","title":"\u4f7f\u7528"},{"location":"eventing/flows/sequence/#_3","text":"\u5e8f\u5217\u6709\u4e09\u4e2a\u90e8\u5206\u7684\u89c4\u8303: Steps \uff0c\u5b83\u5b9a\u4e49\u4e86 Subscriber \u7684\u6309\u987a\u5e8f\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u6309\u5217\u51fa\u7684\u987a\u5e8f\u6267\u884c\u7684\u51fd\u6570\u3002 \u8fd9\u4e9b\u662f\u4f7f\u7528 messaging.v1.SubscriberSpec \u6307\u5b9a\u7684\uff0c\u5c31\u50cf\u521b\u5efa Subscription \u65f6\u4e00\u6837\u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u5e94\u8be5\u662f Addressable \u3002 ChannelTemplate \u5b9a\u4e49\u4e86\u7528\u4e8e\u5728\u6b65\u9aa4\u4e4b\u95f4\u521b\u5efa Channel \u7684\u6a21\u677f\u3002 Reply (\u53ef\u9009)\u5f15\u7528\u5e8f\u5217\u4e2d\u6700\u540e\u4e00\u6b65\u7684\u7ed3\u679c\u88ab\u53d1\u9001\u5230\u7684\u4f4d\u7f6e\u3002","title":"\u5e8f\u5217\u89c4\u8303"},{"location":"eventing/flows/sequence/#_4","text":"\u5e8f\u5217\u6709\u56db\u4e2a\u90e8\u5206\u7684\u72b6\u6001: \u8be6\u7ec6\u63cf\u8ff0\u5e8f\u5217\u5bf9\u8c61\u7684\u6574\u4f53\u72b6\u6001\u7684\u6761\u4ef6 ChannelStatuses \uff0c\u5b83\u4f20\u9012\u4f5c\u4e3a\u8be5\u5e8f\u5217\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42\u201c\u901a\u9053\u201d\u8d44\u6e90\u7684\u72b6\u6001 \u5b83\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u6bcf\u4e2aStatus\u5bf9\u5e94\u4e8eStep\u53f7\uff0c \u56e0\u6b64\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6761\u76ee\u662f\u7b2c\u4e00\u4e2aStep\u4e4b\u524d\u7684 Channel \u7684Status\u3002 SubscriptionStatuses \uff0c\u5b83\u4f20\u9012\u4f5c\u4e3a\u8be5\u5e8f\u5217\u7684\u4e00\u90e8\u5206\u521b\u5efa\u7684\u5e95\u5c42\u201c\u8ba2\u9605\u201d\u8d44\u6e90\u7684\u72b6\u6001\u3002 \u5b83\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u6bcf\u4e2aStatus\u5bf9\u5e94\u4e8eStep\u53f7\uff0c\u56e0\u6b64\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6761\u76ee\u662f\u201cSubscription\u201d\uff0c \u521b\u5efa\u5b83\u662f\u4e3a\u4e86\u5c06\u7b2c\u4e00\u4e2a\u901a\u9053\u8fde\u63a5\u5230\u201cSteps\u201d\u6570\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u3002 AddressStatus \u662f\u516c\u5f00\u7684\uff0c\u4ee5\u4fbfSequence\u53ef\u4ee5\u5728\u53ef\u4ee5\u4f7f\u7528Addressable\u7684\u5730\u65b9\u4f7f\u7528\u3002 \u53d1\u9001\u5230\u6b64\u5730\u5740\u5c06\u9488\u5bf9\u5e8f\u5217\u4e2d\u7b2c\u4e00\u6b65\u524d\u9762\u7684 Channel \u3002","title":"\u5e8f\u5217\u72b6\u6001"},{"location":"eventing/flows/sequence/#_5","text":"\u5bf9\u4e8e\u4e0b\u9762\u7684\u6bcf\u4e2a\u793a\u4f8b\uff0c\u90fd\u4f7f\u7528 PingSource \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u3002 \u6211\u4eec\u8fd8\u4f7f\u7528\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684 \u8f6c\u6362\u5668 \uff0c\u5b83\u5bf9\u4f20\u5165\u7684\u4e8b\u4ef6\u6267\u884c\u975e\u5e38\u7b80\u5355\u7684\u8f6c\u6362\uff0c\u4ee5\u6f14\u793a\u5b83\u4eec\u5df2\u7ecf\u901a\u8fc7\u4e86\u6bcf\u4e2a\u9636\u6bb5\u3002","title":"\u4f8b\u5b50"},{"location":"eventing/flows/sequence/#_6","text":"\u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u53c2\u89c1 \u65e0\u5e94\u7b54\u7684\u5e8f\u5217(\u7ec8\u7aef\u6700\u540e\u4e00\u6b65) .","title":"\u65e0\u5e94\u7b54\u5e8f\u5217"},{"location":"eventing/flows/sequence/#_7","text":"\u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u7684\u76f8\u540c\u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u552f\u4e00\u7684\u533a\u522b\u662f\u6211\u4eec\u5c06\u4f7f\u7528 Subscriber.Spec.Reply \u5b57\u6bb5\u5c06\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa\u8fde\u63a5\u5230\u4e8b\u4ef6\u663e\u793aPod\u3002 \u53c2\u89c1 \u6709\u5e94\u7b54\u5e8f\u5217(\u6700\u540e\u4e00\u6b65\u4ea7\u751f\u8f93\u51fa) .","title":"\u6709\u5e94\u7b54\u5e8f\u5217"},{"location":"eventing/flows/sequence/#_8","text":"\u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u76f4\u63a5\u8fde\u63a5\u5230 PingSource \u7684\u76f8\u540c\u76843\u6b65 Sequence \u3002 \u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b80\u5355\u5730\u9644\u52a0\u5728 - Handled by <STEP NUMBER> \u4e0a\uff0c\u4f8b\u5982 Sequence \u4e2d\u7684\u7b2c\u4e00\u4e2a\u6b65\u9aa4\u5c06\u63a5\u53d7\u4f20\u5165\u6d88\u606f\u5e76\u5c06\"- Handled by 0\"\u9644\u52a0\u5230\u4f20\u5165\u6d88\u606f\u3002 \u552f\u4e00\u7684\u533a\u522b\u662f\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 Subscriber.Spec.Reply \u5b57\u6bb5\u5c06\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa\u8fde\u63a5\u5230\u53e6\u4e00\u4e2a Sequence \uff0c\u8be5 Sequence \u6267\u884c\u4e0e\u7b2c\u4e00\u4e2a\u7ba1\u9053\u76f8\u540c\u7684\u6d88\u606f\u4fee\u6539(\u53ea\u662f\u6b65\u9aa4\u4e0d\u540c)\u3002 \u53c2\u89c1 \u5c06\u5e8f\u5217\u8fde\u63a5\u5728\u4e00\u8d77 .","title":"\u5c06\u5e8f\u5217\u94fe\u63a5\u5728\u4e00\u8d77"},{"location":"eventing/flows/sequence/#_9","text":"\u4f60\u4e5f\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u76ee\u6807\u4e3a Sequence \u7684\u89e6\u53d1\u5668\u3002 \u8fd9\u6b21\u6211\u4eec\u5c06\u8fde\u63a5 PingSource \u5c06\u4e8b\u4ef6\u53d1\u9001\u5230 Broker \uff0c\u7136\u540e\u6211\u4eec\u5c06\u8ba9 Sequence \u5c06\u4ea7\u751f\u7684\u4e8b\u4ef6\u53d1\u9001\u56deBroker\uff0c\u4ee5\u4fbf Sequence \u7684\u7ed3\u679c\u53ef\u4ee5\u88ab\u5176\u4ed6\u89e6\u53d1\u5668\u89c2\u5bdf\u5230\u3002 \u53c2\u89c1 \u4f7f\u7528\u5e26\u6709\u4ee3\u7406/\u89e6\u53d1\u5668\u6a21\u578b\u7684\u5e8f\u5217 .","title":"\u4f7f\u7528\u5e26\u6709\u4ee3\u7406/\u89e6\u53d1\u5668\u6a21\u578b\u7684\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/","text":"\u6709\u5e94\u7b54\u5e8f\u5217 \u5e8f\u5217\u8fde\u63a5\u5230\u4e8b\u4ef6\u663e\u793a \u00b6 \u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u83b7\u53d6\u8be5 Sequence \u7684\u8f93\u51fa\u5e76\u663e\u793a\u7ed3\u679c\u8f93\u51fa\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u4f4d\u4e8e https://github.com/knative/eventing/blob/main/cmd/appender/main.go . \u5148\u51b3\u6761\u4ef6 \u00b6 \u5bf9\u4e8e\u672c\u4f8b\uff0c\u6211\u4eec\u5047\u8bbe\u60a8\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u4ee5\u53caKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002 \u8bbe\u7f6e \u00b6 \u521b\u5efaKnative\u670d\u52a1 \u00b6 \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u6b65\u9aa4: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml \u521b\u5efa\u5e8f\u5217 \u00b6 sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence.yaml \u521b\u5efa\u6309\u987a\u5e8f\u663e\u793a\u4e8b\u4ef6\u7684\u670d\u52a1 \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./event-display.yaml \u521b\u5efa\u4ee5\u5e8f\u5217\u4e3a\u76ee\u6807\u7684PingSource \u00b6 \u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\u201cmessage\":\"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml \u68c0\u67e5\u7ed3\u679c \u00b6 \u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u7a0d\u7b49\u7247\u523b\uff0c\u7136\u540e\u67e5\u770b\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u4e0a\u9762\u3002","title":"\u6709\u5e94\u7b54\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_1","text":"\u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u83b7\u53d6\u8be5 Sequence \u7684\u8f93\u51fa\u5e76\u663e\u793a\u7ed3\u679c\u8f93\u51fa\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u4f4d\u4e8e https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"\u6709\u5e94\u7b54\u5e8f\u5217 \u5e8f\u5217\u8fde\u63a5\u5230\u4e8b\u4ef6\u663e\u793a"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_2","text":"\u5bf9\u4e8e\u672c\u4f8b\uff0c\u6211\u4eec\u5047\u8bbe\u60a8\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u4ee5\u53caKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_3","text":"","title":"\u8bbe\u7f6e"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#knative","text":"\u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u6b65\u9aa4: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"\u521b\u5efaKnative\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_4","text":"sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence.yaml","title":"\u521b\u5efa\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_5","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./event-display.yaml","title":"\u521b\u5efa\u6309\u987a\u5e8f\u663e\u793a\u4e8b\u4ef6\u7684\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#pingsource","text":"\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\u201cmessage\":\"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"\u521b\u5efa\u4ee5\u5e8f\u5217\u4e3a\u76ee\u6807\u7684PingSource"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#_6","text":"\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u7a0d\u7b49\u7247\u523b\uff0c\u7136\u540e\u67e5\u770b\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u4e0a\u9762\u3002","title":"\u68c0\u67e5\u7ed3\u679c"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/","text":"\u5e8f\u5217\u8fde\u63a5\u5230\u53e6\u4e00\u4e2a\u5e8f\u5217 \u00b6 \u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u83b7\u53d6\u8be5 Sequence \u7684\u8f93\u51fa\u5e76\u5c06\u5176\u53d1\u9001\u7ed9\u7b2c\u4e8c\u4e2a Sequence \uff0c\u6700\u540e\u663e\u793a\u7ed3\u679c\u8f93\u51fa\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u90fd\u5728 https://github.com/knative/eventing/blob/main/cmd/appender/main.go . \u5148\u51b3\u6761\u4ef6 \u00b6 \u5bf9\u4e8e\u672c\u4f8b\uff0c\u6211\u4eec\u5047\u8bbe\u60a8\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u4ee5\u53caKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002 \u8bbe\u7f6e \u00b6 \u521b\u5efaKnative\u670d\u52a1 \u00b6 \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u6b65\u9aa4: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml \u521b\u5efa\u7b2c\u4e00\u4e2a\u5e8f\u5217 \u00b6 sequence1.yaml \u6587\u4ef6\u5305\u542b\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence1.yaml \u521b\u5efa\u7b2c\u4e8c\u4e2a\u5e8f\u5217 \u00b6 sequence2.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml \u521b\u5efa\u6309\u987a\u5e8f\u663e\u793a\u4e8b\u4ef6\u7684\u670d\u52a1 \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./event-display.yaml \u521b\u5efa\u9488\u5bf9\u7b2c\u4e00\u4e2a\u5e8f\u5217\u7684PingSource \u00b6 \u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\"message\": \"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml \u68c0\u67e5\u7ed3\u679c \u00b6 \u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u7136\u540e\u67e5\u770b\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u4e0a\u9762\u3002","title":"\u5e8f\u5217\u5230\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_1","text":"\u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u83b7\u53d6\u8be5 Sequence \u7684\u8f93\u51fa\u5e76\u5c06\u5176\u53d1\u9001\u7ed9\u7b2c\u4e8c\u4e2a Sequence \uff0c\u6700\u540e\u663e\u793a\u7ed3\u679c\u8f93\u51fa\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u90fd\u5728 https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"\u5e8f\u5217\u8fde\u63a5\u5230\u53e6\u4e00\u4e2a\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_2","text":"\u5bf9\u4e8e\u672c\u4f8b\uff0c\u6211\u4eec\u5047\u8bbe\u60a8\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u4ee5\u53caKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_3","text":"","title":"\u8bbe\u7f6e"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#knative","text":"\u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u6b65\u9aa4: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml","title":"\u521b\u5efaKnative\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_4","text":"sequence1.yaml \u6587\u4ef6\u5305\u542b\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence1.yaml","title":"\u521b\u5efa\u7b2c\u4e00\u4e2a\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_5","text":"sequence2.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml","title":"\u521b\u5efa\u7b2c\u4e8c\u4e2a\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_6","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./event-display.yaml","title":"\u521b\u5efa\u6309\u987a\u5e8f\u663e\u793a\u4e8b\u4ef6\u7684\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#pingsource","text":"\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\"message\": \"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml","title":"\u521b\u5efa\u9488\u5bf9\u7b2c\u4e00\u4e2a\u5e8f\u5217\u7684PingSource"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#_7","text":"\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u7136\u540e\u67e5\u770b\u4e8b\u4ef6\u663e\u793aPod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u4e0a\u9762\u3002","title":"\u68c0\u67e5\u7ed3\u679c"},{"location":"eventing/flows/sequence/sequence-terminal/","text":"\u5e8f\u5217\u7ec8\u7aef \u00b6 \u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\u3002 \u5e8f\u5217\u53ef\u4ee5\u8fdb\u884c\u5916\u90e8\u5de5\u4f5c\uff0c\u6216\u8005\u5728\u5e26\u5916\u521b\u5efa\u989d\u5916\u7684\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u90fd\u4f4d\u4e8e: https://github.com/knative/eventing/blob/main/cmd/appender/main.go . \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u4f60\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u548cKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002 \u8bbe\u7f6e \u00b6 \u521b\u5efaKnative\u670d\u52a1 \u00b6 \u9996\u5148\u521b\u5efa\u5c06\u5728\u6b65\u9aa4\u4e2d\u5f15\u7528\u76843\u4e2a\u6b65\u9aa4\u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml \u521b\u5efa\u5e8f\u5217 \u00b6 sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence.yaml \u521b\u5efa\u9488\u5bf9\u5e8f\u5217\u7684PingSource \u00b6 \u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\"message\": \"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml \u68c0\u67e5\u7ed3\u679c \u00b6 \u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 \u6ce8\u610f\uff0c\u7531\u4e8e\u6211\u4eec\u5c06 PingSource \u8bbe\u7f6e\u4e3a\u6bcf2\u5206\u949f\u53d1\u51fa\u4e00\u6b21\uff0c\u6240\u4ee5\u4e8b\u4ef6\u5728\u65e5\u5fd7\u4e2d\u663e\u793a\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u65f6\u95f4\u3002 kubectl -n default get pods \u8ba9\u6211\u4eec\u770b\u770b Sequence \u4e2d\u7b2c\u4e00\u6b65\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } \u60a8\u53ef\u4ee5\u770b\u5230\uff0c\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u73b0\u5728\u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u7b2c\u4e00\u6b65\u4fee\u6539\u4e3a\u5305\u542b\" - Handled by 0\"\u3002\u6fc0\u52a8\u4eba\u5fc3\u7684:) \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u770b Sequence \u4e2d\u7b2c\u4e8c\u6b65\u7684\u8f93\u51fa: kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } \u6b63\u5982\u9884\u671f\u7684\u90a3\u6837\uff0c\u5b83\u73b0\u5728\u7531\u7b2c\u4e00\u6b65\u548c\u7b2c\u4e8c\u6b65\u5904\u7406\uff0c\u53cd\u6620\u5728\u73b0\u5728\u7684\u6d88\u606f\u4e2d:\"Hello world! - Handled by 0 - Handled by 1\" \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u770b Sequence \u4e2d\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa: kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"\u5e8f\u5217\u7ec8\u7aef"},{"location":"eventing/flows/sequence/sequence-terminal/#_1","text":"\u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411 Sequence \u63d0\u4f9b\u4e8b\u4ef6\u3002 \u5e8f\u5217\u53ef\u4ee5\u8fdb\u884c\u5916\u90e8\u5de5\u4f5c\uff0c\u6216\u8005\u5728\u5e26\u5916\u521b\u5efa\u989d\u5916\u7684\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u90fd\u4f4d\u4e8e: https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"\u5e8f\u5217\u7ec8\u7aef"},{"location":"eventing/flows/sequence/sequence-terminal/#_2","text":"\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u4f60\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u4e00\u4e2a InMemoryChannel \u548cKnative\u670d\u52a1(\u7528\u4e8e\u6211\u4eec\u7684\u51fd\u6570)\u3002 \u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\uff0c\u540c\u6837\uff0c\u5982\u679c\u60a8\u5e0c\u671b\u90e8\u7f72\u5230\u53e6\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539\u793a\u4f8b\u4ee5\u53cd\u6620\u8fd9\u4e00\u70b9\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684 Channel \uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/flows/sequence/sequence-terminal/#_3","text":"","title":"\u8bbe\u7f6e"},{"location":"eventing/flows/sequence/sequence-terminal/#knative","text":"\u9996\u5148\u521b\u5efa\u5c06\u5728\u6b65\u9aa4\u4e2d\u5f15\u7528\u76843\u4e2a\u6b65\u9aa4\u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"\u521b\u5efaKnative\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-terminal/#_4","text":"sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third \u5728\u4e0b\u9762\u7684\u547d\u4ee4\u4e2d\u4fee\u6539 default \uff0c\u5728\u4f60\u60f3\u8981\u521b\u5efa\u8d44\u6e90\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa Sequence : kubectl -n default create -f ./sequence.yaml","title":"\u521b\u5efa\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-terminal/#pingsource","text":"\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf2\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u5e26\u6709 {\"message\": \"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u8d1f\u8f7d\u7684CloudEvent\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"\u521b\u5efa\u9488\u5bf9\u5e8f\u5217\u7684PingSource"},{"location":"eventing/flows/sequence/sequence-terminal/#_5","text":"\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u4e8b\u4ef6\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 \u6ce8\u610f\uff0c\u7531\u4e8e\u6211\u4eec\u5c06 PingSource \u8bbe\u7f6e\u4e3a\u6bcf2\u5206\u949f\u53d1\u51fa\u4e00\u6b21\uff0c\u6240\u4ee5\u4e8b\u4ef6\u5728\u65e5\u5fd7\u4e2d\u663e\u793a\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u65f6\u95f4\u3002 kubectl -n default get pods \u8ba9\u6211\u4eec\u770b\u770b Sequence \u4e2d\u7b2c\u4e00\u6b65\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } \u60a8\u53ef\u4ee5\u770b\u5230\uff0c\u521d\u59cb\u7684PingSource\u6d88\u606f (\"Hello World!\") \u73b0\u5728\u5df2\u7ecf\u88ab\u5e8f\u5217\u4e2d\u7684\u7b2c\u4e00\u6b65\u4fee\u6539\u4e3a\u5305\u542b\" - Handled by 0\"\u3002\u6fc0\u52a8\u4eba\u5fc3\u7684:) \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u770b Sequence \u4e2d\u7b2c\u4e8c\u6b65\u7684\u8f93\u51fa: kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } \u6b63\u5982\u9884\u671f\u7684\u90a3\u6837\uff0c\u5b83\u73b0\u5728\u7531\u7b2c\u4e00\u6b65\u548c\u7b2c\u4e8c\u6b65\u5904\u7406\uff0c\u53cd\u6620\u5728\u73b0\u5728\u7684\u6d88\u606f\u4e2d:\"Hello world! - Handled by 0 - Handled by 1\" \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u770b Sequence \u4e2d\u6700\u540e\u4e00\u6b65\u7684\u8f93\u51fa: kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"\u68c0\u67e5\u7ed3\u679c"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/","text":"\u4f7f\u7528\u5e26\u6709\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u7684\u5e8f\u5217 \u00b6 \u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411\u4ee3\u7406\u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a\u201c\u8fc7\u6ee4\u5668\u201d\uff0c\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u8fde\u63a5\u5230\u4e00\u4e2a\u75313\u4e2a\u6b65\u9aa4\u7ec4\u6210\u7684 \u5e8f\u5217 \u4e2d\u3002 \u7136\u540e\u53d6\u5e8f\u5217\u7684\u672b\u5c3e\uff0c\u5c06\u65b0\u751f\u6210\u7684\u4e8b\u4ef6\u53cd\u9988\u7ed9\u4ee3\u7406\uff0c\u5e76\u521b\u5efa\u53e6\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u8be5\u89e6\u53d1\u5668\u5c06\u663e\u793a\u8fd9\u4e9b\u4e8b\u4ef6\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 Knative \u670d\u52a1 InMemoryChannel Note \u8fd9\u4e9b\u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u201c\u901a\u9053\u201d\uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u4f4d\u4e8e https://github.com/knative/eventing/blob/main/cmd/appender/main.go . \u8bbe\u7f6e \u00b6 \u521b\u5efa\u4ee3\u7406 \u00b6 \u8981\u521b\u5efa\u96c6\u7fa4\u9ed8\u8ba4\u7684Broker\u7c7b\u578b\uff0c\u8bf7\u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u670d\u52a1 \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u670d\u52a1: kubectl -n default create -f ./steps.yaml \u521b\u5efa\u5e8f\u5217 \u00b6 sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 \u53e6\u5916\uff0c\u5c06 spec.reply.name \u66f4\u6539\u4e3a\u6307\u5411\u60a8\u7684\u4ee3\u7406 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u4ee5\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u5e8f\u5217: kubectl -n default create -f ./sequence.yaml \u521b\u5efa\u9488\u5bf9\u4ee3\u7406\u7684PingSource \u00b6 \u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf\u96942\u5206\u949f\u53d1\u9001\u4e00\u4e2aCloudEvent\uff0c\u5e76\u5c06 {\"message\":\"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u6709\u6548\u8d1f\u8f7d\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u548c\u5e8f\u5217\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efaPingSource: kubectl -n default create -f ./ping-source.yaml \u521b\u5efa\u9488\u5bf9\u5e8f\u5217\u7684\u89e6\u53d1\u5668 \u00b6 apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u548c\u5e8f\u5217\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u89e6\u53d1\u5668: kubectl -n default create -f ./trigger.yaml \u521b\u5efa\u670d\u52a1\u548c\u89e6\u53d1\u5668\uff0c\u663e\u793a\u6309\u987a\u5e8f\u521b\u5efa\u7684\u4e8b\u4ef6 \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u4ee5\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u670d\u52a1\u5e76\u89e6\u53d1: kubectl -n default create -f ./display-trigger.yaml \u68c0\u67e5\u7ed3\u679c \u00b6 \u60a8\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u5e8f\u5217\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u67e5\u770b sequence-display Pod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f {\"Hello World!\"} \u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u540e\u9762\u3002","title":"\u4ee3\u7406\u548c\u89e6\u53d1\u5668"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_1","text":"\u6211\u4eec\u5c06\u521b\u5efa\u4ee5\u4e0b\u903b\u8f91\u914d\u7f6e\u3002 \u6211\u4eec\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5411\u4ee3\u7406\u63d0\u4f9b\u4e8b\u4ef6\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a\u201c\u8fc7\u6ee4\u5668\u201d\uff0c\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u8fde\u63a5\u5230\u4e00\u4e2a\u75313\u4e2a\u6b65\u9aa4\u7ec4\u6210\u7684 \u5e8f\u5217 \u4e2d\u3002 \u7136\u540e\u53d6\u5e8f\u5217\u7684\u672b\u5c3e\uff0c\u5c06\u65b0\u751f\u6210\u7684\u4e8b\u4ef6\u53cd\u9988\u7ed9\u4ee3\u7406\uff0c\u5e76\u521b\u5efa\u53e6\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u8be5\u89e6\u53d1\u5668\u5c06\u663e\u793a\u8fd9\u4e9b\u4e8b\u4ef6\u3002","title":"\u4f7f\u7528\u5e26\u6709\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u7684\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_2","text":"Knative \u670d\u52a1 InMemoryChannel Note \u8fd9\u4e9b\u793a\u4f8b\u4f7f\u7528 default \u540d\u79f0\u7a7a\u95f4\u3002 \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u201c\u901a\u9053\u201d\uff0c\u4f60\u5fc5\u987b\u4fee\u6539 Sequence.Spec.ChannelTemplate \u6765\u521b\u5efa\u9002\u5f53\u7684\u901a\u9053\u8d44\u6e90\u3002 \u8fd9\u4e9b\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\u4f4d\u4e8e https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_3","text":"","title":"\u8bbe\u7f6e"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_4","text":"\u8981\u521b\u5efa\u96c6\u7fa4\u9ed8\u8ba4\u7684Broker\u7c7b\u578b\uff0c\u8bf7\u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4ee3\u7406"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_5","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u670d\u52a1: kubectl -n default create -f ./steps.yaml","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_6","text":"sequence.yaml \u6587\u4ef6\u5305\u542b\u4e86\u521b\u5efa\u5e8f\u5217\u7684\u89c4\u8303\u3002 \u5982\u679c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u901a\u9053\uff0c\u5219\u9700\u8981\u66f4\u6539 spec.channelTemplate \u4ee5\u6307\u5411\u6240\u9700\u7684\u901a\u9053\u3002 \u53e6\u5916\uff0c\u5c06 spec.reply.name \u66f4\u6539\u4e3a\u6307\u5411\u60a8\u7684\u4ee3\u7406 apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u4ee5\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u5e8f\u5217: kubectl -n default create -f ./sequence.yaml","title":"\u521b\u5efa\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#pingsource","text":"\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u5b83\u5c06\u6bcf\u96942\u5206\u949f\u53d1\u9001\u4e00\u4e2aCloudEvent\uff0c\u5e76\u5c06 {\"message\":\"Hello world!\"} \u4f5c\u4e3a\u6570\u636e\u6709\u6548\u8d1f\u8f7d\u3002 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u548c\u5e8f\u5217\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efaPingSource: kubectl -n default create -f ./ping-source.yaml","title":"\u521b\u5efa\u9488\u5bf9\u4ee3\u7406\u7684PingSource"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_7","text":"apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u548c\u5e8f\u5217\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u89e6\u53d1\u5668: kubectl -n default create -f ./trigger.yaml","title":"\u521b\u5efa\u9488\u5bf9\u5e8f\u5217\u7684\u89e6\u53d1\u5668"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_8","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- \u5728\u4ee5\u4e0b\u547d\u4ee4\u4e2d\u66f4\u6539 default \uff0c\u4ee5\u5728\u60a8\u914d\u7f6e\u4ee3\u7406\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u670d\u52a1\u5e76\u89e6\u53d1: kubectl -n default create -f ./display-trigger.yaml","title":"\u521b\u5efa\u670d\u52a1\u548c\u89e6\u53d1\u5668\uff0c\u663e\u793a\u6309\u987a\u5e8f\u521b\u5efa\u7684\u4e8b\u4ef6"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#_9","text":"\u60a8\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u5e8f\u5217\u663e\u793aPods\u7684\u65e5\u5fd7\u770b\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u3002 kubectl -n default get pods \u67e5\u770b sequence-display Pod\u7684\u65e5\u5fd7: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } \u4f60\u53ef\u4ee5\u770b\u5230\u521d\u59cb\u7684PingSource\u6d88\u606f {\"Hello World!\"} \u88ab\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u6b65\u9aa4\u9644\u52a0\u5230\u5b83\u540e\u9762\u3002","title":"\u68c0\u67e5\u7ed3\u679c"},{"location":"eventing/observability/logging/collecting-logs/","text":"\u65e5\u5fd7 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528\u65e5\u5fd7\u5904\u7406\u5668\u548c\u8f6c\u53d1\u5668 Fluent Bit \u5728\u4e2d\u5fc3\u76ee\u5f55\u4e2d\u6536\u96c6Kubernetes\u65e5\u5fd7\u3002 \u8fd0\u884cKnative\u65f6\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff0c\u4f46\u662f\u4f7f\u7528 Knative\u670d\u52a1 \u4f1a\u5f88\u6709\u5e2e\u52a9\uff0c\u5b83\u4f1a\u5728\u4e0d\u518d\u9700\u8981Pod\u548c\u76f8\u5173\u65e5\u5fd7\u65f6\u81ea\u52a8\u5220\u9664\u5b83\u4eec\u3002 Fluent Bit\u652f\u6301\u5bfc\u51fa\u5230\u8bb8\u591a\u5176\u4ed6\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\u3002 \u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u4f8b\u5982Splunk\u3001datdog\u3001ElasticSearch\u6216Stackdriver\uff0c\u60a8\u53ef\u4ee5\u6309\u7167 FluentBit\u6587\u6863 \u914d\u7f6e\u65e5\u5fd7\u8f6c\u53d1\u5668\u3002 \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6 \u00b6 \u8bbe\u7f6e\u65e5\u5fd7\u6536\u96c6\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4: \u5728\u5404\u8282\u70b9\u4e0a\u8fd0\u884c\u65e5\u5fd7\u8f6c\u53d1DaemonSet\u3002 \u5728\u96c6\u7fa4\u7684\u67d0\u4e2a\u5730\u65b9\u8fd0\u884c\u6536\u96c6\u5668\u3002 Tip \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u7684\u662f StatefulSet\uff0c\u5b83\u5c06\u65e5\u5fd7\u5b58\u50a8\u5728Kubernetes PersistentVolumeClaim\u4e0a\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528HostPath\u3002 \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 fluent-bit-collector.yaml \u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aStatefulSet\uff0c\u4ee5\u53ca\u4e00\u4e2aKubernetes\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u5141\u8bb8\u4ece\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u548c\u8bfb\u53d6\u65e5\u5fd7\u3002 \u63d0\u4f9b\u7684\u914d\u7f6e\u5c06\u5728\u540d\u4e3a logging \u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u76d1\u89c6\u914d\u7f6e\u3002 Important \u5728\u8f6c\u53d1\u4e4b\u524d\u8bbe\u7acb\u6536\u8d27\u4eba\u3002\u5728\u914d\u7f6e\u8f6c\u53d1\u5668\u65f6\uff0c\u60a8\u5c06\u9700\u8981\u6536\u96c6\u5668\u7684\u5730\u5740\uff0c\u5e76\u4e14\u8f6c\u53d1\u5668\u53ef\u80fd\u4f1a\u5c06\u65e5\u5fd7\u6392\u961f\uff0c\u76f4\u5230\u6536\u96c6\u5668\u51c6\u5907\u597d\u4e3a\u6b62\u3002 \u8fc7\u7a0b \u00b6 \u8f93\u5165\u547d\u4ee4\u5e94\u7528\u914d\u7f6e: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml \u9ed8\u8ba4\u914d\u7f6e\u5c06\u65e5\u5fd7\u5206\u4e3a: Knative\u670d\u52a1\uff0c\u6216\u5e26\u6709 app=Knative \u6807\u7b7e\u7684Pod\u3002 Non-Knative apps. Note \u65e5\u5fd7\u9ed8\u8ba4\u4f7f\u7528Pod\u540d\u79f0\u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55;\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5728\u5b89\u88c5\u4e4b\u524d\u6216\u4e4b\u540e\u66f4\u65b0 log-collector-config ConfigMap\u6765\u66f4\u6539\u3002 Warning ConfigMap\u66f4\u65b0\u540e\uff0c\u5fc5\u987b\u91cd\u65b0\u542f\u52a8Fluent Bit\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5220\u9664Pod\u5e76\u8ba9StatefulSet\u91cd\u65b0\u521b\u5efa\u5b83\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 \u8981\u901a\u8fc7web\u6d4f\u89c8\u5668\u8bbf\u95ee\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl port-forward --namespace logging service/log-collector 8080 :80 \u5bfc\u822a\u5230 http://localhost:8080/ . \u53ef\u9009:\u60a8\u53ef\u4ee5\u5728 nginx Pod\u4e2d\u6253\u5f00\u4e00\u4e2ashell\uff0c\u5e76\u4f7f\u7528Unix\u5de5\u5177\u641c\u7d22\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 \u5efa\u7acb\u8f6c\u53d1 \u00b6 \u8bf7\u53c2\u9605 Fluent Bit \u6587\u6863\uff0c\u8bbe\u7f6e\u4e00\u4e2a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c06\u65e5\u5fd7\u8f6c\u53d1\u5230ElasticSearch\u7684Fluent Bit DaemonSet\u3002 \u5f53\u60a8\u5728\u5b89\u88c5\u6b65\u9aa4\u4e2d\u521b\u5efaConfigMap\u65f6\uff0c\u5fc5\u987b: \u5c06ElasticSearch\u914d\u7f6e\u66ff\u6362\u4e3a fluent-bit-configmap.yaml , or \u5c06\u4e0b\u9762\u7684\u5757\u6dfb\u52a0\u5230ConfigMap\u4e2d\uff0c\u5e76\u5c06 @INCLUDE output-elasticsearch.conf \u66f4\u65b0\u4e3a @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True \u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668 \u00b6 Warning \u6b64\u8fc7\u7a0b\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\u8bbe\u7f6e\uff0c\u4e0d\u9002\u5408\u751f\u4ea7\u4f7f\u7528\u3002 \u5982\u679c\u4f7f\u7528\u672c\u5730Kubernetes\u96c6\u7fa4\u8fdb\u884c\u5f00\u53d1\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a hostPath PersistentVolume \u5728\u684c\u9762\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b58\u50a8\u65e5\u5fd7\u3002 \u8fd9\u5141\u8bb8\u60a8\u5728\u6587\u4ef6\u4e0a\u4f7f\u7528\u5e38\u7528\u7684\u684c\u9762\u5de5\u5177\uff0c\u800c\u4e0d\u9700\u8981\u7279\u5b9a\u4e8ekubernetes\u7684\u5de5\u5177\u3002 PersistentVolumeClaim \u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note hostPath \u5c06\u6839\u636e\u60a8\u7684Kubernetes\u8f6f\u4ef6\u548c\u4e3b\u673a\u64cd\u4f5c\u7cfb\u7edf\u800c\u6709\u6240\u4e0d\u540c\u3002 \u60a8\u5fc5\u987b\u66f4\u65b0StatefulSet volumeClaimTemplates \u4ee5\u5f15\u7528 shared-logs \u5377\uff0c\u793a\u4f8b\u5982\u4e0b: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 \u5f53\u521b\u5efa\u96c6\u7fa4\u65f6\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 kind-config.yaml \u5e76\u4e3a\u6bcf\u4e2a\u8282\u70b9\u6307\u5b9a extraMounts \uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs \u7136\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528 /shared/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 spec.hostPath.path \u3002 \u6ce8\u610f\u76ee\u5f55\u8def\u5f84 ./logs \u662f\u76f8\u5bf9\u4e8e\u521b\u5efaKind\u96c6\u7fa4\u7684\u76ee\u5f55\u7684\u3002 Docker \u684c\u9762 \u00b6 Docker\u684c\u9762\u81ea\u52a8\u5728\u4e3b\u673a\u548c\u5ba2\u6237\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u95f4\u521b\u5efa\u4e00\u4e9b\u5171\u4eab\u6302\u8f7d\uff0c\u56e0\u6b64\u60a8\u53ea\u9700\u8981\u77e5\u9053\u5230\u60a8\u7684\u4e3b\u76ee\u5f55\u7684\u8def\u5f84\u3002 \u4ee5\u4e0b\u662f\u4e0d\u540c\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e00\u4e9b\u4f8b\u5b50: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube\u9700\u8981\u4e00\u4e2a\u663e\u5f0f\u547d\u4ee4\u5c06\u4e00\u4e2a \u76ee\u5f55\u6302\u8f7d \u5230\u8fd0\u884cKubernetes\u7684\u865a\u62df\u673a\u4e2d\u3002 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 logs \u76ee\u5f55\u6302\u8f7d\u5230\u865a\u62df\u673a\u7684 /mnt/logs \u76ee\u5f55\u4e2d: minikube mount ./logs:/mnt/logs \u60a8\u8fd8\u5fc5\u987b\u5f15\u7528 /mnt/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 hostPath.path \u3002","title":"\u6536\u96c6\u65e5\u5fd7"},{"location":"eventing/observability/logging/collecting-logs/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528\u65e5\u5fd7\u5904\u7406\u5668\u548c\u8f6c\u53d1\u5668 Fluent Bit \u5728\u4e2d\u5fc3\u76ee\u5f55\u4e2d\u6536\u96c6Kubernetes\u65e5\u5fd7\u3002 \u8fd0\u884cKnative\u65f6\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff0c\u4f46\u662f\u4f7f\u7528 Knative\u670d\u52a1 \u4f1a\u5f88\u6709\u5e2e\u52a9\uff0c\u5b83\u4f1a\u5728\u4e0d\u518d\u9700\u8981Pod\u548c\u76f8\u5173\u65e5\u5fd7\u65f6\u81ea\u52a8\u5220\u9664\u5b83\u4eec\u3002 Fluent Bit\u652f\u6301\u5bfc\u51fa\u5230\u8bb8\u591a\u5176\u4ed6\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\u3002 \u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u4f8b\u5982Splunk\u3001datdog\u3001ElasticSearch\u6216Stackdriver\uff0c\u60a8\u53ef\u4ee5\u6309\u7167 FluentBit\u6587\u6863 \u914d\u7f6e\u65e5\u5fd7\u8f6c\u53d1\u5668\u3002","title":"\u65e5\u5fd7"},{"location":"eventing/observability/logging/collecting-logs/#_2","text":"\u8bbe\u7f6e\u65e5\u5fd7\u6536\u96c6\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4: \u5728\u5404\u8282\u70b9\u4e0a\u8fd0\u884c\u65e5\u5fd7\u8f6c\u53d1DaemonSet\u3002 \u5728\u96c6\u7fa4\u7684\u67d0\u4e2a\u5730\u65b9\u8fd0\u884c\u6536\u96c6\u5668\u3002 Tip \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u7684\u662f StatefulSet\uff0c\u5b83\u5c06\u65e5\u5fd7\u5b58\u50a8\u5728Kubernetes PersistentVolumeClaim\u4e0a\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528HostPath\u3002","title":"\u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6"},{"location":"eventing/observability/logging/collecting-logs/#_3","text":"fluent-bit-collector.yaml \u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aStatefulSet\uff0c\u4ee5\u53ca\u4e00\u4e2aKubernetes\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u5141\u8bb8\u4ece\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u548c\u8bfb\u53d6\u65e5\u5fd7\u3002 \u63d0\u4f9b\u7684\u914d\u7f6e\u5c06\u5728\u540d\u4e3a logging \u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u76d1\u89c6\u914d\u7f6e\u3002 Important \u5728\u8f6c\u53d1\u4e4b\u524d\u8bbe\u7acb\u6536\u8d27\u4eba\u3002\u5728\u914d\u7f6e\u8f6c\u53d1\u5668\u65f6\uff0c\u60a8\u5c06\u9700\u8981\u6536\u96c6\u5668\u7684\u5730\u5740\uff0c\u5e76\u4e14\u8f6c\u53d1\u5668\u53ef\u80fd\u4f1a\u5c06\u65e5\u5fd7\u6392\u961f\uff0c\u76f4\u5230\u6536\u96c6\u5668\u51c6\u5907\u597d\u4e3a\u6b62\u3002","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"eventing/observability/logging/collecting-logs/#_4","text":"\u8f93\u5165\u547d\u4ee4\u5e94\u7528\u914d\u7f6e: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml \u9ed8\u8ba4\u914d\u7f6e\u5c06\u65e5\u5fd7\u5206\u4e3a: Knative\u670d\u52a1\uff0c\u6216\u5e26\u6709 app=Knative \u6807\u7b7e\u7684Pod\u3002 Non-Knative apps. Note \u65e5\u5fd7\u9ed8\u8ba4\u4f7f\u7528Pod\u540d\u79f0\u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55;\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5728\u5b89\u88c5\u4e4b\u524d\u6216\u4e4b\u540e\u66f4\u65b0 log-collector-config ConfigMap\u6765\u66f4\u6539\u3002 Warning ConfigMap\u66f4\u65b0\u540e\uff0c\u5fc5\u987b\u91cd\u65b0\u542f\u52a8Fluent Bit\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5220\u9664Pod\u5e76\u8ba9StatefulSet\u91cd\u65b0\u521b\u5efa\u5b83\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 \u8981\u901a\u8fc7web\u6d4f\u89c8\u5668\u8bbf\u95ee\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl port-forward --namespace logging service/log-collector 8080 :80 \u5bfc\u822a\u5230 http://localhost:8080/ . \u53ef\u9009:\u60a8\u53ef\u4ee5\u5728 nginx Pod\u4e2d\u6253\u5f00\u4e00\u4e2ashell\uff0c\u5e76\u4f7f\u7528Unix\u5de5\u5177\u641c\u7d22\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"\u8fc7\u7a0b"},{"location":"eventing/observability/logging/collecting-logs/#_5","text":"\u8bf7\u53c2\u9605 Fluent Bit \u6587\u6863\uff0c\u8bbe\u7f6e\u4e00\u4e2a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c06\u65e5\u5fd7\u8f6c\u53d1\u5230ElasticSearch\u7684Fluent Bit DaemonSet\u3002 \u5f53\u60a8\u5728\u5b89\u88c5\u6b65\u9aa4\u4e2d\u521b\u5efaConfigMap\u65f6\uff0c\u5fc5\u987b: \u5c06ElasticSearch\u914d\u7f6e\u66ff\u6362\u4e3a fluent-bit-configmap.yaml , or \u5c06\u4e0b\u9762\u7684\u5757\u6dfb\u52a0\u5230ConfigMap\u4e2d\uff0c\u5e76\u5c06 @INCLUDE output-elasticsearch.conf \u66f4\u65b0\u4e3a @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"\u5efa\u7acb\u8f6c\u53d1"},{"location":"eventing/observability/logging/collecting-logs/#_6","text":"Warning \u6b64\u8fc7\u7a0b\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\u8bbe\u7f6e\uff0c\u4e0d\u9002\u5408\u751f\u4ea7\u4f7f\u7528\u3002 \u5982\u679c\u4f7f\u7528\u672c\u5730Kubernetes\u96c6\u7fa4\u8fdb\u884c\u5f00\u53d1\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a hostPath PersistentVolume \u5728\u684c\u9762\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b58\u50a8\u65e5\u5fd7\u3002 \u8fd9\u5141\u8bb8\u60a8\u5728\u6587\u4ef6\u4e0a\u4f7f\u7528\u5e38\u7528\u7684\u684c\u9762\u5de5\u5177\uff0c\u800c\u4e0d\u9700\u8981\u7279\u5b9a\u4e8ekubernetes\u7684\u5de5\u5177\u3002 PersistentVolumeClaim \u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note hostPath \u5c06\u6839\u636e\u60a8\u7684Kubernetes\u8f6f\u4ef6\u548c\u4e3b\u673a\u64cd\u4f5c\u7cfb\u7edf\u800c\u6709\u6240\u4e0d\u540c\u3002 \u60a8\u5fc5\u987b\u66f4\u65b0StatefulSet volumeClaimTemplates \u4ee5\u5f15\u7528 shared-logs \u5377\uff0c\u793a\u4f8b\u5982\u4e0b: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"\u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668"},{"location":"eventing/observability/logging/collecting-logs/#kind","text":"\u5f53\u521b\u5efa\u96c6\u7fa4\u65f6\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 kind-config.yaml \u5e76\u4e3a\u6bcf\u4e2a\u8282\u70b9\u6307\u5b9a extraMounts \uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs \u7136\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528 /shared/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 spec.hostPath.path \u3002 \u6ce8\u610f\u76ee\u5f55\u8def\u5f84 ./logs \u662f\u76f8\u5bf9\u4e8e\u521b\u5efaKind\u96c6\u7fa4\u7684\u76ee\u5f55\u7684\u3002","title":"Kind"},{"location":"eventing/observability/logging/collecting-logs/#docker","text":"Docker\u684c\u9762\u81ea\u52a8\u5728\u4e3b\u673a\u548c\u5ba2\u6237\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u95f4\u521b\u5efa\u4e00\u4e9b\u5171\u4eab\u6302\u8f7d\uff0c\u56e0\u6b64\u60a8\u53ea\u9700\u8981\u77e5\u9053\u5230\u60a8\u7684\u4e3b\u76ee\u5f55\u7684\u8def\u5f84\u3002 \u4ee5\u4e0b\u662f\u4e0d\u540c\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e00\u4e9b\u4f8b\u5b50: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker \u684c\u9762"},{"location":"eventing/observability/logging/collecting-logs/#minikube","text":"Minikube\u9700\u8981\u4e00\u4e2a\u663e\u5f0f\u547d\u4ee4\u5c06\u4e00\u4e2a \u76ee\u5f55\u6302\u8f7d \u5230\u8fd0\u884cKubernetes\u7684\u865a\u62df\u673a\u4e2d\u3002 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 logs \u76ee\u5f55\u6302\u8f7d\u5230\u865a\u62df\u673a\u7684 /mnt/logs \u76ee\u5f55\u4e2d: minikube mount ./logs:/mnt/logs \u60a8\u8fd8\u5fc5\u987b\u5f15\u7528 /mnt/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 hostPath.path \u3002","title":"Minikube"},{"location":"eventing/observability/logging/config-logging/","text":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e \u00b6 \u6240\u6709Knative\u7ec4\u4ef6\u7684\u65e5\u5fd7\u914d\u7f6e\u90fd\u901a\u8fc7\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging ConfigMap\u8fdb\u884c\u7ba1\u7406\u3002 \u4f8b\u5982\uff0c\u670d\u52a1\u7ec4\u4ef6\u901a\u8fc7 knative-serving \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u4e8b\u4ef6\u7ec4\u4ef6\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u7b49\u7b49\u3002 Knative\u7ec4\u4ef6\u4f7f\u7528 zap \u65e5\u5fd7\u5e93;\u9009\u9879 \u5728\u8be5\u9879\u76ee\u4e2d\u6709\u66f4\u8be6\u7ec6\u7684\u6587\u6863 \u3002 \u9664\u4e86 zap-logger-config \uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u952e\uff0c\u9002\u7528\u4e8e\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u7ec4\u4ef6\uff0c config-logging ConfigMap\u652f\u6301\u8986\u76d6\u5355\u4e2a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u7ea7\u522b\u3002 ConfigMap key Description zap-logger-config \u7528\u4e8ezap\u8bb0\u5f55\u5668\u914d\u7f6e\u7684JSON\u5bf9\u8c61\u5bb9\u5668\u3002\u5173\u952e\u5b57\u6bb5\u5728\u4e0b\u9762\u7a81\u51fa\u663e\u793a\u3002 zap-logger-config.level \u7ec4\u4ef6\u7684\u9ed8\u8ba4\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 zap-logger-config.encoding \u7ec4\u4ef6\u65e5\u5fd7\u7684\u65e5\u5fd7\u7f16\u7801\u683c\u5f0f(\u9ed8\u8ba4\u4e3aJSON)\u3002 zap-logger-config.encoderConfig \u7528\u4e8e\u81ea\u5b9a\u4e49\u8bb0\u5f55\u5185\u5bb9\u7684 zap EncoderConfig \u3002 loglevel.<component> \u4ec5\u8986\u76d6\u7ed9\u5b9a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 Zap\u652f\u6301\u7684\u65e5\u5fd7\u7ea7\u522b\u6709: debug - \u7ec6\u7c92\u5ea6\u7684\u8c03\u8bd5 info - \u6b63\u5e38\u7684\u65e5\u5fd7 warn - \u610f\u5916\u4f46\u975e\u5173\u952e\u7684\u9519\u8bef error - \u5173\u952e\u7684\u9519\u8bef;\u6b63\u5e38\u64cd\u4f5c\u65f6\u51fa\u73b0\u610f\u5916 dpanic - \u5728\u8c03\u8bd5\u6a21\u5f0f\u4e0b\uff0c\u89e6\u53d1\u6050\u614c(\u5d29\u6e83) panic - \u5f15\u53d1\u6050\u614c(\u5d29\u6e83) fatal - \u7acb\u5373\u9000\u51fa\uff0c\u9000\u51fa\u72b6\u6001\u4e3a1(\u5931\u8d25)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bb0\u5f55"},{"location":"eventing/observability/logging/config-logging/#_1","text":"\u6240\u6709Knative\u7ec4\u4ef6\u7684\u65e5\u5fd7\u914d\u7f6e\u90fd\u901a\u8fc7\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging ConfigMap\u8fdb\u884c\u7ba1\u7406\u3002 \u4f8b\u5982\uff0c\u670d\u52a1\u7ec4\u4ef6\u901a\u8fc7 knative-serving \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u4e8b\u4ef6\u7ec4\u4ef6\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u7b49\u7b49\u3002 Knative\u7ec4\u4ef6\u4f7f\u7528 zap \u65e5\u5fd7\u5e93;\u9009\u9879 \u5728\u8be5\u9879\u76ee\u4e2d\u6709\u66f4\u8be6\u7ec6\u7684\u6587\u6863 \u3002 \u9664\u4e86 zap-logger-config \uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u952e\uff0c\u9002\u7528\u4e8e\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u7ec4\u4ef6\uff0c config-logging ConfigMap\u652f\u6301\u8986\u76d6\u5355\u4e2a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u7ea7\u522b\u3002 ConfigMap key Description zap-logger-config \u7528\u4e8ezap\u8bb0\u5f55\u5668\u914d\u7f6e\u7684JSON\u5bf9\u8c61\u5bb9\u5668\u3002\u5173\u952e\u5b57\u6bb5\u5728\u4e0b\u9762\u7a81\u51fa\u663e\u793a\u3002 zap-logger-config.level \u7ec4\u4ef6\u7684\u9ed8\u8ba4\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 zap-logger-config.encoding \u7ec4\u4ef6\u65e5\u5fd7\u7684\u65e5\u5fd7\u7f16\u7801\u683c\u5f0f(\u9ed8\u8ba4\u4e3aJSON)\u3002 zap-logger-config.encoderConfig \u7528\u4e8e\u81ea\u5b9a\u4e49\u8bb0\u5f55\u5185\u5bb9\u7684 zap EncoderConfig \u3002 loglevel.<component> \u4ec5\u8986\u76d6\u7ed9\u5b9a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 Zap\u652f\u6301\u7684\u65e5\u5fd7\u7ea7\u522b\u6709: debug - \u7ec6\u7c92\u5ea6\u7684\u8c03\u8bd5 info - \u6b63\u5e38\u7684\u65e5\u5fd7 warn - \u610f\u5916\u4f46\u975e\u5173\u952e\u7684\u9519\u8bef error - \u5173\u952e\u7684\u9519\u8bef;\u6b63\u5e38\u64cd\u4f5c\u65f6\u51fa\u73b0\u610f\u5916 dpanic - \u5728\u8c03\u8bd5\u6a21\u5f0f\u4e0b\uff0c\u89e6\u53d1\u6050\u614c(\u5d29\u6e83) panic - \u5f15\u53d1\u6050\u614c(\u5d29\u6e83) fatal - \u7acb\u5373\u9000\u51fa\uff0c\u9000\u51fa\u72b6\u6001\u4e3a1(\u5931\u8d25)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e"},{"location":"eventing/observability/metrics/collecting-metrics/","text":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf \u00b6 Knative\u652f\u6301\u6536\u96c6\u6307\u6807\u7684\u4e0d\u540c\u6d41\u884c\u5de5\u5177: Prometheus OpenTelemetry Collector Grafana \u4eea\u8868\u677f\u53ef\u7528\u4e8e\u76f4\u63a5\u4ecePrometheus\u6536\u96c6\u7684\u6307\u6807\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6eOpenTelemetry Collector\uff0c\u4ee5\u4fbf\u4eceKnative\u7ec4\u4ef6\u63a5\u6536\u5ea6\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u53d1\u7ed9\u652f\u6301OpenTelemetry\u7684\u5176\u4ed6\u5ea6\u91cf\u63d0\u4f9b\u7a0b\u5e8f\u3002 Warning \u60a8\u4e0d\u80fd\u540c\u65f6\u4f7f\u7528OpenTelemetry Collector\u548cPrometheus\u3002 \u9ed8\u8ba4\u7684\u5ea6\u91cf\u540e\u7aef\u662fPrometheus\u3002 \u4f60\u9700\u8981\u4ece config-observability Configmap\u4e2d\u5220\u9664 metrics.backend-destination \u548c metrics.request-metrics-backend-destination \u952e\u6765\u542f\u7528Prometheus\u5ea6\u91cf\u3002 \u5173\u4e8e Prometheus \u00b6 Prometheus \u662f\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u3001\u805a\u5408\u65f6\u95f4\u5e8f\u5217\u5ea6\u91cf\u548c\u8b66\u62a5\u7684\u5f00\u6e90\u5de5\u5177\u3002 \u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u522e\u9664OpenTelemetry Collector\uff0c\u4e0b\u9762\u5c06\u5728\u4f7f\u7528Prometheus\u65f6\u6f14\u793a\u8fd9\u4e00\u70b9\u3002 \u914d\u7f6e Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml \u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . \u5173\u4e8e OpenTelemetry \u00b6 OpenTelemetry\u662f\u4e00\u4e2a\u9488\u5bf9\u4e91\u539f\u751f\u8f6f\u4ef6\u7684CNCF\u53ef\u89c2\u5bdf\u6027\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u7ec4\u5de5\u5177\u3001api\u548csdk\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528OpenTelemetry\u6765\u6d4b\u91cf\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u3002 \u8fd9\u4e9b\u6570\u636e\u5305\u62ec\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u6765\u4e86\u89e3Knative\u7ec4\u4ef6\u7684\u6027\u80fd\u548c\u884c\u4e3a\u3002 OpenTelemetry\u5141\u8bb8\u60a8\u8f7b\u677e\u5730\u5c06\u6307\u6807\u5bfc\u51fa\u5230\u591a\u4e2a\u76d1\u89c6\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u6784\u5efa\u6216\u91cd\u65b0\u914d\u7f6eKnative\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u7406\u89e3\u6536\u96c6\u5668 \u00b6 \u6536\u96c6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u5404\u79cdKnative\u7ec4\u4ef6\u53ef\u4ee5\u5728\u5176\u4e2d\u63a8\u9001\u7531\u76d1\u89c6\u670d\u52a1\u4fdd\u7559\u548c\u6536\u96c6\u7684\u6307\u6807\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528ConfigMap\u548cDeployment\u914d\u7f6e\u5355\u4e2a\u6536\u96c6\u5668\u5b9e\u4f8b\u3002 Tip \u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u90e8\u7f72\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 OpenTelemetry Operator \u81ea\u52a8\u5316\u8fd9\u4e9b\u6b65\u9aa4\u4e2d\u7684\u4e00\u4e9b\u3002 Caution https://github.com/knative-sandbox/monitoring/tree/main/grafana\u4e0a\u7684Grafana\u4eea\u8868\u677f\u4e0d\u80fd\u4f7f\u7528\u4eceOpenTelemetry Collector\u4e2d\u63d0\u53d6\u7684\u6307\u6807\u3002 \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' \u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u6536\u96c6\u5ea6\u91cf\u6807\u51c6"},{"location":"eventing/observability/metrics/collecting-metrics/#knative","text":"Knative\u652f\u6301\u6536\u96c6\u6307\u6807\u7684\u4e0d\u540c\u6d41\u884c\u5de5\u5177: Prometheus OpenTelemetry Collector Grafana \u4eea\u8868\u677f\u53ef\u7528\u4e8e\u76f4\u63a5\u4ecePrometheus\u6536\u96c6\u7684\u6307\u6807\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6eOpenTelemetry Collector\uff0c\u4ee5\u4fbf\u4eceKnative\u7ec4\u4ef6\u63a5\u6536\u5ea6\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u53d1\u7ed9\u652f\u6301OpenTelemetry\u7684\u5176\u4ed6\u5ea6\u91cf\u63d0\u4f9b\u7a0b\u5e8f\u3002 Warning \u60a8\u4e0d\u80fd\u540c\u65f6\u4f7f\u7528OpenTelemetry Collector\u548cPrometheus\u3002 \u9ed8\u8ba4\u7684\u5ea6\u91cf\u540e\u7aef\u662fPrometheus\u3002 \u4f60\u9700\u8981\u4ece config-observability Configmap\u4e2d\u5220\u9664 metrics.backend-destination \u548c metrics.request-metrics-backend-destination \u952e\u6765\u542f\u7528Prometheus\u5ea6\u91cf\u3002","title":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus","text":"Prometheus \u662f\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u3001\u805a\u5408\u65f6\u95f4\u5e8f\u5217\u5ea6\u91cf\u548c\u8b66\u62a5\u7684\u5f00\u6e90\u5de5\u5177\u3002 \u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u522e\u9664OpenTelemetry Collector\uff0c\u4e0b\u9762\u5c06\u5728\u4f7f\u7528Prometheus\u65f6\u6f14\u793a\u8fd9\u4e00\u70b9\u3002","title":"\u5173\u4e8e Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus_1","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"\u914d\u7f6e Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus_2","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"\u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b"},{"location":"eventing/observability/metrics/collecting-metrics/#opentelemetry","text":"OpenTelemetry\u662f\u4e00\u4e2a\u9488\u5bf9\u4e91\u539f\u751f\u8f6f\u4ef6\u7684CNCF\u53ef\u89c2\u5bdf\u6027\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u7ec4\u5de5\u5177\u3001api\u548csdk\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528OpenTelemetry\u6765\u6d4b\u91cf\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u3002 \u8fd9\u4e9b\u6570\u636e\u5305\u62ec\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u6765\u4e86\u89e3Knative\u7ec4\u4ef6\u7684\u6027\u80fd\u548c\u884c\u4e3a\u3002 OpenTelemetry\u5141\u8bb8\u60a8\u8f7b\u677e\u5730\u5c06\u6307\u6807\u5bfc\u51fa\u5230\u591a\u4e2a\u76d1\u89c6\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u6784\u5efa\u6216\u91cd\u65b0\u914d\u7f6eKnative\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002","title":"\u5173\u4e8e OpenTelemetry"},{"location":"eventing/observability/metrics/collecting-metrics/#_1","text":"\u6536\u96c6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u5404\u79cdKnative\u7ec4\u4ef6\u53ef\u4ee5\u5728\u5176\u4e2d\u63a8\u9001\u7531\u76d1\u89c6\u670d\u52a1\u4fdd\u7559\u548c\u6536\u96c6\u7684\u6307\u6807\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528ConfigMap\u548cDeployment\u914d\u7f6e\u5355\u4e2a\u6536\u96c6\u5668\u5b9e\u4f8b\u3002 Tip \u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u90e8\u7f72\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 OpenTelemetry Operator \u81ea\u52a8\u5316\u8fd9\u4e9b\u6b65\u9aa4\u4e2d\u7684\u4e00\u4e9b\u3002 Caution https://github.com/knative-sandbox/monitoring/tree/main/grafana\u4e0a\u7684Grafana\u4eea\u8868\u677f\u4e0d\u80fd\u4f7f\u7528\u4eceOpenTelemetry Collector\u4e2d\u63d0\u53d6\u7684\u6307\u6807\u3002","title":"\u7406\u89e3\u6536\u96c6\u5668"},{"location":"eventing/observability/metrics/collecting-metrics/#_2","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"eventing/observability/metrics/collecting-metrics/#_3","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e"},{"location":"eventing/observability/metrics/eventing-metrics/","text":"Knative \u4e8b\u4ef6\u5ea6\u91cf \u00b6 \u7ba1\u7406\u5458\u53ef\u4ee5\u67e5\u770bKnative\u4e8b\u4ef6\u7ec4\u4ef6\u7684\u5ea6\u91cf\u3002 Broker - Ingress \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u5ea6\u91cf\u6765\u8c03\u8bd5\u4ee3\u7406\u5165\u63a5\u53e3\u7684\u6267\u884c\u65b9\u5f0f\u4ee5\u53ca\u901a\u8fc7\u5165\u63a5\u53e3\u7ec4\u4ef6\u5206\u6d3e\u7684\u4e8b\u4ef6\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable Broker - Filter \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6307\u6807\u6765\u8c03\u8bd5\u4ee3\u7406\u7b5b\u9009\u5668\u5982\u4f55\u6267\u884c\u4ee5\u53ca\u901a\u8fc7\u7b5b\u9009\u5668\u7ec4\u4ef6\u5206\u6d3e\u4ec0\u4e48\u4e8b\u4ef6\u3002 \u6b64\u5916\uff0c\u7528\u6237\u8fd8\u53ef\u4ee5\u6d4b\u91cf\u4e8b\u4ef6\u4e0a\u5b9e\u9645\u8fc7\u6ee4\u64cd\u4f5c\u7684\u5ef6\u8fdf\u65f6\u95f4\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable In-memory Dispatcher \u00b6 \u5185\u5b58\u901a\u9053\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section . \u4e8b\u4ef6\u6e90 \u00b6 \u4e8b\u4ef6\u6e90\u7531\u62e5\u6709\u76f8\u5173\u7cfb\u7edf\u7684\u7528\u6237\u521b\u5efa\uff0c\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u7528\u4e8b\u4ef6\u89e6\u53d1\u5e94\u7528\u7a0b\u5e8f\u3002 \u6bcf\u4e2a\u6e90\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u90fd\u516c\u5f00\u4e86\u4e00\u4e9b\u5ea6\u91cf\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u76d1\u89c6\u5206\u6d3e\u7684\u4e8b\u4ef6\u3002 \u4f7f\u7528\u4ee5\u4e0b\u5ea6\u91cf\u6765\u9a8c\u8bc1\u4e8b\u4ef6\u5df2\u7ecf\u4ece\u6e90\u7aef\u4ea4\u4ed8\uff0c\u4ece\u800c\u9a8c\u8bc1\u6e90\u548c\u4e0e\u6e90\u7684\u4efb\u4f55\u8fde\u63a5\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002 Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"\u4e8b\u4ef6\u6307\u6807"},{"location":"eventing/observability/metrics/eventing-metrics/#knative","text":"\u7ba1\u7406\u5458\u53ef\u4ee5\u67e5\u770bKnative\u4e8b\u4ef6\u7ec4\u4ef6\u7684\u5ea6\u91cf\u3002","title":"Knative \u4e8b\u4ef6\u5ea6\u91cf"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-ingress","text":"\u4f7f\u7528\u4ee5\u4e0b\u5ea6\u91cf\u6765\u8c03\u8bd5\u4ee3\u7406\u5165\u63a5\u53e3\u7684\u6267\u884c\u65b9\u5f0f\u4ee5\u53ca\u901a\u8fc7\u5165\u63a5\u53e3\u7ec4\u4ef6\u5206\u6d3e\u7684\u4e8b\u4ef6\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable","title":"Broker - Ingress"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-filter","text":"\u4f7f\u7528\u4ee5\u4e0b\u6307\u6807\u6765\u8c03\u8bd5\u4ee3\u7406\u7b5b\u9009\u5668\u5982\u4f55\u6267\u884c\u4ee5\u53ca\u901a\u8fc7\u7b5b\u9009\u5668\u7ec4\u4ef6\u5206\u6d3e\u4ec0\u4e48\u4e8b\u4ef6\u3002 \u6b64\u5916\uff0c\u7528\u6237\u8fd8\u53ef\u4ee5\u6d4b\u91cf\u4e8b\u4ef6\u4e0a\u5b9e\u9645\u8fc7\u6ee4\u64cd\u4f5c\u7684\u5ef6\u8fdf\u65f6\u95f4\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable","title":"Broker - Filter"},{"location":"eventing/observability/metrics/eventing-metrics/#in-memory-dispatcher","text":"\u5185\u5b58\u901a\u9053\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002 \u901a\u8fc7\u5728http\u4ee3\u7801\u4e0a\u805a\u5408\u5ea6\u91cf\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b:\u6210\u529f\u4e8b\u4ef6(2xx)\u548c\u5931\u8d25\u4e8b\u4ef6(5xx)\u3002 Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section .","title":"In-memory Dispatcher"},{"location":"eventing/observability/metrics/eventing-metrics/#_1","text":"\u4e8b\u4ef6\u6e90\u7531\u62e5\u6709\u76f8\u5173\u7cfb\u7edf\u7684\u7528\u6237\u521b\u5efa\uff0c\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u7528\u4e8b\u4ef6\u89e6\u53d1\u5e94\u7528\u7a0b\u5e8f\u3002 \u6bcf\u4e2a\u6e90\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u90fd\u516c\u5f00\u4e86\u4e00\u4e9b\u5ea6\u91cf\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u76d1\u89c6\u5206\u6d3e\u7684\u4e8b\u4ef6\u3002 \u4f7f\u7528\u4ee5\u4e0b\u5ea6\u91cf\u6765\u9a8c\u8bc1\u4e8b\u4ef6\u5df2\u7ecf\u4ece\u6e90\u7aef\u4ea4\u4ed8\uff0c\u4ece\u800c\u9a8c\u8bc1\u6e90\u548c\u4e0e\u6e90\u7684\u4efb\u4f55\u8fde\u63a5\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002 Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"\u4e8b\u4ef6\u6e90"},{"location":"eventing/reference/eventing-api/","text":"\u5728\u6784\u5efa\u8fc7\u7a0b\u4e2d\uff0c\u8be5\u6587\u4ef6\u4ece \u4e8b\u4ef6repo \u4e2d\u66f4\u65b0\u5230\u6b63\u786e\u7684\u7248\u672c\u3002","title":"Eventing API"},{"location":"eventing/sinks/","text":"\u5173\u4e8e\u63a5\u6536\u5668 \u00b6 \u5728\u521b\u5efa\u4e8b\u4ef6\u6e90\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e2a \u63a5\u6536\u5668 \uff0c\u5c06\u4e8b\u4ef6\u4ece\u6e90\u53d1\u9001\u5230\u8be5 \u63a5\u6536\u5668 \u3002 \u63a5\u6536\u5668\u662f\u4e00\u4e2a \u53ef\u5bfb\u5740 \u6216 \u53ef\u8c03\u7528\u7684 \u8d44\u6e90\uff0c\u53ef\u4ee5\u4ece\u5176\u4ed6\u8d44\u6e90\u63a5\u6536\u4f20\u5165\u7684\u4e8b\u4ef6\u3002 Knative\u670d\u52a1\u3001\u901a\u9053\u548c\u4ee3\u7406\u90fd\u662f\u63a5\u6536\u5668\u7684\u4f8b\u5b50\u3002 \u53ef\u5bfb\u5740\u5bf9\u8c61\u63a5\u6536\u5e76\u786e\u8ba4\u901a\u8fc7HTTP\u4f20\u9012\u5230 status.address.url \u5b57\u6bb5\u4e2d\u5b9a\u4e49\u7684\u5730\u5740\u7684\u4e8b\u4ef6\u3002 \u4f5c\u4e3a\u4e00\u4e2a\u7279\u4f8b\uff0c\u6838\u5fc3 Kubernetes\u670d\u52a1\u5bf9\u8c61 \u4e5f\u5b9e\u73b0\u4e86\u53ef\u5bfb\u5740\u63a5\u53e3\u3002 \u53ef\u8c03\u7528\u5bf9\u8c61\u80fd\u591f\u63a5\u6536\u901a\u8fc7HTTP\u4f20\u9012\u7684\u4e8b\u4ef6\u5e76\u8f6c\u6362\u8be5\u4e8b\u4ef6\uff0c\u5728HTTP\u54cd\u5e94\u4e2d\u8fd4\u56de0\u62161\u4e2a\u65b0\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u8fd4\u56de\u7684\u4e8b\u4ef6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5904\u7406\uff0c\u5904\u7406\u65b9\u5f0f\u4e0e\u5904\u7406\u6765\u81ea\u5916\u90e8\u4e8b\u4ef6\u6e90\u7684\u4e8b\u4ef6\u76f8\u540c\u3002 Sink\u4f5c\u4e3a\u53c2\u6570 \u00b6 Sink\u7528\u4f5c\u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u63a5\u6536\u5668 \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u9009 or \u53ef\u9009 ref \u8fd9\u6307\u5411\u4e00\u4e2a\u53ef\u5bfb\u5740\u7684\u3002 \u5fc5\u9009 \u5982\u679c \u4e0d \u4f7f\u7528 uri ref.apiVersion \u5f15\u7528\u7684API\u7248\u672c\u3002 \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref ref.kind \u5f15\u7528\u5bf9\u8c61Kind. \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref ref.namespace \u5f15\u7528\u5bf9\u8c61\u7684\u547d\u540d\u7a7a\u95f4\u3002\u5982\u679c\u7701\u7565\u8be5\u53c2\u6570\uff0c\u9ed8\u8ba4\u4e3a\u4fdd\u5b58\u5b83\u7684\u5bf9\u8c61\u3002 \u53ef\u9009 ref.name \u5f15\u7528\u5bf9\u8c61\u540d\u5b57 \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref uri \u8fd9\u53ef\u4ee5\u662f\u4e00\u4e2a\u5e26\u6709\u975e\u7a7a\u65b9\u6848\u548c\u6307\u5411\u76ee\u6807\u6216\u76f8\u5bf9URI\u7684\u975e\u7a7a\u4e3b\u673a\u7684\u7edd\u5bf9URL\u3002\u4f7f\u7528\u4eceRef\u68c0\u7d22\u7684\u57faURI\u89e3\u6790\u76f8\u5bf9URI\u3002 \u5fc5\u9009 \u5982\u679c \u4e0d \u4f7f\u7528 ref Note \u81f3\u5c11\u9700\u8981 ref or uri \u4e2d\u7684\u4e00\u4e2a\u3002 \u5982\u679c\u4e24\u8005\u90fd\u6307\u5b9a\u4e86\uff0c uri \u5c06\u4ece\u53ef\u5bfb\u5740 ref \u7ed3\u679c\u89e3\u6790\u4e3aURL\u3002 \u53c2\u6570\u793a\u4f8b \u00b6 \u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5982\u679c ref \u89e3\u6790\u4e3a \"http://mysink.default.svc.cluster.local\" \uff0c\u5219 uri \u88ab\u6dfb\u52a0\u5230\u5176\u4e2d\uff0c\u7ed3\u679c\u4e3a \"http://mysink.default.svc.cluster.local/extra/path\" \u3002 apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract \u8fd9\u5c06\u5bfc\u81f4\u5728 subject \u4e0a\u5c06 K_SINK \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a \"http://mysink.default.svc.cluster.local/extra/path\" \u3002 \u4f7f\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u4f5c\u4e3a\u63a5\u6536\u5668 \u00b6 \u8981\u4f7f\u7528Kubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4f5c\u4e3a\u4e8b\u4ef6\u63a5\u6536\u5668\uff0c\u60a8\u5fc5\u987b: \u4f7fCR\u53ef\u5bfb\u5740\u3002 \u60a8\u5fc5\u987b\u786e\u4fddCR\u5305\u542b status.address.url \u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u53ef\u5bfb\u5740\u8d44\u6e90 \u89c4\u8303\u3002 \u521b\u5efa\u4e00\u4e2a\u53ef\u5bfb\u5740\u89e3\u6790\u5668ClusterRole\uff0c\u4ee5\u83b7\u53d6\u63a5\u6536\u4e8b\u4ef6\u6240\u9700\u7684RBAC\u89c4\u5219\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a kafkasinks-addressable-resolver ClusterRole\u6765\u5141\u8bb8 get , list , and watch \u8bbf\u95eeKafkaSink\u5bf9\u8c61\u548c\u72b6\u6001: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch \u901a\u8fc7\u4f7f\u7528\u89e6\u53d1\u5668\u8fc7\u6ee4\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6 \u00b6 \u60a8\u53ef\u4ee5\u5c06\u89e6\u53d1\u5668\u8fde\u63a5\u5230\u63a5\u6536\u5668\uff0c\u4ee5\u4fbf\u5728\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u63a5\u6536\u5668\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u8fc7\u6ee4\u3002 \u8fde\u63a5\u5230\u89e6\u53d1\u5668\u7684\u63a5\u6536\u5668\u5728\u89e6\u53d1\u5668\u8d44\u6e90\u89c4\u8303\u4e2d\u88ab\u914d\u7f6e\u4e3a subscriber \u3002 \u4f8b\u5982: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> \u5b83\u662f\u8fde\u63a5\u5230\u63a5\u6536\u5668\u7684\u89e6\u53d1\u5668\u7684\u540d\u79f0\u3002 <kafka-sink-name> \u5b83\u662f\u4e00\u4e2aKafkaSink\u5bf9\u8c61\u7684\u540d\u79f0\u3002 \u4f7f\u7528 kn CLI --sink \u6807\u5fd7\u6307\u5b9a\u63a5\u6536\u5668 \u00b6 \u5f53\u60a8\u4f7f\u7528Knative ( kn ) CLI\u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\u751f\u6210CR\u65f6\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 --sink \u6807\u5fd7\u6307\u5b9a\u4e00\u4e2a\u4ece\u8be5\u8d44\u6e90\u53d1\u9001\u4e8b\u4ef6\u7684\u63a5\u6536\u5668\u3002 \u4e0b\u9762\u7684\u4f8b\u5b50\u521b\u5efa\u4e86\u4e00\u4e2a\u4f7f\u7528\u670d\u52a1 http://event-display.svc.cluster.local \u4f5c\u4e3a\u63a5\u6536\u5668\u7684SinkBinding: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" http://event-display.svc.cluster.local \u4e2d\u7684 svc \u51b3\u5b9a\u63a5\u6536\u5668\u662fKnative\u670d\u52a1\u3002 \u5176\u4ed6\u9ed8\u8ba4\u63a5\u6536\u5668\u524d\u7f00\u5305\u62ec\u901a\u9053\u548c\u4ee3\u7406\u3002 Tip \u4f60\u53ef\u4ee5\u901a\u8fc7 \u81ea\u5b9a\u4e49 kn \u4e3a kn CLI\u547d\u4ee4\u4f7f\u7528 --sink \u6807\u5fd7\u6765\u914d\u7f6e\u54ea\u4e9b\u8d44\u6e90\u53ef\u4ee5\u88ab\u4f7f\u7528. \u652f\u6301\u7684\u7b2c\u4e09\u65b9\u63a5\u6536\u5668\u7c7b\u578b \u00b6 Name Maintainer Description KafkaSink Knative \u53d1\u9001\u4e8b\u4ef6\u5230Kafka\u4e3b\u9898 RedisStreamSink Knative \u53d1\u9001\u4e8b\u4ef6\u5230Redis\u6d41","title":"\u5173\u4e8e\u63a5\u6536\u5668"},{"location":"eventing/sinks/#_1","text":"\u5728\u521b\u5efa\u4e8b\u4ef6\u6e90\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e2a \u63a5\u6536\u5668 \uff0c\u5c06\u4e8b\u4ef6\u4ece\u6e90\u53d1\u9001\u5230\u8be5 \u63a5\u6536\u5668 \u3002 \u63a5\u6536\u5668\u662f\u4e00\u4e2a \u53ef\u5bfb\u5740 \u6216 \u53ef\u8c03\u7528\u7684 \u8d44\u6e90\uff0c\u53ef\u4ee5\u4ece\u5176\u4ed6\u8d44\u6e90\u63a5\u6536\u4f20\u5165\u7684\u4e8b\u4ef6\u3002 Knative\u670d\u52a1\u3001\u901a\u9053\u548c\u4ee3\u7406\u90fd\u662f\u63a5\u6536\u5668\u7684\u4f8b\u5b50\u3002 \u53ef\u5bfb\u5740\u5bf9\u8c61\u63a5\u6536\u5e76\u786e\u8ba4\u901a\u8fc7HTTP\u4f20\u9012\u5230 status.address.url \u5b57\u6bb5\u4e2d\u5b9a\u4e49\u7684\u5730\u5740\u7684\u4e8b\u4ef6\u3002 \u4f5c\u4e3a\u4e00\u4e2a\u7279\u4f8b\uff0c\u6838\u5fc3 Kubernetes\u670d\u52a1\u5bf9\u8c61 \u4e5f\u5b9e\u73b0\u4e86\u53ef\u5bfb\u5740\u63a5\u53e3\u3002 \u53ef\u8c03\u7528\u5bf9\u8c61\u80fd\u591f\u63a5\u6536\u901a\u8fc7HTTP\u4f20\u9012\u7684\u4e8b\u4ef6\u5e76\u8f6c\u6362\u8be5\u4e8b\u4ef6\uff0c\u5728HTTP\u54cd\u5e94\u4e2d\u8fd4\u56de0\u62161\u4e2a\u65b0\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u8fd4\u56de\u7684\u4e8b\u4ef6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5904\u7406\uff0c\u5904\u7406\u65b9\u5f0f\u4e0e\u5904\u7406\u6765\u81ea\u5916\u90e8\u4e8b\u4ef6\u6e90\u7684\u4e8b\u4ef6\u76f8\u540c\u3002","title":"\u5173\u4e8e\u63a5\u6536\u5668"},{"location":"eventing/sinks/#sink","text":"Sink\u7528\u4f5c\u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u63a5\u6536\u5668 \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u9009 or \u53ef\u9009 ref \u8fd9\u6307\u5411\u4e00\u4e2a\u53ef\u5bfb\u5740\u7684\u3002 \u5fc5\u9009 \u5982\u679c \u4e0d \u4f7f\u7528 uri ref.apiVersion \u5f15\u7528\u7684API\u7248\u672c\u3002 \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref ref.kind \u5f15\u7528\u5bf9\u8c61Kind. \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref ref.namespace \u5f15\u7528\u5bf9\u8c61\u7684\u547d\u540d\u7a7a\u95f4\u3002\u5982\u679c\u7701\u7565\u8be5\u53c2\u6570\uff0c\u9ed8\u8ba4\u4e3a\u4fdd\u5b58\u5b83\u7684\u5bf9\u8c61\u3002 \u53ef\u9009 ref.name \u5f15\u7528\u5bf9\u8c61\u540d\u5b57 \u5fc5\u9009 \u5982\u679c\u4f7f\u7528 ref uri \u8fd9\u53ef\u4ee5\u662f\u4e00\u4e2a\u5e26\u6709\u975e\u7a7a\u65b9\u6848\u548c\u6307\u5411\u76ee\u6807\u6216\u76f8\u5bf9URI\u7684\u975e\u7a7a\u4e3b\u673a\u7684\u7edd\u5bf9URL\u3002\u4f7f\u7528\u4eceRef\u68c0\u7d22\u7684\u57faURI\u89e3\u6790\u76f8\u5bf9URI\u3002 \u5fc5\u9009 \u5982\u679c \u4e0d \u4f7f\u7528 ref Note \u81f3\u5c11\u9700\u8981 ref or uri \u4e2d\u7684\u4e00\u4e2a\u3002 \u5982\u679c\u4e24\u8005\u90fd\u6307\u5b9a\u4e86\uff0c uri \u5c06\u4ece\u53ef\u5bfb\u5740 ref \u7ed3\u679c\u89e3\u6790\u4e3aURL\u3002","title":"Sink\u4f5c\u4e3a\u53c2\u6570"},{"location":"eventing/sinks/#_2","text":"\u7ed9\u5b9a\u4e0b\u9762\u7684YAML\uff0c\u5982\u679c ref \u89e3\u6790\u4e3a \"http://mysink.default.svc.cluster.local\" \uff0c\u5219 uri \u88ab\u6dfb\u52a0\u5230\u5176\u4e2d\uff0c\u7ed3\u679c\u4e3a \"http://mysink.default.svc.cluster.local/extra/path\" \u3002 apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract \u8fd9\u5c06\u5bfc\u81f4\u5728 subject \u4e0a\u5c06 K_SINK \u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a \"http://mysink.default.svc.cluster.local/extra/path\" \u3002","title":"\u53c2\u6570\u793a\u4f8b"},{"location":"eventing/sinks/#_3","text":"\u8981\u4f7f\u7528Kubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4f5c\u4e3a\u4e8b\u4ef6\u63a5\u6536\u5668\uff0c\u60a8\u5fc5\u987b: \u4f7fCR\u53ef\u5bfb\u5740\u3002 \u60a8\u5fc5\u987b\u786e\u4fddCR\u5305\u542b status.address.url \u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u53ef\u5bfb\u5740\u8d44\u6e90 \u89c4\u8303\u3002 \u521b\u5efa\u4e00\u4e2a\u53ef\u5bfb\u5740\u89e3\u6790\u5668ClusterRole\uff0c\u4ee5\u83b7\u53d6\u63a5\u6536\u4e8b\u4ef6\u6240\u9700\u7684RBAC\u89c4\u5219\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a kafkasinks-addressable-resolver ClusterRole\u6765\u5141\u8bb8 get , list , and watch \u8bbf\u95eeKafkaSink\u5bf9\u8c61\u548c\u72b6\u6001: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u4f5c\u4e3a\u63a5\u6536\u5668"},{"location":"eventing/sinks/#_4","text":"\u60a8\u53ef\u4ee5\u5c06\u89e6\u53d1\u5668\u8fde\u63a5\u5230\u63a5\u6536\u5668\uff0c\u4ee5\u4fbf\u5728\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u63a5\u6536\u5668\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u8fc7\u6ee4\u3002 \u8fde\u63a5\u5230\u89e6\u53d1\u5668\u7684\u63a5\u6536\u5668\u5728\u89e6\u53d1\u5668\u8d44\u6e90\u89c4\u8303\u4e2d\u88ab\u914d\u7f6e\u4e3a subscriber \u3002 \u4f8b\u5982: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> \u5b83\u662f\u8fde\u63a5\u5230\u63a5\u6536\u5668\u7684\u89e6\u53d1\u5668\u7684\u540d\u79f0\u3002 <kafka-sink-name> \u5b83\u662f\u4e00\u4e2aKafkaSink\u5bf9\u8c61\u7684\u540d\u79f0\u3002","title":"\u901a\u8fc7\u4f7f\u7528\u89e6\u53d1\u5668\u8fc7\u6ee4\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6"},{"location":"eventing/sinks/#kn-cli-sink","text":"\u5f53\u60a8\u4f7f\u7528Knative ( kn ) CLI\u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\u751f\u6210CR\u65f6\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 --sink \u6807\u5fd7\u6307\u5b9a\u4e00\u4e2a\u4ece\u8be5\u8d44\u6e90\u53d1\u9001\u4e8b\u4ef6\u7684\u63a5\u6536\u5668\u3002 \u4e0b\u9762\u7684\u4f8b\u5b50\u521b\u5efa\u4e86\u4e00\u4e2a\u4f7f\u7528\u670d\u52a1 http://event-display.svc.cluster.local \u4f5c\u4e3a\u63a5\u6536\u5668\u7684SinkBinding: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" http://event-display.svc.cluster.local \u4e2d\u7684 svc \u51b3\u5b9a\u63a5\u6536\u5668\u662fKnative\u670d\u52a1\u3002 \u5176\u4ed6\u9ed8\u8ba4\u63a5\u6536\u5668\u524d\u7f00\u5305\u62ec\u901a\u9053\u548c\u4ee3\u7406\u3002 Tip \u4f60\u53ef\u4ee5\u901a\u8fc7 \u81ea\u5b9a\u4e49 kn \u4e3a kn CLI\u547d\u4ee4\u4f7f\u7528 --sink \u6807\u5fd7\u6765\u914d\u7f6e\u54ea\u4e9b\u8d44\u6e90\u53ef\u4ee5\u88ab\u4f7f\u7528.","title":"\u4f7f\u7528 kn CLI --sink \u6807\u5fd7\u6307\u5b9a\u63a5\u6536\u5668"},{"location":"eventing/sinks/#_5","text":"Name Maintainer Description KafkaSink Knative \u53d1\u9001\u4e8b\u4ef6\u5230Kafka\u4e3b\u9898 RedisStreamSink Knative \u53d1\u9001\u4e8b\u4ef6\u5230Redis\u6d41","title":"\u652f\u6301\u7684\u7b2c\u4e09\u65b9\u63a5\u6536\u5668\u7c7b\u578b"},{"location":"eventing/sinks/kafka-sink/","text":"Apache Kafka Sink \u00b6 \u672c\u9875\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5\u548c\u914d\u7f6eApache KafkaSink\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u5fc5\u987b\u80fd\u591f\u8bbf\u95ee\u5b89\u88c5\u4e86 Knative\u4e8b\u4ef6 \u7684Kubernetes\u96c6\u7fa4. \u5b89\u88c5 \u00b6 \u5b89\u88c5Kafka\u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml \u5b89\u88c5KafkaSink\u6570\u636e\u5e73\u9762: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml \u9a8c\u8bc1 kafka-controller \u548c kafka-sink-receiver \u90e8\u7f72\u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s KafkaSink\u4f8b\u5b50 \u00b6 KafkaSink\u5bf9\u8c61\u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 \u8f93\u51fa\u4e3b\u9898\u5185\u5bb9\u6a21\u5f0f \u00b6 CloudEvent\u89c4\u8303\u5b9a\u4e49\u4e86\u4e24\u79cd\u4f20\u8f93clouddevent\u7684\u6a21\u5f0f:\u7ed3\u6784\u5316\u548c\u4e8c\u8fdb\u5236\u3002 A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: binary. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured Security \u00b6 Knative supports the following Apache Kafka security features: Apache Kafka Sink \u5148\u51b3\u6761\u4ef6 \u5b89\u88c5 KafkaSink\u4f8b\u5b50 \u8f93\u51fa\u4e3b\u9898\u5185\u5bb9\u6a21\u5f0f Security Enabling security features Authentication using SASL Authentication using SASL without encryption Authentication using SASL and encryption using SSL Encryption using SSL without client authentication Authentication and encryption using SSL Kafka Producer configurations Enable debug logging for data plane components Enabling security features \u00b6 To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._ Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set. Kafka Producer configurations \u00b6 A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations . Enable debug logging for data plane components \u00b6 To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"KafkaSink"},{"location":"eventing/sinks/kafka-sink/#apache-kafka-sink","text":"\u672c\u9875\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5\u548c\u914d\u7f6eApache KafkaSink\u3002","title":"Apache Kafka Sink"},{"location":"eventing/sinks/kafka-sink/#_1","text":"\u60a8\u5fc5\u987b\u80fd\u591f\u8bbf\u95ee\u5b89\u88c5\u4e86 Knative\u4e8b\u4ef6 \u7684Kubernetes\u96c6\u7fa4.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/sinks/kafka-sink/#_2","text":"\u5b89\u88c5Kafka\u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml \u5b89\u88c5KafkaSink\u6570\u636e\u5e73\u9762: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml \u9a8c\u8bc1 kafka-controller \u548c kafka-sink-receiver \u90e8\u7f72\u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s","title":"\u5b89\u88c5"},{"location":"eventing/sinks/kafka-sink/#kafkasink","text":"KafkaSink\u5bf9\u8c61\u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092","title":"KafkaSink\u4f8b\u5b50"},{"location":"eventing/sinks/kafka-sink/#_3","text":"CloudEvent\u89c4\u8303\u5b9a\u4e49\u4e86\u4e24\u79cd\u4f20\u8f93clouddevent\u7684\u6a21\u5f0f:\u7ed3\u6784\u5316\u548c\u4e8c\u8fdb\u5236\u3002 A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: binary. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured","title":"\u8f93\u51fa\u4e3b\u9898\u5185\u5bb9\u6a21\u5f0f"},{"location":"eventing/sinks/kafka-sink/#security","text":"Knative supports the following Apache Kafka security features: Apache Kafka Sink \u5148\u51b3\u6761\u4ef6 \u5b89\u88c5 KafkaSink\u4f8b\u5b50 \u8f93\u51fa\u4e3b\u9898\u5185\u5bb9\u6a21\u5f0f Security Enabling security features Authentication using SASL Authentication using SASL without encryption Authentication using SASL and encryption using SSL Encryption using SSL without client authentication Authentication and encryption using SSL Kafka Producer configurations Enable debug logging for data plane components","title":"Security"},{"location":"eventing/sinks/kafka-sink/#enabling-security-features","text":"To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._","title":"Enabling security features"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/sinks/kafka-sink/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#kafka-producer-configurations","text":"A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations .","title":"Kafka Producer configurations"},{"location":"eventing/sinks/kafka-sink/#enable-debug-logging-for-data-plane-components","text":"To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"Enable debug logging for data plane components"},{"location":"eventing/sinks/redis-sink/","text":"Redis Stream Sink \u00b6 Knative\u7684Redis\u6d41\u4e8b\u4ef6\u63a5\u6536\u5668\u63a5\u6536CloudEvents\u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230Redis\u5b9e\u4f8b\u7684\u6307\u5b9a stream \u3002 Redis\u6d41\u63a5\u6536\u5668\u53ef\u4ee5\u4e0e\u672c\u5730\u7248\u672c\u7684Redis\u6570\u636e\u5e93\u5b9e\u4f8b\u6216\u57fa\u4e8e\u4e91\u7684\u5b9e\u4f8b\u4e00\u8d77\u5de5\u4f5c\uff0c\u5176 address \u5c06\u5728\u63a5\u6536\u5668\u89c4\u8303\u4e2d\u6307\u5b9a\u3002 \u6b64\u5916\uff0c\u6307\u5b9a\u7684 stream \u540d\u79f0\u5c06\u7531\u63a5\u6536\u65b9\u521b\u5efa\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u5b58\u5728\u7684\u8bdd\u3002 \u5f00\u59cb \u00b6 \u5b89\u88c5 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 Knative\u670d\u52a1\u5b89\u88c5\u8bf4\u660e\u5728 \u8fd9\u91cc \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u672c\u5730Redis\u5b9e\u4f8b\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002 \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis\u7684\u4e91\u5b9e\u4f8b(\u4f8b\u5982\uff0cIBM cloud\u4e0a\u7684Redis DB)\uff0c\u5219\u9700\u8981\u5728\u5b89\u88c5\u4e8b\u4ef6\u63a5\u6536\u5668\u4e4b\u524d\u914d\u7f6eTLS\u8bc1\u4e66\u3002 \u7f16\u8f91 tls-secret Secret\u5c06\u60a8\u7684Redis\u4e91\u5b9e\u4f8b\u4e2d\u7684TLS\u8bc1\u4e66\u6dfb\u52a0\u5230 TLS_CERT \u6570\u636e\u5bc6\u94a5: vi config/sink/tls-secret.yaml \u5c06\u60a8\u7684\u8bc1\u4e66\u6dfb\u52a0\u5230\u6587\u4ef6\u4e2d\uff0c\u5e76\u4fdd\u5b58\u8be5\u6587\u4ef6\u3002\u5c06\u5728\u4e0b\u4e00\u6b65\u5e94\u7528\u3002 \u521b\u5efa RedisStreamSink \u63a5\u6536\u5668\u5b9a\u4e49\u53ca\u5176\u6240\u6709\u7ec4\u4ef6: \u00b6 \u5e94\u7528 config/sink ko apply -f config/sink \u4f8b\u5b50 \u00b6 \u5728\u672c\u4f8b\u4e2d\uff0c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2aRedis\u6d41\u4e8b\u4ef6\u63a5\u6536\u5668\uff0c\u5b83\u5c06\u63a5\u6536CloudEvent\u4e8b\u4ef6\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u9879\u76ee\u6dfb\u52a0\u5230 mystream \u6d41\u4e2d\u3002 \u901a\u8fc7\u8be5\u547d\u4ee4\u5b89\u88c5\u672c\u5730Redis\u3002\u4f7f\u7528\u5b9e\u4f8b: kubectl apply -f samples/redis \u4e3a\u8fd9\u4e2a\u793a\u4f8b\u63a5\u6536\u5668\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4: kubectl create ns redex \u901a\u8fc7\u8fd0\u884c\u8fd9\u4e2a\u547d\u4ee4\u5b89\u88c5\u4e00\u4e2aRedis\u6d41\u63a5\u6536\u793a\u4f8b: Note \u9664\u4e86\u914d\u7f6e\u60a8\u7684TLS\u8bc1\u4e66\uff0c\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis DB\u7684\u4e91\u5b9e\u4f8b\uff0c\u60a8\u5c06\u9700\u8981\u5728 redisstreamsink \u63a5\u6536\u5668 yaml\u4e2d\u8bbe\u7f6e\u9002\u5f53\u7684\u5730\u5740\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u8fde\u63a5\u5b57\u7b26\u4e32\u7684\u4f8b\u5b50: address: \"rediss://$USERNAME:$PASSWORD@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u7136\u540e\uff0c\u5e94\u7528 samples/sink \u521b\u5efa\u4e00\u4e2aRedis\u6d41\u63a5\u6536\u5668\u8d44\u6e90 kubectl apply -n redex -f samples/sink \u9a8c\u8bc1Redis\u6d41\u63a5\u6536\u5668\u662f\u5426\u51c6\u5907\u5c31\u7eea: kubectl get -n redex redisstreamsink.sinks.knative.dev mystream NAME URL AGE READY REASON mystream http://redistreamsinkmystream.redex.svc.cluster.local 35s True \u5411\u63a5\u6536\u5668\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: curl $( kubectl get ksvc redistreamsinkmystream -ojsonpath = '{.status.url}' -n redex ) \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-type: dev.knative.sources.redisstream\" \\ -H \"ce-source: cli\" \\ -H \"ce-id: 1\" \\ -H \"datacontenttype: application/json\" \\ -d '[\"fruit\", \"orange\"]' \u68c0\u67e5redis\u4e2d\u662f\u5426\u6dfb\u52a0\u4e86\u65b0\u6d88\u606f: kubectl exec -n redis svc/redis redis-cli xinfo stream mystream ... last-entry 1598652372717 -0 fruit orange \u8981\u8fdb\u884c\u6e05\u7406\uff0c\u8bf7\u5220\u9664Redis\u6d41\u63a5\u6536\u5668\u793a\u4f8b\u548credex\u547d\u540d\u7a7a\u95f4: kubectl delete -f samples/sink -n redex kubectl delete ns redex \u53c2\u8003 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 Redis\u5b89\u88c5\u3002\u90e8\u7f72\u672c\u5730Redis\u7684\u8bf4\u660e\u5728\u4e0a\u9762\u3002 \u4e86\u89e3 Redis\u6d41\u7684\u57fa\u7840\u77e5\u8bc6 , \u4ee5\u53ca\u4e00\u4e9b \u7279\u5b9a\u4e8e\u6d41\u7684\u547d\u4ee4 \u6e90\u5b57\u6bb5 \u00b6 RedisStreamSink \u8d44\u6e90\u662fKubernetes\u5bf9\u8c61\u3002 \u9664\u4e86\u6807\u51c6\u7684Kubernetes apiVersion \uff0c kind \u548c metadata \uff0c\u5b83\u4eec\u8fd8\u6709\u4ee5\u4e0b\u7684 spec \u5b57\u6bb5: \u5b57\u6bb5 \u503c address Redis TCP\u5730\u5740 stream Redis\u6d41\u7684\u540d\u79f0 \u4e00\u65e6\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e86\u5bf9\u8c61\uff0c\u63a5\u6536\u5668\u5c06\u901a\u8fc7\u5bf9\u8c61\u4e0a\u7684 status \u5b57\u6bb5\u63d0\u4f9b\u5173\u4e8e\u5c31\u7eea\u6216\u9519\u8bef\u7684\u8f93\u51fa\u4fe1\u606f\u3002 \u8c03\u8bd5\u6280\u5de7 \u00b6 \u4f60\u53ef\u4ee5\u68c0\u67e5Redis\u6d41\u63a5\u6536\u5668\u8d44\u6e90\u7684 status.condition \u503c\u6765\u8bca\u65ad\u4efb\u4f55\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl get redisstreamsinks -n redex kubectl describe redisstreamsinks mystream -n redex \u60a8\u8fd8\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u4ee5\u68c0\u67e5\u63a5\u6536\u5668\u90e8\u7f72\u7684\u95ee\u9898: kubectl get pods -n redex kubectl logs {podname} -n redex \u60a8\u8fd8\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u4ee5\u68c0\u67e5\u63a5\u6536\u5668\u63a7\u5236\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redis-controller-manager-0 -n knative-sinks KO\u5b89\u88c5\u95ee\u9898? \u53c2\u8003: https://github.com/google/ko/issues/106 \u5c1d\u8bd5\u91cd\u65b0\u5b89\u88c5KO\u548c\u8bbe\u7f6e export GOROOT=$(go env GOROOT) \u914d\u7f6e\u9009\u9879 \u00b6 config-observability \u548c config-logging configmap \u53ef\u7528\u4e8e\u7ba1\u7406\u65e5\u5fd7\u8bb0\u5f55\u548c\u5ea6\u91cf\u914d\u7f6e\u3002","title":"RedisStreamSink"},{"location":"eventing/sinks/redis-sink/#redis-stream-sink","text":"Knative\u7684Redis\u6d41\u4e8b\u4ef6\u63a5\u6536\u5668\u63a5\u6536CloudEvents\u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230Redis\u5b9e\u4f8b\u7684\u6307\u5b9a stream \u3002 Redis\u6d41\u63a5\u6536\u5668\u53ef\u4ee5\u4e0e\u672c\u5730\u7248\u672c\u7684Redis\u6570\u636e\u5e93\u5b9e\u4f8b\u6216\u57fa\u4e8e\u4e91\u7684\u5b9e\u4f8b\u4e00\u8d77\u5de5\u4f5c\uff0c\u5176 address \u5c06\u5728\u63a5\u6536\u5668\u89c4\u8303\u4e2d\u6307\u5b9a\u3002 \u6b64\u5916\uff0c\u6307\u5b9a\u7684 stream \u540d\u79f0\u5c06\u7531\u63a5\u6536\u65b9\u521b\u5efa\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u5b58\u5728\u7684\u8bdd\u3002","title":"Redis Stream Sink"},{"location":"eventing/sinks/redis-sink/#_1","text":"","title":"\u5f00\u59cb"},{"location":"eventing/sinks/redis-sink/#_2","text":"","title":"\u5b89\u88c5"},{"location":"eventing/sinks/redis-sink/#_3","text":"Knative\u670d\u52a1\u5b89\u88c5\u8bf4\u660e\u5728 \u8fd9\u91cc \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u672c\u5730Redis\u5b9e\u4f8b\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002 \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis\u7684\u4e91\u5b9e\u4f8b(\u4f8b\u5982\uff0cIBM cloud\u4e0a\u7684Redis DB)\uff0c\u5219\u9700\u8981\u5728\u5b89\u88c5\u4e8b\u4ef6\u63a5\u6536\u5668\u4e4b\u524d\u914d\u7f6eTLS\u8bc1\u4e66\u3002 \u7f16\u8f91 tls-secret Secret\u5c06\u60a8\u7684Redis\u4e91\u5b9e\u4f8b\u4e2d\u7684TLS\u8bc1\u4e66\u6dfb\u52a0\u5230 TLS_CERT \u6570\u636e\u5bc6\u94a5: vi config/sink/tls-secret.yaml \u5c06\u60a8\u7684\u8bc1\u4e66\u6dfb\u52a0\u5230\u6587\u4ef6\u4e2d\uff0c\u5e76\u4fdd\u5b58\u8be5\u6587\u4ef6\u3002\u5c06\u5728\u4e0b\u4e00\u6b65\u5e94\u7528\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/sinks/redis-sink/#redisstreamsink","text":"\u5e94\u7528 config/sink ko apply -f config/sink","title":"\u521b\u5efaRedisStreamSink\u63a5\u6536\u5668\u5b9a\u4e49\u53ca\u5176\u6240\u6709\u7ec4\u4ef6:"},{"location":"eventing/sinks/redis-sink/#_4","text":"\u5728\u672c\u4f8b\u4e2d\uff0c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2aRedis\u6d41\u4e8b\u4ef6\u63a5\u6536\u5668\uff0c\u5b83\u5c06\u63a5\u6536CloudEvent\u4e8b\u4ef6\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u9879\u76ee\u6dfb\u52a0\u5230 mystream \u6d41\u4e2d\u3002 \u901a\u8fc7\u8be5\u547d\u4ee4\u5b89\u88c5\u672c\u5730Redis\u3002\u4f7f\u7528\u5b9e\u4f8b: kubectl apply -f samples/redis \u4e3a\u8fd9\u4e2a\u793a\u4f8b\u63a5\u6536\u5668\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4: kubectl create ns redex \u901a\u8fc7\u8fd0\u884c\u8fd9\u4e2a\u547d\u4ee4\u5b89\u88c5\u4e00\u4e2aRedis\u6d41\u63a5\u6536\u793a\u4f8b: Note \u9664\u4e86\u914d\u7f6e\u60a8\u7684TLS\u8bc1\u4e66\uff0c\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis DB\u7684\u4e91\u5b9e\u4f8b\uff0c\u60a8\u5c06\u9700\u8981\u5728 redisstreamsink \u63a5\u6536\u5668 yaml\u4e2d\u8bbe\u7f6e\u9002\u5f53\u7684\u5730\u5740\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u8fde\u63a5\u5b57\u7b26\u4e32\u7684\u4f8b\u5b50: address: \"rediss://$USERNAME:$PASSWORD@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u7136\u540e\uff0c\u5e94\u7528 samples/sink \u521b\u5efa\u4e00\u4e2aRedis\u6d41\u63a5\u6536\u5668\u8d44\u6e90 kubectl apply -n redex -f samples/sink \u9a8c\u8bc1Redis\u6d41\u63a5\u6536\u5668\u662f\u5426\u51c6\u5907\u5c31\u7eea: kubectl get -n redex redisstreamsink.sinks.knative.dev mystream NAME URL AGE READY REASON mystream http://redistreamsinkmystream.redex.svc.cluster.local 35s True \u5411\u63a5\u6536\u5668\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: curl $( kubectl get ksvc redistreamsinkmystream -ojsonpath = '{.status.url}' -n redex ) \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-type: dev.knative.sources.redisstream\" \\ -H \"ce-source: cli\" \\ -H \"ce-id: 1\" \\ -H \"datacontenttype: application/json\" \\ -d '[\"fruit\", \"orange\"]' \u68c0\u67e5redis\u4e2d\u662f\u5426\u6dfb\u52a0\u4e86\u65b0\u6d88\u606f: kubectl exec -n redis svc/redis redis-cli xinfo stream mystream ... last-entry 1598652372717 -0 fruit orange \u8981\u8fdb\u884c\u6e05\u7406\uff0c\u8bf7\u5220\u9664Redis\u6d41\u63a5\u6536\u5668\u793a\u4f8b\u548credex\u547d\u540d\u7a7a\u95f4: kubectl delete -f samples/sink -n redex kubectl delete ns redex","title":"\u4f8b\u5b50"},{"location":"eventing/sinks/redis-sink/#_5","text":"","title":"\u53c2\u8003"},{"location":"eventing/sinks/redis-sink/#_6","text":"Redis\u5b89\u88c5\u3002\u90e8\u7f72\u672c\u5730Redis\u7684\u8bf4\u660e\u5728\u4e0a\u9762\u3002 \u4e86\u89e3 Redis\u6d41\u7684\u57fa\u7840\u77e5\u8bc6 , \u4ee5\u53ca\u4e00\u4e9b \u7279\u5b9a\u4e8e\u6d41\u7684\u547d\u4ee4","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/sinks/redis-sink/#_7","text":"RedisStreamSink \u8d44\u6e90\u662fKubernetes\u5bf9\u8c61\u3002 \u9664\u4e86\u6807\u51c6\u7684Kubernetes apiVersion \uff0c kind \u548c metadata \uff0c\u5b83\u4eec\u8fd8\u6709\u4ee5\u4e0b\u7684 spec \u5b57\u6bb5: \u5b57\u6bb5 \u503c address Redis TCP\u5730\u5740 stream Redis\u6d41\u7684\u540d\u79f0 \u4e00\u65e6\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e86\u5bf9\u8c61\uff0c\u63a5\u6536\u5668\u5c06\u901a\u8fc7\u5bf9\u8c61\u4e0a\u7684 status \u5b57\u6bb5\u63d0\u4f9b\u5173\u4e8e\u5c31\u7eea\u6216\u9519\u8bef\u7684\u8f93\u51fa\u4fe1\u606f\u3002","title":"\u6e90\u5b57\u6bb5"},{"location":"eventing/sinks/redis-sink/#_8","text":"\u4f60\u53ef\u4ee5\u68c0\u67e5Redis\u6d41\u63a5\u6536\u5668\u8d44\u6e90\u7684 status.condition \u503c\u6765\u8bca\u65ad\u4efb\u4f55\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl get redisstreamsinks -n redex kubectl describe redisstreamsinks mystream -n redex \u60a8\u8fd8\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u4ee5\u68c0\u67e5\u63a5\u6536\u5668\u90e8\u7f72\u7684\u95ee\u9898: kubectl get pods -n redex kubectl logs {podname} -n redex \u60a8\u8fd8\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u4ee5\u68c0\u67e5\u63a5\u6536\u5668\u63a7\u5236\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redis-controller-manager-0 -n knative-sinks KO\u5b89\u88c5\u95ee\u9898? \u53c2\u8003: https://github.com/google/ko/issues/106 \u5c1d\u8bd5\u91cd\u65b0\u5b89\u88c5KO\u548c\u8bbe\u7f6e export GOROOT=$(go env GOROOT)","title":"\u8c03\u8bd5\u6280\u5de7"},{"location":"eventing/sinks/redis-sink/#_9","text":"config-observability \u548c config-logging configmap \u53ef\u7528\u4e8e\u7ba1\u7406\u65e5\u5fd7\u8bb0\u5f55\u548c\u5ea6\u91cf\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u9009\u9879"},{"location":"eventing/sources/","text":"\u4e8b\u4ef6\u6e90 \u00b6 \u4e8b\u4ef6\u6e90\u662f\u4e00\u4e2aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\uff0c\u7531\u5f00\u53d1\u4eba\u5458\u6216\u96c6\u7fa4\u7ba1\u7406\u5458\u521b\u5efa\uff0c\u5145\u5f53\u4e8b\u4ef6\u751f\u6210\u5668\u548c\u4e8b\u4ef6 sink \u4e4b\u95f4\u7684\u94fe\u63a5\u3002 \u63a5\u6536\u5668\u53ef\u4ee5\u662fk8s\u670d\u52a1\uff0c\u5305\u62ecKnative\u670d\u52a1\u3001\u901a\u9053\u6216\u4ece\u4e8b\u4ef6\u6e90\u63a5\u6536\u4e8b\u4ef6\u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4ece\u6e90\u5bf9\u8c61\u5b9e\u4f8b\u5316CR\u6765\u521b\u5efa\u4e8b\u4ef6\u6e90\u3002 \u6e90\u5bf9\u8c61\u5b9a\u4e49\u5b9e\u4f8b\u5316CR\u6240\u9700\u7684\u53c2\u6570\u548c\u53c2\u6570\u3002 \u6240\u6709\u6765\u6e90\u90fd\u662f \u6765\u6e90 \u7c7b\u522b\u7684\u4e00\u90e8\u5206\u3002 kn kubectl \u901a\u8fc7\u8f93\u5165kn\u547d\u4ee4\uff0c\u53ef\u4ee5\u5217\u51fa\u96c6\u7fa4\u4e2d\u73b0\u6709\u7684\u4e8b\u4ef6\u6e90: kn source list \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u96c6\u7fa4\u4e0a\u5df2\u6709\u7684\u4e8b\u4ef6\u6e90: kubectl get sources Note \u4ece\u5176\u4ed6\u6d88\u606f\u6280\u672f(\u5982Kafka\u6216RabbitMQ)\u5bfc\u5165\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u6e90\u4e0d\u8d1f\u8d23\u8bbe\u7f6e \u53ef\u9009\u5c5e\u6027 \u6bd4\u5982 datacontenttype . \u8fd9\u662f\u6700\u521d\u7684\u4e8b\u4ef6\u5236\u4f5c\u4eba\u7684\u8d23\u4efb;\u5982\u679c\u5c5e\u6027\u5b58\u5728\uff0c\u6e90\u53ea\u8ffd\u52a0\u5c5e\u6027\u3002 Knative \u6e90 \u00b6 \u540d\u79f0 \u72b6\u6001 Maintainer \u63cf\u8ff0 APIServerSource Stable Knative \u5c06Kubernetes API\u670d\u52a1\u5668\u4e8b\u4ef6\u5f15\u5165Knative\u3002\u6bcf\u6b21\u521b\u5efa\u3001\u66f4\u65b0\u6216\u5220\u9664Kubernetes\u8d44\u6e90\u65f6\uff0cAPIServerSource\u90fd\u4f1a\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002 Apache CouchDB Alpha Knative \u5c06 Apache CouchDB \u6d88\u606f\u5f15\u5165Knative\u3002 Apache Kafka Stable Knative \u5c06 Apache Kafka \u6d88\u606f\u5f15\u5165Knative\u3002KafkaSource\u4eceApache Kafka Cluster\u4e2d\u8bfb\u53d6\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u4f20\u9012\u7ed9\u4e00\u4e2a\u63a5\u6536\u5668\uff0c\u4ee5\u4fbf\u5b83\u4eec\u53ef\u4ee5\u88ab\u4f7f\u7528\u3002\u53c2\u89c1 Kafka\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 ContainerSource Stable Knative ContainerSource\u5b9e\u4f8b\u5316\u53ef\u4ee5\u751f\u6210\u4e8b\u4ef6\u7684\u5bb9\u5668\u6620\u50cf\uff0c\u76f4\u5230\u5220\u9664ContainerSource\u4e3a\u6b62\u3002\u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u7528\u4e8e\u8f6e\u8be2FTP\u670d\u52a1\u5668\u4ee5\u83b7\u53d6\u65b0\u6587\u4ef6\u6216\u4ee5\u8bbe\u7f6e\u7684\u65f6\u95f4\u95f4\u9694\u751f\u6210\u4e8b\u4ef6\u3002\u7ed9\u5b9a\u4e00\u4e2a\u81f3\u5c11\u6307\u5b9a\u4e86\u4e00\u4e2a\u5bb9\u5668\u6620\u50cf\u7684 spec.template \uff0c ContainerSource\u5c06\u4fdd\u6301\u4e00\u4e2aPod\u8fd0\u884c\u5728\u6307\u5b9a\u7684\u6620\u50cf\u4e2d\u3002 K_SINK (\u76ee\u6807\u5730\u5740)\u548c KE_CE_OVERRIDES (JSON CloudEvents\u5c5e\u6027)\u73af\u5883\u53d8\u91cf\u88ab\u6ce8\u5165\u5230\u8fd0\u884c\u6620\u50cf\u4e2d\u3002\u5b83\u88ab\u591a\u4e2a\u5176\u4ed6\u6e90\u7528\u4f5c\u5e95\u5c42\u57fa\u7840\u7ed3\u6784\u3002\u6709\u5173\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2 \u9605\u5bb9\u5668\u6e90 \u793a\u4f8b\u3002 GitHub Beta Knative \u5728\u6307\u5b9a\u7684GitHub\u7ec4\u7ec7\u6216\u5b58\u50a8\u5e93\u4e2d\u6ce8\u518c\u6307\u5b9a\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u5e26\u5165Knative\u3002GitHubSource\u4e3a\u9009\u5b9a\u7684 GitHub\u4e8b\u4ef6\u7c7b\u578b \u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u53c2\u89c1 GitHub\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 GitLab Beta Knative \u5728\u6307\u5b9a\u7684GitLab\u5b58\u50a8\u5e93\u4e2d\u6ce8\u518c\u6307\u5b9a\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u5e26\u5165Knative\u3002GitLabSource\u4e3a\u6307\u5b9a\u7684 \u4e8b\u4ef6\u7c7b\u578b \u521b\u5efa\u4e00\u4e2awebhook\uff0c\u76d1\u542c\u4f20\u5165\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u4f7f\u7528\u8005\u3002\u53c2\u89c1 GitLab\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 KogitoSource Alpha Knative \u7531 Kogito Operator \u7ba1\u7406\u7684 Kogito Runtime \u81ea\u5b9a\u4e49\u8d44\u6e90\u7684\u5b9e\u73b0\u3002 PingSource Stable Knative \u5728\u6307\u5b9a\u7684 Cron \u8c03\u5ea6\u4e0a\u751f\u6210\u5177\u6709\u56fa\u5b9a\u6709\u6548\u8d1f\u8f7d\u7684\u4e8b\u4ef6\u3002\u53c2\u89c1 Ping\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002 RabbitMQ Stable Knative \u5c06 RabbitMQ \u6d88\u606f\u5e26\u5165Knative\u3002 RedisSource Alpha Knative \u5c06 Redis\u6d41 \u5f15\u5165Knative\u3002 SinkBinding Stable Knative \u53ef\u4ee5\u4f7f\u7528Kubernetes\u63d0\u4f9b\u7684\u4efb\u4f55\u719f\u6089\u7684\u8ba1\u7b97\u62bd\u8c61(\u4f8b\u5982Deployment\u3001Job\u3001DaemonSet\u3001StatefulSet)\u6216Knative\u62bd\u8c61(\u4f8b\u5982Service\u3001Configuration)\u6765\u7f16\u5199\u65b0\u7684\u4e8b\u4ef6\u6e90\u3002SinkBinding\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5c06 K_SINK (\u76ee\u6807\u5730\u5740)\u548c K_CE_OVERRIDES (JSON cloudevents\u5c5e\u6027)\u73af\u5883\u53d8\u91cf\u6ce8\u5165\u5230\u4efb\u4f55Kubernetes\u8d44\u6e90\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u6709\u4e00\u4e2a\u770b\u8d77\u6765\u50cfPod(\u53c8\u540dPodSpecable)\u7684 spec.template \u3002\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u89c1 SinkBinding \u793a\u4f8b\u3002 \u7b2c\u4e09\u65b9\u6e90 \u00b6 Name Status Maintainer Description Amazon CloudWatch Stable TriggerMesh \u4ece Amazon CloudWatch \u6536\u96c6\u6307\u6807. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon CloudWatch Logs Stable TriggerMesh \u8ba2\u9605\u6765\u81ea Amazon CloudWatch Logs \u6d41\u7684\u65e5\u5fd7\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) AWS CodeCommit Stable TriggerMesh \u6ce8\u518c\u7531 AWS CodeCommit \u6e90\u4ee3\u7801\u5b58\u50a8\u5e93\u53d1\u51fa\u7684\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Cognito Identity Stable TriggerMesh Registers for events from Amazon Cognito identity pools. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Cognito User Stable TriggerMesh Registers for events from Amazon Cognito user pools. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon DynamoDB Stable TriggerMesh Reads records from an Amazon DynamoDB stream. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Kinesis Stable TriggerMesh Reads records from an Amazon Kinesis stream. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon RDS Performance Insights Stable TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon S3 Stable TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon SNS Stable TriggerMesh Subscribes to messages from an Amazon SNS topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon SQS Stable TriggerMesh Consumes messages from an Amazon SQS queue. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Apache Camel Stable Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Azure Activity Logs Stable TriggerMesh Capture activity logs from Azure Activity Logs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Blob Storage Stable TriggerMesh Subscribes to events from an Azure Blob Storage account. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Event Grid Stable TriggerMesh Retrieves events from Azure Event Grid . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Event Hubs Stable TriggerMesh Consumes events from Azure Event Hubs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure IoT Hub Stable TriggerMesh Consumes event from Azure IoT Hub . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Queue Storage Stable TriggerMesh Retrieves messages from Azure Queue Storage . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Service Bus Queues Stable TriggerMesh Consumes messages from an Azure Service Bus queue. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Service Bus Topics Stable TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Direktiv Alpha Direktiv Receive events from Direktiv . DockerHubSource Alpha None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. Google Cloud Audit Logs Stable TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Billing Stable TriggerMesh Captures budget notifications from Google Cloud Billing . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud IoT Stable TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Pub/Sub Stable TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Source Repositories Stable TriggerMesh Consumes events from Google Cloud Source Repositories . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Storage Stable TriggerMesh \u6355\u83b7\u6765\u81ea \u8c37\u6b4cCloud Storage \u6876\u7684\u66f4\u6539\u901a\u77e5\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) HTTP Poller Stable TriggerMesh \u5b9a\u671f\u4eceHTTP/S URL\u63d0\u53d6\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Oracle Cloud Infrastructure Stable TriggerMesh \u4ece Oracle\u4e91\u57fa\u7840\u8bbe\u65bd \u68c0\u7d22\u6307\u6807. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Salesforce Stable TriggerMesh \u4f7f\u7528\u6765\u81ea Salesforce \u901a\u9053\u7684\u4e8b\u4ef6\u3002( \u5b89\u88c5 ) ( \u793a\u4f8b ) Slack Stable TriggerMesh \u8ba2\u9605 Slack \u7684\u6d3b\u52a8\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Twilio Supported TriggerMesh \u4ece Twilio \u63a5\u6536\u4e8b\u4ef6. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) VMware Alpha VMware \u5c06 vSphere \u4e8b\u4ef6\u5e26\u5165Knative\u3002 Webhook Stable TriggerMesh \u4f7f\u7528HTTP\u4ecewebhook\u4e2d\u6444\u53d6\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Zendesk Stable TriggerMesh \u8ba2\u9605Zendesk\u7684\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) \u989d\u5916\u7684\u6e90 \u00b6 \u5982\u679c\u60a8\u7684\u4ee3\u7801\u9700\u8981\u5c06\u4e8b\u4ef6\u4f5c\u4e3a\u5176\u4e1a\u52a1\u903b\u8f91\u7684\u4e00\u90e8\u5206\u53d1\u9001\uff0c\u5e76\u4e14\u4e0d\u9002\u5408\u6e90\u7684\u6a21\u578b\uff0c\u8bf7\u8003\u8651 \u5c06\u4e8b\u4ef6\u76f4\u63a5\u63d0\u4f9b\u7ed9\u4ee3\u7406 \u3002 \u6709\u5173\u4f7f\u7528 kn \u6e90\u76f8\u5173\u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 kn source \u53c2\u8003\u6587\u6863 \u3002","title":"\u5173\u4e8e\u4e8b\u4ef6\u6e90"},{"location":"eventing/sources/#_1","text":"\u4e8b\u4ef6\u6e90\u662f\u4e00\u4e2aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\uff0c\u7531\u5f00\u53d1\u4eba\u5458\u6216\u96c6\u7fa4\u7ba1\u7406\u5458\u521b\u5efa\uff0c\u5145\u5f53\u4e8b\u4ef6\u751f\u6210\u5668\u548c\u4e8b\u4ef6 sink \u4e4b\u95f4\u7684\u94fe\u63a5\u3002 \u63a5\u6536\u5668\u53ef\u4ee5\u662fk8s\u670d\u52a1\uff0c\u5305\u62ecKnative\u670d\u52a1\u3001\u901a\u9053\u6216\u4ece\u4e8b\u4ef6\u6e90\u63a5\u6536\u4e8b\u4ef6\u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4ece\u6e90\u5bf9\u8c61\u5b9e\u4f8b\u5316CR\u6765\u521b\u5efa\u4e8b\u4ef6\u6e90\u3002 \u6e90\u5bf9\u8c61\u5b9a\u4e49\u5b9e\u4f8b\u5316CR\u6240\u9700\u7684\u53c2\u6570\u548c\u53c2\u6570\u3002 \u6240\u6709\u6765\u6e90\u90fd\u662f \u6765\u6e90 \u7c7b\u522b\u7684\u4e00\u90e8\u5206\u3002 kn kubectl \u901a\u8fc7\u8f93\u5165kn\u547d\u4ee4\uff0c\u53ef\u4ee5\u5217\u51fa\u96c6\u7fa4\u4e2d\u73b0\u6709\u7684\u4e8b\u4ef6\u6e90: kn source list \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u96c6\u7fa4\u4e0a\u5df2\u6709\u7684\u4e8b\u4ef6\u6e90: kubectl get sources Note \u4ece\u5176\u4ed6\u6d88\u606f\u6280\u672f(\u5982Kafka\u6216RabbitMQ)\u5bfc\u5165\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u6e90\u4e0d\u8d1f\u8d23\u8bbe\u7f6e \u53ef\u9009\u5c5e\u6027 \u6bd4\u5982 datacontenttype . \u8fd9\u662f\u6700\u521d\u7684\u4e8b\u4ef6\u5236\u4f5c\u4eba\u7684\u8d23\u4efb;\u5982\u679c\u5c5e\u6027\u5b58\u5728\uff0c\u6e90\u53ea\u8ffd\u52a0\u5c5e\u6027\u3002","title":"\u4e8b\u4ef6\u6e90"},{"location":"eventing/sources/#knative","text":"\u540d\u79f0 \u72b6\u6001 Maintainer \u63cf\u8ff0 APIServerSource Stable Knative \u5c06Kubernetes API\u670d\u52a1\u5668\u4e8b\u4ef6\u5f15\u5165Knative\u3002\u6bcf\u6b21\u521b\u5efa\u3001\u66f4\u65b0\u6216\u5220\u9664Kubernetes\u8d44\u6e90\u65f6\uff0cAPIServerSource\u90fd\u4f1a\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002 Apache CouchDB Alpha Knative \u5c06 Apache CouchDB \u6d88\u606f\u5f15\u5165Knative\u3002 Apache Kafka Stable Knative \u5c06 Apache Kafka \u6d88\u606f\u5f15\u5165Knative\u3002KafkaSource\u4eceApache Kafka Cluster\u4e2d\u8bfb\u53d6\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u4f20\u9012\u7ed9\u4e00\u4e2a\u63a5\u6536\u5668\uff0c\u4ee5\u4fbf\u5b83\u4eec\u53ef\u4ee5\u88ab\u4f7f\u7528\u3002\u53c2\u89c1 Kafka\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 ContainerSource Stable Knative ContainerSource\u5b9e\u4f8b\u5316\u53ef\u4ee5\u751f\u6210\u4e8b\u4ef6\u7684\u5bb9\u5668\u6620\u50cf\uff0c\u76f4\u5230\u5220\u9664ContainerSource\u4e3a\u6b62\u3002\u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u7528\u4e8e\u8f6e\u8be2FTP\u670d\u52a1\u5668\u4ee5\u83b7\u53d6\u65b0\u6587\u4ef6\u6216\u4ee5\u8bbe\u7f6e\u7684\u65f6\u95f4\u95f4\u9694\u751f\u6210\u4e8b\u4ef6\u3002\u7ed9\u5b9a\u4e00\u4e2a\u81f3\u5c11\u6307\u5b9a\u4e86\u4e00\u4e2a\u5bb9\u5668\u6620\u50cf\u7684 spec.template \uff0c ContainerSource\u5c06\u4fdd\u6301\u4e00\u4e2aPod\u8fd0\u884c\u5728\u6307\u5b9a\u7684\u6620\u50cf\u4e2d\u3002 K_SINK (\u76ee\u6807\u5730\u5740)\u548c KE_CE_OVERRIDES (JSON CloudEvents\u5c5e\u6027)\u73af\u5883\u53d8\u91cf\u88ab\u6ce8\u5165\u5230\u8fd0\u884c\u6620\u50cf\u4e2d\u3002\u5b83\u88ab\u591a\u4e2a\u5176\u4ed6\u6e90\u7528\u4f5c\u5e95\u5c42\u57fa\u7840\u7ed3\u6784\u3002\u6709\u5173\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2 \u9605\u5bb9\u5668\u6e90 \u793a\u4f8b\u3002 GitHub Beta Knative \u5728\u6307\u5b9a\u7684GitHub\u7ec4\u7ec7\u6216\u5b58\u50a8\u5e93\u4e2d\u6ce8\u518c\u6307\u5b9a\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u5e26\u5165Knative\u3002GitHubSource\u4e3a\u9009\u5b9a\u7684 GitHub\u4e8b\u4ef6\u7c7b\u578b \u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u53c2\u89c1 GitHub\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 GitLab Beta Knative \u5728\u6307\u5b9a\u7684GitLab\u5b58\u50a8\u5e93\u4e2d\u6ce8\u518c\u6307\u5b9a\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4e8b\u4ef6\u5e26\u5165Knative\u3002GitLabSource\u4e3a\u6307\u5b9a\u7684 \u4e8b\u4ef6\u7c7b\u578b \u521b\u5efa\u4e00\u4e2awebhook\uff0c\u76d1\u542c\u4f20\u5165\u7684\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u4f7f\u7528\u8005\u3002\u53c2\u89c1 GitLab\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\u3002 KogitoSource Alpha Knative \u7531 Kogito Operator \u7ba1\u7406\u7684 Kogito Runtime \u81ea\u5b9a\u4e49\u8d44\u6e90\u7684\u5b9e\u73b0\u3002 PingSource Stable Knative \u5728\u6307\u5b9a\u7684 Cron \u8c03\u5ea6\u4e0a\u751f\u6210\u5177\u6709\u56fa\u5b9a\u6709\u6548\u8d1f\u8f7d\u7684\u4e8b\u4ef6\u3002\u53c2\u89c1 Ping\u6e90 \u793a\u4f8b\u4e86\u89e3\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002 RabbitMQ Stable Knative \u5c06 RabbitMQ \u6d88\u606f\u5e26\u5165Knative\u3002 RedisSource Alpha Knative \u5c06 Redis\u6d41 \u5f15\u5165Knative\u3002 SinkBinding Stable Knative \u53ef\u4ee5\u4f7f\u7528Kubernetes\u63d0\u4f9b\u7684\u4efb\u4f55\u719f\u6089\u7684\u8ba1\u7b97\u62bd\u8c61(\u4f8b\u5982Deployment\u3001Job\u3001DaemonSet\u3001StatefulSet)\u6216Knative\u62bd\u8c61(\u4f8b\u5982Service\u3001Configuration)\u6765\u7f16\u5199\u65b0\u7684\u4e8b\u4ef6\u6e90\u3002SinkBinding\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5c06 K_SINK (\u76ee\u6807\u5730\u5740)\u548c K_CE_OVERRIDES (JSON cloudevents\u5c5e\u6027)\u73af\u5883\u53d8\u91cf\u6ce8\u5165\u5230\u4efb\u4f55Kubernetes\u8d44\u6e90\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u6709\u4e00\u4e2a\u770b\u8d77\u6765\u50cfPod(\u53c8\u540dPodSpecable)\u7684 spec.template \u3002\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u89c1 SinkBinding \u793a\u4f8b\u3002","title":"Knative \u6e90"},{"location":"eventing/sources/#_2","text":"Name Status Maintainer Description Amazon CloudWatch Stable TriggerMesh \u4ece Amazon CloudWatch \u6536\u96c6\u6307\u6807. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon CloudWatch Logs Stable TriggerMesh \u8ba2\u9605\u6765\u81ea Amazon CloudWatch Logs \u6d41\u7684\u65e5\u5fd7\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) AWS CodeCommit Stable TriggerMesh \u6ce8\u518c\u7531 AWS CodeCommit \u6e90\u4ee3\u7801\u5b58\u50a8\u5e93\u53d1\u51fa\u7684\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Cognito Identity Stable TriggerMesh Registers for events from Amazon Cognito identity pools. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Cognito User Stable TriggerMesh Registers for events from Amazon Cognito user pools. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon DynamoDB Stable TriggerMesh Reads records from an Amazon DynamoDB stream. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon Kinesis Stable TriggerMesh Reads records from an Amazon Kinesis stream. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon RDS Performance Insights Stable TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon S3 Stable TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon SNS Stable TriggerMesh Subscribes to messages from an Amazon SNS topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Amazon SQS Stable TriggerMesh Consumes messages from an Amazon SQS queue. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Apache Camel Stable Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Azure Activity Logs Stable TriggerMesh Capture activity logs from Azure Activity Logs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Blob Storage Stable TriggerMesh Subscribes to events from an Azure Blob Storage account. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Event Grid Stable TriggerMesh Retrieves events from Azure Event Grid . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Event Hubs Stable TriggerMesh Consumes events from Azure Event Hubs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure IoT Hub Stable TriggerMesh Consumes event from Azure IoT Hub . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Queue Storage Stable TriggerMesh Retrieves messages from Azure Queue Storage . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Service Bus Queues Stable TriggerMesh Consumes messages from an Azure Service Bus queue. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Azure Service Bus Topics Stable TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Direktiv Alpha Direktiv Receive events from Direktiv . DockerHubSource Alpha None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. Google Cloud Audit Logs Stable TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Billing Stable TriggerMesh Captures budget notifications from Google Cloud Billing . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud IoT Stable TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Pub/Sub Stable TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Source Repositories Stable TriggerMesh Consumes events from Google Cloud Source Repositories . ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Google Cloud Storage Stable TriggerMesh \u6355\u83b7\u6765\u81ea \u8c37\u6b4cCloud Storage \u6876\u7684\u66f4\u6539\u901a\u77e5\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) HTTP Poller Stable TriggerMesh \u5b9a\u671f\u4eceHTTP/S URL\u63d0\u53d6\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Oracle Cloud Infrastructure Stable TriggerMesh \u4ece Oracle\u4e91\u57fa\u7840\u8bbe\u65bd \u68c0\u7d22\u6307\u6807. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Salesforce Stable TriggerMesh \u4f7f\u7528\u6765\u81ea Salesforce \u901a\u9053\u7684\u4e8b\u4ef6\u3002( \u5b89\u88c5 ) ( \u793a\u4f8b ) Slack Stable TriggerMesh \u8ba2\u9605 Slack \u7684\u6d3b\u52a8\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Twilio Supported TriggerMesh \u4ece Twilio \u63a5\u6536\u4e8b\u4ef6. ( \u5b89\u88c5 ) ( \u793a\u4f8b ) VMware Alpha VMware \u5c06 vSphere \u4e8b\u4ef6\u5e26\u5165Knative\u3002 Webhook Stable TriggerMesh \u4f7f\u7528HTTP\u4ecewebhook\u4e2d\u6444\u53d6\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b ) Zendesk Stable TriggerMesh \u8ba2\u9605Zendesk\u7684\u4e8b\u4ef6\u3002 ( \u5b89\u88c5 ) ( \u793a\u4f8b )","title":"\u7b2c\u4e09\u65b9\u6e90"},{"location":"eventing/sources/#_3","text":"\u5982\u679c\u60a8\u7684\u4ee3\u7801\u9700\u8981\u5c06\u4e8b\u4ef6\u4f5c\u4e3a\u5176\u4e1a\u52a1\u903b\u8f91\u7684\u4e00\u90e8\u5206\u53d1\u9001\uff0c\u5e76\u4e14\u4e0d\u9002\u5408\u6e90\u7684\u6a21\u578b\uff0c\u8bf7\u8003\u8651 \u5c06\u4e8b\u4ef6\u76f4\u63a5\u63d0\u4f9b\u7ed9\u4ee3\u7406 \u3002 \u6709\u5173\u4f7f\u7528 kn \u6e90\u76f8\u5173\u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 kn source \u53c2\u8003\u6587\u6863 \u3002","title":"\u989d\u5916\u7684\u6e90"},{"location":"eventing/sources/apiserversource/","text":"\u5173\u4e8e ApiServerSource \u00b6 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6 Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u76d1\u542c Kubernetes API \u670d\u52a1\u5668\u53d1\u51fa\u7684\u4e8b\u4ef6(\u5982 Pod \u521b\u5efa\u3001\u90e8\u7f72\u66f4\u65b0\u7b49)\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u8f6c\u53d1\u5230\u4e00\u4e2a\u63a5\u6536\u5668\u3002 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6\u5904\u7406\u6838\u5fc3\u7ec4\u4ef6\u7684\u4e00\u90e8\u5206\uff0c\u5728\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406\u65f6\u9ed8\u8ba4\u63d0\u4f9b\u3002 \u7528\u6237\u53ef\u4ee5\u521b\u5efa ApiServerSource \u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6e90"},{"location":"eventing/sources/apiserversource/#apiserversource","text":"API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6 Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u76d1\u542c Kubernetes API \u670d\u52a1\u5668\u53d1\u51fa\u7684\u4e8b\u4ef6(\u5982 Pod \u521b\u5efa\u3001\u90e8\u7f72\u66f4\u65b0\u7b49)\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u8f6c\u53d1\u5230\u4e00\u4e2a\u63a5\u6536\u5668\u3002 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6\u5904\u7406\u6838\u5fc3\u7ec4\u4ef6\u7684\u4e00\u90e8\u5206\uff0c\u5728\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406\u65f6\u9ed8\u8ba4\u63d0\u4f9b\u3002 \u7528\u6237\u53ef\u4ee5\u521b\u5efa ApiServerSource \u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e ApiServerSource"},{"location":"eventing/sources/apiserversource/getting-started/","text":"\u521b\u5efa ApiServerSource \u5bf9\u8c61 \u00b6 \u4ecb\u7ecd\u5982\u4f55\u521b\u5efa ApiServerSource \u5bf9\u8c61\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u521b\u5efa ApiServerSource \u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5 Knative \u4e8b\u4ef6 \u3002 \u5fc5\u987b\u5b89\u88c5 kubectl CLI \u5de5\u5177\u3002 \u53ef\u9009:\u5982\u679c\u8981\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u5de5\u5177\u3002 \u521b\u5efa ApiServerSource \u5bf9\u8c61 \u00b6 \u53ef\u9009:\u4e3a API \u670d\u52a1\u5668\u6e90\u5b9e\u4f8b\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 Note \u4e3a\u60a8\u7684ApiServerSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c \u5141\u8bb8\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c \u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u53ef\u80fd\u5b58\u5728\u4e8e default \u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa ServiceAccount: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> \u662f\u8981\u521b\u5efa\u7684 ServiceAccount \u7684\u540d\u79f0\u3002 <namespace> \u662f\u60a8\u5728\u524d\u9762\u7684\u6b65\u9aa4 1 \u4e2d\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u89d2\u8272: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch !!! note The only required verbs are `get`, `list` and `watch`. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: === \"kn\" - To create the ApiServerSource, run the command: ```bash kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> ``` Where: - `<apiserversource>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<sink-name>` is the name of your sink, for example, `http://event-display.pingsource-example.svc.cluster.local`. For a list of available options, see the [Knative client documentation](https://github.com/knative/client/blob/main/docs/cmd/kn_source_apiserver_create.md#kn-source-apiserver-create). === \"YAML\" 1. Create a YAML file using the following template: ```yaml apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: <apiserversource-name> namespace: <namespace> spec: serviceAccountName: <service-account> mode: <event-mode> resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: <sink-kind> name: <sink-name> ``` Where: - `<apiserversource-name>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<event-mode>` is either `Resource` or `Reference`. If set to `Resource`, the event payload contains the entire resource that the event is for. If set to `Reference`, the event payload only contains a reference to the resource that the event is for. The default is `Reference`. - `<sink-kind>` is any supported Addressable object that you want to use as a sink, for example, a `Service` or `Deployment`. - `<sink-name>` is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see [ApiServerSource reference](reference.md). 1. Apply the YAML file by running the command: ```bash kubectl apply -f <filename>.yaml ``` Where `<filename>` is the name of the file you created in the previous step. \u9a8c\u8bc1 ApiServerSource \u5bf9\u8c61 \u00b6 Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" } \u5220\u9664 ApiServerSource \u5bf9\u8c61 \u00b6 To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource","text":"\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa ApiServerSource \u5bf9\u8c61\u3002","title":"\u521b\u5efa ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#_1","text":"\u5728\u521b\u5efa ApiServerSource \u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5 Knative \u4e8b\u4ef6 \u3002 \u5fc5\u987b\u5b89\u88c5 kubectl CLI \u5de5\u5177\u3002 \u53ef\u9009:\u5982\u679c\u8981\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u5de5\u5177\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_1","text":"\u53ef\u9009:\u4e3a API \u670d\u52a1\u5668\u6e90\u5b9e\u4f8b\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 Note \u4e3a\u60a8\u7684ApiServerSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c \u5141\u8bb8\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c \u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u53ef\u80fd\u5b58\u5728\u4e8e default \u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa ServiceAccount: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> \u662f\u8981\u521b\u5efa\u7684 ServiceAccount \u7684\u540d\u79f0\u3002 <namespace> \u662f\u60a8\u5728\u524d\u9762\u7684\u6b65\u9aa4 1 \u4e2d\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u89d2\u8272: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch !!! note The only required verbs are `get`, `list` and `watch`. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: === \"kn\" - To create the ApiServerSource, run the command: ```bash kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> ``` Where: - `<apiserversource>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<sink-name>` is the name of your sink, for example, `http://event-display.pingsource-example.svc.cluster.local`. For a list of available options, see the [Knative client documentation](https://github.com/knative/client/blob/main/docs/cmd/kn_source_apiserver_create.md#kn-source-apiserver-create). === \"YAML\" 1. Create a YAML file using the following template: ```yaml apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: <apiserversource-name> namespace: <namespace> spec: serviceAccountName: <service-account> mode: <event-mode> resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: <sink-kind> name: <sink-name> ``` Where: - `<apiserversource-name>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<event-mode>` is either `Resource` or `Reference`. If set to `Resource`, the event payload contains the entire resource that the event is for. If set to `Reference`, the event payload only contains a reference to the resource that the event is for. The default is `Reference`. - `<sink-kind>` is any supported Addressable object that you want to use as a sink, for example, a `Service` or `Deployment`. - `<sink-name>` is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see [ApiServerSource reference](reference.md). 1. Apply the YAML file by running the command: ```bash kubectl apply -f <filename>.yaml ``` Where `<filename>` is the name of the file you created in the previous step.","title":"\u521b\u5efa ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_2","text":"Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" }","title":"\u9a8c\u8bc1 ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_3","text":"To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"\u5220\u9664 ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/reference/","text":"ApiServerSource \u53c2\u8003 \u00b6 This topic provides reference information about the configurable fields for the ApiServerSource object. ApiServerSource \u00b6 An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional \u8d44\u6e90\u53c2\u6570 \u00b6 The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels Example: Resources parameter \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment \u793a\u4f8b:\u4f7f\u7528matchExpressions\u7684\u8d44\u6e90\u53c2\u6570 \u00b6 Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp \u793a\u4f8b:\u4f7f\u7528matchLabels\u7684\u8d44\u6e90\u53c2\u6570 \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp ServiceAccountName parameter \u00b6 ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object. \u4f8b\u5b50:\u8ffd\u8e2aPod \u00b6 The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ... \u4e3b\u53c2\u6570 \u00b6 ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required \u793a\u4f8b:\u4e3b\u53c2\u6570 \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ... CloudEvent \u8986\u76d6 \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. \u4f8b\u5982:CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u6e90\u53c2\u8003"},{"location":"eventing/sources/apiserversource/reference/#apiserversource","text":"This topic provides reference information about the configurable fields for the ApiServerSource object.","title":"ApiServerSource \u53c2\u8003"},{"location":"eventing/sources/apiserversource/reference/#apiserversource_1","text":"An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"ApiServerSource"},{"location":"eventing/sources/apiserversource/reference/#_1","text":"The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels","title":"\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter","text":"Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment","title":"Example: Resources parameter"},{"location":"eventing/sources/apiserversource/reference/#matchexpressions","text":"Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp","title":"\u793a\u4f8b:\u4f7f\u7528matchExpressions\u7684\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#matchlabels","text":"Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp","title":"\u793a\u4f8b:\u4f7f\u7528matchLabels\u7684\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#serviceaccountname-parameter","text":"ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object.","title":"ServiceAccountName parameter"},{"location":"eventing/sources/apiserversource/reference/#pod","text":"The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ...","title":"\u4f8b\u5b50:\u8ffd\u8e2aPod"},{"location":"eventing/sources/apiserversource/reference/#_2","text":"ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required","title":"\u4e3b\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#_3","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ...","title":"\u793a\u4f8b:\u4e3b\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#cloudevent","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent \u8986\u76d6"},{"location":"eventing/sources/apiserversource/reference/#cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u4f8b\u5982:CloudEvent Overrides"},{"location":"eventing/sources/kafka-source/","text":"Apache Kafka \u6e90 \u00b6 KafkaSource \u8bfb\u53d6\u5b58\u50a8\u5728\u73b0\u6709 Apache Kafka \u4e3b\u9898\u4e2d\u7684\u6d88\u606f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6d88\u606f\u4f5c\u4e3aCloudEvents\u901a\u8fc7HTTP\u53d1\u9001\u5230\u5176\u914d\u7f6e\u7684 \u63a5\u6536\u5668 \u3002 KafkaSource \u4fdd\u7559\u4e86\u5b58\u50a8\u5728\u4e3b\u9898\u5206\u533a\u4e2d\u7684\u6d88\u606f\u7684\u987a\u5e8f\u3002 \u5b83\u901a\u8fc7\u7b49\u5f85\u6765\u81ea \u63a5\u6536\u5668 \u7684\u6210\u529f\u54cd\u5e94\uff0c\u7136\u540e\u5728\u540c\u4e00\u5206\u533a\u4e2d\u4f20\u9012\u4e0b\u4e00\u4e2a\u6d88\u606f\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 \u5b89\u88c5KafkaSource\u63a7\u5236\u5668 \u00b6 Install the KafkaSource controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s \u53ef\u9009:\u521b\u5efaKafka\u4e3b\u9898 \u00b6 Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME CLUSTER PARTITIONS REPLICATION FACTOR knative-demo-topic my-cluster 3 1 \u521b\u5efa\u670d\u52a1 \u00b6 Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s \u5361\u592b\u5361\u4e8b\u4ef6\u6e90 \u00b6 Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h \u9a8c\u8bc1 \u00b6 Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" } \u53ef\u9009:\u6307\u5b9a\u952e\u53cd\u5e8f\u5217\u5316\u5668 \u00b6 When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display \u53ef\u9009:\u6307\u5b9a\u521d\u59cb\u504f\u79fb\u91cf \u00b6 By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group. \u8fde\u63a5\u5230\u4e00\u4e2a\u652f\u6301tls\u7684Kafka\u4ee3\u7406 \u00b6 KafkaSource\u652f\u6301TLS\u548cSASL\u8ba4\u8bc1\u65b9\u5f0f\u3002\u542f\u7528TLS\u8ba4\u8bc1\u9700\u8981\u4ee5\u4e0b\u6587\u4ef6: \u8bc1\u4e66 \u5ba2\u6237\u7aef\u8bc1\u4e66\u548c\u5bc6\u94a5 KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display \u542f\u7528 SASL for KafkaSources \u00b6 Simple Authentication and Security Layer (SASL) is used by Apache Kafka for authentication. If you use SASL authentication on your cluster, users must provide credentials to Knative for communicating with the Kafka cluster, otherwise events cannot be produced or consumed. Prerequisites \u00b6 You have access to a Kafka cluster that has Simple Authentication and Security Layer (SASL). Procedure \u00b6 Create a secret that uses the Kafka cluster's SASL information, by running the following commands: STRIMZI_CRT = $( kubectl -n kafka get secret example-cluster-cluster-ca-cert --template = '{{index.data \"ca.crt\"}}' | base64 --decode ) SASL_PASSWD = $( kubectl -n kafka get secret example-user --template = '{{index.data \"password\"}}' | base64 --decode ) kubectl create secret -n default generic <secret_name> \\ --from-literal = ca.crt = \" $STRIMZI_CRT \" \\ --from-literal = password = \" $SASL_PASSWD \" \\ --from-literal = saslType = \"SCRAM-SHA-512\" \\ --from-literal = user = \"example-user\" Create or modify a KafkaSource so that it contains the following spec options: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : example-source spec : ... net : sasl : enable : true user : secretKeyRef : name : <secret_name> key : user password : secretKeyRef : name : <secret_name> key : password saslType : secretKeyRef : name : <secret_name> key : saslType tls : enable : true caCert : secretKeyRef : name : <secret_name> key : ca.crt ... Where <secret_name> is the name of the secret generated in the previous step. \u6e05\u7406\u6b65\u9aa4 \u00b6 Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"KafkaSource"},{"location":"eventing/sources/kafka-source/#apache-kafka","text":"KafkaSource \u8bfb\u53d6\u5b58\u50a8\u5728\u73b0\u6709 Apache Kafka \u4e3b\u9898\u4e2d\u7684\u6d88\u606f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6d88\u606f\u4f5c\u4e3aCloudEvents\u901a\u8fc7HTTP\u53d1\u9001\u5230\u5176\u914d\u7f6e\u7684 \u63a5\u6536\u5668 \u3002 KafkaSource \u4fdd\u7559\u4e86\u5b58\u50a8\u5728\u4e3b\u9898\u5206\u533a\u4e2d\u7684\u6d88\u606f\u7684\u987a\u5e8f\u3002 \u5b83\u901a\u8fc7\u7b49\u5f85\u6765\u81ea \u63a5\u6536\u5668 \u7684\u6210\u529f\u54cd\u5e94\uff0c\u7136\u540e\u5728\u540c\u4e00\u5206\u533a\u4e2d\u4f20\u9012\u4e0b\u4e00\u4e2a\u6d88\u606f\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002","title":"Apache Kafka \u6e90"},{"location":"eventing/sources/kafka-source/#kafkasource","text":"Install the KafkaSource controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s","title":"\u5b89\u88c5KafkaSource\u63a7\u5236\u5668"},{"location":"eventing/sources/kafka-source/#kafka","text":"Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME CLUSTER PARTITIONS REPLICATION FACTOR knative-demo-topic my-cluster 3 1","title":"\u53ef\u9009:\u521b\u5efaKafka\u4e3b\u9898"},{"location":"eventing/sources/kafka-source/#_1","text":"Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/sources/kafka-source/#_2","text":"Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h","title":"\u5361\u592b\u5361\u4e8b\u4ef6\u6e90"},{"location":"eventing/sources/kafka-source/#_3","text":"Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" }","title":"\u9a8c\u8bc1"},{"location":"eventing/sources/kafka-source/#_4","text":"When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"\u53ef\u9009:\u6307\u5b9a\u952e\u53cd\u5e8f\u5217\u5316\u5668"},{"location":"eventing/sources/kafka-source/#_5","text":"By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group.","title":"\u53ef\u9009:\u6307\u5b9a\u521d\u59cb\u504f\u79fb\u91cf"},{"location":"eventing/sources/kafka-source/#tlskafka","text":"KafkaSource\u652f\u6301TLS\u548cSASL\u8ba4\u8bc1\u65b9\u5f0f\u3002\u542f\u7528TLS\u8ba4\u8bc1\u9700\u8981\u4ee5\u4e0b\u6587\u4ef6: \u8bc1\u4e66 \u5ba2\u6237\u7aef\u8bc1\u4e66\u548c\u5bc6\u94a5 KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"\u8fde\u63a5\u5230\u4e00\u4e2a\u652f\u6301tls\u7684Kafka\u4ee3\u7406"},{"location":"eventing/sources/kafka-source/#sasl-for-kafkasources","text":"Simple Authentication and Security Layer (SASL) is used by Apache Kafka for authentication. If you use SASL authentication on your cluster, users must provide credentials to Knative for communicating with the Kafka cluster, otherwise events cannot be produced or consumed.","title":"\u542f\u7528 SASL for KafkaSources"},{"location":"eventing/sources/kafka-source/#prerequisites","text":"You have access to a Kafka cluster that has Simple Authentication and Security Layer (SASL).","title":"Prerequisites"},{"location":"eventing/sources/kafka-source/#procedure","text":"Create a secret that uses the Kafka cluster's SASL information, by running the following commands: STRIMZI_CRT = $( kubectl -n kafka get secret example-cluster-cluster-ca-cert --template = '{{index.data \"ca.crt\"}}' | base64 --decode ) SASL_PASSWD = $( kubectl -n kafka get secret example-user --template = '{{index.data \"password\"}}' | base64 --decode ) kubectl create secret -n default generic <secret_name> \\ --from-literal = ca.crt = \" $STRIMZI_CRT \" \\ --from-literal = password = \" $SASL_PASSWD \" \\ --from-literal = saslType = \"SCRAM-SHA-512\" \\ --from-literal = user = \"example-user\" Create or modify a KafkaSource so that it contains the following spec options: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : example-source spec : ... net : sasl : enable : true user : secretKeyRef : name : <secret_name> key : user password : secretKeyRef : name : <secret_name> key : password saslType : secretKeyRef : name : <secret_name> key : saslType tls : enable : true caCert : secretKeyRef : name : <secret_name> key : ca.crt ... Where <secret_name> is the name of the secret generated in the previous step.","title":"Procedure"},{"location":"eventing/sources/kafka-source/#_6","text":"Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"\u6e05\u7406\u6b65\u9aa4"},{"location":"eventing/sources/ping-source/","text":"\u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61 \u00b6 \u4ecb\u7ecd\u5982\u4f55\u521b\u5efaPingSource\u5bf9\u8c61\u3002 PingSource\u662f\u4e00\u79cd\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684 cron \u8c03\u5ea6\u4e0a\u4f7f\u7528\u56fa\u5b9a\u7684\u6709\u6548\u8d1f\u8f7d\u751f\u6210\u4e8b\u4ef6\u3002 \u4e0b\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5c06PingSource\u914d\u7f6e\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u8be5\u4e8b\u4ef6\u6e90\u6bcf\u5206\u949f\u5411\u540d\u4e3a event-display \u7684Knative\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\uff0c\u8be5\u670d\u52a1\u7528\u4f5c\u63a5\u6536\u5668\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u521b\u5efaPingSource: \u4f60\u5fc5\u987b\u5b89\u88c5 Knative\u4e8b\u4ef6 \u3002 \u5728\u5b89\u88c5Knative\u4e8b\u4ef6\u65f6\uff0c\u9ed8\u8ba4\u542f\u7528PingSource\u4e8b\u4ef6\u6e90\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl \u6216 kn \u547d\u4ee4\u6765\u521b\u5efa\u63a5\u6536\u5668\u548cPingSource\u7b49\u7ec4\u4ef6\u3002 \u5728\u6b64\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u6b65\u9aa4\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl \u6216 kail \u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55\u3002 \u521b\u5efaPingSource\u5bf9\u8c61 \u00b6 \u53ef\u9009: \u4e3a\u4f60\u7684PingSource\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u5e0c\u671bPingSource\u4f7f\u7528\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u4f8b\u5982, pingsource-example . Note \u4e3aPingSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u53ef\u4ee5\u8ba9\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u201c\u9ed8\u8ba4\u201d\u540d\u79f0\u7a7a\u95f4\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u5668\u3002\u5982\u679c\u60a8\u6ca1\u6709\u81ea\u5df1\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u670d\u52a1\u5c06\u4f20\u5165\u7684\u6d88\u606f\u8f6c\u50a8\u5230\u65e5\u5fd7\u4e2d: \u5c06\u4e0b\u9762\u7684YAML\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 \u5176\u4e2d <namespace> \u662f\u60a8\u5728\u4e0a\u9762\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efaPingSource\u5bf9\u8c61\u3002 Note \u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u5fc5\u987b\u5728PingSource YAML\u6587\u4ef6\u4e2d\u8868\u793a\u4e3a\u6587\u672c\u3002 \u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684\u4e8b\u4ef6\u4e0d\u80fd\u5728YAML\u4e2d\u76f4\u63a5\u5e8f\u5217\u5316\u3002 \u7136\u800c\uff0c\u4f60\u53ef\u4ee5\u53d1\u9001base64\u7f16\u7801\u7684\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u5728PingSource\u89c4\u8303\u4e2d\u4f7f\u7528 dataBase64 \u6765\u4ee3\u66ff data \u3002 \u4f7f\u7528\u4ee5\u4e0b\u9009\u9879\u4e4b\u4e00: kn kn: binary data YAML YAML: binary data \u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u53d1\u9001\u53ef\u4ee5\u8868\u793a\u4e3a\u7eaf\u6587\u672c\u7684\u6570\u636e\uff0c\u5982\u6587\u672c\u3001JSON\u6216XML\uff0c\u8fd0\u884c\u547d\u4ee4: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u3002\u6b64\u6570\u636e\u5fc5\u987b\u8868\u793a\u4e3a\u6587\u672c\uff0c\u800c\u4e0d\u662f\u4e8c\u8fdb\u5236\u3002\u4f8b\u5982\uff0c\u4e00\u4e2aJSON\u5bf9\u8c61\uff0c\u5982 {\"message\": \"Hello world!\"} \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u5b57\uff0c\u4f8b\u5982\uff0c http://event-display.pingsource-example.svc.cluster.local \u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u521b\u5efa\u4e00\u4e2a\u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684PingSource\uff0c\u4f7f\u7528\u547d\u4ee4: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<base64-data>' \\ --encoding 'base64' \\ --sink <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <base64-data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684base64\u7f16\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u4f8b\u5982\uff0c ZGF0YQ== \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u5b57\uff0c\u4f8b\u5982\uff0c http://event-display.pingsource-example.svc.cluster.local \u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u53d1\u9001\u53ef\u4ee5\u8868\u793a\u4e3a\u7eaf\u6587\u672c\u7684\u6570\u636e\uff0c\u5982\u6587\u672c\u3001JSON\u6216XML: \u4f7f\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <content-type> \u5b83\u662f\u4f60\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u7684\u5a92\u4f53\u7c7b\u578b\uff0c\u4f8b\u5982\uff0c application/json \u3002 <data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u3002\u6b64\u6570\u636e\u5fc5\u987b\u8868\u793a\u4e3a\u6587\u672c\uff0c\u800c\u4e0d\u662f\u4e8c\u8fdb\u5236\u3002\u4f8b\u5982\uff0c\u4e00\u4e2aJSON\u5bf9\u8c61\uff0c\u5982 {\"message\": \"Hello world!\"} \u3002 <sink-kind> \u5b83\u662f\u4f60\u60f3\u7528\u4f5c\u63a5\u6536\u5668\u7684\u4efb\u4f55\u53d7\u652f\u6301\u7684\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982 \u670d\u52a1 \u6216 \u90e8\u7f72 \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c event-display \u3002 \u6709\u5173\u53ef\u4ee5\u4e3aPingSource\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2a\u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684PingSource: \u4f7f\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source-binary \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <content-type> \u5b83\u662f\u4f60\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u7684\u5a92\u4f53\u7c7b\u578b\uff0c\u4f8b\u5982\uff0c application/json \u3002 <base64-data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684base64\u7f16\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u4f8b\u5982\uff0c ZGF0YQ== \u3002 <sink-kind> \u5b83\u662f\u60a8\u5e0c\u671b\u7528\u4f5c\u63a5\u6536\u5668\u7684\u4efb\u4f55\u53d7\u652f\u6301\u7684\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982\uff0cKubernetes\u670d\u52a1\u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c event-display \u3002 \u6709\u5173\u53ef\u4ee5\u4e3aPingSource\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u9a8c\u8bc1PingSource\u5bf9\u8c61 \u00b6 \u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m \u9a8c\u8bc1\u8f93\u51fa\u662f\u5426\u8fd4\u56dePingSource\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u5c5e\u6027\u3002 \u5728\u4e0b\u4f8b\u4e2d\uff0c\u8be5\u547d\u4ee4\u8fd4\u56dePingSource\u53d1\u9001\u7ed9event-display\u670d\u52a1\u7684\u4e8b\u4ef6\u7684\u201cAttributes\u201d\u548c\u201cData\u201d\u5c5e\u6027: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } \u5220\u9664PingSource\u5bf9\u8c61 \u00b6 \u60a8\u53ef\u4ee5\u5220\u9664PingSource\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u5220\u9664\u8d44\u6e90: \u8981\u5220\u9664PingSource\u5bf9\u8c61\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u547d\u540d\u7a7a\u95f4: kubectl delete namespace <namespace> \u5176\u4e2d <namespace> \u662f\u5305\u542bPingSource\u5bf9\u8c61\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u53ea\u5220\u9664PingSource\u5b9e\u4f8b\uff0c\u4f7f\u7528\u547d\u4ee4: kn kubectl kn source ping delete <pingsource-name> \u5176\u4e2d <pingsource-name> \u662f\u8981\u5220\u9664\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982 test-ping-source \u3002 kubectl delete pingsources.sources.knative.dev <pingsource-name> \u5176\u4e2d <pingsource-name> \u662f\u8981\u5220\u9664\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982 test-ping-source \u3002 \u53ea\u5220\u9664sink\uff0c\u4f7f\u7528\u547d\u4ee4: kn kubectl kn service delete <sink-name> \u5176\u4e2d <sink-name> \u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982 event-display \u3002 kubectl delete service.serving.knative.dev <sink-name> \u5176\u4e2d <sink-name> \u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982 event-display \u3002","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource","text":"\u4ecb\u7ecd\u5982\u4f55\u521b\u5efaPingSource\u5bf9\u8c61\u3002 PingSource\u662f\u4e00\u79cd\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684 cron \u8c03\u5ea6\u4e0a\u4f7f\u7528\u56fa\u5b9a\u7684\u6709\u6548\u8d1f\u8f7d\u751f\u6210\u4e8b\u4ef6\u3002 \u4e0b\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5c06PingSource\u914d\u7f6e\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u8be5\u4e8b\u4ef6\u6e90\u6bcf\u5206\u949f\u5411\u540d\u4e3a event-display \u7684Knative\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\uff0c\u8be5\u670d\u52a1\u7528\u4f5c\u63a5\u6536\u5668\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002","title":"\u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#_1","text":"\u521b\u5efaPingSource: \u4f60\u5fc5\u987b\u5b89\u88c5 Knative\u4e8b\u4ef6 \u3002 \u5728\u5b89\u88c5Knative\u4e8b\u4ef6\u65f6\uff0c\u9ed8\u8ba4\u542f\u7528PingSource\u4e8b\u4ef6\u6e90\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl \u6216 kn \u547d\u4ee4\u6765\u521b\u5efa\u63a5\u6536\u5668\u548cPingSource\u7b49\u7ec4\u4ef6\u3002 \u5728\u6b64\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u6b65\u9aa4\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl \u6216 kail \u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/sources/ping-source/#pingsource_1","text":"\u53ef\u9009: \u4e3a\u4f60\u7684PingSource\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u5e0c\u671bPingSource\u4f7f\u7528\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u4f8b\u5982, pingsource-example . Note \u4e3aPingSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u53ef\u4ee5\u8ba9\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u201c\u9ed8\u8ba4\u201d\u540d\u79f0\u7a7a\u95f4\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u5668\u3002\u5982\u679c\u60a8\u6ca1\u6709\u81ea\u5df1\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u670d\u52a1\u5c06\u4f20\u5165\u7684\u6d88\u606f\u8f6c\u50a8\u5230\u65e5\u5fd7\u4e2d: \u5c06\u4e0b\u9762\u7684YAML\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 \u5176\u4e2d <namespace> \u662f\u60a8\u5728\u4e0a\u9762\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efaPingSource\u5bf9\u8c61\u3002 Note \u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u5fc5\u987b\u5728PingSource YAML\u6587\u4ef6\u4e2d\u8868\u793a\u4e3a\u6587\u672c\u3002 \u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684\u4e8b\u4ef6\u4e0d\u80fd\u5728YAML\u4e2d\u76f4\u63a5\u5e8f\u5217\u5316\u3002 \u7136\u800c\uff0c\u4f60\u53ef\u4ee5\u53d1\u9001base64\u7f16\u7801\u7684\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u5728PingSource\u89c4\u8303\u4e2d\u4f7f\u7528 dataBase64 \u6765\u4ee3\u66ff data \u3002 \u4f7f\u7528\u4ee5\u4e0b\u9009\u9879\u4e4b\u4e00: kn kn: binary data YAML YAML: binary data \u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u53d1\u9001\u53ef\u4ee5\u8868\u793a\u4e3a\u7eaf\u6587\u672c\u7684\u6570\u636e\uff0c\u5982\u6587\u672c\u3001JSON\u6216XML\uff0c\u8fd0\u884c\u547d\u4ee4: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u3002\u6b64\u6570\u636e\u5fc5\u987b\u8868\u793a\u4e3a\u6587\u672c\uff0c\u800c\u4e0d\u662f\u4e8c\u8fdb\u5236\u3002\u4f8b\u5982\uff0c\u4e00\u4e2aJSON\u5bf9\u8c61\uff0c\u5982 {\"message\": \"Hello world!\"} \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u5b57\uff0c\u4f8b\u5982\uff0c http://event-display.pingsource-example.svc.cluster.local \u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u521b\u5efa\u4e00\u4e2a\u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684PingSource\uff0c\u4f7f\u7528\u547d\u4ee4: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<base64-data>' \\ --encoding 'base64' \\ --sink <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <base64-data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684base64\u7f16\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u4f8b\u5982\uff0c ZGF0YQ== \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u5b57\uff0c\u4f8b\u5982\uff0c http://event-display.pingsource-example.svc.cluster.local \u3002 \u6709\u5173\u53ef\u7528\u9009\u9879\u7684\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative\u5ba2\u6237\u7aef\u6587\u6863 . \u521b\u5efa\u4e00\u4e2aPingSource\uff0c\u53d1\u9001\u53ef\u4ee5\u8868\u793a\u4e3a\u7eaf\u6587\u672c\u7684\u6570\u636e\uff0c\u5982\u6587\u672c\u3001JSON\u6216XML: \u4f7f\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <content-type> \u5b83\u662f\u4f60\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u7684\u5a92\u4f53\u7c7b\u578b\uff0c\u4f8b\u5982\uff0c application/json \u3002 <data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u3002\u6b64\u6570\u636e\u5fc5\u987b\u8868\u793a\u4e3a\u6587\u672c\uff0c\u800c\u4e0d\u662f\u4e8c\u8fdb\u5236\u3002\u4f8b\u5982\uff0c\u4e00\u4e2aJSON\u5bf9\u8c61\uff0c\u5982 {\"message\": \"Hello world!\"} \u3002 <sink-kind> \u5b83\u662f\u4f60\u60f3\u7528\u4f5c\u63a5\u6536\u5668\u7684\u4efb\u4f55\u53d7\u652f\u6301\u7684\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982 \u670d\u52a1 \u6216 \u90e8\u7f72 \u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c event-display \u3002 \u6709\u5173\u53ef\u4ee5\u4e3aPingSource\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2a\u53d1\u9001\u4e8c\u8fdb\u5236\u6570\u636e\u7684PingSource: \u4f7f\u7528\u4e0b\u9762\u7684\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> \u5b83\u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c test-ping-source-binary \u3002 <namespace> \u5b83\u662f\u60a8\u5728\u4e0a\u9762\u7684\u7b2c1\u6b65\u4e2d\u521b\u5efa\u7684\u547d\u540d\u7a7a\u95f4\u7684\u540d\u79f0\u3002 <cron-schedule> \u5b83\u662fPingSource\u53d1\u9001\u4e8b\u4ef6\u7684\u65f6\u95f4\u8868\u7684cron\u8868\u8fbe\u5f0f\uff0c\u4f8b\u5982\uff0c */1 * * * * \u6bcf\u5206\u949f\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u3002 <content-type> \u5b83\u662f\u4f60\u60f3\u8981\u53d1\u9001\u7684\u6570\u636e\u7684\u5a92\u4f53\u7c7b\u578b\uff0c\u4f8b\u5982\uff0c application/json \u3002 <base64-data> \u5b83\u662f\u60a8\u60f3\u8981\u53d1\u9001\u7684base64\u7f16\u7801\u4e8c\u8fdb\u5236\u6570\u636e\uff0c\u4f8b\u5982\uff0c ZGF0YQ== \u3002 <sink-kind> \u5b83\u662f\u60a8\u5e0c\u671b\u7528\u4f5c\u63a5\u6536\u5668\u7684\u4efb\u4f55\u53d7\u652f\u6301\u7684\u53ef\u5bfb\u5740\u5bf9\u8c61\uff0c\u4f8b\u5982\uff0cKubernetes\u670d\u52a1\u3002 <sink-name> \u5b83\u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982\uff0c event-display \u3002 \u6709\u5173\u53ef\u4ee5\u4e3aPingSource\u5bf9\u8c61\u914d\u7f6e\u7684\u5b57\u6bb5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 PingSource reference \u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efaPingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource_2","text":"\u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m \u9a8c\u8bc1\u8f93\u51fa\u662f\u5426\u8fd4\u56dePingSource\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u5c5e\u6027\u3002 \u5728\u4e0b\u4f8b\u4e2d\uff0c\u8be5\u547d\u4ee4\u8fd4\u56dePingSource\u53d1\u9001\u7ed9event-display\u670d\u52a1\u7684\u4e8b\u4ef6\u7684\u201cAttributes\u201d\u548c\u201cData\u201d\u5c5e\u6027: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" }","title":"\u9a8c\u8bc1PingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource_3","text":"\u60a8\u53ef\u4ee5\u5220\u9664PingSource\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u5220\u9664\u8d44\u6e90: \u8981\u5220\u9664PingSource\u5bf9\u8c61\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u547d\u540d\u7a7a\u95f4: kubectl delete namespace <namespace> \u5176\u4e2d <namespace> \u662f\u5305\u542bPingSource\u5bf9\u8c61\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u53ea\u5220\u9664PingSource\u5b9e\u4f8b\uff0c\u4f7f\u7528\u547d\u4ee4: kn kubectl kn source ping delete <pingsource-name> \u5176\u4e2d <pingsource-name> \u662f\u8981\u5220\u9664\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982 test-ping-source \u3002 kubectl delete pingsources.sources.knative.dev <pingsource-name> \u5176\u4e2d <pingsource-name> \u662f\u8981\u5220\u9664\u7684PingSource\u7684\u540d\u79f0\uff0c\u4f8b\u5982 test-ping-source \u3002 \u53ea\u5220\u9664sink\uff0c\u4f7f\u7528\u547d\u4ee4: kn kubectl kn service delete <sink-name> \u5176\u4e2d <sink-name> \u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982 event-display \u3002 kubectl delete service.serving.knative.dev <sink-name> \u5176\u4e2d <sink-name> \u662f\u4f60\u7684\u63a5\u6536\u5668\u7684\u540d\u79f0\uff0c\u4f8b\u5982 event-display \u3002","title":"\u5220\u9664PingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/reference/","text":"PingSource \u6e90 \u00b6 \u4ecb\u7ecdPingSource\u5bf9\u8c61\u53ef\u914d\u7f6e\u5b57\u6bb5\u7684\u53c2\u8003\u4fe1\u606f\u3002 PingSource \u00b6 PingSource\u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b\u6216\u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u6b64\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3aPingSource\u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6PingSource\u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a name \u3002 \u5fc5\u987b spec \u6307\u5b9a\u6b64PingSource\u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.contentType \u5a92\u4f53\u7c7b\u578b\u4e3a data \u6216 dataBase64 \u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002 \u53ef\u9009 spec.data \u7528\u4f5c\u53d1\u5e03\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u4e3b\u4f53\u7684\u6570\u636e\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u4e0e dataBase64 \u4e92\u65a5\u3002 \u5982\u679c\u6ca1\u6709\u53d1\u9001base64\u7f16\u7801\u7684\u6570\u636e\uff0c\u5219\u9700\u8981 spec.dataBase64 \u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u5b9e\u9645\u4e8b\u4ef6\u4e3b\u4f53\u7684base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u4e0e data \u76f8\u4e92\u6392\u65a5\u3002 \u5982\u679c\u53d1\u9001base64\u7f16\u7801\u7684\u6570\u636e\uff0c\u5219\u9700\u8981 spec.schedule \u6307\u5b9acron\u8ba1\u5212\u3002\u9ed8\u8ba4\u4e3a * * * * * . \u53ef\u9009 spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.timezone \u4fee\u6539\u76f8\u5bf9\u4e8e\u6307\u5b9a\u65f6\u533a\u7684\u5b9e\u9645\u65f6\u95f4\u3002\u9ed8\u8ba4\u4e3a\u7cfb\u7edf\u65f6\u533a\u3002 \u53c2\u89c1Wikipedia\u4e0a\u7684 \u6709\u6548tz\u6570\u636e\u5e93\u65f6\u533a\u5217\u8868 \u3002\u6709\u5173\u65f6\u533a\u7684\u4e00\u822c\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 IANA \u7f51\u7ad9\u3002 \u53ef\u9009 spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009 status \u5b9a\u4e49PingSource\u7684\u89c2\u5bdf\u72b6\u6001\u3002 \u53ef\u9009 status.observedGeneration \u6700\u540e\u7531\u63a7\u5236\u5668\u5904\u7406\u7684\u670d\u52a1\u7684 Generation \u3002 \u53ef\u9009 status.conditions \u8d44\u6e90\u5f53\u524d\u72b6\u6001\u7684\u6700\u65b0\u53ef\u7528\u89c2\u5bdf\u3002 \u53ef\u9009 status.sinkUri \u4e3aSource\u914d\u7f6e\u7684\u5f53\u524d\u6d3b\u52a8\u63a5\u6536\u5668URI\u3002 \u53ef\u9009 CloudEvent Overrides\uff08\u8986\u76d6\uff09 \u00b6 CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 ceOverrides \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b\u6216\u53ef\u9009 extensions \u6307\u5b9a\u5728\u51fa\u7ad9\u4e8b\u4ef6\u4e0a\u6dfb\u52a0\u6216\u8986\u76d6\u54ea\u4e9b\u5c5e\u6027\u3002\u6bcf\u4e2a extensions key-value \u5bf9\u5728\u4e8b\u4ef6\u4e0a\u4f5c\u4e3a\u5c5e\u6027\u6269\u5c55\u72ec\u7acb\u8bbe\u7f6e\u3002 \u53ef\u9009 Note \u53ea\u5141\u8bb8\u6709\u6548\u7684 CloudEvent\u5c5e\u6027\u540d \u4f5c\u4e3a\u6269\u5c55\u3002 \u60a8\u4e0d\u80fd\u4ece\u6269\u5c55\u8986\u76d6\u914d\u7f6e\u4e2d\u8bbe\u7f6e\u89c4\u8303\u5b9a\u4e49\u7684\u5c5e\u6027\u3002 \u4f8b\u5982\uff0c\u4f60\u4e0d\u80fd\u4fee\u6539 type \u5c5e\u6027\u3002 \u793a\u4f8b: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 \u5408\u540c \u8fd9\u5bfc\u81f4\u5728 subject \u4e0a\u8bbe\u7f6e K_CE_OVERRIDES \u73af\u5883\u53d8\u91cf\uff0c\u5982\u4e0b\u6240\u793a: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u6e90\u53c2\u8003"},{"location":"eventing/sources/ping-source/reference/#pingsource","text":"\u4ecb\u7ecdPingSource\u5bf9\u8c61\u53ef\u914d\u7f6e\u5b57\u6bb5\u7684\u53c2\u8003\u4fe1\u606f\u3002","title":"PingSource \u6e90"},{"location":"eventing/sources/ping-source/reference/#pingsource_1","text":"PingSource\u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: Field Description \u5fc5\u987b\u6216\u53ef\u9009 apiVersion \u6307\u5b9aAPI\u7248\u672c\uff0c\u4f8b\u5982 sources.knative.dev/v1 . \u5fc5\u987b kind \u5c06\u6b64\u8d44\u6e90\u5bf9\u8c61\u6807\u8bc6\u4e3aPingSource\u5bf9\u8c61\u3002 \u5fc5\u987b metadata \u6307\u5b9a\u552f\u4e00\u6807\u8bc6PingSource\u5bf9\u8c61\u7684\u5143\u6570\u636e\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a name \u3002 \u5fc5\u987b spec \u6307\u5b9a\u6b64PingSource\u5bf9\u8c61\u7684\u914d\u7f6e\u4fe1\u606f\u3002 \u5fc5\u987b spec.contentType \u5a92\u4f53\u7c7b\u578b\u4e3a data \u6216 dataBase64 \u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002 \u53ef\u9009 spec.data \u7528\u4f5c\u53d1\u5e03\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u4e3b\u4f53\u7684\u6570\u636e\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u4e0e dataBase64 \u4e92\u65a5\u3002 \u5982\u679c\u6ca1\u6709\u53d1\u9001base64\u7f16\u7801\u7684\u6570\u636e\uff0c\u5219\u9700\u8981 spec.dataBase64 \u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u5b9e\u9645\u4e8b\u4ef6\u4e3b\u4f53\u7684base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u3002\u4e0e data \u76f8\u4e92\u6392\u65a5\u3002 \u5982\u679c\u53d1\u9001base64\u7f16\u7801\u7684\u6570\u636e\uff0c\u5219\u9700\u8981 spec.schedule \u6307\u5b9acron\u8ba1\u5212\u3002\u9ed8\u8ba4\u4e3a * * * * * . \u53ef\u9009 spec.sink \u5bf9\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684URI\u7684\u5bf9\u8c61\u7684\u5f15\u7528\u3002 \u5fc5\u987b spec.timezone \u4fee\u6539\u76f8\u5bf9\u4e8e\u6307\u5b9a\u65f6\u533a\u7684\u5b9e\u9645\u65f6\u95f4\u3002\u9ed8\u8ba4\u4e3a\u7cfb\u7edf\u65f6\u533a\u3002 \u53c2\u89c1Wikipedia\u4e0a\u7684 \u6709\u6548tz\u6570\u636e\u5e93\u65f6\u533a\u5217\u8868 \u3002\u6709\u5173\u65f6\u533a\u7684\u4e00\u822c\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 IANA \u7f51\u7ad9\u3002 \u53ef\u9009 spec.ceOverrides \u5b9a\u4e49\u8986\u76d6\u4ee5\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 \u53ef\u9009 status \u5b9a\u4e49PingSource\u7684\u89c2\u5bdf\u72b6\u6001\u3002 \u53ef\u9009 status.observedGeneration \u6700\u540e\u7531\u63a7\u5236\u5668\u5904\u7406\u7684\u670d\u52a1\u7684 Generation \u3002 \u53ef\u9009 status.conditions \u8d44\u6e90\u5f53\u524d\u72b6\u6001\u7684\u6700\u65b0\u53ef\u7528\u89c2\u5bdf\u3002 \u53ef\u9009 status.sinkUri \u4e3aSource\u914d\u7f6e\u7684\u5f53\u524d\u6d3b\u52a8\u63a5\u6536\u5668URI\u3002 \u53ef\u9009","title":"PingSource"},{"location":"eventing/sources/ping-source/reference/#cloudevent-overrides","text":"CloudEvent Overrides\u5b9a\u4e49\u4e86\u8986\u76d6\u6765\u63a7\u5236\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u8f93\u51fa\u683c\u5f0f\u548c\u4fee\u6539\u3002 ceOverrides \u5b9a\u4e49\u652f\u6301\u4ee5\u4e0b\u5b57\u6bb5: \u5b57\u6bb5 \u63cf\u8ff0 \u5fc5\u987b\u6216\u53ef\u9009 extensions \u6307\u5b9a\u5728\u51fa\u7ad9\u4e8b\u4ef6\u4e0a\u6dfb\u52a0\u6216\u8986\u76d6\u54ea\u4e9b\u5c5e\u6027\u3002\u6bcf\u4e2a extensions key-value \u5bf9\u5728\u4e8b\u4ef6\u4e0a\u4f5c\u4e3a\u5c5e\u6027\u6269\u5c55\u72ec\u7acb\u8bbe\u7f6e\u3002 \u53ef\u9009 Note \u53ea\u5141\u8bb8\u6709\u6548\u7684 CloudEvent\u5c5e\u6027\u540d \u4f5c\u4e3a\u6269\u5c55\u3002 \u60a8\u4e0d\u80fd\u4ece\u6269\u5c55\u8986\u76d6\u914d\u7f6e\u4e2d\u8bbe\u7f6e\u89c4\u8303\u5b9a\u4e49\u7684\u5c5e\u6027\u3002 \u4f8b\u5982\uff0c\u4f60\u4e0d\u80fd\u4fee\u6539 type \u5c5e\u6027\u3002","title":"CloudEvent Overrides\uff08\u8986\u76d6\uff09"},{"location":"eventing/sources/ping-source/reference/#cloudevent-overrides_1","text":"apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 \u5408\u540c \u8fd9\u5bfc\u81f4\u5728 subject \u4e0a\u8bbe\u7f6e K_CE_OVERRIDES \u73af\u5883\u53d8\u91cf\uff0c\u5982\u4e0b\u6240\u793a: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u793a\u4f8b: CloudEvent Overrides"},{"location":"eventing/sources/rabbitmq-source/","text":"\u521b\u5efa\u4e00\u4e2aRabbitMQSource \u00b6 \u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efaRabbitMQSource\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 Knative\u4e8b\u4ef6 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 CertManager v1.5.4 \u2014\u4e0eRabbitMQ\u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u5668\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 RabbitMQ\u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u5668 -\u6211\u4eec\u7684\u5efa\u8bae\u662f \u6700\u65b0\u7248\u672c \u4e0eCertManager \u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684RabbitMQ\u5b9e\u4f8b\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u7b26 \u521b\u5efa\u4e00\u4e2a.\u6709\u5173\u914d\u7f6e RabbitmqCluster CRD\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u7f51\u7ad9 \u5b89\u88c5RabbitMQ\u63a7\u5236\u5668 \u00b6 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5RabbitMQSource\u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-source.yaml \u9a8c\u8bc1 rabbitmq-controller-manager \u548c rabbitmq-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources Example output: NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-controller-manager 1 /1 1 1 3s rabbitmq-webhook 1 /1 1 1 4s \u521b\u5efa\u670d\u52a1 \u00b6 \u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s \u521b\u5efa\u4e00\u4e2aRabbitMQSource\u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1alpha1 kind : RabbitmqSource metadata : name : <source-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials rabbitmqResourcesConfig : parallelism : 10 exchangeName : \"eventing-rabbitmq-source\" queueName : \"eventing-rabbitmq-source\" delivery : retry : 5 backoffPolicy : \"linear\" backoffDelay : \"PT1S\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Where: <source-name> \u662f\u4f60\u60f3\u8981\u7684RabbitMQSource\u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4f60\u4e4b\u524d\u521b\u5efa\u7684RabbitMQ\u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u60a8\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c\u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eSource\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c\u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u9a8c\u8bc1 \u00b6 \u68c0\u67e5\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\uff0c\u770b\u5b83\u662f\u5426\u6b63\u5728\u63a5\u6536\u4e8b\u4ef6\u3002 Source\u5f00\u59cb\u5411Sink\u53d1\u9001\u4e8b\u4ef6\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\u3002 kubectl -l = 'serving.knative.dev/service=event-display' logs -c user-container \u2601\ufe0f cloudevents.Event Context Attributes, specversion: 1 .0 type: dev.knative.rabbitmq.event source: /apis/v1/namespaces/default/rabbitmqsources/<source-name> subject: f147099d-c64d-41f7-b8eb-a2e53b228349 id: f147099d-c64d-41f7-b8eb-a2e53b228349 time: 2021 -12-16T20:11:39.052276498Z datacontenttype: application/json Data, { ... Random Data ... } \u6e05\u7406 \u00b6 \u5220\u9664RabbitMQSource: kubectl delete -f <source-yaml-filename> \u5220\u9664RabbitMQ\u7684\u8bc1\u4e66\u79d8\u5bc6: kubectl delete -f <secret-yaml-filename> \u5220\u9664\u4e8b\u4ef6\u663e\u793a\u670d\u52a1: kubectl delete -f <service-yaml-filename> \u989d\u5916\u4fe1\u606f \u00b6 \u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github\u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2abug\u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github\u5e93\u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898 .","title":"RabbitMQSource"},{"location":"eventing/sources/rabbitmq-source/#rabbitmqsource","text":"\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efaRabbitMQSource\u3002","title":"\u521b\u5efa\u4e00\u4e2aRabbitMQSource"},{"location":"eventing/sources/rabbitmq-source/#_1","text":"\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 Knative\u4e8b\u4ef6 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 CertManager v1.5.4 \u2014\u4e0eRabbitMQ\u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u5668\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 RabbitMQ\u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u5668 -\u6211\u4eec\u7684\u5efa\u8bae\u662f \u6700\u65b0\u7248\u672c \u4e0eCertManager \u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684RabbitMQ\u5b9e\u4f8b\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u7b26 \u521b\u5efa\u4e00\u4e2a.\u6709\u5173\u914d\u7f6e RabbitmqCluster CRD\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u7f51\u7ad9","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/sources/rabbitmq-source/#rabbitmq","text":"\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5RabbitMQSource\u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-source.yaml \u9a8c\u8bc1 rabbitmq-controller-manager \u548c rabbitmq-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources Example output: NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-controller-manager 1 /1 1 1 3s rabbitmq-webhook 1 /1 1 1 4s","title":"\u5b89\u88c5RabbitMQ\u63a7\u5236\u5668"},{"location":"eventing/sources/rabbitmq-source/#_2","text":"\u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/sources/rabbitmq-source/#rabbitmqsource_1","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : sources.knative.dev/v1alpha1 kind : RabbitmqSource metadata : name : <source-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials rabbitmqResourcesConfig : parallelism : 10 exchangeName : \"eventing-rabbitmq-source\" queueName : \"eventing-rabbitmq-source\" delivery : retry : 5 backoffPolicy : \"linear\" backoffDelay : \"PT1S\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Where: <source-name> \u662f\u4f60\u60f3\u8981\u7684RabbitMQSource\u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4f60\u4e4b\u524d\u521b\u5efa\u7684RabbitMQ\u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u60a8\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c\u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eSource\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c\u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2aRabbitMQSource\u5bf9\u8c61"},{"location":"eventing/sources/rabbitmq-source/#_3","text":"\u68c0\u67e5\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\uff0c\u770b\u5b83\u662f\u5426\u6b63\u5728\u63a5\u6536\u4e8b\u4ef6\u3002 Source\u5f00\u59cb\u5411Sink\u53d1\u9001\u4e8b\u4ef6\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\u3002 kubectl -l = 'serving.knative.dev/service=event-display' logs -c user-container \u2601\ufe0f cloudevents.Event Context Attributes, specversion: 1 .0 type: dev.knative.rabbitmq.event source: /apis/v1/namespaces/default/rabbitmqsources/<source-name> subject: f147099d-c64d-41f7-b8eb-a2e53b228349 id: f147099d-c64d-41f7-b8eb-a2e53b228349 time: 2021 -12-16T20:11:39.052276498Z datacontenttype: application/json Data, { ... Random Data ... }","title":"\u9a8c\u8bc1"},{"location":"eventing/sources/rabbitmq-source/#_4","text":"\u5220\u9664RabbitMQSource: kubectl delete -f <source-yaml-filename> \u5220\u9664RabbitMQ\u7684\u8bc1\u4e66\u79d8\u5bc6: kubectl delete -f <secret-yaml-filename> \u5220\u9664\u4e8b\u4ef6\u663e\u793a\u670d\u52a1: kubectl delete -f <service-yaml-filename>","title":"\u6e05\u7406"},{"location":"eventing/sources/rabbitmq-source/#_5","text":"\u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github\u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2abug\u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github\u5e93\u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898 .","title":"\u989d\u5916\u4fe1\u606f"},{"location":"eventing/sources/redis/","text":"\u5173\u4e8e RedisStreamSource \u00b6 RedisStreamSource \u4ece Redis \u6d41 \u8bfb\u53d6\u6d88\u606f\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u53d1\u9001\u5230\u5f15\u7528\u7684\u63a5\u6536\u5668\uff0c\u8be5\u63a5\u6536\u5668\u53ef\u4ee5\u662fKubernetes\u670d\u52a1\u6216Knative\u670d\u52a1\u7b49\u3002 \u5b83\u88ab\u914d\u7f6e\u4e3a\u91cd\u8bd5\u53d1\u9001CloudEvents\uff0c\u4ee5\u4fbf\u4e8b\u4ef6\u4e0d\u4f1a\u4e22\u5931\u3002 Redis\u6d41\u6e90\u53ef\u4ee5\u4e0e\u672c\u5730\u7248\u672c\u7684Redis\u6570\u636e\u5e93\u5b9e\u4f8b\u6216\u57fa\u4e8e\u4e91\u7684\u5b9e\u4f8b\u4e00\u8d77\u5de5\u4f5c\uff0c\u5176 address \u5c06\u5728\u6e90\u89c4\u8303\u4e2d\u6307\u5b9a\u3002 \u6b64\u5916\uff0c\u6307\u5b9a\u7684 stream \u540d\u79f0\u548c\u6d88\u8d39\u8005 group \u540d\u79f0\u5c06\u7531\u63a5\u6536\u9002\u914d\u5668\u521b\u5efa\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u5b58\u5728\u7684\u8bdd\u3002 \u6d88\u8d39\u8005\u7ec4\u4e2d\u7684\u6d88\u8d39\u8005\u6570\u91cf\u4e5f\u53ef\u4ee5\u901a\u8fc7 config-redis \u4e2d\u7684\u6570\u636e\u8fdb\u884c\u914d\u7f6e\u3002 \u8fd9\u4f7f\u5f97\u6bcf\u4e2a\u4f7f\u7528\u8005\u53ef\u4ee5\u4f7f\u7528\u5230\u8fbe\u6d41\u4e2d\u7684\u4e0d\u540c\u6d88\u606f\u3002 \u6bcf\u4e2a\u6d88\u8d39\u8005\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u6d88\u8d39\u8005\u540d\uff0c\u8fd9\u662f\u7531\u63a5\u6536\u9002\u914d\u5668\u521b\u5efa\u7684\u5b57\u7b26\u4e32\u3002 \u5f53\u4e00\u4e2aRedis\u6d41\u6e90\u8d44\u6e90\u88ab\u5220\u9664\u65f6\uff0c\u7ec4\u4e2d\u7684\u6240\u6709\u6d88\u8d39\u8005\u90fd\u88ab\u4f18\u96c5\u5730\u5173\u95ed/\u5220\u9664\uff0c\u7136\u540e\u6d88\u8d39\u7ec4\u672c\u8eab\u88ab\u9500\u6bc1\u3002 \u5728\u4e00\u4e2a\u6d88\u8d39\u8005\u88ab\u5173\u95ed\u4e4b\u524d\uff0c\u5b83\u7684\u6240\u6709\u6302\u8d77\u7684\u6d88\u606f\u90fd\u4f5c\u4e3aCloudEvents\u53d1\u9001\u5e76\u88ab\u786e\u8ba4\u3002 \u5f00\u59cb \u00b6 \u5b89\u88c5 \u00b6 \u4f7f\u7528\u57fa\u4e8e\u4e91\u7684Redis\u5b9e\u4f8b\u7684\u524d\u63d0\u6761\u4ef6: \u00b6 \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u672c\u5730Redis\u5b9e\u4f8b\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002 \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis\u7684\u4e91\u5b9e\u4f8b(\u4f8b\u5982\uff0cIBM\u4e91\u4e0a\u7684Redis DB)\uff0c\u5219\u9700\u8981\u5728\u5b89\u88c5\u4e8b\u4ef6\u6e90\u4e4b\u524d\u914d\u7f6eTLS\u8bc1\u4e66\u3002 \u7f16\u8f91 tls-secret Secret\u5c06\u60a8\u7684Redis\u4e91\u5b9e\u4f8b\u4e2d\u7684TLS\u8bc1\u4e66\u6dfb\u52a0\u5230 TLS_CERT \u6570\u636e\u5bc6\u94a5: vi config/source/tls-secret.yaml \u5c06\u60a8\u7684\u8bc1\u4e66\u6dfb\u52a0\u5230\u6587\u4ef6\u4e2d\uff0c\u5e76\u4fdd\u5b58\u8be5\u6587\u4ef6\u3002\u5c06\u5728\u4e0b\u4e00\u6b65\u5e94\u7528\u3002 \u521b\u5efa RedisStreamSource \u6e90\u7801\u5b9a\u4e49\u53ca\u5176\u6240\u6709\u7ec4\u4ef6: \u00b6 \u5728\u5b89\u88c5\u4e8b\u4ef6\u6e90\u4e4b\u524d\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u7ec4\u4e2d\u7684\u6d88\u8d39\u8005\u6570\u91cf\u914d\u7f6e\u63a5\u6536\u9002\u914d\u5668\u3002 \u7f16\u8f91 config-redis ConfigMap\u6765\u7f16\u8f91 numConsumers \u6570\u636e\u952e: vi config/source/config-redis.yaml \u7136\u540e,\u5e94\u7528 config/source ko apply -f config/source \u4f8b\u5b50 \u00b6 \u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4f60\u521b\u5efa\u4e86\u4e00\u4e2aRedis Stream\u4e8b\u4ef6\u6e90\uff0c\u76d1\u542c\u6dfb\u52a0\u5230\u201cmystream\u201d\u6d41\u4e2d\u7684\u9879\u76ee\u3002 \u7136\u540e\u5c06\u8fd9\u4e9b\u9879\u4f5c\u4e3aCloudEvent\u4e8b\u4ef6\u53d1\u9001\u5230\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\u3002 \u5b89\u88c5\u672c\u5730Redis\u547d\u4ee4\u5982\u4e0b: kubectl apply -f samples/redis \u4e3a\u8fd9\u4e2a\u793a\u4f8b\u6e90\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4: kubectl create ns redex \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5Redis\u6d41\u6e90\u793a\u4f8b\u8d44\u6e90: Note \u9664\u4e86\u914d\u7f6e\u60a8\u7684TLS\u8bc1\u4e66\uff0c\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis DB\u7684\u4e91\u5b9e\u4f8b\uff0c\u60a8\u5c06\u9700\u8981\u5728 redisstreamsource \u6e90yaml\u4e2d\u8bbe\u7f6e\u9002\u5f53\u7684\u5730\u5740\u3002 \u5bf9\u4e8eredis v5\u53ca\u66f4\u8001\u7248\u672c\uff0c\u4e0d\u9700\u8981\u6307\u5b9a\u7528\u6237\u540d: address: \"rediss://:password@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u5bf9\u4e8eredis v6\uff0c\u9700\u8981\u4e00\u4e2a\u7528\u6237\u540d: address: \"rediss://username:password@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u7136\u540e\uff0c\u5e94\u7528 samples/source \u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\u548c\u4e00\u4e2aRedis\u6d41\u6e90\u8d44\u6e90 kubectl apply -n redex -f samples/source \u9a8c\u8bc1Redis\u6d41\u6e90\u5df2\u7ecf\u51c6\u5907\u597d: kubectl get -n redex redisstreamsources.sources.knative.dev mystream NAME SINK AGE READY REASON mystream http://event-display.redex.svc.cluster.local/ 38s True \u6dfb\u52a0\u4e00\u4e2a\u9879\u76ee\u5230 mystream : kubectl exec -n redis svc/redis redis-cli xadd mystream '*' fruit banana color yellow \u68c0\u67e5\u4e8b\u4ef6\u663e\u793a\u63a5\u6536\u5668\uff0c\u770b\u770b\u662f\u5426\u6536\u5230\u4e86\u4e8b\u4ef6: kubectl logs -n redex svc/event-display \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ] \u6570\u636e\u5305\u542b\u6dfb\u52a0\u5230\u6d41\u4e2d\u7684\u5b57\u6bb5\u503c\u5bf9\u5217\u8868\u3002 \u8981\u6e05\u7406\uff0c\u5220\u9664Redis\u6d41\u6e90\u793a\u4f8b\u548credex\u547d\u540d\u7a7a\u95f4: kubectl delete -f samples/source -n redex kubectl delete ns redex \u53c2\u8003 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 Redis\u5b89\u88c5\u3002(\u90e8\u7f72\u672c\u5730Redis\u7684\u8bf4\u660e\u5728\u4e0a\u9762) \u4e86\u89e3 Redis Stream\u7684\u57fa\u7840\u77e5\u8bc6 , \u4ee5\u53ca\u4e00\u4e9b \u7279\u5b9a\u4e8eStreams\u7684\u547d\u4ee4 . \u6e90\u5b57\u6bb5 \u00b6 RedisStreamSource \u6e90\u662fKubernetes\u5bf9\u8c61\u3002 \u9664\u4e86\u6807\u51c6\u7684Kubernetes apiVersion , kind , and metadata \uff0c\u5b83\u4eec\u8fd8\u6709\u4ee5\u4e0b\u7684 spec \u5b57\u6bb5: \u5b57\u6bb5 \u503c \u5fc5\u9009\u6216\u53ef\u9009 address Redis TCP\u5730\u5740 \u5fc5\u9009 stream Redis\u6d41\u7684\u540d\u79f0 \u5fc5\u9009 group \u4e0e\u6b64\u6e90\u5173\u8054\u7684\u4f7f\u7528\u8005\u7ec4\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002 \u53ef\u9009 sink \u5bf9 \u53ef\u5bfb\u5740 Kubernetes\u5bf9\u8c61\u7684\u5f15\u7528\uff0c\u8be5\u5bf9\u8c61\u5c06\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684uri \u5fc5\u9009 \u4e00\u65e6\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e86\u5bf9\u8c61\uff0c\u6e90\u5c06\u901a\u8fc7\u5bf9\u8c61\u4e0a\u7684 status \u5b57\u6bb5\u63d0\u4f9b\u5173\u4e8e\u5c31\u7eea\u6216\u9519\u8bef\u7684\u8f93\u51fa\u4fe1\u606f\u3002 \u8c03\u8bd5\u6280\u5de7 \u00b6 \u60a8\u53ef\u4ee5\u67e5\u770bRedis\u6d41\u6e90\u8d44\u6e90\u7684\u72b6\u6001\u3002\u6761\u4ef6\u7684\u503c\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e4b\u4e00\u6765\u8bca\u65ad\u4efb\u4f55\u95ee\u9898: kubectl get redisstreamsource -n redex kubectl describe redisstreamsource mystream -n redex \u4f60\u4e5f\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u6765\u68c0\u67e5\u63a5\u6536\u9002\u914d\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redissource-mystream-1234-0 -n redex \u4f60\u4e5f\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u6765\u68c0\u67e5\u6e90\u63a7\u5236\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redis-controller-manager-0 -n knative-sources KO\u5b89\u88c5\u95ee\u9898? \u53c2\u8003: https://github.com/google/ko/issues/106 \u5c1d\u8bd5\u91cd\u65b0\u5b89\u88c5KO\u548c\u8bbe\u7f6e export GOROOT=$(go env GOROOT) \u914d\u7f6e\u9009\u9879 \u00b6 config-observability \u548c config-logging configmap\u53ef\u7528\u4e8e\u7ba1\u7406\u65e5\u5fd7\u8bb0\u5f55\u548c\u5ea6\u91cf\u914d\u7f6e\u3002","title":"\u5173\u4e8e\u6e90"},{"location":"eventing/sources/redis/#redisstreamsource","text":"RedisStreamSource \u4ece Redis \u6d41 \u8bfb\u53d6\u6d88\u606f\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u53d1\u9001\u5230\u5f15\u7528\u7684\u63a5\u6536\u5668\uff0c\u8be5\u63a5\u6536\u5668\u53ef\u4ee5\u662fKubernetes\u670d\u52a1\u6216Knative\u670d\u52a1\u7b49\u3002 \u5b83\u88ab\u914d\u7f6e\u4e3a\u91cd\u8bd5\u53d1\u9001CloudEvents\uff0c\u4ee5\u4fbf\u4e8b\u4ef6\u4e0d\u4f1a\u4e22\u5931\u3002 Redis\u6d41\u6e90\u53ef\u4ee5\u4e0e\u672c\u5730\u7248\u672c\u7684Redis\u6570\u636e\u5e93\u5b9e\u4f8b\u6216\u57fa\u4e8e\u4e91\u7684\u5b9e\u4f8b\u4e00\u8d77\u5de5\u4f5c\uff0c\u5176 address \u5c06\u5728\u6e90\u89c4\u8303\u4e2d\u6307\u5b9a\u3002 \u6b64\u5916\uff0c\u6307\u5b9a\u7684 stream \u540d\u79f0\u548c\u6d88\u8d39\u8005 group \u540d\u79f0\u5c06\u7531\u63a5\u6536\u9002\u914d\u5668\u521b\u5efa\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u5b58\u5728\u7684\u8bdd\u3002 \u6d88\u8d39\u8005\u7ec4\u4e2d\u7684\u6d88\u8d39\u8005\u6570\u91cf\u4e5f\u53ef\u4ee5\u901a\u8fc7 config-redis \u4e2d\u7684\u6570\u636e\u8fdb\u884c\u914d\u7f6e\u3002 \u8fd9\u4f7f\u5f97\u6bcf\u4e2a\u4f7f\u7528\u8005\u53ef\u4ee5\u4f7f\u7528\u5230\u8fbe\u6d41\u4e2d\u7684\u4e0d\u540c\u6d88\u606f\u3002 \u6bcf\u4e2a\u6d88\u8d39\u8005\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684\u6d88\u8d39\u8005\u540d\uff0c\u8fd9\u662f\u7531\u63a5\u6536\u9002\u914d\u5668\u521b\u5efa\u7684\u5b57\u7b26\u4e32\u3002 \u5f53\u4e00\u4e2aRedis\u6d41\u6e90\u8d44\u6e90\u88ab\u5220\u9664\u65f6\uff0c\u7ec4\u4e2d\u7684\u6240\u6709\u6d88\u8d39\u8005\u90fd\u88ab\u4f18\u96c5\u5730\u5173\u95ed/\u5220\u9664\uff0c\u7136\u540e\u6d88\u8d39\u7ec4\u672c\u8eab\u88ab\u9500\u6bc1\u3002 \u5728\u4e00\u4e2a\u6d88\u8d39\u8005\u88ab\u5173\u95ed\u4e4b\u524d\uff0c\u5b83\u7684\u6240\u6709\u6302\u8d77\u7684\u6d88\u606f\u90fd\u4f5c\u4e3aCloudEvents\u53d1\u9001\u5e76\u88ab\u786e\u8ba4\u3002","title":"\u5173\u4e8e RedisStreamSource"},{"location":"eventing/sources/redis/#_1","text":"","title":"\u5f00\u59cb"},{"location":"eventing/sources/redis/#_2","text":"","title":"\u5b89\u88c5"},{"location":"eventing/sources/redis/#redis","text":"\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u672c\u5730Redis\u5b9e\u4f8b\uff0c\u5219\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002 \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis\u7684\u4e91\u5b9e\u4f8b(\u4f8b\u5982\uff0cIBM\u4e91\u4e0a\u7684Redis DB)\uff0c\u5219\u9700\u8981\u5728\u5b89\u88c5\u4e8b\u4ef6\u6e90\u4e4b\u524d\u914d\u7f6eTLS\u8bc1\u4e66\u3002 \u7f16\u8f91 tls-secret Secret\u5c06\u60a8\u7684Redis\u4e91\u5b9e\u4f8b\u4e2d\u7684TLS\u8bc1\u4e66\u6dfb\u52a0\u5230 TLS_CERT \u6570\u636e\u5bc6\u94a5: vi config/source/tls-secret.yaml \u5c06\u60a8\u7684\u8bc1\u4e66\u6dfb\u52a0\u5230\u6587\u4ef6\u4e2d\uff0c\u5e76\u4fdd\u5b58\u8be5\u6587\u4ef6\u3002\u5c06\u5728\u4e0b\u4e00\u6b65\u5e94\u7528\u3002","title":"\u4f7f\u7528\u57fa\u4e8e\u4e91\u7684Redis\u5b9e\u4f8b\u7684\u524d\u63d0\u6761\u4ef6:"},{"location":"eventing/sources/redis/#redisstreamsource_1","text":"\u5728\u5b89\u88c5\u4e8b\u4ef6\u6e90\u4e4b\u524d\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u7ec4\u4e2d\u7684\u6d88\u8d39\u8005\u6570\u91cf\u914d\u7f6e\u63a5\u6536\u9002\u914d\u5668\u3002 \u7f16\u8f91 config-redis ConfigMap\u6765\u7f16\u8f91 numConsumers \u6570\u636e\u952e: vi config/source/config-redis.yaml \u7136\u540e,\u5e94\u7528 config/source ko apply -f config/source","title":"\u521b\u5efa RedisStreamSource \u6e90\u7801\u5b9a\u4e49\u53ca\u5176\u6240\u6709\u7ec4\u4ef6:"},{"location":"eventing/sources/redis/#_3","text":"\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4f60\u521b\u5efa\u4e86\u4e00\u4e2aRedis Stream\u4e8b\u4ef6\u6e90\uff0c\u76d1\u542c\u6dfb\u52a0\u5230\u201cmystream\u201d\u6d41\u4e2d\u7684\u9879\u76ee\u3002 \u7136\u540e\u5c06\u8fd9\u4e9b\u9879\u4f5c\u4e3aCloudEvent\u4e8b\u4ef6\u53d1\u9001\u5230\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\u3002 \u5b89\u88c5\u672c\u5730Redis\u547d\u4ee4\u5982\u4e0b: kubectl apply -f samples/redis \u4e3a\u8fd9\u4e2a\u793a\u4f8b\u6e90\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4: kubectl create ns redex \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5Redis\u6d41\u6e90\u793a\u4f8b\u8d44\u6e90: Note \u9664\u4e86\u914d\u7f6e\u60a8\u7684TLS\u8bc1\u4e66\uff0c\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528Redis DB\u7684\u4e91\u5b9e\u4f8b\uff0c\u60a8\u5c06\u9700\u8981\u5728 redisstreamsource \u6e90yaml\u4e2d\u8bbe\u7f6e\u9002\u5f53\u7684\u5730\u5740\u3002 \u5bf9\u4e8eredis v5\u53ca\u66f4\u8001\u7248\u672c\uff0c\u4e0d\u9700\u8981\u6307\u5b9a\u7528\u6237\u540d: address: \"rediss://:password@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u5bf9\u4e8eredis v6\uff0c\u9700\u8981\u4e00\u4e2a\u7528\u6237\u540d: address: \"rediss://username:password@7f41ece8-ccb3-43df-b35b-7716e27b222e.b2b5a92ee2df47d58bad0fa448c15585.databases.appdomain.cloud:32086\" \u7136\u540e\uff0c\u5e94\u7528 samples/source \u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\u663e\u793a\u670d\u52a1\u548c\u4e00\u4e2aRedis\u6d41\u6e90\u8d44\u6e90 kubectl apply -n redex -f samples/source \u9a8c\u8bc1Redis\u6d41\u6e90\u5df2\u7ecf\u51c6\u5907\u597d: kubectl get -n redex redisstreamsources.sources.knative.dev mystream NAME SINK AGE READY REASON mystream http://event-display.redex.svc.cluster.local/ 38s True \u6dfb\u52a0\u4e00\u4e2a\u9879\u76ee\u5230 mystream : kubectl exec -n redis svc/redis redis-cli xadd mystream '*' fruit banana color yellow \u68c0\u67e5\u4e8b\u4ef6\u663e\u793a\u63a5\u6536\u5668\uff0c\u770b\u770b\u662f\u5426\u6536\u5230\u4e86\u4e8b\u4ef6: kubectl logs -n redex svc/event-display \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ] \u6570\u636e\u5305\u542b\u6dfb\u52a0\u5230\u6d41\u4e2d\u7684\u5b57\u6bb5\u503c\u5bf9\u5217\u8868\u3002 \u8981\u6e05\u7406\uff0c\u5220\u9664Redis\u6d41\u6e90\u793a\u4f8b\u548credex\u547d\u540d\u7a7a\u95f4: kubectl delete -f samples/source -n redex kubectl delete ns redex","title":"\u4f8b\u5b50"},{"location":"eventing/sources/redis/#_4","text":"","title":"\u53c2\u8003"},{"location":"eventing/sources/redis/#_5","text":"Redis\u5b89\u88c5\u3002(\u90e8\u7f72\u672c\u5730Redis\u7684\u8bf4\u660e\u5728\u4e0a\u9762) \u4e86\u89e3 Redis Stream\u7684\u57fa\u7840\u77e5\u8bc6 , \u4ee5\u53ca\u4e00\u4e9b \u7279\u5b9a\u4e8eStreams\u7684\u547d\u4ee4 .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/sources/redis/#_6","text":"RedisStreamSource \u6e90\u662fKubernetes\u5bf9\u8c61\u3002 \u9664\u4e86\u6807\u51c6\u7684Kubernetes apiVersion , kind , and metadata \uff0c\u5b83\u4eec\u8fd8\u6709\u4ee5\u4e0b\u7684 spec \u5b57\u6bb5: \u5b57\u6bb5 \u503c \u5fc5\u9009\u6216\u53ef\u9009 address Redis TCP\u5730\u5740 \u5fc5\u9009 stream Redis\u6d41\u7684\u540d\u79f0 \u5fc5\u9009 group \u4e0e\u6b64\u6e90\u5173\u8054\u7684\u4f7f\u7528\u8005\u7ec4\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002 \u53ef\u9009 sink \u5bf9 \u53ef\u5bfb\u5740 Kubernetes\u5bf9\u8c61\u7684\u5f15\u7528\uff0c\u8be5\u5bf9\u8c61\u5c06\u89e3\u6790\u4e3a\u7528\u4f5c\u63a5\u6536\u5668\u7684uri \u5fc5\u9009 \u4e00\u65e6\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u4e86\u5bf9\u8c61\uff0c\u6e90\u5c06\u901a\u8fc7\u5bf9\u8c61\u4e0a\u7684 status \u5b57\u6bb5\u63d0\u4f9b\u5173\u4e8e\u5c31\u7eea\u6216\u9519\u8bef\u7684\u8f93\u51fa\u4fe1\u606f\u3002","title":"\u6e90\u5b57\u6bb5"},{"location":"eventing/sources/redis/#_7","text":"\u60a8\u53ef\u4ee5\u67e5\u770bRedis\u6d41\u6e90\u8d44\u6e90\u7684\u72b6\u6001\u3002\u6761\u4ef6\u7684\u503c\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e4b\u4e00\u6765\u8bca\u65ad\u4efb\u4f55\u95ee\u9898: kubectl get redisstreamsource -n redex kubectl describe redisstreamsource mystream -n redex \u4f60\u4e5f\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u6765\u68c0\u67e5\u63a5\u6536\u9002\u914d\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redissource-mystream-1234-0 -n redex \u4f60\u4e5f\u53ef\u4ee5\u9605\u8bfb\u65e5\u5fd7\u6765\u68c0\u67e5\u6e90\u63a7\u5236\u5668\u7684\u90e8\u7f72\u95ee\u9898: kubectl logs redis-controller-manager-0 -n knative-sources KO\u5b89\u88c5\u95ee\u9898? \u53c2\u8003: https://github.com/google/ko/issues/106 \u5c1d\u8bd5\u91cd\u65b0\u5b89\u88c5KO\u548c\u8bbe\u7f6e export GOROOT=$(go env GOROOT)","title":"\u8c03\u8bd5\u6280\u5de7"},{"location":"eventing/sources/redis/#_8","text":"config-observability \u548c config-logging configmap\u53ef\u7528\u4e8e\u7ba1\u7406\u65e5\u5fd7\u8bb0\u5f55\u548c\u5ea6\u91cf\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u9009\u9879"},{"location":"eventing/sources/redis/getting-started/","text":"\u521b\u5efa RedisStreamSource \u00b6 \u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efa\u4e00\u4e2a RedisStreamSource \u5bf9\u8c61\u3002 \u5b89\u88c5 RedisStreamSource \u63d2\u4ef6 \u00b6 RedisStreamSource \u662f\u4e00\u4e2a Knative \u4e8b\u4ef6\u9644\u52a0\u7ec4\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RedisStreamSource: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-redis/latest/redis-source.yaml \u9a8c\u8bc1 redis-controller-manager \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE redis-controller-manager 1 /1 1 1 3s \u521b\u5efa\u670d\u52a1 \u00b6 \u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s \u521b\u5efa RedisStreamSource \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684 YAML \u6a21\u677f\u521b\u5efa RedisStreamSource \u5bf9\u8c61: apiVersion : sources.knative.dev/v1alpha1 kind : RedisStreamSource metadata : name : <redis-stream-source> spec : address : <redis-uri> stream : <redis-stream-name> group : <consumer-group-name> sink : <sink> Where: <redis-stream-source> \u662f\u4f60\u7684\u6e90\u540d\u5b57\u3002(\u5fc5\u9700) <redis-uri> \u662f Redis URI. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Redis \u6587\u6863 \u3002(\u5fc5\u9700) <redis-stream-name> \u662f Redis \u6d41\u7684\u540d\u79f0. (\u5fc5\u9700) <consumer-group-name> \u662f Redis \u6d88\u8d39\u7fa4\u4f53\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002(\u53ef\u9009) <sink> \u5b83\u662f\u53d1\u9001\u4e8b\u4ef6\u3002(\u5fc5\u9700) \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u9a8c\u8bc1 RedisStreamSource \u5bf9\u8c61 \u00b6 \u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl logs -l app = event-display --tail = 100 \u6837\u4f8b\u8f93\u51fa: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ] \u5220\u9664 RedisStreamSource \u5bf9\u8c61 \u00b6 \u5220\u9664 RedisStreamSource \u5bf9\u8c61: kubectl delete -f <filename> \u989d\u5916\u7684\u4fe1\u606f \u00b6 \u6709\u5173 Redis \u6d41\u6e90\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 eventing-redis Github \u5e93","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource","text":"\u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efa\u4e00\u4e2a RedisStreamSource \u5bf9\u8c61\u3002","title":"\u521b\u5efa RedisStreamSource"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_1","text":"RedisStreamSource \u662f\u4e00\u4e2a Knative \u4e8b\u4ef6\u9644\u52a0\u7ec4\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RedisStreamSource: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-redis/latest/redis-source.yaml \u9a8c\u8bc1 redis-controller-manager \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE redis-controller-manager 1 /1 1 1 3s","title":"\u5b89\u88c5 RedisStreamSource \u63d2\u4ef6"},{"location":"eventing/sources/redis/getting-started/#_1","text":"\u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684 YAML \u6a21\u677f\u521b\u5efa RedisStreamSource \u5bf9\u8c61: apiVersion : sources.knative.dev/v1alpha1 kind : RedisStreamSource metadata : name : <redis-stream-source> spec : address : <redis-uri> stream : <redis-stream-name> group : <consumer-group-name> sink : <sink> Where: <redis-stream-source> \u662f\u4f60\u7684\u6e90\u540d\u5b57\u3002(\u5fc5\u9700) <redis-uri> \u662f Redis URI. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Redis \u6587\u6863 \u3002(\u5fc5\u9700) <redis-stream-name> \u662f Redis \u6d41\u7684\u540d\u79f0. (\u5fc5\u9700) <consumer-group-name> \u662f Redis \u6d88\u8d39\u7fa4\u4f53\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002(\u53ef\u9009) <sink> \u5b83\u662f\u53d1\u9001\u4e8b\u4ef6\u3002(\u5fc5\u9700) \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_3","text":"\u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl logs -l app = event-display --tail = 100 \u6837\u4f8b\u8f93\u51fa: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ]","title":"\u9a8c\u8bc1 RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_4","text":"\u5220\u9664 RedisStreamSource \u5bf9\u8c61: kubectl delete -f <filename>","title":"\u5220\u9664 RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#_2","text":"\u6709\u5173 Redis \u6d41\u6e90\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 eventing-redis Github \u5e93","title":"\u989d\u5916\u7684\u4fe1\u606f"},{"location":"eventing/sugar/","text":"Knative \u4e8b\u4ef6\u7cd6\u63a7 \u00b6 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5c06\u5bf9\u914d\u7f6e\u7684\u6807\u7b7e\u505a\u51fa\u53cd\u5e94\uff0c\u4ee5\u751f\u6210\u6216\u63a7\u5236\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u4e8b\u4ef6\u6e90\u3002 \u8fd9\u5141\u8bb8\u96c6\u7fa4\u64cd\u4f5c\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e13\u6ce8\u4e8e\u521b\u5efa\u66f4\u5c11\u7684\u8d44\u6e90\uff0c\u5e76\u6309\u9700\u521b\u5efa\u5e95\u5c42\u4e8b\u4ef6\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u5728\u4e0d\u518d\u9700\u8981\u65f6\u8fdb\u884c\u6e05\u7406\u3002 \u5b89\u88c5 \u00b6 \u7cd6\u63a7\u9ed8\u8ba4\u662f disabled \u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e config-sugar ConfigMap \u542f\u7528\u3002 \u53c2\u89c1\u4e0b\u9762\u7684\u7b80\u5355\u793a\u4f8b\uff0c\u5e76\u914d\u7f6e\u7cd6\u63a7\u4ee5\u83b7\u5f97\u66f4\u591a\u7ec6\u8282\u3002 \u81ea\u52a8\u521b\u5efa\u4ee3\u7406 \u00b6 \u521b\u5efa\u4ee3\u7406\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u624b\u52a8\u5c06\u8d44\u6e90\u5e94\u7528\u5230\u96c6\u7fa4: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d` ``\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u4f8b\u5982\u5728\u540d\u79f0\u7a7a\u95f4\u521b\u5efa\u65f6\uff0c\u6216\u8005\u5728\u89e6\u53d1\u5668\u521b\u5efa\u65f6\u3002 \u7cd6\u63a7\u4f7f\u8fd9\u4e9b\u7528\u4f8b\u6210\u4e3a\u53ef\u80fd\u3002 \u4e0b\u9762\u662f sugar-config ConfigMap \u7684\u793a\u4f8b\u914d\u7f6e\uff0c\u4e3a\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u6240\u6709\u89e6\u53d1\u5668\u542f\u7528\u7cd6\u63a7\u3002 apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Specify a label selector to selectively apply sugaring to certain namespaces namespace-selector : | matchExpressions: - key: \"my.custom.injection.key\" operator: \"In\" values: [\"enabled\"] # Use an empty object to enable for all triggers trigger-selector : | {} \u5f53\u4f7f\u7528\u6807\u7b7e my.custom.injection.key: enabled \u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u4e00\u4e2a\u4ee3\u7406\u88ab\u5220\u9664\uff0c\u4f46\u5f15\u7528\u7684\u6807\u7b7e\u9009\u62e9\u5668\u6b63\u5728\u4f7f\u7528\u65f6\uff0c\u7cd6\u63a7\u5c06\u81ea\u52a8\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u9ed8\u8ba4\u4ee3\u7406\u3002 \u540d\u79f0\u7a7a\u95f4\u7684\u4f8b\u5b50 \u00b6 \u5728\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : v1 kind : Namespace metadata : name : example labels : my.custom.injection.key : enabled \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u8981\u5728\u547d\u540d\u7a7a\u95f4\u5b58\u5728\u540e\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u8bf7\u5c06\u547d\u540d\u7a7a\u95f4\u6807\u8bb0\u4e3a: kubectl label namespace default my.custom.injection.key = enabled \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\u3002 \u89e6\u53d1\u7684\u4f8b\u5b50 \u00b6 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u5728\u89e6\u53d1\u5668\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF \u8fd9\u5c06\u5728\u547d\u540d\u7a7a\u95f4\u201chello\u201d\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\uff0c\u5e76\u5c1d\u8bd5\u5411\u201cevent-display\u201d\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\u3002 \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\uff0c\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u62e5\u6709\u73b0\u6709\u7684\u4ee3\u7406\u3002","title":"\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/sugar/#knative","text":"Knative \u4e8b\u4ef6\u7cd6\u63a7\u5c06\u5bf9\u914d\u7f6e\u7684\u6807\u7b7e\u505a\u51fa\u53cd\u5e94\uff0c\u4ee5\u751f\u6210\u6216\u63a7\u5236\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u4e8b\u4ef6\u6e90\u3002 \u8fd9\u5141\u8bb8\u96c6\u7fa4\u64cd\u4f5c\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e13\u6ce8\u4e8e\u521b\u5efa\u66f4\u5c11\u7684\u8d44\u6e90\uff0c\u5e76\u6309\u9700\u521b\u5efa\u5e95\u5c42\u4e8b\u4ef6\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u5728\u4e0d\u518d\u9700\u8981\u65f6\u8fdb\u884c\u6e05\u7406\u3002","title":"Knative \u4e8b\u4ef6\u7cd6\u63a7"},{"location":"eventing/sugar/#_1","text":"\u7cd6\u63a7\u9ed8\u8ba4\u662f disabled \u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e config-sugar ConfigMap \u542f\u7528\u3002 \u53c2\u89c1\u4e0b\u9762\u7684\u7b80\u5355\u793a\u4f8b\uff0c\u5e76\u914d\u7f6e\u7cd6\u63a7\u4ee5\u83b7\u5f97\u66f4\u591a\u7ec6\u8282\u3002","title":"\u5b89\u88c5"},{"location":"eventing/sugar/#_2","text":"\u521b\u5efa\u4ee3\u7406\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u624b\u52a8\u5c06\u8d44\u6e90\u5e94\u7528\u5230\u96c6\u7fa4: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d` ``\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u4f8b\u5982\u5728\u540d\u79f0\u7a7a\u95f4\u521b\u5efa\u65f6\uff0c\u6216\u8005\u5728\u89e6\u53d1\u5668\u521b\u5efa\u65f6\u3002 \u7cd6\u63a7\u4f7f\u8fd9\u4e9b\u7528\u4f8b\u6210\u4e3a\u53ef\u80fd\u3002 \u4e0b\u9762\u662f sugar-config ConfigMap \u7684\u793a\u4f8b\u914d\u7f6e\uff0c\u4e3a\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u6240\u6709\u89e6\u53d1\u5668\u542f\u7528\u7cd6\u63a7\u3002 apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Specify a label selector to selectively apply sugaring to certain namespaces namespace-selector : | matchExpressions: - key: \"my.custom.injection.key\" operator: \"In\" values: [\"enabled\"] # Use an empty object to enable for all triggers trigger-selector : | {} \u5f53\u4f7f\u7528\u6807\u7b7e my.custom.injection.key: enabled \u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u4e00\u4e2a\u4ee3\u7406\u88ab\u5220\u9664\uff0c\u4f46\u5f15\u7528\u7684\u6807\u7b7e\u9009\u62e9\u5668\u6b63\u5728\u4f7f\u7528\u65f6\uff0c\u7cd6\u63a7\u5c06\u81ea\u52a8\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u9ed8\u8ba4\u4ee3\u7406\u3002","title":"\u81ea\u52a8\u521b\u5efa\u4ee3\u7406"},{"location":"eventing/sugar/#_3","text":"\u5728\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : v1 kind : Namespace metadata : name : example labels : my.custom.injection.key : enabled \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u8981\u5728\u547d\u540d\u7a7a\u95f4\u5b58\u5728\u540e\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u8bf7\u5c06\u547d\u540d\u7a7a\u95f4\u6807\u8bb0\u4e3a: kubectl label namespace default my.custom.injection.key = enabled \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\u3002","title":"\u540d\u79f0\u7a7a\u95f4\u7684\u4f8b\u5b50"},{"location":"eventing/sugar/#_4","text":"\u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u5728\u89e6\u53d1\u5668\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF \u8fd9\u5c06\u5728\u547d\u540d\u7a7a\u95f4\u201chello\u201d\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\uff0c\u5e76\u5c1d\u8bd5\u5411\u201cevent-display\u201d\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\u3002 \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\uff0c\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u62e5\u6709\u73b0\u6709\u7684\u4ee3\u7406\u3002","title":"\u89e6\u53d1\u7684\u4f8b\u5b50"},{"location":"eventing/triggers/","text":"\u4f7f\u7528\u89e6\u53d1\u5668 \u00b6 \u89e6\u53d1\u5668\u8868\u793a\u4ece\u7279\u5b9a\u4ee3\u7406\u8ba2\u9605\u4e8b\u4ef6\u7684\u613f\u671b\u3002 subscriber \u503c\u5fc5\u987b\u662f type Destination . \u89e6\u53d1\u5668\u4e3e\u4f8b \u00b6 \u4e0b\u9762\u7684\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9 Knative \u670d\u52a1 my-service : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u4ee5\u4e0b\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4ea4\u4ed8\u5230 Kubernetes \u670d\u52a1 my-service \u7684\u81ea\u5b9a\u4e49\u8def\u5f84 /my-custom-path : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u89e6\u53d1\u8fc7\u6ee4 \u00b6 \u652f\u6301\u5bf9\u4efb\u610f\u6570\u91cf\u7684 CloudEvents \u5c5e\u6027\u548c\u6269\u5c55\u8fdb\u884c\u7cbe\u786e\u5339\u914d\u7b5b\u9009\u3002 \u5982\u679c\u8fc7\u6ee4\u5668\u8bbe\u7f6e\u4e86\u591a\u4e2a\u5c5e\u6027\uff0c\u90a3\u4e48\u4e00\u4e2a\u4e8b\u4ef6\u5fc5\u987b\u5177\u6709\u89e6\u53d1\u5668\u7b5b\u9009\u5b83\u6240\u9700\u7684\u6240\u6709\u5c5e\u6027\u3002 \u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u652f\u6301\u5b57\u7b26\u4e32\u503c\u7684\u7cbe\u786e\u5339\u914d\u3002 \u4e3e\u4f8b \u00b6 \u6b64\u793a\u4f8b\u8fc7\u6ee4\u6765\u81ea default \u4ee3\u7406\u7684\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u7c7b\u578b\u4e3a dev.knative.foo.bar \uff0c\u6269\u5c55\u540d\u4e3a myextension \uff0c\u503c\u4e3a my-extension-value \u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u89e6\u53d1\u5668\u6ce8\u91ca \u00b6 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u4ee5\u4e0b\u4e24\u4e2a\u6ce8\u91ca\u6765\u4fee\u6539\u89e6\u53d1\u5668\u7684\u884c\u4e3a: eventing.knative.dev/injection : \u5982\u679c\u8bbe\u7f6e\u4e3a enabled \uff0c\u4e8b\u4ef6\u5c06\u81ea\u52a8\u4e3a\u89e6\u53d1\u5668\u521b\u5efa\u4e00\u4e2a\u4e0d\u5b58\u5728\u7684\u4ee3\u7406\u3002 \u4ee3\u7406\u5728\u521b\u5efa\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u6b64\u6ce8\u91ca\u4ec5\u5728\u542f\u7528 \u7cd6\u63a7\u5236\u5668 \u65f6\u6709\u6548\uff0c\u8fd9\u662f\u53ef\u9009\u7684\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u542f\u7528\u3002 knative.dev/dependency : \u6b64\u6ce8\u91ca\u7528\u4e8e\u6807\u8bb0\u89e6\u53d1\u5668\u6240\u4f9d\u8d56\u7684\u6e90\u3002 \u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u4f9d\u8d56\u9879\u6ca1\u6709\u51c6\u5907\u597d\uff0c\u90a3\u4e48\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u51c6\u5907\u597d\u3002 \u4e0b\u9762\u7684 YAML \u662f\u4e00\u4e2a\u5e26\u6709\u4f9d\u8d56\u7684\u89e6\u53d1\u5668\u7684\u4f8b\u5b50: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"\u4f7f\u7528\u89e6\u53d1\u5668"},{"location":"eventing/triggers/#_1","text":"\u89e6\u53d1\u5668\u8868\u793a\u4ece\u7279\u5b9a\u4ee3\u7406\u8ba2\u9605\u4e8b\u4ef6\u7684\u613f\u671b\u3002 subscriber \u503c\u5fc5\u987b\u662f type Destination .","title":"\u4f7f\u7528\u89e6\u53d1\u5668"},{"location":"eventing/triggers/#_2","text":"\u4e0b\u9762\u7684\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9 Knative \u670d\u52a1 my-service : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u4ee5\u4e0b\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4ea4\u4ed8\u5230 Kubernetes \u670d\u52a1 my-service \u7684\u81ea\u5b9a\u4e49\u8def\u5f84 /my-custom-path : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u89e6\u53d1\u5668\u4e3e\u4f8b"},{"location":"eventing/triggers/#_3","text":"\u652f\u6301\u5bf9\u4efb\u610f\u6570\u91cf\u7684 CloudEvents \u5c5e\u6027\u548c\u6269\u5c55\u8fdb\u884c\u7cbe\u786e\u5339\u914d\u7b5b\u9009\u3002 \u5982\u679c\u8fc7\u6ee4\u5668\u8bbe\u7f6e\u4e86\u591a\u4e2a\u5c5e\u6027\uff0c\u90a3\u4e48\u4e00\u4e2a\u4e8b\u4ef6\u5fc5\u987b\u5177\u6709\u89e6\u53d1\u5668\u7b5b\u9009\u5b83\u6240\u9700\u7684\u6240\u6709\u5c5e\u6027\u3002 \u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u652f\u6301\u5b57\u7b26\u4e32\u503c\u7684\u7cbe\u786e\u5339\u914d\u3002","title":"\u89e6\u53d1\u8fc7\u6ee4"},{"location":"eventing/triggers/#_4","text":"\u6b64\u793a\u4f8b\u8fc7\u6ee4\u6765\u81ea default \u4ee3\u7406\u7684\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u7c7b\u578b\u4e3a dev.knative.foo.bar \uff0c\u6269\u5c55\u540d\u4e3a myextension \uff0c\u503c\u4e3a my-extension-value \u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u4e3e\u4f8b"},{"location":"eventing/triggers/#_5","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u4ee5\u4e0b\u4e24\u4e2a\u6ce8\u91ca\u6765\u4fee\u6539\u89e6\u53d1\u5668\u7684\u884c\u4e3a: eventing.knative.dev/injection : \u5982\u679c\u8bbe\u7f6e\u4e3a enabled \uff0c\u4e8b\u4ef6\u5c06\u81ea\u52a8\u4e3a\u89e6\u53d1\u5668\u521b\u5efa\u4e00\u4e2a\u4e0d\u5b58\u5728\u7684\u4ee3\u7406\u3002 \u4ee3\u7406\u5728\u521b\u5efa\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u6b64\u6ce8\u91ca\u4ec5\u5728\u542f\u7528 \u7cd6\u63a7\u5236\u5668 \u65f6\u6709\u6548\uff0c\u8fd9\u662f\u53ef\u9009\u7684\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u542f\u7528\u3002 knative.dev/dependency : \u6b64\u6ce8\u91ca\u7528\u4e8e\u6807\u8bb0\u89e6\u53d1\u5668\u6240\u4f9d\u8d56\u7684\u6e90\u3002 \u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u4f9d\u8d56\u9879\u6ca1\u6709\u51c6\u5907\u597d\uff0c\u90a3\u4e48\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u51c6\u5907\u597d\u3002 \u4e0b\u9762\u7684 YAML \u662f\u4e00\u4e2a\u5e26\u6709\u4f9d\u8d56\u7684\u89e6\u53d1\u5668\u7684\u4f8b\u5b50: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"\u89e6\u53d1\u5668\u6ce8\u91ca"},{"location":"eventing/troubleshooting/","text":"\u8c03\u8bd5Knative\u4e8b\u4ef6 \u00b6 \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u5982\u4f55\u8c03\u8bd5\u4e00\u4e2a\u4e0d\u80fd\u5de5\u4f5c\u7684Knative\u4e8b\u4ef6\u8bbe\u7f6e\u7684\u4e0d\u65ad\u53d1\u5c55\u7684\u6587\u6863\u3002 \u89c2\u4f17 \u00b6 \u672c\u6587\u6863\u9002\u7528\u4e8e\u719f\u6089 Knative\u4e8b\u4ef6 \u7684\u5bf9\u8c61\u6a21\u578b\u7684\u4eba\u3002 \u4f60\u4e0d\u9700\u8981\u6210\u4e3a\u4e13\u5bb6\uff0c\u4f46\u9700\u8981\u5927\u81f4\u4e86\u89e3\u4e8b\u7269\u662f\u5982\u4f55\u7ec4\u5408\u5728\u4e00\u8d77\u7684\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bbe\u7f6e Knative\u4e8b\u4ef6\u548c\u4e8b\u4ef6\u8d21\u732e\u8d44\u6e90 . \u4f8b\u5b50 \u00b6 \u672c\u6307\u5357\u4f7f\u7528\u4e86\u4e00\u4e2a\u7531\u4e8b\u4ef6\u6e90\u7ec4\u6210\u7684\u793a\u4f8b\uff0c\u8be5\u4e8b\u4ef6\u6e90\u5411\u51fd\u6570\u53d1\u9001\u4e8b\u4ef6\u3002 \u53c2\u89c1 example.yaml \u4e86\u89e3\u6574\u4e2aYAML\u3002 \u8981\u4f7f\u672c\u6307\u5357\u4e2d\u7684\u4efb\u4f55\u547d\u4ee4\u751f\u6548\uff0c\u5fc5\u987b\u5e94\u7528 example.yaml : kubectl apply --filename example.yaml \u89e6\u53d1\u4e8b\u4ef6 \u00b6 Knative\u4e8b\u4ef6\u5c06\u5728 knative-debug \u547d\u540d\u7a7a\u95f4\u4e2d\u53d1\u751fKubernetes Event \u65f6\u53d1\u751f\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b9e\u73b0: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u5230Kubernetes\u7684\u4e8b\u4ef6(\u6ce8\u610f\uff0c\u8fd9\u4e9b\u4e0d\u662fKnative\u4e8b\u4ef6!): kubectl --namespace knative-debug get events \u8fd9\u5c06\u4ea7\u751f\u5982\u4e0b\u7684\u8f93\u51fa: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2 \u6211\u7684\u6d3b\u52a8\u5728\u54ea\u91cc? \u00b6 \u4f60\u5df2\u7ecf\u5e94\u7528\u4e86 example.yaml \uff0c\u5e76\u4e14\u6b63\u5728\u68c0\u67e5 fn \u7684\u65e5\u5fd7: kubectl --namespace knative-debug logs -l app = fn -c user-container \u4f46\u662f\u4f60\u770b\u4e0d\u5230\u4efb\u4f55\u4e8b\u4ef6\u7684\u5230\u6765\u3002\u95ee\u9898\u5728\u54ea\u91cc? \u68c0\u67e5\u5df2\u521b\u5efa\u7684\u8d44\u6e90 \u00b6 \u9996\u5148\u8981\u68c0\u67e5\u7684\u662f\u6240\u6709\u521b\u5efa\u7684\u8d44\u6e90\uff0c\u4ed6\u4eec\u7684\u72b6\u6001\u5305\u542b ready true? \u6211\u4eec\u5c06\u5c1d\u8bd5\u4ece\u6700\u57fa\u672c\u7684\u90e8\u5206\u6765\u786e\u5b9a\u539f\u56e0: fn - Deployment \u5728Knative\u4e2d\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u3002 svc - Service \u5728Knative\u5185\u90e8\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u3002 chan - Channel \u53d6\u51b3\u4e8e\u5b83\u7684\u652f\u6301 channel implementation \u548c\u4e00\u4e9b\u53d6\u51b3\u4e8e sub \u3002 src - Source \u53d6\u51b3\u4e8e chan . sub - Subscription \u53d6\u51b3\u4e8e chan \u548c svc . fn \u00b6 kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' \u6211\u4eec\u60f3\u770b\u5230 1 \u3002 \u5982\u679c\u4f60\u6ca1\u6709\uff0c\u90a3\u4e48\u4f60\u9700\u8981\u8c03\u8bd5 Deployment \u3002 status \u4e2d\u6709\u4ec0\u4e48\u660e\u663e\u7684\u9519\u8bef\u5417? kubectl --namespace knative-debug get deployment fn --output yaml \u5982\u679c\u95ee\u9898\u4e0d\u660e\u663e\uff0c\u90a3\u4e48\u60a8\u9700\u8981\u8c03\u8bd5\u201c\u90e8\u7f72\u201d\uff0c\u8fd9\u8d85\u51fa\u4e86\u672c\u6587\u7684\u8303\u56f4\u3002 \u786e\u8ba4 Pod \u4e3a Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide. svc \u00b6 kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn . chan \u00b6 chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready . Service \u00b6 chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller . src \u00b6 src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' sub \u00b6 sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml \u63a7\u5236\u5668 \u00b6 Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document. \u670d\u52a1\u63a7\u5236\u5668 \u00b6 The Kubernetes Service Controller, controlling svc , is out of scope for this document. \u901a\u9053\u63a7\u5236\u5668 \u00b6 There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error . \u6e90\u63a7\u5236\u5668 \u00b6 Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller . ApiServerSource \u63a7\u5236\u5668 \u00b6 The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error . \u8ba2\u9605\u63a7\u5236\u5668 \u00b6 The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error . \u6570\u636e\u5e73\u9762 \u00b6 The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel. \u901a\u9053\u5206\u914d\u5668 \u00b6 The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"\u8c03\u8bd5"},{"location":"eventing/troubleshooting/#knative","text":"\u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u5982\u4f55\u8c03\u8bd5\u4e00\u4e2a\u4e0d\u80fd\u5de5\u4f5c\u7684Knative\u4e8b\u4ef6\u8bbe\u7f6e\u7684\u4e0d\u65ad\u53d1\u5c55\u7684\u6587\u6863\u3002","title":"\u8c03\u8bd5Knative\u4e8b\u4ef6"},{"location":"eventing/troubleshooting/#_1","text":"\u672c\u6587\u6863\u9002\u7528\u4e8e\u719f\u6089 Knative\u4e8b\u4ef6 \u7684\u5bf9\u8c61\u6a21\u578b\u7684\u4eba\u3002 \u4f60\u4e0d\u9700\u8981\u6210\u4e3a\u4e13\u5bb6\uff0c\u4f46\u9700\u8981\u5927\u81f4\u4e86\u89e3\u4e8b\u7269\u662f\u5982\u4f55\u7ec4\u5408\u5728\u4e00\u8d77\u7684\u3002","title":"\u89c2\u4f17"},{"location":"eventing/troubleshooting/#_2","text":"\u8bbe\u7f6e Knative\u4e8b\u4ef6\u548c\u4e8b\u4ef6\u8d21\u732e\u8d44\u6e90 .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/troubleshooting/#_3","text":"\u672c\u6307\u5357\u4f7f\u7528\u4e86\u4e00\u4e2a\u7531\u4e8b\u4ef6\u6e90\u7ec4\u6210\u7684\u793a\u4f8b\uff0c\u8be5\u4e8b\u4ef6\u6e90\u5411\u51fd\u6570\u53d1\u9001\u4e8b\u4ef6\u3002 \u53c2\u89c1 example.yaml \u4e86\u89e3\u6574\u4e2aYAML\u3002 \u8981\u4f7f\u672c\u6307\u5357\u4e2d\u7684\u4efb\u4f55\u547d\u4ee4\u751f\u6548\uff0c\u5fc5\u987b\u5e94\u7528 example.yaml : kubectl apply --filename example.yaml","title":"\u4f8b\u5b50"},{"location":"eventing/troubleshooting/#_4","text":"Knative\u4e8b\u4ef6\u5c06\u5728 knative-debug \u547d\u540d\u7a7a\u95f4\u4e2d\u53d1\u751fKubernetes Event \u65f6\u53d1\u751f\u3002 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b9e\u73b0: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u770b\u5230Kubernetes\u7684\u4e8b\u4ef6(\u6ce8\u610f\uff0c\u8fd9\u4e9b\u4e0d\u662fKnative\u4e8b\u4ef6!): kubectl --namespace knative-debug get events \u8fd9\u5c06\u4ea7\u751f\u5982\u4e0b\u7684\u8f93\u51fa: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2","title":"\u89e6\u53d1\u4e8b\u4ef6"},{"location":"eventing/troubleshooting/#_5","text":"\u4f60\u5df2\u7ecf\u5e94\u7528\u4e86 example.yaml \uff0c\u5e76\u4e14\u6b63\u5728\u68c0\u67e5 fn \u7684\u65e5\u5fd7: kubectl --namespace knative-debug logs -l app = fn -c user-container \u4f46\u662f\u4f60\u770b\u4e0d\u5230\u4efb\u4f55\u4e8b\u4ef6\u7684\u5230\u6765\u3002\u95ee\u9898\u5728\u54ea\u91cc?","title":"\u6211\u7684\u6d3b\u52a8\u5728\u54ea\u91cc?"},{"location":"eventing/troubleshooting/#_6","text":"\u9996\u5148\u8981\u68c0\u67e5\u7684\u662f\u6240\u6709\u521b\u5efa\u7684\u8d44\u6e90\uff0c\u4ed6\u4eec\u7684\u72b6\u6001\u5305\u542b ready true? \u6211\u4eec\u5c06\u5c1d\u8bd5\u4ece\u6700\u57fa\u672c\u7684\u90e8\u5206\u6765\u786e\u5b9a\u539f\u56e0: fn - Deployment \u5728Knative\u4e2d\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u3002 svc - Service \u5728Knative\u5185\u90e8\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u3002 chan - Channel \u53d6\u51b3\u4e8e\u5b83\u7684\u652f\u6301 channel implementation \u548c\u4e00\u4e9b\u53d6\u51b3\u4e8e sub \u3002 src - Source \u53d6\u51b3\u4e8e chan . sub - Subscription \u53d6\u51b3\u4e8e chan \u548c svc .","title":"\u68c0\u67e5\u5df2\u521b\u5efa\u7684\u8d44\u6e90"},{"location":"eventing/troubleshooting/#fn","text":"kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' \u6211\u4eec\u60f3\u770b\u5230 1 \u3002 \u5982\u679c\u4f60\u6ca1\u6709\uff0c\u90a3\u4e48\u4f60\u9700\u8981\u8c03\u8bd5 Deployment \u3002 status \u4e2d\u6709\u4ec0\u4e48\u660e\u663e\u7684\u9519\u8bef\u5417? kubectl --namespace knative-debug get deployment fn --output yaml \u5982\u679c\u95ee\u9898\u4e0d\u660e\u663e\uff0c\u90a3\u4e48\u60a8\u9700\u8981\u8c03\u8bd5\u201c\u90e8\u7f72\u201d\uff0c\u8fd9\u8d85\u51fa\u4e86\u672c\u6587\u7684\u8303\u56f4\u3002 \u786e\u8ba4 Pod \u4e3a Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide.","title":"fn"},{"location":"eventing/troubleshooting/#svc","text":"kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn .","title":"svc"},{"location":"eventing/troubleshooting/#chan","text":"chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready .","title":"chan"},{"location":"eventing/troubleshooting/#service","text":"chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller .","title":"Service"},{"location":"eventing/troubleshooting/#src","text":"src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}'","title":"src"},{"location":"eventing/troubleshooting/#sub","text":"sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml","title":"sub"},{"location":"eventing/troubleshooting/#_7","text":"Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document.","title":"\u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#_8","text":"The Kubernetes Service Controller, controlling svc , is out of scope for this document.","title":"\u670d\u52a1\u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#_9","text":"There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error .","title":"\u901a\u9053\u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#_10","text":"Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller .","title":"\u6e90\u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#apiserversource","text":"The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"ApiServerSource \u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#_11","text":"The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"\u8ba2\u9605\u63a7\u5236\u5668"},{"location":"eventing/troubleshooting/#_12","text":"The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel.","title":"\u6570\u636e\u5e73\u9762"},{"location":"eventing/troubleshooting/#_13","text":"The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"\u901a\u9053\u5206\u914d\u5668"},{"location":"functions/","text":"Knative \u51fd\u6570\u6982\u8ff0 \u00b6 Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002 \u51fd\u6570\u6a21\u677f \u00b6 Knative\u51fd\u6570\u901a\u8fc7\u5728\u8fd0\u884c create \u547d\u4ee4\u65f6\u542f\u52a8\u51fd\u6570\u9879\u76ee\u6837\u677f\uff0c\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u521b\u5efa\u57fa\u672c\u51fd\u6570\u7684\u6a21\u677f\u3002 \u6a21\u677f\u5141\u8bb8\u60a8\u4e3a\u51fd\u6570\u9009\u62e9\u8bed\u8a00\u548c\u8c03\u7528\u683c\u5f0f\u3002\u4ee5\u4e0b\u6a21\u677f\u9002\u7528\u4e8eCloudEvent\u548cHTTP\u8c03\u7528\u683c\u5f0f: Node.js Python Go Quarkus Rust TypeScript \u8bed\u8a00\u5305 \u00b6 \u51fd\u6570\u53ef\u4ee5\u7528\u53ef\u7528 \u8bed\u8a00\u5305 \u652f\u6301\u7684\u4efb\u4f55\u8bed\u8a00\u7f16\u5199. \u51fd\u6570\u5165\u95e8 \u00b6 \u5728\u4f7f\u7528Knative\u51fd\u6570\u4e4b\u524d\uff0c\u5fc5\u987b\u80fd\u591f\u8bbf\u95eeKnative\u5f00\u53d1\u73af\u5883\u3002 \u8981\u5efa\u7acb\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\uff0c\u60a8\u53ef\u4ee5\u8ddf\u968f Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b \u3002","title":"\u51fd\u6570\u6982\u8ff0"},{"location":"functions/#knative","text":"Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"Knative \u51fd\u6570\u6982\u8ff0"},{"location":"functions/#_1","text":"Knative\u51fd\u6570\u901a\u8fc7\u5728\u8fd0\u884c create \u547d\u4ee4\u65f6\u542f\u52a8\u51fd\u6570\u9879\u76ee\u6837\u677f\uff0c\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u521b\u5efa\u57fa\u672c\u51fd\u6570\u7684\u6a21\u677f\u3002 \u6a21\u677f\u5141\u8bb8\u60a8\u4e3a\u51fd\u6570\u9009\u62e9\u8bed\u8a00\u548c\u8c03\u7528\u683c\u5f0f\u3002\u4ee5\u4e0b\u6a21\u677f\u9002\u7528\u4e8eCloudEvent\u548cHTTP\u8c03\u7528\u683c\u5f0f: Node.js Python Go Quarkus Rust TypeScript","title":"\u51fd\u6570\u6a21\u677f"},{"location":"functions/#_2","text":"\u51fd\u6570\u53ef\u4ee5\u7528\u53ef\u7528 \u8bed\u8a00\u5305 \u652f\u6301\u7684\u4efb\u4f55\u8bed\u8a00\u7f16\u5199.","title":"\u8bed\u8a00\u5305"},{"location":"functions/#_3","text":"\u5728\u4f7f\u7528Knative\u51fd\u6570\u4e4b\u524d\uff0c\u5fc5\u987b\u80fd\u591f\u8bbf\u95eeKnative\u5f00\u53d1\u73af\u5883\u3002 \u8981\u5efa\u7acb\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\uff0c\u60a8\u53ef\u4ee5\u8ddf\u968f Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b \u3002","title":"\u51fd\u6570\u5165\u95e8"},{"location":"functions/building-functions/","text":"\u6784\u5efa\u51fd\u6570 \u00b6 \u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u672c\u5730\u6784\u5efa \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 build \u547d\u4ee4\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u800c\u65e0\u9700\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build \u96c6\u7fa4\u6784\u5efa \u00b6 \u5982\u679c\u60a8\u6ca1\u6709\u8fd0\u884c\u672c\u5730Docker\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6216\u8005\u60a8\u6b63\u5728\u4f7f\u7528CI/CD\u7ba1\u9053\uff0c\u90a3\u4e48\u60a8\u53ef\u80fd\u5e0c\u671b\u5728\u96c6\u7fa4\u4e0a\u6784\u5efa\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u672c\u5730\u6784\u5efa\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func deploy --remote \u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u96c6\u7fa4\u4e0a\u6784\u5efa\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8be5\u51fd\u6570\u5fc5\u987b\u5b58\u5728\u4e8eGit\u5b58\u50a8\u5e93\u4e2d\u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684\u96c6\u7fa4\u4ee5\u4f7f\u7528Tekton pipeline\u3002\u8bf7\u53c2\u9605 \u96c6\u7fa4\u6784\u5efa \u6587\u6863\u3002 \u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\u65f6\uff0c\u5fc5\u987b\u6307\u5b9a\u8be5\u51fd\u6570\u7684Git URL: func kn func func deploy --remote --registry <registry> --git-url <git-url> -p hello kn func deploy --remote --registry <registry> --git-url <git-url> -p hello \u5728\u4e3a\u51fd\u6570\u6307\u5b9aGit URL\u4e00\u6b21\u4e4b\u540e\uff0c\u53ef\u4ee5\u5728\u540e\u7eed\u547d\u4ee4\u4e2d\u7701\u7565\u5b83\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"functions/building-functions/#_1","text":"\u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"functions/building-functions/#_2","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 build \u547d\u4ee4\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u800c\u65e0\u9700\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002","title":"\u672c\u5730\u6784\u5efa"},{"location":"functions/building-functions/#_3","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/building-functions/#_4","text":"build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u8fc7\u7a0b"},{"location":"functions/building-functions/#_5","text":"\u5982\u679c\u60a8\u6ca1\u6709\u8fd0\u884c\u672c\u5730Docker\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6216\u8005\u60a8\u6b63\u5728\u4f7f\u7528CI/CD\u7ba1\u9053\uff0c\u90a3\u4e48\u60a8\u53ef\u80fd\u5e0c\u671b\u5728\u96c6\u7fa4\u4e0a\u6784\u5efa\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u672c\u5730\u6784\u5efa\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func deploy --remote \u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u96c6\u7fa4\u4e0a\u6784\u5efa\u3002","title":"\u96c6\u7fa4\u6784\u5efa"},{"location":"functions/building-functions/#_6","text":"\u8be5\u51fd\u6570\u5fc5\u987b\u5b58\u5728\u4e8eGit\u5b58\u50a8\u5e93\u4e2d\u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684\u96c6\u7fa4\u4ee5\u4f7f\u7528Tekton pipeline\u3002\u8bf7\u53c2\u9605 \u96c6\u7fa4\u6784\u5efa \u6587\u6863\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/building-functions/#_7","text":"\u7b2c\u4e00\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\u65f6\uff0c\u5fc5\u987b\u6307\u5b9a\u8be5\u51fd\u6570\u7684Git URL: func kn func func deploy --remote --registry <registry> --git-url <git-url> -p hello kn func deploy --remote --registry <registry> --git-url <git-url> -p hello \u5728\u4e3a\u51fd\u6570\u6307\u5b9aGit URL\u4e00\u6b21\u4e4b\u540e\uff0c\u53ef\u4ee5\u5728\u540e\u7eed\u547d\u4ee4\u4e2d\u7701\u7565\u5b83\u3002","title":"\u8fc7\u7a0b"},{"location":"functions/creating-functions/","text":"\u521b\u5efa\u51fd\u6570 \u00b6 \u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"functions/creating-functions/#_1","text":"\u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"functions/deploying-functions/","text":"\u90e8\u7f72\u51fd\u6570 \u00b6 \u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002 \u8fc7\u7a0b \u00b6 deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"functions/deploying-functions/#_1","text":"\u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"functions/deploying-functions/#_2","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/deploying-functions/#_3","text":"deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"functions/install-func/","text":"\u5b89\u88c5 Knative \u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002 \u5b89\u88c5 func \u547d\u4ee4\u884c \u00b6 Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func . \u5b89\u88c5 kn func \u547d\u4ee4\u884c\u63d2\u4ef6 \u00b6 kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5\u51fd\u6570"},{"location":"functions/install-func/#knative","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002","title":"\u5b89\u88c5 Knative \u51fd\u6570"},{"location":"functions/install-func/#func","text":"Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func .","title":"\u5b89\u88c5 func \u547d\u4ee4\u884c"},{"location":"functions/install-func/#kn-func","text":"kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5  kn func \u547d\u4ee4\u884c\u63d2\u4ef6"},{"location":"functions/invoking-functions/","text":"\u8c03\u7528\u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u6765\u8c03\u7528\u672c\u5730\u6216Knative\u96c6\u7fa4\u4e0a\u7684\u51fd\u6570\u3002 \u8fd9\u4e2a\u547d\u4ee4\u53ef\u4ee5\u7528\u6765\u6d4b\u8bd5\u4e00\u4e2a\u51fd\u6570\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff0c\u662f\u5426\u80fd\u591f\u6b63\u786e\u5730\u63a5\u6536HTTP\u8bf7\u6c42\u548cCloudEvents\u3002 \u5982\u679c\u4f60\u7684\u51fd\u6570\u5728\u672c\u5730\u8fd0\u884c\uff0c func invoke \u4f1a\u5411\u672c\u5730\u5b9e\u4f8b\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u9001\u5230\u5e26\u6709 --data \u6807\u5fd7\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5176\u4ed6\u9009\u9879\u6765\u6a21\u62df\u4e0d\u540c\u7c7b\u578b\u7684\u8bf7\u6c42\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 \u51fd\u6570\u8c03\u7528 \u6587\u6863\u3002","title":"\u8c03\u7528\u51fd\u6570"},{"location":"functions/invoking-functions/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u6765\u8c03\u7528\u672c\u5730\u6216Knative\u96c6\u7fa4\u4e0a\u7684\u51fd\u6570\u3002 \u8fd9\u4e2a\u547d\u4ee4\u53ef\u4ee5\u7528\u6765\u6d4b\u8bd5\u4e00\u4e2a\u51fd\u6570\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff0c\u662f\u5426\u80fd\u591f\u6b63\u786e\u5730\u63a5\u6536HTTP\u8bf7\u6c42\u548cCloudEvents\u3002 \u5982\u679c\u4f60\u7684\u51fd\u6570\u5728\u672c\u5730\u8fd0\u884c\uff0c func invoke \u4f1a\u5411\u672c\u5730\u5b9e\u4f8b\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u9001\u5230\u5e26\u6709 --data \u6807\u5fd7\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5176\u4ed6\u9009\u9879\u6765\u6a21\u62df\u4e0d\u540c\u7c7b\u578b\u7684\u8bf7\u6c42\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 \u51fd\u6570\u8c03\u7528 \u6587\u6863\u3002","title":"\u8c03\u7528\u51fd\u6570"},{"location":"functions/language-packs/","text":"\u8bed\u8a00\u5305 \u00b6 \u8bed\u8a00\u5305\u53ef\u7528\u4e8e\u6269\u5c55Knative\u51fd\u6570\uff0c\u4ee5\u652f\u6301\u989d\u5916\u7684\u8fd0\u884c\u65f6\u3001\u51fd\u6570\u7b7e\u540d\u3001\u64cd\u4f5c\u7cfb\u7edf\u548c\u5df2\u5b89\u88c5\u7684\u51fd\u6570\u5de5\u5177\u3002 \u8bed\u8a00\u5305\u901a\u8fc7Git\u5b58\u50a8\u5e93\u6216\u4f5c\u4e3a\u78c1\u76d8\u4e0a\u7684\u76ee\u5f55\u5206\u53d1\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u8bed\u8a00\u5305 \u6587\u6863\u3002 \u4f7f\u7528\u5916\u90e8Git\u5b58\u50a8\u5e93 \u00b6 \u5728\u521b\u5efa\u65b0\u51fd\u6570\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9aGit\u5b58\u50a8\u5e93\u4f5c\u4e3a\u6a21\u677f\u6587\u4ef6\u7684\u6e90\u3002 Knative\u6c99\u76d2\u7ef4\u62a4\u4e86\u4e00\u7ec4 \u793a\u4f8b\u6a21\u677f \uff0c\u53ef\u4ee5\u5728\u9879\u76ee\u521b\u5efa\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e3aNode.js\u4f7f\u7528 metacontroller \u6a21\u677f: func create myfunc -l nodejs -t metacontroller --repository https://github.com/knative-sandbox/func-tastic \u5728\u672c\u5730\u5b89\u88c5\u8bed\u8a00\u5305 \u00b6 \u8bed\u8a00\u5305\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func repository \u547d\u4ee4\u5728\u672c\u5730\u5b89\u88c5\u3002 \u4f8b\u5982\uff0c\u8981\u6dfb\u52a0Knative Sandbox\u793a\u4f8b\u6a21\u677f\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func repository add knative https://github.com/knative-sandbox/func-tastic \u5b89\u88c5Knative\u6c99\u76d2\u793a\u4f8b\u6a21\u677f\u540e\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 create \u547d\u4ee4\u4e2d\u6307\u5b9a Knative \u524d\u7f00\u6765\u4f7f\u7528 metacontroller \u6a21\u677f: func create -t knative/metacontroller -l nodejs my-controller-function","title":"\u8bed\u8a00\u5305"},{"location":"functions/language-packs/#_1","text":"\u8bed\u8a00\u5305\u53ef\u7528\u4e8e\u6269\u5c55Knative\u51fd\u6570\uff0c\u4ee5\u652f\u6301\u989d\u5916\u7684\u8fd0\u884c\u65f6\u3001\u51fd\u6570\u7b7e\u540d\u3001\u64cd\u4f5c\u7cfb\u7edf\u548c\u5df2\u5b89\u88c5\u7684\u51fd\u6570\u5de5\u5177\u3002 \u8bed\u8a00\u5305\u901a\u8fc7Git\u5b58\u50a8\u5e93\u6216\u4f5c\u4e3a\u78c1\u76d8\u4e0a\u7684\u76ee\u5f55\u5206\u53d1\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u8bed\u8a00\u5305 \u6587\u6863\u3002","title":"\u8bed\u8a00\u5305"},{"location":"functions/language-packs/#git","text":"\u5728\u521b\u5efa\u65b0\u51fd\u6570\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9aGit\u5b58\u50a8\u5e93\u4f5c\u4e3a\u6a21\u677f\u6587\u4ef6\u7684\u6e90\u3002 Knative\u6c99\u76d2\u7ef4\u62a4\u4e86\u4e00\u7ec4 \u793a\u4f8b\u6a21\u677f \uff0c\u53ef\u4ee5\u5728\u9879\u76ee\u521b\u5efa\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e3aNode.js\u4f7f\u7528 metacontroller \u6a21\u677f: func create myfunc -l nodejs -t metacontroller --repository https://github.com/knative-sandbox/func-tastic","title":"\u4f7f\u7528\u5916\u90e8Git\u5b58\u50a8\u5e93"},{"location":"functions/language-packs/#_2","text":"\u8bed\u8a00\u5305\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func repository \u547d\u4ee4\u5728\u672c\u5730\u5b89\u88c5\u3002 \u4f8b\u5982\uff0c\u8981\u6dfb\u52a0Knative Sandbox\u793a\u4f8b\u6a21\u677f\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func repository add knative https://github.com/knative-sandbox/func-tastic \u5b89\u88c5Knative\u6c99\u76d2\u793a\u4f8b\u6a21\u677f\u540e\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 create \u547d\u4ee4\u4e2d\u6307\u5b9a Knative \u524d\u7f00\u6765\u4f7f\u7528 metacontroller \u6a21\u677f: func create -t knative/metacontroller -l nodejs my-controller-function","title":"\u5728\u672c\u5730\u5b89\u88c5\u8bed\u8a00\u5305"},{"location":"functions/running-functions/","text":"\u8fd0\u884c\u51fd\u6570 \u00b6 \u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 \u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"functions/running-functions/#_1","text":"\u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"functions/running-functions/#_2","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/running-functions/#_3","text":"\u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/","text":"\u6b22\u8fce\u6765\u5230 Knative \u5feb\u901f\u5165\u95e8\u6559\u7a0b \u00b6 \u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative Quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u7b80\u5355\u7684 Knative \u90e8\u7f72\u6765\u5c1d\u8bd5 Knative \u670d\u52a1 \u548c Knative \u4e8b\u4ef6 \u7684\u5e38\u7528\u7279\u6027\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002 \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002 Tip \u5728\u952e\u76d8\u4e0a\u70b9\u51fb . (\u53e5\u53f7) \u53ef\u4ee5\u5728\u6559\u7a0b\u4e2d\u524d\u8fdb. \u7528 , (\u9017\u53f7) \u968f\u65f6\u53ef\u4ee5\u56de\u53bb.","title":"\u6559\u7a0b\u7b80\u4ecb"},{"location":"getting-started/#knative","text":"\u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative Quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u7b80\u5355\u7684 Knative \u90e8\u7f72\u6765\u5c1d\u8bd5 Knative \u670d\u52a1 \u548c Knative \u4e8b\u4ef6 \u7684\u5e38\u7528\u7279\u6027\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002","title":"\u6b22\u8fce\u6765\u5230 Knative \u5feb\u901f\u5165\u95e8\u6559\u7a0b"},{"location":"getting-started/#_1","text":"Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002 Tip \u5728\u952e\u76d8\u4e0a\u70b9\u51fb . (\u53e5\u53f7) \u53ef\u4ee5\u5728\u6559\u7a0b\u4e2d\u524d\u8fdb. \u7528 , (\u9017\u53f7) \u968f\u65f6\u53ef\u4ee5\u56de\u53bb.","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"getting-started/about-knative-functions/","text":"\u5173\u4e8e Knative \u51fd\u6570 \u00b6 Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"\u5173\u4e8e\u51fd\u6570"},{"location":"getting-started/about-knative-functions/#knative","text":"Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"\u5173\u4e8e Knative \u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/","text":"\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570 \u00b6 \u5728\u521b\u5efa\u4e86\u51fd\u6570\u9879\u76ee\u4e4b\u540e\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u4f8b\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\u3002 \u8fd0\u884c\u51fd\u6570 \u00b6 \u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 \u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body: \u90e8\u7f72\u51fd\u6570 \u00b6 \u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002 \u8fc7\u7a0b \u00b6 deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body: \u6784\u5efa\u51fd\u6570 \u00b6 \u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u6784\u5efa\u8fd0\u884c\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_1","text":"\u5728\u521b\u5efa\u4e86\u51fd\u6570\u9879\u76ee\u4e4b\u540e\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u4f8b\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\u3002","title":"\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_2","text":"\u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_3","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_4","text":"\u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/build-run-deploy-func/#_5","text":"\u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_6","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_7","text":"deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/build-run-deploy-func/#_8","text":"\u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_9","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_10","text":"build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u8fc7\u7a0b"},{"location":"getting-started/clean-up/","text":"\u6e05\u7406 \u00b6 \u6211\u4eec\u5efa\u8bae\u60a8\u5220\u9664\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u96c6\u7fa4\uff0c\u4ee5\u91ca\u653e\u672c\u5730\u673a\u5668\u4e0a\u7684\u8d44\u6e90\u3002 \u5982\u679c\u60a8\u60f3\u5728\u5220\u9664\u96c6\u7fa4\u540e\u7ee7\u7eed\u4f7f\u7528Knative\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 quickstart plugin \u5728\u65b0\u7684\u96c6\u7fa4\u4e0a\u91cd\u65b0\u5b89\u88c5Knative\u3002 \u5220\u9664\u96c6\u7fa4 \u00b6 kind minikube \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u4f60\u7684 kind \u96c6\u7fa4: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u4f60\u7684 minikube \u96c6\u7fa4: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"\u6e05\u7406"},{"location":"getting-started/clean-up/#_1","text":"\u6211\u4eec\u5efa\u8bae\u60a8\u5220\u9664\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u96c6\u7fa4\uff0c\u4ee5\u91ca\u653e\u672c\u5730\u673a\u5668\u4e0a\u7684\u8d44\u6e90\u3002 \u5982\u679c\u60a8\u60f3\u5728\u5220\u9664\u96c6\u7fa4\u540e\u7ee7\u7eed\u4f7f\u7528Knative\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 quickstart plugin \u5728\u65b0\u7684\u96c6\u7fa4\u4e0a\u91cd\u65b0\u5b89\u88c5Knative\u3002","title":"\u6e05\u7406"},{"location":"getting-started/clean-up/#_2","text":"kind minikube \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u4f60\u7684 kind \u96c6\u7fa4: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u4f60\u7684 minikube \u96c6\u7fa4: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"\u5220\u9664\u96c6\u7fa4"},{"location":"getting-started/create-a-function/","text":"\u521b\u5efa\u51fd\u6570 \u00b6 \u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"getting-started/create-a-function/#_1","text":"\u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"getting-started/first-autoscale/","text":"\u81ea\u52a8\u5b9a\u91cf \u00b6 Knative\u670d\u52a1\u63d0\u4f9b\u81ea\u52a8\u7f29\u653e\uff0c\u4e5f\u79f0\u4e3a \u81ea\u52a8\u7f29\u653e \u3002 \u8fd9\u610f\u5473\u7740\uff0cKnative\u670d\u52a1\u5728\u4e0d\u4f7f\u7528\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u51cf\u5c11\u5230\u96f6\u4e2a\u8fd0\u884c\u7684pod\u3002 \u5217\u51fa\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u67e5\u770bKnative\u670d\u52a1\u6240\u5728\u7684URL: kn kubectl \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kn service list Expected output NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kubectl get ksvc Expected output NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True \u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u524d\u9762\u7684URL\u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello World! \u4f60\u770b\u5230 curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io ? \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u60a8\u7684DNS\u670d\u52a1\u5668\u53ef\u80fd\u8bbe\u7f6e\u4e3a\u4e0d\u89e3\u6790 *.sslip.io \u5730\u5740\u3002 \u5982\u679c\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u79f0\u670d\u52a1\u5668\u6765\u89e3\u51b3\u8fd9\u4e9b\u5730\u5740\u3002 \u5177\u4f53\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u53d1\u884c\u7248\u6709\u6240\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu\u6d3e\u751f\u7cfb\u7edf\uff0c\u4f60\u53ef\u4ee5\u5728 /etc/systemd/resolved.conf \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u6761\u76ee: [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. \u7136\u540e\u7b80\u5355\u5730\u7528 sudo service systemd-resolved restart \u91cd\u65b0\u542f\u52a8\u670d\u52a1\u3002 \u5bf9\u4e8eMacOS\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528 \u8fd9\u91cc \u6240\u8ff0\u7684\u7f51\u7edc\u8bbe\u7f6e\u6dfb\u52a0DNS\u548c\u57df\u3002 \u89c2\u5bdf\u81ea\u52a8\u4f38\u7f29 \u00b6 \u89c2\u5bdf\u8fd9\u4e9bPod\uff0c\u770b\u770b\u5728\u6d41\u91cf\u505c\u6b62\u8bbf\u95eeURL\u540e\uff0c\u5b83\u4eec\u662f\u5982\u4f55\u7f29\u5c0f\u5230\u96f6\u7684: kubectl get pod -l serving.knative.dev/service = hello -w Note \u53ef\u80fd\u9700\u89812\u5206\u949f\u624d\u80fd\u8ba9\u4f60\u7684\u8231\u7f29\u5c0f\u3002\u518d\u6b21ping\u60a8\u7684\u670d\u52a1\u5c06\u91cd\u7f6e\u6b64\u8ba1\u65f6\u5668\u3002 Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating \u6269\u5927\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u5728\u6d4f\u89c8\u5668\u4e2d\u91cd\u65b0\u8fd0\u884cKnative\u670d\u52a1\u3002\u4f60\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u65b0\u7684Pod\u518d\u6b21\u8fd0\u884c: Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running \u7528 Ctrl+c \u9000\u51fa kubectl watch \u547d\u4ee4\u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"getting-started/first-autoscale/#_1","text":"Knative\u670d\u52a1\u63d0\u4f9b\u81ea\u52a8\u7f29\u653e\uff0c\u4e5f\u79f0\u4e3a \u81ea\u52a8\u7f29\u653e \u3002 \u8fd9\u610f\u5473\u7740\uff0cKnative\u670d\u52a1\u5728\u4e0d\u4f7f\u7528\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u51cf\u5c11\u5230\u96f6\u4e2a\u8fd0\u884c\u7684pod\u3002","title":"\u81ea\u52a8\u5b9a\u91cf"},{"location":"getting-started/first-autoscale/#knative","text":"\u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u67e5\u770bKnative\u670d\u52a1\u6240\u5728\u7684URL: kn kubectl \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kn service list Expected output NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kubectl get ksvc Expected output NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True","title":"\u5217\u51fa\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-autoscale/#knative_1","text":"\u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u524d\u9762\u7684URL\u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello World! \u4f60\u770b\u5230 curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io ? \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u60a8\u7684DNS\u670d\u52a1\u5668\u53ef\u80fd\u8bbe\u7f6e\u4e3a\u4e0d\u89e3\u6790 *.sslip.io \u5730\u5740\u3002 \u5982\u679c\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u79f0\u670d\u52a1\u5668\u6765\u89e3\u51b3\u8fd9\u4e9b\u5730\u5740\u3002 \u5177\u4f53\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u53d1\u884c\u7248\u6709\u6240\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu\u6d3e\u751f\u7cfb\u7edf\uff0c\u4f60\u53ef\u4ee5\u5728 /etc/systemd/resolved.conf \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u6761\u76ee: [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. \u7136\u540e\u7b80\u5355\u5730\u7528 sudo service systemd-resolved restart \u91cd\u65b0\u542f\u52a8\u670d\u52a1\u3002 \u5bf9\u4e8eMacOS\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528 \u8fd9\u91cc \u6240\u8ff0\u7684\u7f51\u7edc\u8bbe\u7f6e\u6dfb\u52a0DNS\u548c\u57df\u3002","title":"\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-autoscale/#_2","text":"\u89c2\u5bdf\u8fd9\u4e9bPod\uff0c\u770b\u770b\u5728\u6d41\u91cf\u505c\u6b62\u8bbf\u95eeURL\u540e\uff0c\u5b83\u4eec\u662f\u5982\u4f55\u7f29\u5c0f\u5230\u96f6\u7684: kubectl get pod -l serving.knative.dev/service = hello -w Note \u53ef\u80fd\u9700\u89812\u5206\u949f\u624d\u80fd\u8ba9\u4f60\u7684\u8231\u7f29\u5c0f\u3002\u518d\u6b21ping\u60a8\u7684\u670d\u52a1\u5c06\u91cd\u7f6e\u6b64\u8ba1\u65f6\u5668\u3002 Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating","title":"\u89c2\u5bdf\u81ea\u52a8\u4f38\u7f29"},{"location":"getting-started/first-autoscale/#knative_2","text":"\u5728\u6d4f\u89c8\u5668\u4e2d\u91cd\u65b0\u8fd0\u884cKnative\u670d\u52a1\u3002\u4f60\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u65b0\u7684Pod\u518d\u6b21\u8fd0\u884c: Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running \u7528 Ctrl+c \u9000\u51fa kubectl watch \u547d\u4ee4\u3002","title":"\u6269\u5927\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-broker/","text":"\u7b2c\u4e00\u4e2a\u4ee3\u7406 \u00b6 \u4f5c\u4e3a kn quickstart \u5b89\u88c5\u7684\u4e00\u90e8\u5206\uff0c\u4e00\u4e2a InMemoryChannel-backed \u4ee3\u7406\u88ab\u5b89\u88c5\u5728\u4f60\u7684\u96c6\u7fa4\u4e0a\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5df2\u5b89\u88c5: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning InMemoryChannel-backed \u4ee3\u7406\u4ec5\u7528\u4e8e\u5f00\u53d1\uff0c\u4e0d\u80fd\u7528\u4e8e\u751f\u4ea7\u90e8\u7f72\u3002 \u63a5\u4e0b\u6765\uff0c\u60a8\u5c06\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3a\u4e91\u4e8b\u4ef6\u64ad\u653e\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u4e86\u89e3\u6e90\u3001\u4ee3\u7406\u3001\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668\u7684 \u7b80\u5355\u5b9e\u73b0 \u3002","title":"\u7b2c\u4e00\u4e2a\u4ee3\u7406"},{"location":"getting-started/first-broker/#_1","text":"\u4f5c\u4e3a kn quickstart \u5b89\u88c5\u7684\u4e00\u90e8\u5206\uff0c\u4e00\u4e2a InMemoryChannel-backed \u4ee3\u7406\u88ab\u5b89\u88c5\u5728\u4f60\u7684\u96c6\u7fa4\u4e0a\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5df2\u5b89\u88c5: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning InMemoryChannel-backed \u4ee3\u7406\u4ec5\u7528\u4e8e\u5f00\u53d1\uff0c\u4e0d\u80fd\u7528\u4e8e\u751f\u4ea7\u90e8\u7f72\u3002 \u63a5\u4e0b\u6765\uff0c\u60a8\u5c06\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3a\u4e91\u4e8b\u4ef6\u64ad\u653e\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u4e86\u89e3\u6e90\u3001\u4ee3\u7406\u3001\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668\u7684 \u7b80\u5355\u5b9e\u73b0 \u3002","title":"\u7b2c\u4e00\u4e2a\u4ee3\u7406"},{"location":"getting-started/first-service/","text":"\u90e8\u7f72 Knative \u670d\u52a1 \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a Hello world Knative\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u63a5\u53d7\u73af\u5883\u53d8\u91cf TARGET \u5e76\u6253\u5370 Hello ${TARGET}! kn YAML \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72\u670d\u52a1: kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World Expected output Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io The value of ${LOADBALANCER_IP} above depends on your type of cluster, for kind it will be 127.0.0.1 for minikube depends on the local tunnel. \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72Knative Service: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello created","title":"\u90e8\u7f72\u670d\u52a1"},{"location":"getting-started/first-service/#knative","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a Hello world Knative\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u63a5\u53d7\u73af\u5883\u53d8\u91cf TARGET \u5e76\u6253\u5370 Hello ${TARGET}! kn YAML \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72\u670d\u52a1: kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World Expected output Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io The value of ${LOADBALANCER_IP} above depends on your type of cluster, for kind it will be 127.0.0.1 for minikube depends on the local tunnel. \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72Knative Service: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello created","title":"\u90e8\u7f72 Knative \u670d\u52a1"},{"location":"getting-started/first-source/","text":"\u7b2c\u4e00\u4e2a\u6e90 \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4f7f\u7528 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u6765\u5c55\u793a Knative \u4e8b\u4ef6\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u5728\u672c\u6559\u7a0b\u7ed3\u675f\u65f6\uff0c\u60a8\u5e94\u8be5\u62e5\u6709\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u4f53\u7cfb\u7ed3\u6784: \u4e0a\u9762\u7684\u56fe\u50cf\u662f Knative in Action \u7684\u56fe 6.6 \u521b\u5efa\u7b2c\u4e00\u4e2a\u6e90 \u00b6 \u901a\u8fc7\u5c06\u4ee3\u7406\u7684 URL( BROKER_URL )\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0cCloudEvents Player \u5145\u5f53\u4e86 CloudEvents \u7684\u6e90\u3002 \u60a8\u5c06\u901a\u8fc7 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u5411\u4ee3\u7406\u53d1\u9001 CloudEvents\u3002 \u521b\u5efa CloudEvents Player \u670d\u52a1: kn YAML \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u9884\u671f\u7684\u8f93\u51fa Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \u4e3a\u4ec0\u4e48\u6211\u7684\u4fee\u8ba2\u7248\u7684\u540d\u5b57\u4e0d\u4e00\u6837! \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5206\u914d \u7248\u672c\u540d \uff0c\u6240\u4ee5\u670d\u52a1\u4f1a\u81ea\u52a8\u4e3a\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u3002\u5982\u679c\u4f60\u7684\u4fee\u8ba2\u7248\u547d\u540d\u4e0d\u540c\u4e5f\u6ca1\u5173\u7cfb\u3002 \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a cloudevents-player.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created \u6d4b\u8bd5 CloudEvents Player \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536 CloudEvents\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1 URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 \u670d\u52a1URL\u4e3a http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , \u4f8b\u5982, http://cloudevents-player.default.127.0.0.1.sslip.io \u90a3\u79cd . \u8fd9\u4e9b\u5b57\u6bb5\u662f\u4ec0\u4e48\u610f\u601d? Field Description Event ID \u4e00\u4e2a\u60df\u4e00\u7684 ID\u3002\u5355\u51fb\u5faa\u73af\u56fe\u6807\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u5faa\u73af\u3002 Event Type \u4e00\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u3002 Event Source \u4e00\u4e2a\u4e8b\u4ef6\u6e90\u3002 Specversion \u754c\u5b9a\u60a8\u6b63\u5728\u4f7f\u7528\u7684 CloudEvents \u89c4\u8303(\u5e94\u8be5\u603b\u662f 1.0)\u3002 Message CloudEvent \u7684 data \u90e8\u5206\uff0c\u4e00\u4e2a\u627f\u8f7d\u60a8\u60f3\u8981\u4f20\u9012\u7684\u6570\u636e\u7684\u6709\u6548\u8f7d\u8377\u3002 \u6709\u5173 CloudEvents \u89c4\u8303\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CloudEvents \u89c4\u8303 . \u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6 \u00b6 \u5c1d\u8bd5\u4f7f\u7528 CloudEvents Player \u754c\u9762\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: \u5728\u8868\u683c\u4e2d\u586b\u4e0a\u4f60\u60f3\u8981\u7684\u4efb\u4f55\u6570\u636e\u3002 \u786e\u4fdd\u4e8b\u4ef6\u6e90\u4e0d\u5305\u542b\u4efb\u4f55\u7a7a\u683c\u3002 \u70b9\u51fb SEND EVENT . \u5355\u51fb \u5411\u60a8\u5c55\u793a\u4ee3\u7406\u770b\u5230\u7684 CloudEvent\u3002 \u60f3\u8981\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001\u4e8b\u4ef6\u5417? \u4f5c\u4e3a Web \u8868\u5355\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001/\u67e5\u770b\u4e8b\u4ef6\u3002 \u53d1\u5e03\u4e8b\u4ef6: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' \u548c\u67e5\u770b\u4e8b\u4ef6: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages Status \u5217\u4e2d\u7684 \u56fe\u6807\u610f\u5473\u7740\u4e8b\u4ef6\u5df2\u7ecf\u53d1\u9001\u5230\u6211\u4eec\u7684\u4ee3\u7406\u2026 \u4f46\u8fd9\u4e00\u4e8b\u4ef6\u5230\u54ea\u91cc\u53bb\u4e86? \u73b0\u5728\uff0c \u54ea\u513f\u4e5f\u53bb\u4e0d\u4e86! \u4ee3\u7406\u53ea\u662f\u4e8b\u4ef6\u7684\u5bb9\u5668\u3002 \u4e3a\u4e86\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4efb\u4f55\u5730\u65b9\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4e8b\u4ef6\u5e76\u5c06\u5b83\u4eec\u653e\u7f6e\u5230\u67d0\u4e2a\u5730\u65b9\u3002 \u4f60\u5f88\u5e78\u8fd0;\u4f60\u5c06\u5728\u4e0b\u4e00\u9875\u521b\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668!","title":"\u7b2c\u4e00\u4e2a\u6e90"},{"location":"getting-started/first-source/#_1","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4f7f\u7528 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u6765\u5c55\u793a Knative \u4e8b\u4ef6\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u5728\u672c\u6559\u7a0b\u7ed3\u675f\u65f6\uff0c\u60a8\u5e94\u8be5\u62e5\u6709\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u4f53\u7cfb\u7ed3\u6784: \u4e0a\u9762\u7684\u56fe\u50cf\u662f Knative in Action \u7684\u56fe 6.6","title":"\u7b2c\u4e00\u4e2a\u6e90"},{"location":"getting-started/first-source/#_2","text":"\u901a\u8fc7\u5c06\u4ee3\u7406\u7684 URL( BROKER_URL )\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0cCloudEvents Player \u5145\u5f53\u4e86 CloudEvents \u7684\u6e90\u3002 \u60a8\u5c06\u901a\u8fc7 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u5411\u4ee3\u7406\u53d1\u9001 CloudEvents\u3002 \u521b\u5efa CloudEvents Player \u670d\u52a1: kn YAML \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u9884\u671f\u7684\u8f93\u51fa Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \u4e3a\u4ec0\u4e48\u6211\u7684\u4fee\u8ba2\u7248\u7684\u540d\u5b57\u4e0d\u4e00\u6837! \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5206\u914d \u7248\u672c\u540d \uff0c\u6240\u4ee5\u670d\u52a1\u4f1a\u81ea\u52a8\u4e3a\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u3002\u5982\u679c\u4f60\u7684\u4fee\u8ba2\u7248\u547d\u540d\u4e0d\u540c\u4e5f\u6ca1\u5173\u7cfb\u3002 \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a cloudevents-player.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created","title":"\u521b\u5efa\u7b2c\u4e00\u4e2a\u6e90"},{"location":"getting-started/first-source/#cloudevents-player","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536 CloudEvents\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1 URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 \u670d\u52a1URL\u4e3a http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , \u4f8b\u5982, http://cloudevents-player.default.127.0.0.1.sslip.io \u90a3\u79cd . \u8fd9\u4e9b\u5b57\u6bb5\u662f\u4ec0\u4e48\u610f\u601d? Field Description Event ID \u4e00\u4e2a\u60df\u4e00\u7684 ID\u3002\u5355\u51fb\u5faa\u73af\u56fe\u6807\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u5faa\u73af\u3002 Event Type \u4e00\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u3002 Event Source \u4e00\u4e2a\u4e8b\u4ef6\u6e90\u3002 Specversion \u754c\u5b9a\u60a8\u6b63\u5728\u4f7f\u7528\u7684 CloudEvents \u89c4\u8303(\u5e94\u8be5\u603b\u662f 1.0)\u3002 Message CloudEvent \u7684 data \u90e8\u5206\uff0c\u4e00\u4e2a\u627f\u8f7d\u60a8\u60f3\u8981\u4f20\u9012\u7684\u6570\u636e\u7684\u6709\u6548\u8f7d\u8377\u3002 \u6709\u5173 CloudEvents \u89c4\u8303\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CloudEvents \u89c4\u8303 .","title":"\u6d4b\u8bd5 CloudEvents Player"},{"location":"getting-started/first-source/#_3","text":"\u5c1d\u8bd5\u4f7f\u7528 CloudEvents Player \u754c\u9762\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: \u5728\u8868\u683c\u4e2d\u586b\u4e0a\u4f60\u60f3\u8981\u7684\u4efb\u4f55\u6570\u636e\u3002 \u786e\u4fdd\u4e8b\u4ef6\u6e90\u4e0d\u5305\u542b\u4efb\u4f55\u7a7a\u683c\u3002 \u70b9\u51fb SEND EVENT . \u5355\u51fb \u5411\u60a8\u5c55\u793a\u4ee3\u7406\u770b\u5230\u7684 CloudEvent\u3002 \u60f3\u8981\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001\u4e8b\u4ef6\u5417? \u4f5c\u4e3a Web \u8868\u5355\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001/\u67e5\u770b\u4e8b\u4ef6\u3002 \u53d1\u5e03\u4e8b\u4ef6: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' \u548c\u67e5\u770b\u4e8b\u4ef6: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages Status \u5217\u4e2d\u7684 \u56fe\u6807\u610f\u5473\u7740\u4e8b\u4ef6\u5df2\u7ecf\u53d1\u9001\u5230\u6211\u4eec\u7684\u4ee3\u7406\u2026 \u4f46\u8fd9\u4e00\u4e8b\u4ef6\u5230\u54ea\u91cc\u53bb\u4e86? \u73b0\u5728\uff0c \u54ea\u513f\u4e5f\u53bb\u4e0d\u4e86! \u4ee3\u7406\u53ea\u662f\u4e8b\u4ef6\u7684\u5bb9\u5668\u3002 \u4e3a\u4e86\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4efb\u4f55\u5730\u65b9\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4e8b\u4ef6\u5e76\u5c06\u5b83\u4eec\u653e\u7f6e\u5230\u67d0\u4e2a\u5730\u65b9\u3002 \u4f60\u5f88\u5e78\u8fd0;\u4f60\u5c06\u5728\u4e0b\u4e00\u9875\u521b\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668!","title":"\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6"},{"location":"getting-started/first-traffic-split/","text":"\u6d41\u91cf\u5206\u53d1 \u00b6 \u6d41\u91cf\u5206\u53d1\u5bf9\u4e8e \u84dd/\u7eff\u90e8\u7f72 \u548c \u91d1\u4e1d\u96c0\u90e8\u7f72 \u5f88\u6709\u7528\u3002 \u4fee\u8ba2\u7248 \u662f\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u5b9e\u65f6\u5feb\u7167\u3002 \u6bcf\u6b21\u66f4\u6539Knative\u670d\u52a1\u7684\u914d\u7f6e\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u5f53\u5206\u6d41\u6d41\u91cf\u65f6\uff0cKnative\u4f1a\u5728Knative\u670d\u52a1\u7684\u4e0d\u540c\u4fee\u8ba2\u7248\u4e4b\u95f4\u5206\u6d41\u6d41\u91cf\u3002 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2 \u00b6 \u66ff\u6362 TARGET=World \uff0c\u66f4\u65b0Knative\u670d\u52a1\u4e0a\u7684\u73af\u5883\u53d8\u91cf TARGET \u503c\u4e3a\"Knative\"\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kn service update hello \\ --env TARGET = Knative \u548c\u524d\u9762\u4e00\u6837\uff0c kn \u5411CLI\u8f93\u51fa\u4e00\u4e9b\u6709\u7528\u7684\u4fe1\u606f\u3002 Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u7f16\u8f91\u4f60\u73b0\u6709\u7684 hello.yaml \u6587\u4ef6\u4ee5\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured \u56e0\u4e3a\u60a8\u6b63\u5728\u66f4\u65b0\u4e00\u4e2a\u73b0\u6709\u7684Knative\u670d\u52a1\uff0c\u6240\u4ee5URL\u4e0d\u4f1a\u6539\u53d8\uff0c\u4f46\u662f\u65b0\u7684\u4fee\u8ba2\u7248\u672c\u6709\u4e86\u65b0\u7684\u540d\u79f0 hello-00002 \u3002 \u8bbf\u95ee\u65b0\u7684\u4fee\u8ba2\u7248\u672c \u00b6 \u8981\u67e5\u770b\u66f4\u6539\uff0c\u8bf7\u5728\u6d4f\u89c8\u5668\u4e0a\u518d\u6b21\u8bbf\u95eeKnative\u670d\u52a1\u6216\u5728\u7ec8\u7aef\u4e0a\u4f7f\u7528 curl : echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! \u67e5\u770b\u73b0\u6709\u7684\u4fee\u6b63 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528Knative ( kn ) or kubectl \u547d\u4ee4\u884c\u67e5\u770b\u73b0\u6709\u4fee\u8ba2\u7684\u5217\u8868: kn kubectl \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 \u5f53\u8fd0\u884c kn \u547d\u4ee4\u65f6\uff0c\u76f8\u5173\u5217\u4e3a TRAFFIC \u3002 \u4f60\u53ef\u4ee5\u770b\u5230100%\u7684\u6d41\u91cf\u90fd\u6d41\u5411\u4e86\u6700\u65b0\u7684\u7248\u672c hello-00002 \uff0c\u5b83\u4f4d\u4e8e GENERATION \u6700\u9ad8\u7684\u90a3\u4e00\u884c\u3002 0%\u7684\u6d41\u91cf\u5c06\u6d41\u5411\u4fee\u8ba2\u7248 hello-00001 \u3002 \u5f53\u60a8\u521b\u5efaKnative\u670d\u52a1\u7684\u65b0\u7248\u672c\u65f6\uff0cKnative\u9ed8\u8ba4\u5c06100%\u7684\u6d41\u91cf\u5bfc\u5411\u8fd9\u4e2a\u6700\u65b0\u7248\u672c\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5e0c\u671b\u6bcf\u4e2a\u4fee\u8ba2\u63a5\u6536\u591a\u5c11\u6d41\u91cf\u6765\u66f4\u6539\u6b64\u9ed8\u8ba4\u884c\u4e3a\u3002 \u5728\u4fee\u8ba2\u7248\u4e4b\u95f4\u5212\u5206\u6d41\u91cf \u00b6 \u5728\u4e24\u4e2a\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u5212\u5206\u6d41\u91cf: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" traffic : - latestRevision : true percent : 50 - latestRevision : false percent : 50 revisionName : hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest always points to the \"latest\" Revision, which in this case is hello-00002 . \u9a8c\u8bc1\u6d41\u91cf\u5206\u6d41 \u00b6 \u82e5\u8981\u9a8c\u8bc1\u6d41\u91cf\u5206\u5272\u7684\u914d\u7f6e\u662f\u5426\u6b63\u786e\uff0c\u8bf7\u518d\u6b21\u901a\u8fc7\u6267\u884c\u8be5\u547d\u4ee4\u5217\u51fa\u4fee\u6539\u9879: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True \u5728\u6d4f\u89c8\u5668\u4e2d\u591a\u6b21\u8bbf\u95eeKnative Service\uff0c\u4ee5\u67e5\u770b\u6bcf\u4e2aRevision\u63d0\u4f9b\u7684\u4e0d\u540c\u8f93\u51fa\u3002 \u7c7b\u4f3c\u5730\uff0c\u60a8\u53ef\u4ee5\u4ece\u7ec8\u7aef\u591a\u6b21\u8bbf\u95eeService URL\uff0c\u4ee5\u67e5\u770b\u5728\u4fee\u8ba2\u4e4b\u95f4\u5212\u5206\u7684\u6d41\u91cf\u3002 echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"\u6d41\u91cf\u5206\u53d1"},{"location":"getting-started/first-traffic-split/#_1","text":"\u6d41\u91cf\u5206\u53d1\u5bf9\u4e8e \u84dd/\u7eff\u90e8\u7f72 \u548c \u91d1\u4e1d\u96c0\u90e8\u7f72 \u5f88\u6709\u7528\u3002 \u4fee\u8ba2\u7248 \u662f\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u5b9e\u65f6\u5feb\u7167\u3002 \u6bcf\u6b21\u66f4\u6539Knative\u670d\u52a1\u7684\u914d\u7f6e\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u5f53\u5206\u6d41\u6d41\u91cf\u65f6\uff0cKnative\u4f1a\u5728Knative\u670d\u52a1\u7684\u4e0d\u540c\u4fee\u8ba2\u7248\u4e4b\u95f4\u5206\u6d41\u6d41\u91cf\u3002","title":"\u6d41\u91cf\u5206\u53d1"},{"location":"getting-started/first-traffic-split/#_2","text":"\u66ff\u6362 TARGET=World \uff0c\u66f4\u65b0Knative\u670d\u52a1\u4e0a\u7684\u73af\u5883\u53d8\u91cf TARGET \u503c\u4e3a\"Knative\"\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kn service update hello \\ --env TARGET = Knative \u548c\u524d\u9762\u4e00\u6837\uff0c kn \u5411CLI\u8f93\u51fa\u4e00\u4e9b\u6709\u7528\u7684\u4fe1\u606f\u3002 Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u7f16\u8f91\u4f60\u73b0\u6709\u7684 hello.yaml \u6587\u4ef6\u4ee5\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured \u56e0\u4e3a\u60a8\u6b63\u5728\u66f4\u65b0\u4e00\u4e2a\u73b0\u6709\u7684Knative\u670d\u52a1\uff0c\u6240\u4ee5URL\u4e0d\u4f1a\u6539\u53d8\uff0c\u4f46\u662f\u65b0\u7684\u4fee\u8ba2\u7248\u672c\u6709\u4e86\u65b0\u7684\u540d\u79f0 hello-00002 \u3002","title":"\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2"},{"location":"getting-started/first-traffic-split/#_3","text":"\u8981\u67e5\u770b\u66f4\u6539\uff0c\u8bf7\u5728\u6d4f\u89c8\u5668\u4e0a\u518d\u6b21\u8bbf\u95eeKnative\u670d\u52a1\u6216\u5728\u7ec8\u7aef\u4e0a\u4f7f\u7528 curl : echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative!","title":"\u8bbf\u95ee\u65b0\u7684\u4fee\u8ba2\u7248\u672c"},{"location":"getting-started/first-traffic-split/#_4","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528Knative ( kn ) or kubectl \u547d\u4ee4\u884c\u67e5\u770b\u73b0\u6709\u4fee\u8ba2\u7684\u5217\u8868: kn kubectl \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 \u5f53\u8fd0\u884c kn \u547d\u4ee4\u65f6\uff0c\u76f8\u5173\u5217\u4e3a TRAFFIC \u3002 \u4f60\u53ef\u4ee5\u770b\u5230100%\u7684\u6d41\u91cf\u90fd\u6d41\u5411\u4e86\u6700\u65b0\u7684\u7248\u672c hello-00002 \uff0c\u5b83\u4f4d\u4e8e GENERATION \u6700\u9ad8\u7684\u90a3\u4e00\u884c\u3002 0%\u7684\u6d41\u91cf\u5c06\u6d41\u5411\u4fee\u8ba2\u7248 hello-00001 \u3002 \u5f53\u60a8\u521b\u5efaKnative\u670d\u52a1\u7684\u65b0\u7248\u672c\u65f6\uff0cKnative\u9ed8\u8ba4\u5c06100%\u7684\u6d41\u91cf\u5bfc\u5411\u8fd9\u4e2a\u6700\u65b0\u7248\u672c\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5e0c\u671b\u6bcf\u4e2a\u4fee\u8ba2\u63a5\u6536\u591a\u5c11\u6d41\u91cf\u6765\u66f4\u6539\u6b64\u9ed8\u8ba4\u884c\u4e3a\u3002","title":"\u67e5\u770b\u73b0\u6709\u7684\u4fee\u6b63"},{"location":"getting-started/first-traffic-split/#_5","text":"\u5728\u4e24\u4e2a\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u5212\u5206\u6d41\u91cf: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" traffic : - latestRevision : true percent : 50 - latestRevision : false percent : 50 revisionName : hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest always points to the \"latest\" Revision, which in this case is hello-00002 .","title":"\u5728\u4fee\u8ba2\u7248\u4e4b\u95f4\u5212\u5206\u6d41\u91cf"},{"location":"getting-started/first-traffic-split/#_6","text":"\u82e5\u8981\u9a8c\u8bc1\u6d41\u91cf\u5206\u5272\u7684\u914d\u7f6e\u662f\u5426\u6b63\u786e\uff0c\u8bf7\u518d\u6b21\u901a\u8fc7\u6267\u884c\u8be5\u547d\u4ee4\u5217\u51fa\u4fee\u6539\u9879: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True \u5728\u6d4f\u89c8\u5668\u4e2d\u591a\u6b21\u8bbf\u95eeKnative Service\uff0c\u4ee5\u67e5\u770b\u6bcf\u4e2aRevision\u63d0\u4f9b\u7684\u4e0d\u540c\u8f93\u51fa\u3002 \u7c7b\u4f3c\u5730\uff0c\u60a8\u53ef\u4ee5\u4ece\u7ec8\u7aef\u591a\u6b21\u8bbf\u95eeService URL\uff0c\u4ee5\u67e5\u770b\u5728\u4fee\u8ba2\u4e4b\u95f4\u5212\u5206\u7684\u6d41\u91cf\u3002 echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"\u9a8c\u8bc1\u6d41\u91cf\u5206\u6d41"},{"location":"getting-started/first-trigger/","text":"\u4f7f\u7528\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668 \u00b6 \u5728\u4e0a\u4e00\u4e2a\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u5411\u4ee3\u7406\u53d1\u9001\u4e8b\u4ef6\u3002 \u73b0\u5728\u6211\u4eec\u5e0c\u671b\u4e8b\u4ef6\u4ece\u4ee3\u7406\u5668\u8f6c\u5230\u63a5\u6536\u5668\u3002 \u5728\u672c\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u63a5\u6536\u5668\u548c\u6e90\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u6211\u4eec\u5c06\u4f7f\u7528\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4ee3\u7406\u5668\u4e2d\u8981\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u3002 \u521b\u5efa\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668 \u00b6 \u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u4ece\u4e8b\u4ef6\u6e90\u76d1\u542c CloudEvents\uff0c\u5e76\u5c06\u5b83\u4eec\u653e\u5165\u63a5\u6536\u5668\u4e2d\uff0c\u8fd9\u4e5f\u662f CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u3002 kn YAML \u8981\u521b\u5efa\u89e6\u53d1\u5668\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a ce-trigger.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u521b\u5efa\u89e6\u53d1\u5668: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created \u6211\u7684\u89e6\u53d1\u5668\u5728\u76d1\u542c\u4ec0\u4e48 CloudEvents ? \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728 kn \u547d\u4ee4\u4e2d\u6307\u5b9a --filter \uff0c\u6240\u4ee5\u89e6\u53d1\u5668\u6b63\u5728\u76d1\u542c\u8fdb\u5165\u4ee3\u7406\u7684\u4efb\u4f55CloudEvents\u3002 \u5c55\u5f00\u4e0b\u4e00\u4e2a\u6ce8\u91ca\uff0c\u67e5\u770b\u5982\u4f55\u4f7f\u7528\u8fc7\u6ee4\u5668\u3002 \u73b0\u5728\uff0c\u5f53\u6211\u4eec\u8fd4\u56de\u5230 CloudEvents Player \u5e76\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u65f6\uff0c\u6211\u4eec\u770b\u5230 CloudEvents \u65e2\u88ab CloudEvents \u53d1\u9001\uff0c\u4e5f\u88ab CloudEvents Player \u63a5\u6536: \u60a8\u53ef\u80fd\u9700\u8981\u5237\u65b0\u9875\u9762\u4ee5\u67e5\u770b\u66f4\u6539\u3002 \u5982\u679c\u6211\u60f3\u8fc7\u6ee4 CloudEvent \u5c5e\u6027\u600e\u4e48\u529e? \u9996\u5148\uff0c\u5220\u9664\u73b0\u6709\u7684\u89e6\u53d1\u5668: kn trigger delete cloudevents-trigger \u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u6dfb\u52a0\u4e00\u4e2a\u76d1\u542c\u67d0\u4e2a CloudEvent \u7c7b\u578b\u7684\u89e6\u53d1\u5668: kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type=some-type \u5982\u679c\u4f60\u53d1\u9001\u4e00\u4e2a\u7c7b\u578b\u4e3a some-type \u7684CloudEvent\uff0c\u5b83\u4f1a\u53cd\u6620\u5728CloudEvents Player UI\u4e2d\u3002 \u89e6\u53d1\u5668\u5ffd\u7565\u4efb\u4f55\u5176\u4ed6\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u8fc7\u6ee4CloudEvent\u7684\u4efb\u4f55\u65b9\u9762\u3002 \u6709\u4e9b\u4eba\u79f0\u4e4b\u4e3a \u201c\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u201d \uff0c\u53ef\u4ee5\u7528\u6765\u5728 Kubernetes \u4e0a\u521b\u5efa\u81ea\u5df1\u7684 \u201c\u529f\u80fd\u5373\u670d\u52a1\u201d \u3002","title":"\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668"},{"location":"getting-started/first-trigger/#_1","text":"\u5728\u4e0a\u4e00\u4e2a\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u5411\u4ee3\u7406\u53d1\u9001\u4e8b\u4ef6\u3002 \u73b0\u5728\u6211\u4eec\u5e0c\u671b\u4e8b\u4ef6\u4ece\u4ee3\u7406\u5668\u8f6c\u5230\u63a5\u6536\u5668\u3002 \u5728\u672c\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u63a5\u6536\u5668\u548c\u6e90\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u6211\u4eec\u5c06\u4f7f\u7528\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4ee3\u7406\u5668\u4e2d\u8981\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u3002","title":"\u4f7f\u7528\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668"},{"location":"getting-started/first-trigger/#_2","text":"\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u4ece\u4e8b\u4ef6\u6e90\u76d1\u542c CloudEvents\uff0c\u5e76\u5c06\u5b83\u4eec\u653e\u5165\u63a5\u6536\u5668\u4e2d\uff0c\u8fd9\u4e5f\u662f CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u3002 kn YAML \u8981\u521b\u5efa\u89e6\u53d1\u5668\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a ce-trigger.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u521b\u5efa\u89e6\u53d1\u5668: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created \u6211\u7684\u89e6\u53d1\u5668\u5728\u76d1\u542c\u4ec0\u4e48 CloudEvents ? \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728 kn \u547d\u4ee4\u4e2d\u6307\u5b9a --filter \uff0c\u6240\u4ee5\u89e6\u53d1\u5668\u6b63\u5728\u76d1\u542c\u8fdb\u5165\u4ee3\u7406\u7684\u4efb\u4f55CloudEvents\u3002 \u5c55\u5f00\u4e0b\u4e00\u4e2a\u6ce8\u91ca\uff0c\u67e5\u770b\u5982\u4f55\u4f7f\u7528\u8fc7\u6ee4\u5668\u3002 \u73b0\u5728\uff0c\u5f53\u6211\u4eec\u8fd4\u56de\u5230 CloudEvents Player \u5e76\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u65f6\uff0c\u6211\u4eec\u770b\u5230 CloudEvents \u65e2\u88ab CloudEvents \u53d1\u9001\uff0c\u4e5f\u88ab CloudEvents Player \u63a5\u6536: \u60a8\u53ef\u80fd\u9700\u8981\u5237\u65b0\u9875\u9762\u4ee5\u67e5\u770b\u66f4\u6539\u3002 \u5982\u679c\u6211\u60f3\u8fc7\u6ee4 CloudEvent \u5c5e\u6027\u600e\u4e48\u529e? \u9996\u5148\uff0c\u5220\u9664\u73b0\u6709\u7684\u89e6\u53d1\u5668: kn trigger delete cloudevents-trigger \u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u6dfb\u52a0\u4e00\u4e2a\u76d1\u542c\u67d0\u4e2a CloudEvent \u7c7b\u578b\u7684\u89e6\u53d1\u5668: kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type=some-type \u5982\u679c\u4f60\u53d1\u9001\u4e00\u4e2a\u7c7b\u578b\u4e3a some-type \u7684CloudEvent\uff0c\u5b83\u4f1a\u53cd\u6620\u5728CloudEvents Player UI\u4e2d\u3002 \u89e6\u53d1\u5668\u5ffd\u7565\u4efb\u4f55\u5176\u4ed6\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u8fc7\u6ee4CloudEvent\u7684\u4efb\u4f55\u65b9\u9762\u3002 \u6709\u4e9b\u4eba\u79f0\u4e4b\u4e3a \u201c\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u201d \uff0c\u53ef\u4ee5\u7528\u6765\u5728 Kubernetes \u4e0a\u521b\u5efa\u81ea\u5df1\u7684 \u201c\u529f\u80fd\u5373\u670d\u52a1\u201d \u3002","title":"\u521b\u5efa\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668"},{"location":"getting-started/getting-started-eventing/","text":"\u5173\u4e8e Knative \u4e8b\u4ef6 \u00b6 Knative \u4e8b\u4ef6\u901a\u8fc7\u5c06\u4e8b\u4ef6\u6e90\u3001\u89e6\u53d1\u5668\u548c\u5176\u4ed6\u9009\u9879\u9644\u52a0\u5230 Knative \u670d\u52a1\uff0c\u4e3a\u60a8\u63d0\u4f9b\u4e86\u7528\u4e8e\u521b\u5efa\u4e8b\u4ef6\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u7528\u5de5\u5177\u3002 \u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u88ab\u8bbe\u8ba1\u4e3a\u5728\u4e8b\u4ef6\u53d1\u751f\u65f6\u68c0\u6d4b\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u4e8b\u4ef6\u5904\u7406\u8fc7\u7a0b\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u3002 Tip \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784\u548c Knative \u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CNCF \u5173\u4e8e \u5e26\u6709 Knative \u4e8b\u4ef6\u7684\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784 \u7684\u4f1a\u8bae \u5b89\u88c5 Knative \u4e8b\u4ef6\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u3001\u53d1\u9001\u548c\u9a8c\u8bc1\u4e8b\u4ef6\u3002 \u672c\u6559\u7a0b\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u57fa\u672c\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u4f7f\u7528 \u4e8b\u4ef6\u6e90 \u3001 \u4ee3\u7406 \u3001 \u89e6\u53d1\u5668 \u548c \u63a5\u6536\u5668 \u7684\u4e8b\u4ef6\u3002","title":"\u5173\u4e8e\u4e8b\u4ef6"},{"location":"getting-started/getting-started-eventing/#knative","text":"Knative \u4e8b\u4ef6\u901a\u8fc7\u5c06\u4e8b\u4ef6\u6e90\u3001\u89e6\u53d1\u5668\u548c\u5176\u4ed6\u9009\u9879\u9644\u52a0\u5230 Knative \u670d\u52a1\uff0c\u4e3a\u60a8\u63d0\u4f9b\u4e86\u7528\u4e8e\u521b\u5efa\u4e8b\u4ef6\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u7528\u5de5\u5177\u3002 \u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u88ab\u8bbe\u8ba1\u4e3a\u5728\u4e8b\u4ef6\u53d1\u751f\u65f6\u68c0\u6d4b\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u4e8b\u4ef6\u5904\u7406\u8fc7\u7a0b\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u3002 Tip \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784\u548c Knative \u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CNCF \u5173\u4e8e \u5e26\u6709 Knative \u4e8b\u4ef6\u7684\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784 \u7684\u4f1a\u8bae \u5b89\u88c5 Knative \u4e8b\u4ef6\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u3001\u53d1\u9001\u548c\u9a8c\u8bc1\u4e8b\u4ef6\u3002 \u672c\u6559\u7a0b\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u57fa\u672c\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u4f7f\u7528 \u4e8b\u4ef6\u6e90 \u3001 \u4ee3\u7406 \u3001 \u89e6\u53d1\u5668 \u548c \u63a5\u6536\u5668 \u7684\u4e8b\u4ef6\u3002","title":"\u5173\u4e8e Knative \u4e8b\u4ef6"},{"location":"getting-started/install-func/","text":"\u5b89\u88c5 Knative \u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002 \u5b89\u88c5 func CLI \u00b6 Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func . \u5b89\u88c5 kn func CLI \u63d2\u4ef6 \u00b6 kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5\u51fd\u6570"},{"location":"getting-started/install-func/#knative","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002","title":"\u5b89\u88c5 Knative \u51fd\u6570"},{"location":"getting-started/install-func/#func-cli","text":"Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func .","title":"\u5b89\u88c5 func CLI"},{"location":"getting-started/install-func/#kn-func-cli","text":"kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5 kn func CLI \u63d2\u4ef6"},{"location":"getting-started/next-steps/","text":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u672c\u4e3b\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u5217\u8868\uff0c\u4ee5\u5e2e\u52a9\u60a8\u7ee7\u7eed\u60a8\u7684 Knative \u4e4b\u65c5\u3002 Knative \u6837\u54c1 \u00b6 \u8bd5\u8bd5\u4e0b\u9762\u7684 Knative \u793a\u4f8b: \u5c06 Kubernetes \u670d\u52a1\u8f6c\u6362\u4e3a Knative \u670d\u52a1 Knative \u670d\u52a1\u4ee3\u7801\u793a\u4f8b Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u63a2\u7d22 Knative \u6587\u6863 \u00b6 \u8bf7\u53c2\u9605\u4ee5\u4e0b\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u7684\u6587\u6863\u6307\u5357: \u670d\u52a1\u6307\u5357 \u4e8b\u4ef6\u6307\u5357 Knative \u4e66\u7c4d \u00b6 \u5e2e\u52a9\u4f60\u7406\u89e3 Knative \u6982\u5ff5\u5e76\u83b7\u5f97\u989d\u5916\u793a\u4f8b\u7684\u4e66\u7c4d: Knative \u5b9e\u4f8b \u60f3\u8981 Knative \u5b9e\u4f8b\u7684\u514d\u8d39\u6570\u5b57\u62f7\u8d1d? VMWare \u6177\u6168\u5730\u6350\u8d60\u4e86 Knative \u5b9e\u4f8b\u7535\u5b50\u4e66\u7684\u514d\u8d39\u8bbf\u95ee\u6743\u9650\uff0c \u5728\u8fd9\u91cc\u83b7\u53d6\u60a8\u7684\u526f\u672c! Knative Cookbook \u5176\u5b83 Knative \u94fe\u63a5 \u00b6 \u5176\u4ed6\u94fe\u63a5\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5f00\u59cb\u4f7f\u7528 Knative: Knative YouTube \u9891\u9053 Knative.tips \u6211\u4eec\u9057\u6f0f\u4e86\u4ec0\u4e48\u5417? \u00b6 \u6211\u4eec\u5f88\u4e50\u610f\u5728\u60a8\u7684 Knative \u65c5\u7a0b\u7684\u4e0b\u4e00\u6b65\u5e2e\u52a9\u60a8\u3002 \u5982\u679c\u6211\u4eec\u5728\u672c\u9875\u4e0a\u9057\u6f0f\u4e86\u60a8\u8ba4\u4e3a\u5e94\u8be5\u5728\u8fd9\u91cc\u7684\u5185\u5bb9\uff0c\u8bf7 \u7ed9\u6211\u4eec\u53cd\u9988 !","title":"\u63a5\u4e0b\u6765\u662f\u4ec0\u4e48?"},{"location":"getting-started/next-steps/#_1","text":"\u672c\u4e3b\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u5217\u8868\uff0c\u4ee5\u5e2e\u52a9\u60a8\u7ee7\u7eed\u60a8\u7684 Knative \u4e4b\u65c5\u3002","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"getting-started/next-steps/#knative","text":"\u8bd5\u8bd5\u4e0b\u9762\u7684 Knative \u793a\u4f8b: \u5c06 Kubernetes \u670d\u52a1\u8f6c\u6362\u4e3a Knative \u670d\u52a1 Knative \u670d\u52a1\u4ee3\u7801\u793a\u4f8b Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b","title":"Knative \u6837\u54c1"},{"location":"getting-started/next-steps/#knative_1","text":"\u8bf7\u53c2\u9605\u4ee5\u4e0b\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u7684\u6587\u6863\u6307\u5357: \u670d\u52a1\u6307\u5357 \u4e8b\u4ef6\u6307\u5357","title":"\u63a2\u7d22 Knative \u6587\u6863"},{"location":"getting-started/next-steps/#knative_2","text":"\u5e2e\u52a9\u4f60\u7406\u89e3 Knative \u6982\u5ff5\u5e76\u83b7\u5f97\u989d\u5916\u793a\u4f8b\u7684\u4e66\u7c4d: Knative \u5b9e\u4f8b \u60f3\u8981 Knative \u5b9e\u4f8b\u7684\u514d\u8d39\u6570\u5b57\u62f7\u8d1d? VMWare \u6177\u6168\u5730\u6350\u8d60\u4e86 Knative \u5b9e\u4f8b\u7535\u5b50\u4e66\u7684\u514d\u8d39\u8bbf\u95ee\u6743\u9650\uff0c \u5728\u8fd9\u91cc\u83b7\u53d6\u60a8\u7684\u526f\u672c! Knative Cookbook","title":"Knative \u4e66\u7c4d"},{"location":"getting-started/next-steps/#knative_3","text":"\u5176\u4ed6\u94fe\u63a5\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5f00\u59cb\u4f7f\u7528 Knative: Knative YouTube \u9891\u9053 Knative.tips","title":"\u5176\u5b83 Knative \u94fe\u63a5"},{"location":"getting-started/next-steps/#_2","text":"\u6211\u4eec\u5f88\u4e50\u610f\u5728\u60a8\u7684 Knative \u65c5\u7a0b\u7684\u4e0b\u4e00\u6b65\u5e2e\u52a9\u60a8\u3002 \u5982\u679c\u6211\u4eec\u5728\u672c\u9875\u4e0a\u9057\u6f0f\u4e86\u60a8\u8ba4\u4e3a\u5e94\u8be5\u5728\u8fd9\u91cc\u7684\u5185\u5bb9\uff0c\u8bf7 \u7ed9\u6211\u4eec\u53cd\u9988 !","title":"\u6211\u4eec\u9057\u6f0f\u4e86\u4ec0\u4e48\u5417?"},{"location":"getting-started/quickstart-install/","text":"\u4f7f\u7528 quickstart \u5b89\u88c5 Knative \u00b6 \u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002 \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002 \u5b89\u88c5 Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 \u5b89\u88c5 Knative quickstart \u63d2\u4ef6 \u00b6 \u8981\u5f00\u59cb\uff0c\u5b89\u88c5 knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u8981\u4f7f\u7528 Homebrew )\u5b89\u88c5 quickstart \u63d2\u4ef6\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4(\u5982\u679c\u60a8\u662f\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative-sandbox/kn-plugins/quickstart \u4ece quickstart \u53d1\u5e03\u9875\u9762 \u4e0b\u8f7d\u60a8\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6. \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-quickstart-amd64 \u91cd\u547d\u540d\u4e3a kn-quickstart \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-quickstart \u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u67e5\u770b kn-plugin-quickstart \u5e93: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh \u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u8fd0\u884c Knative quickstart \u63d2\u4ef6 \u00b6 quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684 Kubernetes \u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5 Knative \u670d\u52a1 \u5176\u4e2d\uff0cKourier \u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io \u4f5c\u4e3a DNS \u5b89\u88c5 Knative \u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97 Knative \u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube \u4f7f\u7528 kind \u5b89\u88c5Knative\u548cKubernetes: kn quickstart kind \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: kind get clusters \u5728 minikube \u5b9e\u4f8b\u4e2d\u5b89\u88c5Knative\u548cKubernetes: Note minikube\u96c6\u7fa4\u5c06\u4f7f\u75286 GB\u7684RAM\u521b\u5efa\u3002 \u5982\u679c\u6ca1\u6709\u8db3\u591f\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u5728\u6b64\u547d\u4ee4\u4e4b\u524d\u8fd0\u884c\u547d\u4ee4 minikube config set memory 3078 \uff0c\u5c06\u5176\u66f4\u6539\u4e3a\u4e0d\u4f4e\u4e8e3 GB\u7684\u503c\u3002 kn quickstart minikube \u4e0a\u4e00\u4e2a\u547d\u4ee4\u7684\u8f93\u51fa\u8981\u6c42\u60a8\u8fd0\u884cminikube tunnel\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u8f85\u52a9\u7ec8\u7aef\u7a97\u53e3\u4e2d\u542f\u52a8\u8fdb\u7a0b\uff0c\u7136\u540e\u8fd4\u56de\u5230\u4e3b\u7a97\u53e3\u5e76\u6309\u56de\u8f66\u7ee7\u7eed: minikube tunnel --profile knative \u5728\u4f7f\u7528Knative\u201cquickstart\u201d\u73af\u5883\u65f6\uff0c\u96a7\u9053\u5fc5\u987b\u5728\u7ec8\u7aef\u7a97\u53e3\u4e2d\u7ee7\u7eed\u8fd0\u884c\u3002 \u9700\u8981\u4f7f\u7528tunnel\u547d\u4ee4\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u7684\u96c6\u7fa4\u4f5c\u4e3aLoadBalancer\u4ece\u60a8\u7684\u4e3b\u673a\u8bbf\u95eeKnative ingress\u670d\u52a1\u3002 Note \u8f93\u5165 Ctrl-C \u53ef\u4ee5\u7ec8\u6b62\u96a7\u9053\u8fdb\u7a0b\u5e76\u6e05\u7406\u7f51\u7edc\u8def\u7531\u3002 \u6709\u5173 minikube tunnel \u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 minikube\u6587\u6863 . \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: minikube profile list \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u73b0\u5728\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 Knative\uff0c\u60a8\u53ef\u4ee5\u5728\u672c\u6559\u7a0b\u7684\u4e0b\u4e00\u4e2a\u4e3b\u9898\u4e2d\u5b66\u4e60\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2a Knative \u670d\u52a1\u3002","title":"\u4f7f\u7528quickstart\u5b89\u88c5"},{"location":"getting-started/quickstart-install/#quickstart-knative","text":"\u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002","title":"\u4f7f\u7528 quickstart \u5b89\u88c5 Knative"},{"location":"getting-started/quickstart-install/#_1","text":"Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"getting-started/quickstart-install/#knative-cli","text":"Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"\u5b89\u88c5 Knative CLI"},{"location":"getting-started/quickstart-install/#knative-quickstart","text":"\u8981\u5f00\u59cb\uff0c\u5b89\u88c5 knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u8981\u4f7f\u7528 Homebrew )\u5b89\u88c5 quickstart \u63d2\u4ef6\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4(\u5982\u679c\u60a8\u662f\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative-sandbox/kn-plugins/quickstart \u4ece quickstart \u53d1\u5e03\u9875\u9762 \u4e0b\u8f7d\u60a8\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6. \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-quickstart-amd64 \u91cd\u547d\u540d\u4e3a kn-quickstart \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-quickstart \u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u67e5\u770b kn-plugin-quickstart \u5e93: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh \u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help","title":"\u5b89\u88c5 Knative quickstart \u63d2\u4ef6"},{"location":"getting-started/quickstart-install/#knative-quickstart_1","text":"quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684 Kubernetes \u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5 Knative \u670d\u52a1 \u5176\u4e2d\uff0cKourier \u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io \u4f5c\u4e3a DNS \u5b89\u88c5 Knative \u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97 Knative \u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube \u4f7f\u7528 kind \u5b89\u88c5Knative\u548cKubernetes: kn quickstart kind \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: kind get clusters \u5728 minikube \u5b9e\u4f8b\u4e2d\u5b89\u88c5Knative\u548cKubernetes: Note minikube\u96c6\u7fa4\u5c06\u4f7f\u75286 GB\u7684RAM\u521b\u5efa\u3002 \u5982\u679c\u6ca1\u6709\u8db3\u591f\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u5728\u6b64\u547d\u4ee4\u4e4b\u524d\u8fd0\u884c\u547d\u4ee4 minikube config set memory 3078 \uff0c\u5c06\u5176\u66f4\u6539\u4e3a\u4e0d\u4f4e\u4e8e3 GB\u7684\u503c\u3002 kn quickstart minikube \u4e0a\u4e00\u4e2a\u547d\u4ee4\u7684\u8f93\u51fa\u8981\u6c42\u60a8\u8fd0\u884cminikube tunnel\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u8f85\u52a9\u7ec8\u7aef\u7a97\u53e3\u4e2d\u542f\u52a8\u8fdb\u7a0b\uff0c\u7136\u540e\u8fd4\u56de\u5230\u4e3b\u7a97\u53e3\u5e76\u6309\u56de\u8f66\u7ee7\u7eed: minikube tunnel --profile knative \u5728\u4f7f\u7528Knative\u201cquickstart\u201d\u73af\u5883\u65f6\uff0c\u96a7\u9053\u5fc5\u987b\u5728\u7ec8\u7aef\u7a97\u53e3\u4e2d\u7ee7\u7eed\u8fd0\u884c\u3002 \u9700\u8981\u4f7f\u7528tunnel\u547d\u4ee4\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u7684\u96c6\u7fa4\u4f5c\u4e3aLoadBalancer\u4ece\u60a8\u7684\u4e3b\u673a\u8bbf\u95eeKnative ingress\u670d\u52a1\u3002 Note \u8f93\u5165 Ctrl-C \u53ef\u4ee5\u7ec8\u6b62\u96a7\u9053\u8fdb\u7a0b\u5e76\u6e05\u7406\u7f51\u7edc\u8def\u7531\u3002 \u6709\u5173 minikube tunnel \u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 minikube\u6587\u6863 . \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: minikube profile list","title":"\u8fd0\u884c Knative quickstart \u63d2\u4ef6"},{"location":"getting-started/quickstart-install/#_2","text":"\u73b0\u5728\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 Knative\uff0c\u60a8\u53ef\u4ee5\u5728\u672c\u6559\u7a0b\u7684\u4e0b\u4e00\u4e2a\u4e3b\u9898\u4e2d\u5b66\u4e60\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2a Knative \u670d\u52a1\u3002","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"install/","text":"\u5b89\u88c5 Knative \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u90e8\u7f72\u9009\u9879\u4e4b\u4e00\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5: \u4f7f\u7528 Knative Quickstart plugin \u5b89\u88c5\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u53d1\u884c\u7248\uff0c\u7528\u4e8e\u5f00\u53d1\u76ee\u7684\u3002 \u4f7f\u7528\u57fa\u4e8eyaml\u7684\u5b89\u88c5\u6765\u5b89\u88c5\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72: \u4f7f\u7528YAML\u5b89\u88c5Knative\u670d\u52a1 \u4f7f\u7528YAML\u5b89\u88c5Knative\u4e8b\u4ef6 \u4f7f\u7528 Knative Operator \u5b89\u88c5\u548c\u914d\u7f6e\u4e00\u4e2a\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72\u3002 \u9075\u5faa\u4f9b\u5e94\u5546\u7ba1\u7406 Knative\u4ea7\u54c1 \u7684\u6587\u6863. \u60a8\u8fd8\u53ef\u4ee5 \u5347\u7ea7\u73b0\u6709\u7684Knative\u5b89\u88c5 . Note Knative\u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u6b63\u5728\u8fd0\u884c\u5e26\u6709Bash shell\u7684Mac\u6216Linux\u3002","title":"\u5b89\u88c5Knative"},{"location":"install/#knative","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u90e8\u7f72\u9009\u9879\u4e4b\u4e00\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5: \u4f7f\u7528 Knative Quickstart plugin \u5b89\u88c5\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u53d1\u884c\u7248\uff0c\u7528\u4e8e\u5f00\u53d1\u76ee\u7684\u3002 \u4f7f\u7528\u57fa\u4e8eyaml\u7684\u5b89\u88c5\u6765\u5b89\u88c5\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72: \u4f7f\u7528YAML\u5b89\u88c5Knative\u670d\u52a1 \u4f7f\u7528YAML\u5b89\u88c5Knative\u4e8b\u4ef6 \u4f7f\u7528 Knative Operator \u5b89\u88c5\u548c\u914d\u7f6e\u4e00\u4e2a\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72\u3002 \u9075\u5faa\u4f9b\u5e94\u5546\u7ba1\u7406 Knative\u4ea7\u54c1 \u7684\u6587\u6863. \u60a8\u8fd8\u53ef\u4ee5 \u5347\u7ea7\u73b0\u6709\u7684Knative\u5b89\u88c5 . Note Knative\u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u6b63\u5728\u8fd0\u884c\u5e26\u6709Bash shell\u7684Mac\u6216Linux\u3002","title":"\u5b89\u88c5 Knative"},{"location":"install/installing-cert-manager/","text":"\u4e3aTLS\u8bc1\u4e66\u5b89\u88c5\u8bc1\u4e66\u7ba1\u7406\u5668 \u00b6 \u5b89\u88c5 Cert-Manager \u5de5\u5177\u83b7\u53d6TLS\u8bc1\u4e66\uff0c\u53ef\u7528\u4e8eKnative\u4e2d\u7684\u5b89\u5168HTTPS\u8fde\u63a5\u3002 \u6709\u5173\u5728Knative\u4e2d\u542f\u7528HTTPS\u8fde\u63a5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528TLS\u8bc1\u4e66\u914d\u7f6eHTTPS \u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528cert-manager\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u4e5f\u53ef\u4ee5\u542f\u7528Knative\u81ea\u52a8\u53d1\u653e\u8bc1\u4e66\u3002 \u5173\u4e8e\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u7684\u5b8c\u6574\u8bf4\u660e\u5728 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u4e2d\u63d0\u4f9b\u3002 \u65e0\u8bba\u60a8\u662f\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u8fd8\u662f\u914d\u7f6eKnative\u4ee5\u5b9e\u73b0\u81ea\u52a8\u53d1\u653e\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u6b65\u9aa4\u5b89\u88c5cert-manager\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u4e3aKnative\u5b89\u88c5cert-manager\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42: \u5fc5\u987b\u5b89\u88c5Knative\u670d\u52a1\u3002\u5173\u4e8e\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Knative\u5b89\u88c5\u6307\u5357 \u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . Knative\u76ee\u524d\u652f\u6301 1.0.0 \u53ca\u66f4\u9ad8\u7248\u672c\u7684\u8bc1\u4e66\u7ba1\u7406\u5668\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5cert-manager \u00b6 \u8981\u4e0b\u8f7d\u548c\u5b89\u88c5cert-manager\uff0c\u8bf7\u4ece\u5b98\u65b9\u7684 cert-manager \u7f51\u7ad9\u4e0a\u9075\u5faa \u5b89\u88c5\u6b65\u9aa4 \u3002 \u6b63\u5728\u5b8c\u6210TLS\u652f\u6301\u7684Knative\u914d\u7f6e \u00b6 \u5728\u4f7f\u7528TLS\u8bc1\u4e66\u8fdb\u884c\u5b89\u5168\u8fde\u63a5\u4e4b\u524d\uff0c\u5fc5\u987b\u5b8c\u6210Knative\u7684\u914d\u7f6e: \u624b\u52a8 : \u5982\u679c\u60a8\u5b89\u88c5\u4e86cert-manager \u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb\u4ee5\u4e0b\u4e3b\u9898\u4e86\u89e3\u5982\u4f55\u521b\u5efa Kubernetes secret: \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u81ea\u52a8 : \u5982\u679c\u5b89\u88c5\u4e86\u7528\u4e8e\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u7684cert-manager\uff0c\u8bf7\u7ee7\u7eed\u6267\u884c\u4ee5\u4e0b\u4e3b\u9898\u4ee5\u542f\u7528\u8be5\u529f\u80fd: \u5728Knative\u4e2d\u542f\u7528TLS\u8bc1\u4e66\u81ea\u52a8\u53d1\u653e","title":"\u5b89\u88c5cert-manager"},{"location":"install/installing-cert-manager/#tls","text":"\u5b89\u88c5 Cert-Manager \u5de5\u5177\u83b7\u53d6TLS\u8bc1\u4e66\uff0c\u53ef\u7528\u4e8eKnative\u4e2d\u7684\u5b89\u5168HTTPS\u8fde\u63a5\u3002 \u6709\u5173\u5728Knative\u4e2d\u542f\u7528HTTPS\u8fde\u63a5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528TLS\u8bc1\u4e66\u914d\u7f6eHTTPS \u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528cert-manager\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u4e5f\u53ef\u4ee5\u542f\u7528Knative\u81ea\u52a8\u53d1\u653e\u8bc1\u4e66\u3002 \u5173\u4e8e\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u7684\u5b8c\u6574\u8bf4\u660e\u5728 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u4e2d\u63d0\u4f9b\u3002 \u65e0\u8bba\u60a8\u662f\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u8fd8\u662f\u914d\u7f6eKnative\u4ee5\u5b9e\u73b0\u81ea\u52a8\u53d1\u653e\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u6b65\u9aa4\u5b89\u88c5cert-manager\u3002","title":"\u4e3aTLS\u8bc1\u4e66\u5b89\u88c5\u8bc1\u4e66\u7ba1\u7406\u5668"},{"location":"install/installing-cert-manager/#_1","text":"\u4e3aKnative\u5b89\u88c5cert-manager\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42: \u5fc5\u987b\u5b89\u88c5Knative\u670d\u52a1\u3002\u5173\u4e8e\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Knative\u5b89\u88c5\u6307\u5357 \u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . Knative\u76ee\u524d\u652f\u6301 1.0.0 \u53ca\u66f4\u9ad8\u7248\u672c\u7684\u8bc1\u4e66\u7ba1\u7406\u5668\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"install/installing-cert-manager/#cert-manager","text":"\u8981\u4e0b\u8f7d\u548c\u5b89\u88c5cert-manager\uff0c\u8bf7\u4ece\u5b98\u65b9\u7684 cert-manager \u7f51\u7ad9\u4e0a\u9075\u5faa \u5b89\u88c5\u6b65\u9aa4 \u3002","title":"\u4e0b\u8f7d\u5e76\u5b89\u88c5cert-manager"},{"location":"install/installing-cert-manager/#tlsknative","text":"\u5728\u4f7f\u7528TLS\u8bc1\u4e66\u8fdb\u884c\u5b89\u5168\u8fde\u63a5\u4e4b\u524d\uff0c\u5fc5\u987b\u5b8c\u6210Knative\u7684\u914d\u7f6e: \u624b\u52a8 : \u5982\u679c\u60a8\u5b89\u88c5\u4e86cert-manager \u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb\u4ee5\u4e0b\u4e3b\u9898\u4e86\u89e3\u5982\u4f55\u521b\u5efa Kubernetes secret: \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u81ea\u52a8 : \u5982\u679c\u5b89\u88c5\u4e86\u7528\u4e8e\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u7684cert-manager\uff0c\u8bf7\u7ee7\u7eed\u6267\u884c\u4ee5\u4e0b\u4e3b\u9898\u4ee5\u542f\u7528\u8be5\u529f\u80fd: \u5728Knative\u4e2d\u542f\u7528TLS\u8bc1\u4e66\u81ea\u52a8\u53d1\u653e","title":"\u6b63\u5728\u5b8c\u6210TLS\u652f\u6301\u7684Knative\u914d\u7f6e"},{"location":"install/installing-istio/","text":"\u4e3aKnative\u5b89\u88c5Istio \u00b6 \u672c\u6307\u5357\u5c06\u6307\u5bfc\u60a8\u624b\u52a8\u5b89\u88c5\u548c\u81ea\u5b9a\u4e49\u4e0eKnative\u4e00\u8d77\u4f7f\u7528\u7684Istio\u3002 \u5982\u679c\u60a8\u7684\u4e91\u5e73\u53f0\u63d0\u4f9b\u6258\u7ba1\u7684Istio\u5b89\u88c5\uff0c\u6211\u4eec\u5efa\u8bae\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u5b89\u88c5Istio\uff0c\u9664\u975e\u60a8\u9700\u8981\u81ea\u5b9a\u4e49\u60a8\u7684\u5b89\u88c5\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u4f60\u9700\u8981: \u5b8c\u6210Kubernetes\u96c6\u7fa4\u7684\u521b\u5efa\u3002 istioctl \u5b89\u88c5. \u652f\u6301\u7684Istio\u7248\u672c \u00b6 \u60a8\u53ef\u4ee5\u5728 Knative Net Istio\u53d1\u5e03\u9875\u9762 \u4e0a\u67e5\u770b\u6700\u65b0\u6d4b\u8bd5\u7684Istio\u7248\u672c. \u5b89\u88c5Istio \u00b6 When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits. \u9009\u62e9Istio\u5b89\u88c5 \u00b6 \u4f60\u53ef\u4ee5\u5b89\u88c5\u5e26\u6709\u6216\u4e0d\u5e26\u6709\u670d\u52a1\u7f51\u683c\u7684Istio: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars . \u5b89\u88c5Istio\u4e0d\u9700\u8981sidecar\u6ce8\u5165 \u00b6 \u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5Istio: \u5b89\u88c5Istio\u65f6\u4e0d\u9700\u8981sidecar\u6ce8\u5165: istioctl install -y \u5b89\u88c5Istio\u9700\u8981sidecar\u6ce8\u5165 \u00b6 If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled \u4f7f\u7528Istio mTLS\u7279\u6027 \u00b6 Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway . \u66f4\u65b0 config-istio configmap\u4ee5\u4f7f\u7528\u975e\u9ed8\u8ba4\u672c\u5730\u7f51\u5173 \u00b6 If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly. \u9a8c\u8bc1\u60a8\u7684Istio\u5b89\u88c5 \u00b6 View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode. \u914d\u7f6eDNS \u00b6 Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\" Istio \u8d44\u6e90 \u00b6 For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference . \u6e05\u7406Istio \u00b6 See the Uninstall Istio . \u63a5\u4e0b\u6765\u662f\u4ec0\u4e48 \u00b6 View the Knative Serving documentation . Try some Knative Serving code samples .","title":"\u5b89\u88c5Istio"},{"location":"install/installing-istio/#knativeistio","text":"\u672c\u6307\u5357\u5c06\u6307\u5bfc\u60a8\u624b\u52a8\u5b89\u88c5\u548c\u81ea\u5b9a\u4e49\u4e0eKnative\u4e00\u8d77\u4f7f\u7528\u7684Istio\u3002 \u5982\u679c\u60a8\u7684\u4e91\u5e73\u53f0\u63d0\u4f9b\u6258\u7ba1\u7684Istio\u5b89\u88c5\uff0c\u6211\u4eec\u5efa\u8bae\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u5b89\u88c5Istio\uff0c\u9664\u975e\u60a8\u9700\u8981\u81ea\u5b9a\u4e49\u60a8\u7684\u5b89\u88c5\u3002","title":"\u4e3aKnative\u5b89\u88c5Istio"},{"location":"install/installing-istio/#_1","text":"\u4f60\u9700\u8981: \u5b8c\u6210Kubernetes\u96c6\u7fa4\u7684\u521b\u5efa\u3002 istioctl \u5b89\u88c5.","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"install/installing-istio/#istio","text":"\u60a8\u53ef\u4ee5\u5728 Knative Net Istio\u53d1\u5e03\u9875\u9762 \u4e0a\u67e5\u770b\u6700\u65b0\u6d4b\u8bd5\u7684Istio\u7248\u672c.","title":"\u652f\u6301\u7684Istio\u7248\u672c"},{"location":"install/installing-istio/#istio_1","text":"When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits.","title":"\u5b89\u88c5Istio"},{"location":"install/installing-istio/#istio_2","text":"\u4f60\u53ef\u4ee5\u5b89\u88c5\u5e26\u6709\u6216\u4e0d\u5e26\u6709\u670d\u52a1\u7f51\u683c\u7684Istio: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars .","title":"\u9009\u62e9Istio\u5b89\u88c5"},{"location":"install/installing-istio/#istiosidecar","text":"\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5Istio: \u5b89\u88c5Istio\u65f6\u4e0d\u9700\u8981sidecar\u6ce8\u5165: istioctl install -y","title":"\u5b89\u88c5Istio\u4e0d\u9700\u8981sidecar\u6ce8\u5165"},{"location":"install/installing-istio/#istiosidecar_1","text":"If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled","title":"\u5b89\u88c5Istio\u9700\u8981sidecar\u6ce8\u5165"},{"location":"install/installing-istio/#istio-mtls","text":"Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway .","title":"\u4f7f\u7528Istio mTLS\u7279\u6027"},{"location":"install/installing-istio/#config-istio-configmap","text":"If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly.","title":"\u66f4\u65b0 config-istio  configmap\u4ee5\u4f7f\u7528\u975e\u9ed8\u8ba4\u672c\u5730\u7f51\u5173"},{"location":"install/installing-istio/#istio_3","text":"View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode.","title":"\u9a8c\u8bc1\u60a8\u7684Istio\u5b89\u88c5"},{"location":"install/installing-istio/#dns","text":"Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\"","title":"\u914d\u7f6eDNS"},{"location":"install/installing-istio/#istio_4","text":"For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference .","title":"Istio \u8d44\u6e90"},{"location":"install/installing-istio/#istio_5","text":"See the Uninstall Istio .","title":"\u6e05\u7406Istio"},{"location":"install/installing-istio/#_2","text":"View the Knative Serving documentation . Try some Knative Serving code samples .","title":"\u63a5\u4e0b\u6765\u662f\u4ec0\u4e48"},{"location":"install/knative-offerings/","text":"Knative Offerings \u00b6 Knative\u6709\u4e00\u4e2a\u4e30\u5bcc\u7684\u793e\u533a\uff0c\u6709\u8bb8\u591a\u4f9b\u5e94\u5546\u53c2\u4e0e\u5176\u4e2d\uff0c\u5176\u4e2d\u8bb8\u591a\u4f9b\u5e94\u5546\u63d0\u4f9b\u5546\u4e1aKnative\u4ea7\u54c1\u3002 \u8bf7\u4e0e\u8fd9\u4e9b\u4f9b\u5e94\u5546\u68c0\u67e5\u4ec0\u4e48\u662f\u652f\u6301\u7684\u6216\u4e0d\u652f\u6301\u7684\u3002 \u4ee5\u4e0b\u662fKnative\u7684\u5546\u4e1a\u4ea7\u54c1\u5217\u8868(\u6309\u5b57\u6bcd\u987a\u5e8f\u6392\u5217): VMware\u5766\u82cf\u7684\u4e91\u672c\u673a\u8fd0\u884c\u65f6 : Kubernetes\u7684\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff0c\u57fa\u4e8eKnative\uff0c\u8fd0\u884c\u5728\u5355\u4e2aKubernetes\u96c6\u7fa4\u4e0a Gardener : \u5728Gardener\u7684\u666e\u901aKubernetes\u96c6\u7fa4\u4e2d\u5b89\u88c5Knative\uff0c\u4ee5\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u65e0\u670d\u52a1\u5668\u8fd0\u884c\u65f6\u5c42\u3002 Google Cloud Run for Anthos : \u4f7f\u7528\u7075\u6d3b\u7684\u65e0\u670d\u52a1\u5668\u5f00\u53d1\u5e73\u53f0\u6269\u5c55\u8c37\u6b4cKubernetes\u5f15\u64ce\u3002\u901a\u8fc7Anthos\u7684\u4e91\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u83b7\u5f97Kubernetes\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u65e0\u670d\u52a1\u5668\u7684\u5f00\u53d1\u4eba\u5458\u7ecf\u9a8c\uff0c\u5141\u8bb8\u60a8\u5728\u81ea\u5df1\u7684\u96c6\u7fa4\u4e0a\u90e8\u7f72\u548c\u7ba1\u7406\u57fa\u4e8eknativees\u7684\u670d\u52a1\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u8c37\u6b4c\u3001\u7b2c\u4e09\u65b9\u6e90\u548c\u60a8\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u4e8b\u4ef6\u89e6\u53d1\u8fd9\u4e9b\u670d\u52a1\u3002 Google Cloud Run : \u4e00\u4e2a\u5b8c\u5168\u7ba1\u7406\u7684\u57fa\u4e8eKnative\u7684\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u3002\u7531\u4e8e\u4e0d\u9700\u8981\u7ba1\u7406Kubernetes\u96c6\u7fa4\uff0c\u4e91\u8fd0\u884c\u53ef\u4ee5\u8ba9\u60a8\u5728\u51e0\u79d2\u949f\u5185\u4ece\u5bb9\u5668\u5230\u751f\u4ea7\u73af\u5883\u3002 IBM Cloud Code Engine : \u4e00\u4e2a\u5b8c\u5168\u6258\u7ba1\u7684\u65e0\u670d\u52a1\u5668\u5e73\u53f0\uff0c\u5b83\u8fd0\u884c\u6240\u6709\u7684\u5bb9\u5668\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5305\u62echttp\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3001\u6279\u5904\u7406\u4f5c\u4e1a\u6216\u4e8b\u4ef6\u9a71\u52a8\u7684\u51fd\u6570\u3002 Nutanix Karbon : \u901a\u8fc7\u5728Karbon\u7ba1\u7406\u7684Kubernetes\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u6765\u6269\u5c55Nutanix Karbon\u7684\u65e0\u670d\u52a1\u5668\u529f\u80fd\u3002 Red Hat Openshift Serverless : \u652f\u6301\u6709\u72b6\u6001\u3001\u65e0\u72b6\u6001\u548c\u65e0\u670d\u52a1\u5668\u7684\u5de5\u4f5c\u8d1f\u8f7d\u90fd\u8fd0\u884c\u5728\u5355\u4e2a\u5177\u6709\u81ea\u52a8\u5316\u64cd\u4f5c\u7684\u591a\u4e91\u5bb9\u5668\u5e73\u53f0\u4e0a\u3002\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u5355\u4e00\u5e73\u53f0\u6765\u6258\u7ba1\u4ed6\u4eec\u7684\u5fae\u670d\u52a1\u3001\u9057\u7559\u548c\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u3002 TriggerMesh : \u73b0\u573a\u548c\u5b8c\u5168\u7ba1\u7406\u7684Knative\u548c\u4e8b\u4ef6\u9a71\u52a8\u96c6\u6210\u5e73\u53f0\u3002\u652f\u6301AWS\u3001Azure\u3001\u8c37\u6b4c\u548c\u8bb8\u591a\u66f4\u591a\u7684\u4e91\u548c\u4f01\u4e1a\u4e8b\u4ef6\u6e90\u548c\u4ee3\u7406\u3002\u53ef\u63d0\u4f9b\u5546\u4e1aKnative\u652f\u6301\u548c\u4e13\u4e1a\u670d\u52a1\u3002","title":"\u57fa\u4e8eKnative\u4ea7\u54c1"},{"location":"install/knative-offerings/#knative-offerings","text":"Knative\u6709\u4e00\u4e2a\u4e30\u5bcc\u7684\u793e\u533a\uff0c\u6709\u8bb8\u591a\u4f9b\u5e94\u5546\u53c2\u4e0e\u5176\u4e2d\uff0c\u5176\u4e2d\u8bb8\u591a\u4f9b\u5e94\u5546\u63d0\u4f9b\u5546\u4e1aKnative\u4ea7\u54c1\u3002 \u8bf7\u4e0e\u8fd9\u4e9b\u4f9b\u5e94\u5546\u68c0\u67e5\u4ec0\u4e48\u662f\u652f\u6301\u7684\u6216\u4e0d\u652f\u6301\u7684\u3002 \u4ee5\u4e0b\u662fKnative\u7684\u5546\u4e1a\u4ea7\u54c1\u5217\u8868(\u6309\u5b57\u6bcd\u987a\u5e8f\u6392\u5217): VMware\u5766\u82cf\u7684\u4e91\u672c\u673a\u8fd0\u884c\u65f6 : Kubernetes\u7684\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff0c\u57fa\u4e8eKnative\uff0c\u8fd0\u884c\u5728\u5355\u4e2aKubernetes\u96c6\u7fa4\u4e0a Gardener : \u5728Gardener\u7684\u666e\u901aKubernetes\u96c6\u7fa4\u4e2d\u5b89\u88c5Knative\uff0c\u4ee5\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u65e0\u670d\u52a1\u5668\u8fd0\u884c\u65f6\u5c42\u3002 Google Cloud Run for Anthos : \u4f7f\u7528\u7075\u6d3b\u7684\u65e0\u670d\u52a1\u5668\u5f00\u53d1\u5e73\u53f0\u6269\u5c55\u8c37\u6b4cKubernetes\u5f15\u64ce\u3002\u901a\u8fc7Anthos\u7684\u4e91\u8fd0\u884c\uff0c\u60a8\u53ef\u4ee5\u83b7\u5f97Kubernetes\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u65e0\u670d\u52a1\u5668\u7684\u5f00\u53d1\u4eba\u5458\u7ecf\u9a8c\uff0c\u5141\u8bb8\u60a8\u5728\u81ea\u5df1\u7684\u96c6\u7fa4\u4e0a\u90e8\u7f72\u548c\u7ba1\u7406\u57fa\u4e8eknativees\u7684\u670d\u52a1\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u8c37\u6b4c\u3001\u7b2c\u4e09\u65b9\u6e90\u548c\u60a8\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u4e8b\u4ef6\u89e6\u53d1\u8fd9\u4e9b\u670d\u52a1\u3002 Google Cloud Run : \u4e00\u4e2a\u5b8c\u5168\u7ba1\u7406\u7684\u57fa\u4e8eKnative\u7684\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u3002\u7531\u4e8e\u4e0d\u9700\u8981\u7ba1\u7406Kubernetes\u96c6\u7fa4\uff0c\u4e91\u8fd0\u884c\u53ef\u4ee5\u8ba9\u60a8\u5728\u51e0\u79d2\u949f\u5185\u4ece\u5bb9\u5668\u5230\u751f\u4ea7\u73af\u5883\u3002 IBM Cloud Code Engine : \u4e00\u4e2a\u5b8c\u5168\u6258\u7ba1\u7684\u65e0\u670d\u52a1\u5668\u5e73\u53f0\uff0c\u5b83\u8fd0\u884c\u6240\u6709\u7684\u5bb9\u5668\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5305\u62echttp\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3001\u6279\u5904\u7406\u4f5c\u4e1a\u6216\u4e8b\u4ef6\u9a71\u52a8\u7684\u51fd\u6570\u3002 Nutanix Karbon : \u901a\u8fc7\u5728Karbon\u7ba1\u7406\u7684Kubernetes\u96c6\u7fa4\u4e0a\u5b89\u88c5Knative\u6765\u6269\u5c55Nutanix Karbon\u7684\u65e0\u670d\u52a1\u5668\u529f\u80fd\u3002 Red Hat Openshift Serverless : \u652f\u6301\u6709\u72b6\u6001\u3001\u65e0\u72b6\u6001\u548c\u65e0\u670d\u52a1\u5668\u7684\u5de5\u4f5c\u8d1f\u8f7d\u90fd\u8fd0\u884c\u5728\u5355\u4e2a\u5177\u6709\u81ea\u52a8\u5316\u64cd\u4f5c\u7684\u591a\u4e91\u5bb9\u5668\u5e73\u53f0\u4e0a\u3002\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u5355\u4e00\u5e73\u53f0\u6765\u6258\u7ba1\u4ed6\u4eec\u7684\u5fae\u670d\u52a1\u3001\u9057\u7559\u548c\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u3002 TriggerMesh : \u73b0\u573a\u548c\u5b8c\u5168\u7ba1\u7406\u7684Knative\u548c\u4e8b\u4ef6\u9a71\u52a8\u96c6\u6210\u5e73\u53f0\u3002\u652f\u6301AWS\u3001Azure\u3001\u8c37\u6b4c\u548c\u8bb8\u591a\u66f4\u591a\u7684\u4e91\u548c\u4f01\u4e1a\u4e8b\u4ef6\u6e90\u548c\u4ee3\u7406\u3002\u53ef\u63d0\u4f9b\u5546\u4e1aKnative\u652f\u6301\u548c\u4e13\u4e1a\u670d\u52a1\u3002","title":"Knative Offerings"},{"location":"install/quickstart-install/","text":"\u4f7f\u7528 quickstart \u5b89\u88c5 Knative \u00b6 \u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002 \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002 \u5b89\u88c5 Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 \u5b89\u88c5 Knative quickstart \u63d2\u4ef6 \u00b6 \u8981\u5f00\u59cb\uff0c\u5b89\u88c5 knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u8981\u4f7f\u7528 Homebrew )\u5b89\u88c5 quickstart \u63d2\u4ef6\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4(\u5982\u679c\u60a8\u662f\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative-sandbox/kn-plugins/quickstart \u4ece quickstart \u53d1\u5e03\u9875\u9762 \u4e0b\u8f7d\u60a8\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6. \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-quickstart-amd64 \u91cd\u547d\u540d\u4e3a kn-quickstart \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-quickstart \u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u67e5\u770b kn-plugin-quickstart \u5e93: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh \u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u8fd0\u884c Knative quickstart \u63d2\u4ef6 \u00b6 quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684 Kubernetes \u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5 Knative \u670d\u52a1 \u5176\u4e2d\uff0cKourier \u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io \u4f5c\u4e3a DNS \u5b89\u88c5 Knative \u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97 Knative \u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube \u4f7f\u7528 kind \u5b89\u88c5Knative\u548cKubernetes: kn quickstart kind \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: kind get clusters \u5728 minikube \u5b9e\u4f8b\u4e2d\u5b89\u88c5Knative\u548cKubernetes: Note minikube\u96c6\u7fa4\u5c06\u4f7f\u75286 GB\u7684RAM\u521b\u5efa\u3002 \u5982\u679c\u6ca1\u6709\u8db3\u591f\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u5728\u6b64\u547d\u4ee4\u4e4b\u524d\u8fd0\u884c\u547d\u4ee4 minikube config set memory 3078 \uff0c\u5c06\u5176\u66f4\u6539\u4e3a\u4e0d\u4f4e\u4e8e3 GB\u7684\u503c\u3002 kn quickstart minikube \u4e0a\u4e00\u4e2a\u547d\u4ee4\u7684\u8f93\u51fa\u8981\u6c42\u60a8\u8fd0\u884cminikube tunnel\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u8f85\u52a9\u7ec8\u7aef\u7a97\u53e3\u4e2d\u542f\u52a8\u8fdb\u7a0b\uff0c\u7136\u540e\u8fd4\u56de\u5230\u4e3b\u7a97\u53e3\u5e76\u6309\u56de\u8f66\u7ee7\u7eed: minikube tunnel --profile knative \u5728\u4f7f\u7528Knative\u201cquickstart\u201d\u73af\u5883\u65f6\uff0c\u96a7\u9053\u5fc5\u987b\u5728\u7ec8\u7aef\u7a97\u53e3\u4e2d\u7ee7\u7eed\u8fd0\u884c\u3002 \u9700\u8981\u4f7f\u7528tunnel\u547d\u4ee4\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u7684\u96c6\u7fa4\u4f5c\u4e3aLoadBalancer\u4ece\u60a8\u7684\u4e3b\u673a\u8bbf\u95eeKnative ingress\u670d\u52a1\u3002 Note \u8f93\u5165 Ctrl-C \u53ef\u4ee5\u7ec8\u6b62\u96a7\u9053\u8fdb\u7a0b\u5e76\u6e05\u7406\u7f51\u7edc\u8def\u7531\u3002 \u6709\u5173 minikube tunnel \u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 minikube\u6587\u6863 . \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: minikube profile list \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u5728[Knative \u6559\u7a0b]\u4e2d\u4e86\u89e3\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2a\u670d\u52a1(../getting-started/first-service.md). \u5c1d\u8bd5 Knative \u4ee3\u7801\u793a\u4f8b . \u53c2\u89c1 Knative Serving \u548c Knative Eventing \u6307\u5357.","title":"quickstart\u5b89\u88c5"},{"location":"install/quickstart-install/#quickstart-knative","text":"\u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730 Knative \u5b89\u88c5\u3002","title":"\u4f7f\u7528 quickstart \u5b89\u88c5 Knative"},{"location":"install/quickstart-install/#_1","text":"Warning Knative quickstart \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8e yaml \u7684\u5b89\u88c5 \u6216 Knative Operator \u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528 Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5 Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728 Kubernetes \u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u8981 3 \u4e2a cpu \u548c 3 \u4e2a GB \u7684 RAM\u3002","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"install/quickstart-install/#knative-cli","text":"Knative CLI ( kn )\u4e3a\u521b\u5efa Knative \u8d44\u6e90(\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u4f7f\u7528 Homebrew \u5b89\u88c5 kn , \u8fd0\u884c\u547d\u4ee4(\u5982\u679c\u4f60\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative/client/kn \u5728\u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u6709\u95ee\u9898\u5417? \u5982\u679c\u4f60\u5728\u4f7f\u7528Homebrew\u5347\u7ea7\u65f6\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5bf9CLI\u5b58\u50a8\u5e93\u7684\u66f4\u6539\uff0c\u5176\u4e2d master \u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3a main \u5206\u652f\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 \u4e3a\u60a8\u7684\u7cfb\u7edf\u4e0b\u8f7d\u4e8c\u8fdb\u5236\u6587\u4ef6. \u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u4f7f\u5176\u53ef\u6267\u884c: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230PATH\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn version \u67e5\u770b kn \u5ba2\u6237\u7aef\u5b58\u50a8\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u79fb\u52a8\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982: kn version \u6620\u50cf\u94fe\u63a5\u5728\u8fd9\u91cc: \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u4ece\u5bb9\u5668\u6620\u50cf\u4e2d\u8fd0\u884c kn \u3002\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"\u5b89\u88c5 Knative CLI"},{"location":"install/quickstart-install/#knative-quickstart","text":"\u8981\u5f00\u59cb\uff0c\u5b89\u88c5 knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: \u8981\u4f7f\u7528 Homebrew )\u5b89\u88c5 quickstart \u63d2\u4ef6\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4(\u5982\u679c\u60a8\u662f\u4ece\u4ee5\u524d\u7684\u7248\u672c\u5347\u7ea7\uff0c\u8bf7\u4f7f\u7528 brew upgrade \u4ee3\u66ff): brew install knative-sandbox/kn-plugins/quickstart \u4ece quickstart \u53d1\u5e03\u9875\u9762 \u4e0b\u8f7d\u60a8\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6. \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-quickstart-amd64 \u91cd\u547d\u540d\u4e3a kn-quickstart \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-quickstart \u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u67d0\u4e2a\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help \u67e5\u770b kn-plugin-quickstart \u5e93: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ \u6784\u5efa\u4e00\u4e2a\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh \u5c06\u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230\u201cPATH\u201d\u4e0a\u7684\u76ee\u5f55: mv kn-quickstart /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5728\u5de5\u4f5c: kn quickstart --help","title":"\u5b89\u88c5 Knative quickstart \u63d2\u4ef6"},{"location":"install/quickstart-install/#knative-quickstart_1","text":"quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684 Kubernetes \u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5 Knative \u670d\u52a1 \u5176\u4e2d\uff0cKourier \u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io \u4f5c\u4e3a DNS \u5b89\u88c5 Knative \u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97 Knative \u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube \u4f7f\u7528 kind \u5b89\u88c5Knative\u548cKubernetes: kn quickstart kind \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: kind get clusters \u5728 minikube \u5b9e\u4f8b\u4e2d\u5b89\u88c5Knative\u548cKubernetes: Note minikube\u96c6\u7fa4\u5c06\u4f7f\u75286 GB\u7684RAM\u521b\u5efa\u3002 \u5982\u679c\u6ca1\u6709\u8db3\u591f\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u5728\u6b64\u547d\u4ee4\u4e4b\u524d\u8fd0\u884c\u547d\u4ee4 minikube config set memory 3078 \uff0c\u5c06\u5176\u66f4\u6539\u4e3a\u4e0d\u4f4e\u4e8e3 GB\u7684\u503c\u3002 kn quickstart minikube \u4e0a\u4e00\u4e2a\u547d\u4ee4\u7684\u8f93\u51fa\u8981\u6c42\u60a8\u8fd0\u884cminikube tunnel\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728\u8f85\u52a9\u7ec8\u7aef\u7a97\u53e3\u4e2d\u542f\u52a8\u8fdb\u7a0b\uff0c\u7136\u540e\u8fd4\u56de\u5230\u4e3b\u7a97\u53e3\u5e76\u6309\u56de\u8f66\u7ee7\u7eed: minikube tunnel --profile knative \u5728\u4f7f\u7528Knative\u201cquickstart\u201d\u73af\u5883\u65f6\uff0c\u96a7\u9053\u5fc5\u987b\u5728\u7ec8\u7aef\u7a97\u53e3\u4e2d\u7ee7\u7eed\u8fd0\u884c\u3002 \u9700\u8981\u4f7f\u7528tunnel\u547d\u4ee4\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u7684\u96c6\u7fa4\u4f5c\u4e3aLoadBalancer\u4ece\u60a8\u7684\u4e3b\u673a\u8bbf\u95eeKnative ingress\u670d\u52a1\u3002 Note \u8f93\u5165 Ctrl-C \u53ef\u4ee5\u7ec8\u6b62\u96a7\u9053\u8fdb\u7a0b\u5e76\u6e05\u7406\u7f51\u7edc\u8def\u7531\u3002 \u6709\u5173 minikube tunnel \u547d\u4ee4\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 minikube\u6587\u6863 . \u63d2\u4ef6\u5b8c\u6210\u540e\uff0c\u9a8c\u8bc1\u4f60\u6709\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4: minikube profile list","title":"\u8fd0\u884c Knative quickstart \u63d2\u4ef6"},{"location":"install/quickstart-install/#_2","text":"\u5728[Knative \u6559\u7a0b]\u4e2d\u4e86\u89e3\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2a\u670d\u52a1(../getting-started/first-service.md). \u5c1d\u8bd5 Knative \u4ee3\u7801\u793a\u4f8b . \u53c2\u89c1 Knative Serving \u548c Knative Eventing \u6307\u5357.","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"install/uninstall/","text":"\u5378\u8f7d Knative \u00b6 To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure. \u5378\u8f7d\u57fa\u4e8eyaml\u7684Knative\u5b89\u88c5 \u00b6 To uninstall a YAML-based Knative installation: \u6b63\u5728\u5378\u8f7d\u53ef\u9009\u670d\u52a1\u6269\u5c55 \u00b6 Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml \u5378\u8f7d\u7f51\u7edc\u5c42 \u00b6 Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml \u5378\u8f7d\u670d\u52a1\u7ec4\u4ef6 \u00b6 Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml \u6b63\u5728\u5378\u8f7d\u53ef\u9009\u7684\u4e8b\u4ef6\u6269\u5c55 \u00b6 Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml \u5378\u8f7d\u4e00\u4e2a\u53ef\u9009\u7684Broker (event)\u5c42 \u00b6 Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml \u5378\u8f7d\u53ef\u9009\u901a\u9053(\u6d88\u606f\u4f20\u9012)\u5c42 \u00b6 Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub. \u5378\u8f7d\u4e8b\u4ef6\u7ec4\u4ef6 \u00b6 Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml \u5378\u8f7d\u57fa\u4e8eoperator\u7684Knative\u5b89\u88c5 \u00b6 To uninstall an Operator-based Knative installation, follow these procedures: \u5220\u9664Knative\u670d\u52a1\u7ec4\u4ef6 \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving \u5220\u9664Knative\u4e8b\u4ef6\u7ec4\u4ef6 \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. \u5220\u9664Knative\u64cd\u4f5c\u7b26 \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"\u5378\u8f7dKnative"},{"location":"install/uninstall/#knative","text":"To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure.","title":"\u5378\u8f7d Knative"},{"location":"install/uninstall/#yamlknative","text":"To uninstall a YAML-based Knative installation:","title":"\u5378\u8f7d\u57fa\u4e8eyaml\u7684Knative\u5b89\u88c5"},{"location":"install/uninstall/#_1","text":"Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml","title":"\u6b63\u5728\u5378\u8f7d\u53ef\u9009\u670d\u52a1\u6269\u5c55"},{"location":"install/uninstall/#_2","text":"Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml","title":"\u5378\u8f7d\u7f51\u7edc\u5c42"},{"location":"install/uninstall/#_3","text":"Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml","title":"\u5378\u8f7d\u670d\u52a1\u7ec4\u4ef6"},{"location":"install/uninstall/#_4","text":"Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml","title":"\u6b63\u5728\u5378\u8f7d\u53ef\u9009\u7684\u4e8b\u4ef6\u6269\u5c55"},{"location":"install/uninstall/#broker-event","text":"Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml","title":"\u5378\u8f7d\u4e00\u4e2a\u53ef\u9009\u7684Broker (event)\u5c42"},{"location":"install/uninstall/#_5","text":"Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub.","title":"\u5378\u8f7d\u53ef\u9009\u901a\u9053(\u6d88\u606f\u4f20\u9012)\u5c42"},{"location":"install/uninstall/#_6","text":"Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml","title":"\u5378\u8f7d\u4e8b\u4ef6\u7ec4\u4ef6"},{"location":"install/uninstall/#operatorknative","text":"To uninstall an Operator-based Knative installation, follow these procedures:","title":"\u5378\u8f7d\u57fa\u4e8eoperator\u7684Knative\u5b89\u88c5"},{"location":"install/uninstall/#knative_1","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"\u5220\u9664Knative\u670d\u52a1\u7ec4\u4ef6"},{"location":"install/uninstall/#knative_2","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"\u5220\u9664Knative\u4e8b\u4ef6\u7ec4\u4ef6"},{"location":"install/uninstall/#knative_3","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"\u5220\u9664Knative\u64cd\u4f5c\u7b26"},{"location":"install/operator/configuring-eventing-cr/","text":"Configuring the Eventing Operator custom resource \u00b6 You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). Setting a default channel \u00b6 If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting. Setting the default channel for the broker \u00b6 If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 Private repository and private secrets \u00b6 The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In this example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: ```bash kn operator configure images --component eventing --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/broker-eventing-controller:latest -n knative-eventing ``` - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key. Download images from different repositories without secrets \u00b6 If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, given the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook docker.io/knative-images-repo2/eventing-webhook:latest controller docker.io/knative-images-repo3/imc-controller:latest dispatcher docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller eventing-controller docker.io/knative-images-repo5/broker-eventing-controller:latest You must modify the KnativeEventing CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --imageKey DISPATCHER_IMAGE -controller --imageURL docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest -n knative-eventing Download images with secrets \u00b6 If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... Configuring the default broker class \u00b6 Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported. Override the resources \u00b6 The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100M limits : cpu : 1000m memory : 250M You can also run the following command to make the equivalent change: kn operator configure resources --component eventing --deployName eventing-controller --container eventing-controller --requestCPU 300m --requestMemory 100M --limitCPU 1000m --limitMemory 250M -n knative-eventing Override the nodeSelector \u00b6 The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelector --component eventing --deployName eventing-controller --key disktype --value hdd -n knative-eventing Override the tolerations \u00b6 The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component eventing --deployName eventing-controller --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-eventing Override the affinity \u00b6 The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd Override the environment variables \u00b6 The KnativeEventing resource is able to override or add the environment variables for the containers in the Knative Eventing deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container eventing-controller into \"knative.dev/my-repo\" for the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller env : - container : eventing-controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component eventing --deployName eventing-controller --container eventing-controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-eventing Override system services \u00b6 If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported. Override labels and annotations and selector \u00b6 The following KnativeEventing resource overrides the eventing-webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : services : - name : eventing-webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component eventing --serviceName eventing-webhook --key mylabel --value foo -n knative-eventing kn operator configure annotations --component eventing --serviceName eventing-webhook --key myannotations --value bar -n knative-eventing kn operator configure selectors --component eventing --serviceName eventing-webhook --key myselector --value bar -n knative-eventing Override system podDisruptionBudgets \u00b6 A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Eventing based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named eventing-webhook , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : podDisruptionBudgets : - name : eventing-webhook minAvailable : 70%","title":"\u914d\u7f6eKnative\u4e8b\u4ef6CRDs"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-eventing-operator-custom-resource","text":"You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR).","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","text":"If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Setting a default channel"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","text":"If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1","title":"Setting the default channel for the broker"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","text":"The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In this example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: ```bash kn operator configure images --component eventing --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/broker-eventing-controller:latest -n knative-eventing ``` - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key.","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","text":"If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, given the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook docker.io/knative-images-repo2/eventing-webhook:latest controller docker.io/knative-images-repo3/imc-controller:latest dispatcher docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller eventing-controller docker.io/knative-images-repo5/broker-eventing-controller:latest You must modify the KnativeEventing CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --imageKey DISPATCHER_IMAGE -controller --imageURL docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest -n knative-eventing","title":"Download images from different repositories without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","text":"Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker","title":"Configuring the default broker class"},{"location":"install/operator/configuring-eventing-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-eventing-cr/#override-the-resources","text":"The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100M limits : cpu : 1000m memory : 250M You can also run the following command to make the equivalent change: kn operator configure resources --component eventing --deployName eventing-controller --container eventing-controller --requestCPU 300m --requestMemory 100M --limitCPU 1000m --limitMemory 250M -n knative-eventing","title":"Override the resources"},{"location":"install/operator/configuring-eventing-cr/#override-the-nodeselector","text":"The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelector --component eventing --deployName eventing-controller --key disktype --value hdd -n knative-eventing","title":"Override the nodeSelector"},{"location":"install/operator/configuring-eventing-cr/#override-the-tolerations","text":"The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component eventing --deployName eventing-controller --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-eventing","title":"Override the tolerations"},{"location":"install/operator/configuring-eventing-cr/#override-the-affinity","text":"The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-eventing-cr/#override-the-environment-variables","text":"The KnativeEventing resource is able to override or add the environment variables for the containers in the Knative Eventing deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container eventing-controller into \"knative.dev/my-repo\" for the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller env : - container : eventing-controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component eventing --deployName eventing-controller --container eventing-controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-eventing","title":"Override the environment variables"},{"location":"install/operator/configuring-eventing-cr/#override-system-services","text":"If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported.","title":"Override system services"},{"location":"install/operator/configuring-eventing-cr/#override-labels-and-annotations-and-selector","text":"The following KnativeEventing resource overrides the eventing-webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : services : - name : eventing-webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component eventing --serviceName eventing-webhook --key mylabel --value foo -n knative-eventing kn operator configure annotations --component eventing --serviceName eventing-webhook --key myannotations --value bar -n knative-eventing kn operator configure selectors --component eventing --serviceName eventing-webhook --key myselector --value bar -n knative-eventing","title":"Override labels and annotations and selector"},{"location":"install/operator/configuring-eventing-cr/#override-system-poddisruptionbudgets","text":"A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Eventing based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named eventing-webhook , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : podDisruptionBudgets : - name : eventing-webhook minAvailable : 70%","title":"Override system podDisruptionBudgets"},{"location":"install/operator/configuring-serving-cr/","text":"Configuring the Knative Serving Operator custom resource \u00b6 You can modify the KnativeServing CR to configure different options for Knative Serving. Configure the Knative Serving version \u00b6 Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v1.5, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"1.5\" You can also run the following command to make the equivalent change: kn operator install --component serving -v 1 .5 -n knative-serving If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator does nothing. The Knative Operator always includes the latest 3 release versions. For example, if the current version of the Knative Operator is v1.5, the earliest version of Knative Serving available through the Operator is v1.2. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v1.3, you must upgrade to v1.4 before upgrading to v1.5. Install customized Knative Serving \u00b6 There are two modes available that you can use to install customized Knative Serving manifests: overwrite mode and append mode . If you are using overwrite mode, under .spec.manifests , you must define all required manifests to install Knative Serving, because the Operator does not install any default manifests. If you are using append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode \u00b6 You can use overwrite mode when you want to customize all Knative Serving manifests. Important You must specify both the version and valid URLs for your custom Knative Serving manifests. For example, if you want to install customized versions of both Knative Serving and the Istio ingress, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. Important The ordering of manifest URLs is critical. Put the manifest you want to apply first at the top of the list. This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml . Append mode \u00b6 You can use append mode to add your customized Knative Serving manifests in addition to the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving manifests, and then installs the customized resources available at https://my-serving/serving-custom.yaml for the version $spec_version . Private repository and private secrets \u00b6 You can use the spec.registry section of the Operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Container Docker Image activator docker.io/knative-images/activator:latest autoscaler docker.io/knative-images/autoscaler:latest controller docker.io/knative-images/controller:latest webhook docker.io/knative-images/webhook:latest autoscaler-hpa docker.io/knative-images/autoscaler-hpa:latest net-istio-controller docker.io/knative-images/net-istio-controller:latest queue-proxy docker.io/knative-images/queue-proxy:latest Define your operator CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:latest You can also run the following command to make the equivalent change: ```bash kn operator configure images --component serving --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-serving ``` Download images individually without secrets \u00b6 If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:latest autoscaler docker.io/knative-images-repo2/autoscaler:latest controller docker.io/knative-images-repo3/controller:latest webhook docker.io/knative-images-repo4/webhook:latest autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:latest queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:latest You must modify the Operator CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:latest autoscaler : docker.io/knative-images-repo2/autoscaler:latest controller : docker.io/knative-images-repo3/controller:latest webhook : docker.io/knative-images-repo4/webhook:latest autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:latest queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:latest You can also run the following commands to make the equivalent change: kn operator configure images --component serving --imageKey activator --imageURL docker.io/knative-images-repo1/activator:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler --imageURL docker.io/knative-images-repo2/autoscaler:latest -n knative-serving kn operator configure images --component serving --imageKey controller --imageURL docker.io/knative-images-repo3/controller:latest -n knative-serving kn operator configure images --component serving --imageKey webhook --imageURL docker.io/knative-images-repo4/webhook:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler-hpa --imageURL docker.io/knative-images-repo5/autoscaler-hpa:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-controller --imageKey controller --imageURL docker.io/knative-images-repo6/prefix-net-istio-controller:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-webhook --imageKey webhook --imageURL docker.io/knative-images-repo6/net-istio-webhook:latest -n knative-serving kn operator configure images --component serving --imageKey queue-proxy --imageURL docker.io/knative-images-repo7/queue-proxy-suffix:latest -n knative-serving Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook . Download images with secrets \u00b6 If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... SSL certificate for controller \u00b6 To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap Replace the default Istio ingress gateway service \u00b6 Create a gateway Service and Deployment instance . Update the Knative gateway by updating the ingress.istio.knative-ingress-gateway spec to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Replace the ingress gateway \u00b6 Create a gateway . Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Configuration of cluster local gateway \u00b6 Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway: Default local gateway name \u00b6 Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway . Non-default local gateway name \u00b6 If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\" Servers configuration for Istio gateways: \u00b6 You can leverage the KnativeServing CR to configure the hosts and port of the servers stanzas for knative-local-gateway or knative-ingress-gateway gateways. For example, you would like to specify the host into <test-ip> and configure the port with number: 443 , name: https , protocol: HTTPS , and target_port: 8443 for knative-local-gateway , apply the following yaml content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : servers : - port : number : 443 name : https protocol : HTTPS target_port : 8443 hosts : - <test-ip> config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\" High availability \u00b6 By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 You can also run the following command to make the equivalent change: kn operator configure replicas --component serving --replicas 3 -n knative-serving The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 . Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by modifying the deployments spec in the KnativeServing CR. Currently resources , replicas , labels , annotations and nodeSelector are supported. Override the resources \u00b6 The KnativeServing CR is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within a deployment. For example, the following KnativeServing CR configures the container controller in the deployment controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller resources : - container : controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi You can also run the following command to make the equivalent change: kn operator configure resources --component serving --deployName controller --container controller --requestCPU 300m --requestMemory 100Mi --limitCPU 1000m --limitMemory 250Mi -n knative-serving Override replicas, labels and annotations \u00b6 The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotations: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotations : bar You can also run the following commands to make the equivalent change: kn operator configure replicas --component serving --replicas 2 -n knative-serving kn operator configure replicas --component serving --deployName webhook --replicas 3 -n knative-serving kn operator configure labels --component serving --deployName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --deployName webhook --key myannotations --value bar -n knative-serving Note The KnativeServing CR label and annotation settings override the webhook's labels and annotations for Deployments and Pods. Override the nodeSelector \u00b6 The following KnativeServing CR overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelectors --component serving --deployName webhook --key disktype --value hdd -n knative-serving Override the tolerations \u00b6 The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component serving --deployName activator --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-serving Override the affinity \u00b6 The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd Override the environment variables \u00b6 The KnativeServing resource is able to override or add the environment variables for the containers in the Knative Serving deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container controller into \"knative.dev/my-repo\" for the deployment controller , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller env : - container : controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component serving --deployName controller --container controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-serving Override system services \u00b6 If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported. Override labels and annotations and selector \u00b6 The following KnativeServing resource overrides the webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : services : - name : webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component serving --serviceName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --serviceName webhook --key myannotations --value bar -n knative-serving kn operator configure selectors --component serving --serviceName webhook --key myselector --value bar -n knative-serving Override system podDisruptionBudgets \u00b6 A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Serving based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named activator-pdb , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : podDisruptionBudgets : - name : activator-pdb minAvailable : 70%","title":"\u914d\u7f6eKnative\u670d\u52a1CRDs"},{"location":"install/operator/configuring-serving-cr/#configuring-the-knative-serving-operator-custom-resource","text":"You can modify the KnativeServing CR to configure different options for Knative Serving.","title":"Configuring the Knative Serving Operator custom resource"},{"location":"install/operator/configuring-serving-cr/#configure-the-knative-serving-version","text":"Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v1.5, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"1.5\" You can also run the following command to make the equivalent change: kn operator install --component serving -v 1 .5 -n knative-serving If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator does nothing. The Knative Operator always includes the latest 3 release versions. For example, if the current version of the Knative Operator is v1.5, the earliest version of Knative Serving available through the Operator is v1.2. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v1.3, you must upgrade to v1.4 before upgrading to v1.5.","title":"Configure the Knative Serving version"},{"location":"install/operator/configuring-serving-cr/#install-customized-knative-serving","text":"There are two modes available that you can use to install customized Knative Serving manifests: overwrite mode and append mode . If you are using overwrite mode, under .spec.manifests , you must define all required manifests to install Knative Serving, because the Operator does not install any default manifests. If you are using append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.","title":"Install customized Knative Serving"},{"location":"install/operator/configuring-serving-cr/#overwrite-mode","text":"You can use overwrite mode when you want to customize all Knative Serving manifests. Important You must specify both the version and valid URLs for your custom Knative Serving manifests. For example, if you want to install customized versions of both Knative Serving and the Istio ingress, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. Important The ordering of manifest URLs is critical. Put the manifest you want to apply first at the top of the list. This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml .","title":"Overwrite mode"},{"location":"install/operator/configuring-serving-cr/#append-mode","text":"You can use append mode to add your customized Knative Serving manifests in addition to the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving manifests, and then installs the customized resources available at https://my-serving/serving-custom.yaml for the version $spec_version .","title":"Append mode"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","text":"You can use the spec.registry section of the Operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Container Docker Image activator docker.io/knative-images/activator:latest autoscaler docker.io/knative-images/autoscaler:latest controller docker.io/knative-images/controller:latest webhook docker.io/knative-images/webhook:latest autoscaler-hpa docker.io/knative-images/autoscaler-hpa:latest net-istio-controller docker.io/knative-images/net-istio-controller:latest queue-proxy docker.io/knative-images/queue-proxy:latest Define your operator CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:latest You can also run the following command to make the equivalent change: ```bash kn operator configure images --component serving --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-serving ```","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","text":"If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:latest autoscaler docker.io/knative-images-repo2/autoscaler:latest controller docker.io/knative-images-repo3/controller:latest webhook docker.io/knative-images-repo4/webhook:latest autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:latest queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:latest You must modify the Operator CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:latest autoscaler : docker.io/knative-images-repo2/autoscaler:latest controller : docker.io/knative-images-repo3/controller:latest webhook : docker.io/knative-images-repo4/webhook:latest autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:latest queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:latest You can also run the following commands to make the equivalent change: kn operator configure images --component serving --imageKey activator --imageURL docker.io/knative-images-repo1/activator:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler --imageURL docker.io/knative-images-repo2/autoscaler:latest -n knative-serving kn operator configure images --component serving --imageKey controller --imageURL docker.io/knative-images-repo3/controller:latest -n knative-serving kn operator configure images --component serving --imageKey webhook --imageURL docker.io/knative-images-repo4/webhook:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler-hpa --imageURL docker.io/knative-images-repo5/autoscaler-hpa:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-controller --imageKey controller --imageURL docker.io/knative-images-repo6/prefix-net-istio-controller:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-webhook --imageKey webhook --imageURL docker.io/knative-images-repo6/net-istio-webhook:latest -n knative-serving kn operator configure images --component serving --imageKey queue-proxy --imageURL docker.io/knative-images-repo7/queue-proxy-suffix:latest -n knative-serving Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook .","title":"Download images individually without secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","text":"To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap","title":"SSL certificate for controller"},{"location":"install/operator/configuring-serving-cr/#replace-the-default-istio-ingress-gateway-service","text":"Create a gateway Service and Deployment instance . Update the Knative gateway by updating the ingress.istio.knative-ingress-gateway spec to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Replace the default Istio ingress gateway service"},{"location":"install/operator/configuring-serving-cr/#replace-the-ingress-gateway","text":"Create a gateway . Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Replace the ingress gateway"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","text":"Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway:","title":"Configuration of cluster local gateway"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","text":"Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway .","title":"Default local gateway name"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","text":"If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Non-default local gateway name"},{"location":"install/operator/configuring-serving-cr/#servers-configuration-for-istio-gateways","text":"You can leverage the KnativeServing CR to configure the hosts and port of the servers stanzas for knative-local-gateway or knative-ingress-gateway gateways. For example, you would like to specify the host into <test-ip> and configure the port with number: 443 , name: https , protocol: HTTPS , and target_port: 8443 for knative-local-gateway , apply the following yaml content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : servers : - port : number : 443 name : https protocol : HTTPS target_port : 8443 hosts : - <test-ip> config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Servers configuration for Istio gateways:"},{"location":"install/operator/configuring-serving-cr/#high-availability","text":"By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 You can also run the following command to make the equivalent change: kn operator configure replicas --component serving --replicas 3 -n knative-serving The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 .","title":"High availability"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by modifying the deployments spec in the KnativeServing CR. Currently resources , replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-serving-cr/#override-the-resources","text":"The KnativeServing CR is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within a deployment. For example, the following KnativeServing CR configures the container controller in the deployment controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller resources : - container : controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi You can also run the following command to make the equivalent change: kn operator configure resources --component serving --deployName controller --container controller --requestCPU 300m --requestMemory 100Mi --limitCPU 1000m --limitMemory 250Mi -n knative-serving","title":"Override the resources"},{"location":"install/operator/configuring-serving-cr/#override-replicas-labels-and-annotations","text":"The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotations: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotations : bar You can also run the following commands to make the equivalent change: kn operator configure replicas --component serving --replicas 2 -n knative-serving kn operator configure replicas --component serving --deployName webhook --replicas 3 -n knative-serving kn operator configure labels --component serving --deployName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --deployName webhook --key myannotations --value bar -n knative-serving Note The KnativeServing CR label and annotation settings override the webhook's labels and annotations for Deployments and Pods.","title":"Override replicas, labels and annotations"},{"location":"install/operator/configuring-serving-cr/#override-the-nodeselector","text":"The following KnativeServing CR overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelectors --component serving --deployName webhook --key disktype --value hdd -n knative-serving","title":"Override the nodeSelector"},{"location":"install/operator/configuring-serving-cr/#override-the-tolerations","text":"The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component serving --deployName activator --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-serving","title":"Override the tolerations"},{"location":"install/operator/configuring-serving-cr/#override-the-affinity","text":"The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-serving-cr/#override-the-environment-variables","text":"The KnativeServing resource is able to override or add the environment variables for the containers in the Knative Serving deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container controller into \"knative.dev/my-repo\" for the deployment controller , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller env : - container : controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component serving --deployName controller --container controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-serving","title":"Override the environment variables"},{"location":"install/operator/configuring-serving-cr/#override-system-services","text":"If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported.","title":"Override system services"},{"location":"install/operator/configuring-serving-cr/#override-labels-and-annotations-and-selector","text":"The following KnativeServing resource overrides the webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : services : - name : webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component serving --serviceName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --serviceName webhook --key myannotations --value bar -n knative-serving kn operator configure selectors --component serving --serviceName webhook --key myselector --value bar -n knative-serving","title":"Override labels and annotations and selector"},{"location":"install/operator/configuring-serving-cr/#override-system-poddisruptionbudgets","text":"A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Serving based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named activator-pdb , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : podDisruptionBudgets : - name : activator-pdb minAvailable : 70%","title":"Override system podDisruptionBudgets"},{"location":"install/operator/configuring-with-operator/","text":"Configuring Knative by using the Operator \u00b6 The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data . Examples \u00b6 You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"\u4f7f\u7528Operator\u914d\u7f6eKnative"},{"location":"install/operator/configuring-with-operator/#configuring-knative-by-using-the-operator","text":"The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data .","title":"Configuring Knative by using the Operator"},{"location":"install/operator/configuring-with-operator/#examples","text":"You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"Examples"},{"location":"install/operator/knative-with-operator-cli/","text":"Install by using the Knative Operator CLI Plugin \u00b6 Knative provides a CLI Plugin to install, configure and manage Knative via the command lines. This CLI plugin facilitates you with a parameter-driven way to configure the Knative cluster, without interacting with the complexities of the custom resources. \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Operator CLI Plugin \u00b6 Before you install the Knative Operator CLI Plugin, first install the Knative CLI . MacOS Linux Download the binary kn-operator-darwin-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-darwin-amd64 kn-operator Download the binary kn-operator-linux-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-linux-amd64 kn-operator Make the plugin executable by running the command: chmod +x kn-operator Create the directory for the kn plugin: mkdir -p ~/.config/kn/plugins Move the file to a plugin directory for kn : cp kn-operator ~/.config/kn/plugins Verify the installation of the Knative Operator CLI Plugin \u00b6 You can run the following command to verify the installation: kn operator -h You should see more information about how to use this CLI plugin. Install the Knative Operator \u00b6 You can install Knative Operator of any specific version under any specific namespace. By default, the namespace is default , and the version is the latest. To install the latest version of Knative Operator, run: kn operator install To install Knative Operator under a certain namespace, e.g. knative-operator, run: kn operator install -n knative-operator To install Knative Operator of a specific version, e.g. 1.7.1, run: kn operator install -v 1 .7.1 Installing the Knative Serving component \u00b6 You can install Knative Serving of any specific version under any specific namespace. By default, the namespace is knative-serving , and the version is the latest. To install the latest version of Knative Serving, run: kn operator install --component serving To install Knative Serving under a certain namespace, e.g. knative-serving, run: kn operator install --component serving -n knative-serving To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component serving -n knative-serving -v \"1.7\" To install the ingress plugin, e.g Kourier, together with the install command, run: kn operator install --component serving -n knative-serving -v \"1.7\" --kourier If you do not specify the ingress plugin, istio is used as the default. However, you need to make sure you install Istio first. Install the networking layer \u00b6 You can configure the network layer option via the Operator CLI Plugin. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, run the command as follows: kn operator enable ingress --kourier -n knative-serving The following steps install Istio to enable its Knative integration: Install Istio . To configure Knative Serving to use Istio, run the command as follows: kn operator enable ingress --istio -n knative-serving The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, run the command as follows: kn operator enable ingress --contour -n knative-serving Installing the Knative Eventing component \u00b6 You can install Knative Eventing of any specific version under any specific namespace. By default, the namespace is knative-eventing , and the version is the latest. To install the latest version of Knative Eventing, run: kn operator install --component eventing To install Knative Eventing under a certain namespace, e.g. knative-eventing, run: kn operator install --component eventing -n knative-eventing To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component eventing -n knative-eventing -v \"1.7\" Installing Knative Eventing with event sources \u00b6 Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To install the eventing source Ceph, run the following command: kn operator enable eventing-source --ceph --namespace knative-eventing To install the eventing source Github, run the following command: kn operator enable eventing-source --github --namespace knative-eventing To install the eventing source Gitlab, run the following command: kn operator enable eventing-source --gitlab --namespace knative-eventing To install the eventing source Kafka, run the following command: kn operator enable eventing-source --kafka --namespace knative-eventing To install the eventing source RabbitMQ, run the following command: kn operator enable eventing-source --rabbitmq --namespace knative-eventing To install the eventing source Redis, run the following command: kn operator enable eventing-source --redis --namespace knative-eventing What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"\u4f7f\u7528Knative Operator CLI\u63d2\u4ef6\u8fdb\u884c\u5b89\u88c5"},{"location":"install/operator/knative-with-operator-cli/#install-by-using-the-knative-operator-cli-plugin","text":"Knative provides a CLI Plugin to install, configure and manage Knative via the command lines. This CLI plugin facilitates you with a parameter-driven way to configure the Knative cluster, without interacting with the complexities of the custom resources.","title":"Install by using the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator-cli-plugin","text":"Before you install the Knative Operator CLI Plugin, first install the Knative CLI . MacOS Linux Download the binary kn-operator-darwin-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-darwin-amd64 kn-operator Download the binary kn-operator-linux-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-linux-amd64 kn-operator Make the plugin executable by running the command: chmod +x kn-operator Create the directory for the kn plugin: mkdir -p ~/.config/kn/plugins Move the file to a plugin directory for kn : cp kn-operator ~/.config/kn/plugins","title":"Install the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#verify-the-installation-of-the-knative-operator-cli-plugin","text":"You can run the following command to verify the installation: kn operator -h You should see more information about how to use this CLI plugin.","title":"Verify the installation of the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator","text":"You can install Knative Operator of any specific version under any specific namespace. By default, the namespace is default , and the version is the latest. To install the latest version of Knative Operator, run: kn operator install To install Knative Operator under a certain namespace, e.g. knative-operator, run: kn operator install -n knative-operator To install Knative Operator of a specific version, e.g. 1.7.1, run: kn operator install -v 1 .7.1","title":"Install the Knative Operator"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-serving-component","text":"You can install Knative Serving of any specific version under any specific namespace. By default, the namespace is knative-serving , and the version is the latest. To install the latest version of Knative Serving, run: kn operator install --component serving To install Knative Serving under a certain namespace, e.g. knative-serving, run: kn operator install --component serving -n knative-serving To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component serving -n knative-serving -v \"1.7\" To install the ingress plugin, e.g Kourier, together with the install command, run: kn operator install --component serving -n knative-serving -v \"1.7\" --kourier If you do not specify the ingress plugin, istio is used as the default. However, you need to make sure you install Istio first.","title":"Installing the Knative Serving component"},{"location":"install/operator/knative-with-operator-cli/#install-the-networking-layer","text":"You can configure the network layer option via the Operator CLI Plugin. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, run the command as follows: kn operator enable ingress --kourier -n knative-serving The following steps install Istio to enable its Knative integration: Install Istio . To configure Knative Serving to use Istio, run the command as follows: kn operator enable ingress --istio -n knative-serving The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, run the command as follows: kn operator enable ingress --contour -n knative-serving","title":"Install the networking layer"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-eventing-component","text":"You can install Knative Eventing of any specific version under any specific namespace. By default, the namespace is knative-eventing , and the version is the latest. To install the latest version of Knative Eventing, run: kn operator install --component eventing To install Knative Eventing under a certain namespace, e.g. knative-eventing, run: kn operator install --component eventing -n knative-eventing To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component eventing -n knative-eventing -v \"1.7\"","title":"Installing the Knative Eventing component"},{"location":"install/operator/knative-with-operator-cli/#installing-knative-eventing-with-event-sources","text":"Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To install the eventing source Ceph, run the following command: kn operator enable eventing-source --ceph --namespace knative-eventing To install the eventing source Github, run the following command: kn operator enable eventing-source --github --namespace knative-eventing To install the eventing source Gitlab, run the following command: kn operator enable eventing-source --gitlab --namespace knative-eventing To install the eventing source Kafka, run the following command: kn operator enable eventing-source --kafka --namespace knative-eventing To install the eventing source RabbitMQ, run the following command: kn operator enable eventing-source --rabbitmq --namespace knative-eventing To install the eventing source Redis, run the following command: kn operator enable eventing-source --redis --namespace knative-eventing","title":"Installing Knative Eventing with event sources"},{"location":"install/operator/knative-with-operator-cli/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/operator/knative-with-operators/","text":"Install by using the Knative Operator \u00b6 Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. The following table describes the supported versions of Serving and Eventing for the Knative Operator: Operator Serving Eventing v1.8 v1.8.0 v1.7.0, v1.7.1 and v1.7.2 v1.6.0 and v1.6.1 v1.5.0 v1.8.0 v1.7.0, v1.7.1, v1.7.2 and v1.7.3 v1.6.0, v1.6.1 and v1.6.2 v1.5.0, v1.5.1, v1.5.2, v1.5.3, v1.5.4, v1.5.5 and v1.5.6 \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Operator \u00b6 Before you install the Knative Serving and Eventing components, first install the Knative Operator. Warning Knative Operator 1.5 is the last version that supports CRDs with both v1alpha1 and v1beta1 . If you are upgrading an existing Operator install from v1.2 or earlier to v1.3 or later, run the following command to upgrade the existing custom resources to v1beta1 before installing the current version: kubectl create -f https://github.com/knative/operator/releases/download/knative-v1.5.1/operator-post-install.yaml To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page . Verify your Knative Operator installation \u00b6 Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h Track the log \u00b6 To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator Install Knative Serving \u00b6 To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS. Create the Knative Serving custom resource \u00b6 To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Install the networking layer \u00b6 Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later. Verify the Knative Serving deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True \u914d\u7f6e DNS \u00b6 \u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) \u771f\u6b63\u7684DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Install Knative Eventing \u00b6 To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources. Create the Knative Eventing custom resource \u00b6 To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Installing a specific version of Eventing \u00b6 Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v1.7, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"1.7\" You can also run the following command to make the equivalent change: kn operator install --component eventing -v 1 .7 -n knative-eventing If spec.version is not specified, the Knative Operator installs the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 1.4, you must upgrade to 1.5 before upgrading to 1.6. Installing customized Knative Eventing \u00b6 The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, under .spec.manifests , you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode \u00b6 Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. Append mode \u00b6 You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them. Installing Knative Eventing with event sources \u00b6 Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the Knative Eventing deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True Uninstalling Knative \u00b6 Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Serving component \u00b6 To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing Removing the Knative Operator: \u00b6 If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/ What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"\u4f7f\u7528Operator\u5b89\u88c5"},{"location":"install/operator/knative-with-operators/#install-by-using-the-knative-operator","text":"Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. The following table describes the supported versions of Serving and Eventing for the Knative Operator: Operator Serving Eventing v1.8 v1.8.0 v1.7.0, v1.7.1 and v1.7.2 v1.6.0 and v1.6.1 v1.5.0 v1.8.0 v1.7.0, v1.7.1, v1.7.2 and v1.7.3 v1.6.0, v1.6.1 and v1.6.2 v1.5.0, v1.5.1, v1.5.2, v1.5.3, v1.5.4, v1.5.5 and v1.5.6","title":"Install by using the Knative Operator"},{"location":"install/operator/knative-with-operators/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/operator/knative-with-operators/#install-the-knative-operator","text":"Before you install the Knative Serving and Eventing components, first install the Knative Operator. Warning Knative Operator 1.5 is the last version that supports CRDs with both v1alpha1 and v1beta1 . If you are upgrading an existing Operator install from v1.2 or earlier to v1.3 or later, run the following command to upgrade the existing custom resources to v1beta1 before installing the current version: kubectl create -f https://github.com/knative/operator/releases/download/knative-v1.5.1/operator-post-install.yaml To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page .","title":"Install the Knative Operator"},{"location":"install/operator/knative-with-operators/#verify-your-knative-operator-installation","text":"Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h","title":"Verify your Knative Operator installation"},{"location":"install/operator/knative-with-operators/#track-the-log","text":"To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator","title":"Track the log"},{"location":"install/operator/knative-with-operators/#install-knative-serving","text":"To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS.","title":"Install Knative Serving"},{"location":"install/operator/knative-with-operators/#create-the-knative-serving-custom-resource","text":"To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create the Knative Serving custom resource"},{"location":"install/operator/knative-with-operators/#install-the-networking-layer","text":"Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later.","title":"Install the networking layer"},{"location":"install/operator/knative-with-operators/#verify-the-knative-serving-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True","title":"Verify the Knative Serving deployment"},{"location":"install/operator/knative-with-operators/#dns","text":"\u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) \u771f\u6b63\u7684DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"\u914d\u7f6e DNS"},{"location":"install/operator/knative-with-operators/#install-knative-eventing","text":"To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources.","title":"Install Knative Eventing"},{"location":"install/operator/knative-with-operators/#create-the-knative-eventing-custom-resource","text":"To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create the Knative Eventing custom resource"},{"location":"install/operator/knative-with-operators/#installing-a-specific-version-of-eventing","text":"Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v1.7, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"1.7\" You can also run the following command to make the equivalent change: kn operator install --component eventing -v 1 .7 -n knative-eventing If spec.version is not specified, the Knative Operator installs the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 1.4, you must upgrade to 1.5 before upgrading to 1.6.","title":"Installing a specific version of Eventing"},{"location":"install/operator/knative-with-operators/#installing-customized-knative-eventing","text":"The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, under .spec.manifests , you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.","title":"Installing customized Knative Eventing"},{"location":"install/operator/knative-with-operators/#overwrite-mode","text":"Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field.","title":"Overwrite mode"},{"location":"install/operator/knative-with-operators/#append-mode","text":"You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them.","title":"Append mode"},{"location":"install/operator/knative-with-operators/#installing-knative-eventing-with-event-sources","text":"Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Installing Knative Eventing with event sources"},{"location":"install/operator/knative-with-operators/#verify-the-knative-eventing-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True","title":"Verify the Knative Eventing deployment"},{"location":"install/operator/knative-with-operators/#uninstalling-knative","text":"Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Uninstalling Knative"},{"location":"install/operator/knative-with-operators/#removing-the-knative-serving-component","text":"To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/operator/knative-with-operators/#removing-knative-eventing-component","text":"To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing","title":"Removing Knative Eventing component"},{"location":"install/operator/knative-with-operators/#removing-the-knative-operator","text":"If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/operator/knative-with-operators/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/upgrade/","text":"\u66f4\u65b0 Knative \u00b6 Knative \u652f\u6301\u901a\u8fc7\u5355\u4e2a minor \u7248\u672c\u53f7\u8fdb\u884c\u5347\u7ea7\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e86 v0.21.0\uff0c\u90a3\u4e48\u5728\u5c1d\u8bd5\u5347\u7ea7\u5230 v0.23.0 \u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5347\u7ea7\u5230 v0.22.0\u3002 \u8981\u9a8c\u8bc1\u5f53\u524d\u7248\u672c\uff0c\u8bf7\u53c2\u89c1 \u68c0\u67e5 Knative \u7248\u672c \u3002 \u5347\u7ea7 Knative: \u5982\u679c\u60a8\u4f7f\u7528 YAML \u5b89\u88c5 Knative\uff0c\u8bf7\u53c2\u89c1 \u7528 kubectl \u5347\u7ea7 . \u5982\u679c\u60a8\u4f7f\u7528 Knative \u64cd\u4f5c\u7b26\u5b89\u88c5 Knative\uff0c\u8bf7\u53c2\u9605 \u4f7f\u7528 Knative Operator \u5347\u7ea7 .","title":"\u5347\u7ea7Knative"},{"location":"install/upgrade/#knative","text":"Knative \u652f\u6301\u901a\u8fc7\u5355\u4e2a minor \u7248\u672c\u53f7\u8fdb\u884c\u5347\u7ea7\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u5b89\u88c5\u4e86 v0.21.0\uff0c\u90a3\u4e48\u5728\u5c1d\u8bd5\u5347\u7ea7\u5230 v0.23.0 \u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5347\u7ea7\u5230 v0.22.0\u3002 \u8981\u9a8c\u8bc1\u5f53\u524d\u7248\u672c\uff0c\u8bf7\u53c2\u89c1 \u68c0\u67e5 Knative \u7248\u672c \u3002 \u5347\u7ea7 Knative: \u5982\u679c\u60a8\u4f7f\u7528 YAML \u5b89\u88c5 Knative\uff0c\u8bf7\u53c2\u89c1 \u7528 kubectl \u5347\u7ea7 . \u5982\u679c\u60a8\u4f7f\u7528 Knative \u64cd\u4f5c\u7b26\u5b89\u88c5 Knative\uff0c\u8bf7\u53c2\u9605 \u4f7f\u7528 Knative Operator \u5347\u7ea7 .","title":"\u66f4\u65b0 Knative"},{"location":"install/upgrade/check-install-version/","text":"\u68c0\u6d4b Knative \u7248\u672c \u00b6 \u8981\u68c0\u67e5Knative\u5b89\u88c5\u7684\u7248\u672c\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u4e4b\u4e00\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u60a8\u662f\u7528YAML\u8fd8\u662f\u7528Operator\u5b89\u88c5Knative\u3002 \u5982\u679c\u4f60\u4f7f\u7528 YAML \u5b89\u88c5 \u00b6 \u8981\u9a8c\u8bc1\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684Knative\u7ec4\u4ef6\u7684\u7248\u672c\uff0c\u8bf7\u67e5\u8be2 <component>.knative.dev/release \u6807\u7b7e\u3002 Knative Serving Knative Eventing \u8fd0\u884c\u547d\u4ee4\u67e5\u770b\u5df2\u5b89\u88c5\u7684Knative\u670d\u52a1\u7248\u672c: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' \u793a\u4f8b\u8f93\u51fa: v0.23.0 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684Knative event\u7248\u672c: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' \u793a\u4f8b\u8f93\u51fa: v0.23.0 \u5982\u679c\u4f60\u4f7f\u7528 Operator \u5b89\u88c5 \u00b6 \u8981\u9a8c\u8bc1\u60a8\u5f53\u524d\u5b89\u88c5\u7684Knative\u7248\u672c: Knative Serving Knative Eventing \u8fd0\u884c\u547d\u4ee4\u67e5\u770b\u5df2\u5b89\u88c5\u7684Knative\u670d\u52a1\u7248\u672c: kubectl get KnativeServing knative-serving --namespace knative-serving \u793a\u4f8b\u8f93\u51fa: NAME VERSION READY REASON knative-serving 0 .23.0 True \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684Knative event\u7248\u672c: kubectl get KnativeEventing knative-eventing --namespace knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"\u68c0\u67e5Knative\u7248\u672c"},{"location":"install/upgrade/check-install-version/#knative","text":"\u8981\u68c0\u67e5Knative\u5b89\u88c5\u7684\u7248\u672c\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u4e4b\u4e00\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u60a8\u662f\u7528YAML\u8fd8\u662f\u7528Operator\u5b89\u88c5Knative\u3002","title":"\u68c0\u6d4b Knative \u7248\u672c"},{"location":"install/upgrade/check-install-version/#yaml","text":"\u8981\u9a8c\u8bc1\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684Knative\u7ec4\u4ef6\u7684\u7248\u672c\uff0c\u8bf7\u67e5\u8be2 <component>.knative.dev/release \u6807\u7b7e\u3002 Knative Serving Knative Eventing \u8fd0\u884c\u547d\u4ee4\u67e5\u770b\u5df2\u5b89\u88c5\u7684Knative\u670d\u52a1\u7248\u672c: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' \u793a\u4f8b\u8f93\u51fa: v0.23.0 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684Knative event\u7248\u672c: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' \u793a\u4f8b\u8f93\u51fa: v0.23.0","title":"\u5982\u679c\u4f60\u4f7f\u7528 YAML \u5b89\u88c5"},{"location":"install/upgrade/check-install-version/#operator","text":"\u8981\u9a8c\u8bc1\u60a8\u5f53\u524d\u5b89\u88c5\u7684Knative\u7248\u672c: Knative Serving Knative Eventing \u8fd0\u884c\u547d\u4ee4\u67e5\u770b\u5df2\u5b89\u88c5\u7684Knative\u670d\u52a1\u7248\u672c: kubectl get KnativeServing knative-serving --namespace knative-serving \u793a\u4f8b\u8f93\u51fa: NAME VERSION READY REASON knative-serving 0 .23.0 True \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u7684Knative event\u7248\u672c: kubectl get KnativeEventing knative-eventing --namespace knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"\u5982\u679c\u4f60\u4f7f\u7528 Operator \u5b89\u88c5"},{"location":"install/upgrade/upgrade-installation-with-operator/","text":"\u4f7f\u7528 Knative Operator \u66f4\u65b0 \u00b6 This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"1.1\" to upgrade to the 1.1 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 1.5, it bundles and supports the installation of Knative versions 1.5, 1.4, 1.3 and 1.2. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace. \u6267\u884c\u5347\u7ea7 \u00b6 To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u901a\u8fc7\u67e5\u770bPod\u9a8c\u8bc1\u5347\u7ea7 \u00b6 You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods -n knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods -n knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s \u901a\u8fc7\u67e5\u770b\u81ea\u5b9a\u4e49\u8d44\u6e90\u9a8c\u8bc1\u5347\u7ea7 \u00b6 You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving -n knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 1 .1.0 True kubectl get KnativeEventing knative-eventing -n knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 1 .1.0 True \u56de\u6eda\u5230\u8f83\u65e9\u7248\u672c \u00b6 If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 1.2, and your previous version is 1.1, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 1.1. Knative Serving Knative Eventing To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u4f7f\u7528Operator\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation-with-operator/#knative-operator","text":"This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"1.1\" to upgrade to the 1.1 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 1.5, it bundles and supports the installation of Knative versions 1.5, 1.4, 1.3 and 1.2. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace.","title":"\u4f7f\u7528 Knative Operator \u66f4\u65b0"},{"location":"install/upgrade/upgrade-installation-with-operator/#_1","text":"To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u6267\u884c\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation-with-operator/#pod","text":"You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods -n knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods -n knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s","title":"\u901a\u8fc7\u67e5\u770bPod\u9a8c\u8bc1\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation-with-operator/#_2","text":"You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving -n knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 1 .1.0 True kubectl get KnativeEventing knative-eventing -n knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 1 .1.0 True","title":"\u901a\u8fc7\u67e5\u770b\u81ea\u5b9a\u4e49\u8d44\u6e90\u9a8c\u8bc1\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation-with-operator/#_3","text":"If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 1.2, and your previous version is 1.1, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 1.1. Knative Serving Knative Eventing To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u56de\u6eda\u5230\u8f83\u65e9\u7248\u672c"},{"location":"install/upgrade/upgrade-installation/","text":"\u4f7f\u7528kubectl\u5347\u7ea7 \u00b6 \u5982\u679c\u4f7f\u7528YAML\u5b89\u88c5Knative\uff0c\u5219\u53ef\u4ee5\u5728\u672c\u4e3b\u9898\u4e2d\u4f7f\u7528 kubectl apply \u547d\u4ee4\u5347\u7ea7Knative\u7ec4\u4ef6\u548c\u63d2\u4ef6\u3002 \u5982\u679c\u4f7f\u7528Operator\u5b89\u88c5\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528Knative Operator\u5347\u7ea7 . \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u5fc5\u987b\u91c7\u53d6\u4e00\u4e9b\u6b65\u9aa4\u4ee5\u786e\u4fdd\u5347\u7ea7\u8fc7\u7a0b\u6210\u529f\u3002 \u8bc6\u522b\u7834\u574f\u6027\u66f4\u6539 \u00b6 \u60a8\u5e94\u8be5\u6ce8\u610fKnative\u5f53\u524d\u7248\u672c\u548c\u671f\u671b\u7248\u672c\u4e4b\u95f4\u7684\u4efb\u4f55\u7834\u574f\u6027\u66f4\u6539\u3002 Knative\u7248\u672c\u4e4b\u95f4\u7684\u7a81\u7834\u6027\u66f4\u6539\u8bb0\u5f55\u5728Knative\u53d1\u5e03\u8bf4\u660e\u4e2d\u3002 \u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u8bf7\u67e5\u770b\u76ee\u6807\u7248\u672c\u7684\u53d1\u5e03\u8bf4\u660e\uff0c\u4e86\u89e3\u60a8\u53ef\u80fd\u9700\u8981\u5bf9Knative\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u7684\u4efb\u4f55\u66f4\u6539: Serving Eventing \u6bcf\u4e2a\u7248\u672c\u7684\u53d1\u5e03\u8bf4\u660e\u90fd\u53d1\u5e03\u5728GitHub\u4e2d\u5404\u81ea\u5b58\u50a8\u5e93\u7684\"Releases\"\u9875\u9762\u4e0a\u3002 \u67e5\u770b\u5f53\u524dPod\u72b6\u6001 \u00b6 \u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u67e5\u770b\u60a8\u8ba1\u5212\u5347\u7ea7\u7684\u540d\u79f0\u7a7a\u95f4\u7684 pods \u7684\u72b6\u6001\u3002 \u8fd9\u5141\u8bb8\u60a8\u6bd4\u8f83\u540d\u79f0\u7a7a\u95f4\u7684\u524d\u540e\u72b6\u6001\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u6b63\u5728\u5347\u7ea7Knative\u670d\u52a1\u548c\u4e8b\u4ef6\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u7684\u5f53\u524d\u72b6\u6001: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing \u5347\u7ea7\u63d2\u4ef6 \u00b6 \u5982\u679c\u60a8\u5b89\u88c5\u4e86\u4e00\u4e2a\u63d2\u4ef6\uff0c\u8bf7\u786e\u4fdd\u5728\u5347\u7ea7Knative\u7ec4\u4ef6\u7684\u540c\u65f6\u5347\u7ea7\u5b83\u3002 \u5347\u7ea7\u524d\u8fd0\u884c\u9884\u5b89\u88c5\u5de5\u5177 \u00b6 \u5bf9\u4e8e\u67d0\u4e9b\u5347\u7ea7\uff0c\u5728\u5b9e\u9645\u5347\u7ea7\u4e4b\u524d\u5fc5\u987b\u5b8c\u6210\u4e00\u4e9b\u6b65\u9aa4\u3002 \u8fd9\u4e9b\u6b65\u9aa4(\u5982\u679c\u9002\u7528\u7684\u8bdd)\u5728\u53d1\u5e03\u8bf4\u660e\u4e2d\u8fdb\u884c\u4e86\u6807\u8bc6\u3002 \u5c06\u73b0\u6709\u8d44\u6e90\u5347\u7ea7\u5230\u6700\u65b0\u7684\u5b58\u50a8\u7248\u672c \u00b6 Knative\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b58\u50a8\u5728Kubernetes\u7684\u7279\u5b9a\u7248\u672c\u4e2d\u3002 \u5f53\u6211\u4eec\u5f15\u5165\u8f83\u65b0\u7684\u652f\u6301\u7248\u672c\u5e76\u5220\u9664\u8f83\u65e7\u7684\u652f\u6301\u7248\u672c\u65f6\uff0c\u60a8\u5fc5\u987b\u5c06\u8d44\u6e90\u8fc1\u79fb\u5230\u6307\u5b9a\u7684\u5b58\u50a8\u7248\u672c\u3002 \u8fd9\u786e\u4fdd\u4e86\u5728\u5347\u7ea7\u65f6\u5220\u9664\u65e7\u7248\u672c\u4f1a\u6210\u529f\u3002 \u5bf9\u4e8e\u5404\u79cd\u5b50\u9879\u76ee\uff0c\u6709\u4e00\u4e2aK8s\u5de5\u4f5c\u6765\u5e2e\u52a9\u64cd\u4f5c\u4eba\u5458\u6267\u884c\u8fd9\u79cd\u8fc1\u79fb\u3002 \u6bcf\u4e2a\u7248\u672c\u7684\u7248\u672c\u8bf4\u660e\u5c06\u660e\u786e\u8bf4\u660e\u662f\u5426\u9700\u8981\u8fc1\u79fb\u3002 \u6267\u884c\u5347\u7ea7 \u00b6 \u8981\u5347\u7ea7\uff0c\u8bf7\u4e3a\u6240\u6709\u5df2\u5b89\u88c5Knative\u7ec4\u4ef6\u548c\u7279\u6027\u7684\u540e\u7eed\u5c0f\u7248\u672c\u5e94\u7528YAML\u6587\u4ef6\uff0c\u8bb0\u4f4f\u6bcf\u6b21\u53ea\u5347\u7ea7\u4e00\u4e2a\u5c0f\u7248\u672c\u3002 \u5347\u7ea7\u524d\uff0c \u68c0\u67e5\u60a8\u7684Knative\u7248\u672c . \u5bf9\u4e8e\u8fd0\u884cKnative Serving\u548cKnative event\u7ec4\u4ef61.1\u7248\u672c\u7684\u96c6\u7fa4\uff0c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u5b89\u88c5\u5347\u7ea7\u52301.2\u7248\u672c: kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/knative-v1.2.0/eventing.yaml \\ \u5347\u7ea7\u5b8c\u6210\u540e\u8fd0\u884c\u5b89\u88c5\u540e\u5de5\u5177 \u00b6 \u5728\u67d0\u4e9b\u5347\u7ea7\u4e2d\uff0c\u6709\u4e9b\u6b65\u9aa4\u5fc5\u987b\u5728\u5b9e\u9645\u5347\u7ea7\u4e4b\u540e\u8fdb\u884c\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u5728\u53d1\u5e03\u8bf4\u660e\u4e2d\u8fdb\u884c\u4e86\u6807\u8bc6\u3002 \u5347\u7ea7\u540e\u9a8c\u8bc1 \u00b6 \u8981\u786e\u8ba4\u7ec4\u4ef6\u548c\u63d2\u4ef6\u5df2\u7ecf\u6210\u529f\u5347\u7ea7\uff0c\u8bf7\u5728\u76f8\u5173\u540d\u79f0\u7a7a\u95f4\u4e2d\u67e5\u770b\u5b83\u4eec\u7684 pods \u7684\u72b6\u6001\u3002 \u6240\u6709\u7684 Pod \u5c06\u5728\u5347\u7ea7\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u4eec\u7684\u5e74\u9f84\u5c06\u91cd\u7f6e\u3002 \u5982\u679c\u5347\u7ea7\u4e86Knative\u670d\u52a1\u548c\u4e8b\u4ef6\uff0c\u8bf7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u6765\u83b7\u53d6\u5173\u4e8e\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u7684 Pod \u7684\u4fe1\u606f: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing \u8fd9\u4e9b\u547d\u4ee4\u8fd4\u56de\u7c7b\u4f3c\u4e8e: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s \u5982\u679c\u6240\u6709 Pod \u7684\u5e74\u9f84\u90fd\u5df2\u91cd\u7f6e\uff0c\u5e76\u4e14\u6240\u6709 Pod \u90fd\u5df2\u542f\u52a8\u5e76\u8fd0\u884c\uff0c\u5219\u5347\u7ea7\u5df2\u6210\u529f\u5b8c\u6210\u3002 \u4f60\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\u65e7 Pod \u88ab\u6e05\u7406\u65f6\u7684 Terminating \u72b6\u6001\u3002 \u5982\u679c\u9700\u8981\uff0c\u91cd\u590d\u5347\u7ea7\u8fc7\u7a0b\uff0c\u76f4\u5230\u8fbe\u5230\u6240\u9700\u7684\u6b21\u8981\u7248\u672c\u53f7\u3002","title":"\u4f7f\u7528kubectl\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation/#kubectl","text":"\u5982\u679c\u4f7f\u7528YAML\u5b89\u88c5Knative\uff0c\u5219\u53ef\u4ee5\u5728\u672c\u4e3b\u9898\u4e2d\u4f7f\u7528 kubectl apply \u547d\u4ee4\u5347\u7ea7Knative\u7ec4\u4ef6\u548c\u63d2\u4ef6\u3002 \u5982\u679c\u4f7f\u7528Operator\u5b89\u88c5\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528Knative Operator\u5347\u7ea7 .","title":"\u4f7f\u7528kubectl\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation/#_1","text":"\u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u5fc5\u987b\u91c7\u53d6\u4e00\u4e9b\u6b65\u9aa4\u4ee5\u786e\u4fdd\u5347\u7ea7\u8fc7\u7a0b\u6210\u529f\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"install/upgrade/upgrade-installation/#_2","text":"\u60a8\u5e94\u8be5\u6ce8\u610fKnative\u5f53\u524d\u7248\u672c\u548c\u671f\u671b\u7248\u672c\u4e4b\u95f4\u7684\u4efb\u4f55\u7834\u574f\u6027\u66f4\u6539\u3002 Knative\u7248\u672c\u4e4b\u95f4\u7684\u7a81\u7834\u6027\u66f4\u6539\u8bb0\u5f55\u5728Knative\u53d1\u5e03\u8bf4\u660e\u4e2d\u3002 \u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u8bf7\u67e5\u770b\u76ee\u6807\u7248\u672c\u7684\u53d1\u5e03\u8bf4\u660e\uff0c\u4e86\u89e3\u60a8\u53ef\u80fd\u9700\u8981\u5bf9Knative\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u7684\u4efb\u4f55\u66f4\u6539: Serving Eventing \u6bcf\u4e2a\u7248\u672c\u7684\u53d1\u5e03\u8bf4\u660e\u90fd\u53d1\u5e03\u5728GitHub\u4e2d\u5404\u81ea\u5b58\u50a8\u5e93\u7684\"Releases\"\u9875\u9762\u4e0a\u3002","title":"\u8bc6\u522b\u7834\u574f\u6027\u66f4\u6539"},{"location":"install/upgrade/upgrade-installation/#pod","text":"\u5728\u5347\u7ea7\u4e4b\u524d\uff0c\u67e5\u770b\u60a8\u8ba1\u5212\u5347\u7ea7\u7684\u540d\u79f0\u7a7a\u95f4\u7684 pods \u7684\u72b6\u6001\u3002 \u8fd9\u5141\u8bb8\u60a8\u6bd4\u8f83\u540d\u79f0\u7a7a\u95f4\u7684\u524d\u540e\u72b6\u6001\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u6b63\u5728\u5347\u7ea7Knative\u670d\u52a1\u548c\u4e8b\u4ef6\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u7684\u5f53\u524d\u72b6\u6001: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing","title":"\u67e5\u770b\u5f53\u524dPod\u72b6\u6001"},{"location":"install/upgrade/upgrade-installation/#_3","text":"\u5982\u679c\u60a8\u5b89\u88c5\u4e86\u4e00\u4e2a\u63d2\u4ef6\uff0c\u8bf7\u786e\u4fdd\u5728\u5347\u7ea7Knative\u7ec4\u4ef6\u7684\u540c\u65f6\u5347\u7ea7\u5b83\u3002","title":"\u5347\u7ea7\u63d2\u4ef6"},{"location":"install/upgrade/upgrade-installation/#_4","text":"\u5bf9\u4e8e\u67d0\u4e9b\u5347\u7ea7\uff0c\u5728\u5b9e\u9645\u5347\u7ea7\u4e4b\u524d\u5fc5\u987b\u5b8c\u6210\u4e00\u4e9b\u6b65\u9aa4\u3002 \u8fd9\u4e9b\u6b65\u9aa4(\u5982\u679c\u9002\u7528\u7684\u8bdd)\u5728\u53d1\u5e03\u8bf4\u660e\u4e2d\u8fdb\u884c\u4e86\u6807\u8bc6\u3002","title":"\u5347\u7ea7\u524d\u8fd0\u884c\u9884\u5b89\u88c5\u5de5\u5177"},{"location":"install/upgrade/upgrade-installation/#_5","text":"Knative\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b58\u50a8\u5728Kubernetes\u7684\u7279\u5b9a\u7248\u672c\u4e2d\u3002 \u5f53\u6211\u4eec\u5f15\u5165\u8f83\u65b0\u7684\u652f\u6301\u7248\u672c\u5e76\u5220\u9664\u8f83\u65e7\u7684\u652f\u6301\u7248\u672c\u65f6\uff0c\u60a8\u5fc5\u987b\u5c06\u8d44\u6e90\u8fc1\u79fb\u5230\u6307\u5b9a\u7684\u5b58\u50a8\u7248\u672c\u3002 \u8fd9\u786e\u4fdd\u4e86\u5728\u5347\u7ea7\u65f6\u5220\u9664\u65e7\u7248\u672c\u4f1a\u6210\u529f\u3002 \u5bf9\u4e8e\u5404\u79cd\u5b50\u9879\u76ee\uff0c\u6709\u4e00\u4e2aK8s\u5de5\u4f5c\u6765\u5e2e\u52a9\u64cd\u4f5c\u4eba\u5458\u6267\u884c\u8fd9\u79cd\u8fc1\u79fb\u3002 \u6bcf\u4e2a\u7248\u672c\u7684\u7248\u672c\u8bf4\u660e\u5c06\u660e\u786e\u8bf4\u660e\u662f\u5426\u9700\u8981\u8fc1\u79fb\u3002","title":"\u5c06\u73b0\u6709\u8d44\u6e90\u5347\u7ea7\u5230\u6700\u65b0\u7684\u5b58\u50a8\u7248\u672c"},{"location":"install/upgrade/upgrade-installation/#_6","text":"\u8981\u5347\u7ea7\uff0c\u8bf7\u4e3a\u6240\u6709\u5df2\u5b89\u88c5Knative\u7ec4\u4ef6\u548c\u7279\u6027\u7684\u540e\u7eed\u5c0f\u7248\u672c\u5e94\u7528YAML\u6587\u4ef6\uff0c\u8bb0\u4f4f\u6bcf\u6b21\u53ea\u5347\u7ea7\u4e00\u4e2a\u5c0f\u7248\u672c\u3002 \u5347\u7ea7\u524d\uff0c \u68c0\u67e5\u60a8\u7684Knative\u7248\u672c . \u5bf9\u4e8e\u8fd0\u884cKnative Serving\u548cKnative event\u7ec4\u4ef61.1\u7248\u672c\u7684\u96c6\u7fa4\uff0c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u5b89\u88c5\u5347\u7ea7\u52301.2\u7248\u672c: kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/knative-v1.2.0/eventing.yaml \\","title":"\u6267\u884c\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation/#_7","text":"\u5728\u67d0\u4e9b\u5347\u7ea7\u4e2d\uff0c\u6709\u4e9b\u6b65\u9aa4\u5fc5\u987b\u5728\u5b9e\u9645\u5347\u7ea7\u4e4b\u540e\u8fdb\u884c\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u5728\u53d1\u5e03\u8bf4\u660e\u4e2d\u8fdb\u884c\u4e86\u6807\u8bc6\u3002","title":"\u5347\u7ea7\u5b8c\u6210\u540e\u8fd0\u884c\u5b89\u88c5\u540e\u5de5\u5177"},{"location":"install/upgrade/upgrade-installation/#_8","text":"\u8981\u786e\u8ba4\u7ec4\u4ef6\u548c\u63d2\u4ef6\u5df2\u7ecf\u6210\u529f\u5347\u7ea7\uff0c\u8bf7\u5728\u76f8\u5173\u540d\u79f0\u7a7a\u95f4\u4e2d\u67e5\u770b\u5b83\u4eec\u7684 pods \u7684\u72b6\u6001\u3002 \u6240\u6709\u7684 Pod \u5c06\u5728\u5347\u7ea7\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u542f\u52a8\uff0c\u5b83\u4eec\u7684\u5e74\u9f84\u5c06\u91cd\u7f6e\u3002 \u5982\u679c\u5347\u7ea7\u4e86Knative\u670d\u52a1\u548c\u4e8b\u4ef6\uff0c\u8bf7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u6765\u83b7\u53d6\u5173\u4e8e\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u7684 Pod \u7684\u4fe1\u606f: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing \u8fd9\u4e9b\u547d\u4ee4\u8fd4\u56de\u7c7b\u4f3c\u4e8e: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s \u5982\u679c\u6240\u6709 Pod \u7684\u5e74\u9f84\u90fd\u5df2\u91cd\u7f6e\uff0c\u5e76\u4e14\u6240\u6709 Pod \u90fd\u5df2\u542f\u52a8\u5e76\u8fd0\u884c\uff0c\u5219\u5347\u7ea7\u5df2\u6210\u529f\u5b8c\u6210\u3002 \u4f60\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\u65e7 Pod \u88ab\u6e05\u7406\u65f6\u7684 Terminating \u72b6\u6001\u3002 \u5982\u679c\u9700\u8981\uff0c\u91cd\u590d\u5347\u7ea7\u8fc7\u7a0b\uff0c\u76f4\u5230\u8fbe\u5230\u6240\u9700\u7684\u6b21\u8981\u7248\u672c\u53f7\u3002","title":"\u5347\u7ea7\u540e\u9a8c\u8bc1"},{"location":"install/yaml-install/","text":"\u57fa\u4e8e YAML \u5b89\u88c5 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u5e94\u7528 YAML \u6587\u4ef6\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5\u3002 \u7528 YAML \u5b89\u88c5 Knative \u670d\u52a1 \u7528 YAML \u5b89\u88c5 Knative \u4e8b\u4ef6","title":"\u57fa\u4e8eYAML\u5b89\u88c5"},{"location":"install/yaml-install/#yaml","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u5e94\u7528 YAML \u6587\u4ef6\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5\u3002 \u7528 YAML \u5b89\u88c5 Knative \u670d\u52a1 \u7528 YAML \u5b89\u88c5 Knative \u4e8b\u4ef6","title":"\u57fa\u4e8e YAML \u5b89\u88c5"},{"location":"install/yaml-install/eventing/eventing-installation-files/","text":"Knative Eventing installation files \u00b6 This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"\u4e8b\u4ef6\u5b89\u88c5\u6587\u4ef6"},{"location":"install/yaml-install/eventing/eventing-installation-files/#knative-eventing-installation-files","text":"This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing installation files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/","text":"Installing Knative Eventing using YAML files \u00b6 This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI. \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install Knative Eventing \u00b6 To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files . Verify the installation \u00b6 Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s Optional: Install a default Channel (messaging) layer \u00b6 The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel In-Memory (standalone) NATS Channel The following commands install the KafkaChannel and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaChannel data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section. Optional: Install a Broker layer \u00b6 The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub. Install optional Eventing extensions \u00b6 The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml To learn more, try the Apache Kafka source sample . Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"YAML\u5b89\u88c5\u4e8b\u4ef6\u5904\u7406"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#installing-knative-eventing-using-yaml-files","text":"This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI.","title":"Installing Knative Eventing using YAML files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-knative-eventing","text":"To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files .","title":"Install Knative Eventing"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#verify-the-installation","text":"Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s","title":"Verify the installation"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","text":"The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel In-Memory (standalone) NATS Channel The following commands install the KafkaChannel and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaChannel data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section.","title":"Optional: Install a default Channel (messaging) layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-broker-layer","text":"The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub.","title":"Optional: Install a Broker layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-optional-eventing-extensions","text":"The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml To learn more, try the Apache Kafka source sample . Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"Install optional Eventing extensions"},{"location":"install/yaml-install/serving/install-serving-with-yaml/","text":"\u4f7f\u7528 YAML \u6587\u4ef6\u5b89\u88c5 Knative \u670d\u52a1 \u00b6 \u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 kubectl \u547d\u4ee4\u884c\u5e94\u7528 YAML \u6587\u4ef6\u6765\u5b89\u88c5 Knative \u670d\u52a1\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. \u5b89\u88c5 Knative Serving \u7ec4\u4ef6 \u00b6 \u5b89\u88c5 Knative Serving \u7ec4\u4ef6\u3002 Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files . \u5b89\u88c5\u7f51\u7edc\u5c42 \u00b6 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u5b89\u88c5\u7f51\u7edc\u5c42\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9\u7f51\u7edc\u5c42: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section. \u9a8c\u8bc1\u5b89\u88c5 \u00b6 Success \u76d1\u89c6Knative\u7ec4\u4ef6\uff0c\u76f4\u5230\u6240\u6709\u7ec4\u4ef6\u663e\u793a STATUS \u4e3a Running \u6216 Completed \u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e76\u68c0\u67e5\u8f93\u51fa: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s \u914d\u7f6e DNS \u00b6 \u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) Real DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. \u5b89\u88c5\u53ef\u9009\u7684\u670d\u52a1\u6269\u5c55 \u00b6 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u5b89\u88c5\u6bcf\u4e2a\u670d\u52a1\u6269\u5c55\u7684\u8bf4\u660e\u3002 HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"YAML\u5b89\u88c5\u670d\u52a1"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#yaml-knative","text":"\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 kubectl \u547d\u4ee4\u884c\u5e94\u7528 YAML \u6587\u4ef6\u6765\u5b89\u88c5 Knative \u670d\u52a1\u3002","title":"\u4f7f\u7528 YAML \u6587\u4ef6\u5b89\u88c5 Knative \u670d\u52a1"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#knative-serving","text":"\u5b89\u88c5 Knative Serving \u7ec4\u4ef6\u3002 Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files .","title":"\u5b89\u88c5 Knative Serving \u7ec4\u4ef6"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#_2","text":"\u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u5b89\u88c5\u7f51\u7edc\u5c42\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9\u7f51\u7edc\u5c42: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section.","title":"\u5b89\u88c5\u7f51\u7edc\u5c42"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#_3","text":"Success \u76d1\u89c6Knative\u7ec4\u4ef6\uff0c\u76f4\u5230\u6240\u6709\u7ec4\u4ef6\u663e\u793a STATUS \u4e3a Running \u6216 Completed \u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e76\u68c0\u67e5\u8f93\u51fa: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s","title":"\u9a8c\u8bc1\u5b89\u88c5"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#dns","text":"\u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) Real DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"\u914d\u7f6e DNS"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#_4","text":"\u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u5b89\u88c5\u6bcf\u4e2a\u670d\u52a1\u6269\u5c55\u7684\u8bf4\u660e\u3002 HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"\u5b89\u88c5\u53ef\u9009\u7684\u670d\u52a1\u6269\u5c55"},{"location":"install/yaml-install/serving/serving-installation-files/","text":"Knative \u670d\u52a1\u5b89\u88c5\u6587\u4ef6 \u00b6 \u672c\u6307\u5357\u63d0\u4f9b\u4e86\u5173\u4e8e\u6838\u5fc3 Knative \u670d\u52a1 YAML \u6587\u4ef6\u7684\u53c2\u8003\u4fe1\u606f\uff0c\u5305\u62ec: \u5b89\u88c5 Knative \u670d\u52a1\u6240\u9700\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u548c\u6838\u5fc3\u7ec4\u4ef6\u3002 \u53ef\u7528\u4e8e\u81ea\u5b9a\u4e49\u5b89\u88c5\u7684\u53ef\u9009\u7ec4\u4ef6\u3002 \u6709\u5173\u5b89\u88c5\u8fd9\u4e9b\u6587\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528 YAML \u6587\u4ef6\u5b89\u88c5 Knative \u670d\u52a1 . \u4e0b\u8868\u63cf\u8ff0\u4e86 Knative \u670d\u52a1\u4e2d\u5305\u542b\u7684\u5b89\u88c5\u6587\u4ef6: \u6587\u4ef6\u540d \u63cf\u8ff0 \u4f9d\u8d56 serving-core.yaml \u5fc5\u987b:Knative \u670d\u52a1\u6838\u5fc3\u7ec4\u4ef6\u3002 serving-crds.yaml serving-crds.yaml \u5fc5\u987b:Knative \u670d\u52a1\u6838\u5fc3 CRDs\u3002 none serving-default-domain.yaml \u914d\u7f6e Knative \u670d\u52a1\u4f7f\u7528 http://sslip.io \u4f5c\u4e3a\u9ed8\u8ba4\u7684 DNS \u540e\u7f00\u3002 serving-core.yaml serving-hpa.yaml \u901a\u8fc7 Kubernetes \u6c34\u5e73 Pod \u81ea\u52a8\u7f29\u653e\u5668\u81ea\u52a8\u7f29\u653e Knative \u4fee\u8ba2\u7684\u7ec4\u4ef6\u3002 serving-core.yaml serving-post-install-jobs.yaml \u5b89\u88c5 serving-core.yaml \u540e\u7684\u9644\u52a0\u4efb\u52a1\u3002\u76ee\u524d\u5b83\u4e0e serving-storage-version-migration.yaml \u76f8\u540c\u3002 serving-core.yaml serving-storage-version-migration.yaml \u5c06 Knative \u8d44\u6e90(\u5305\u62ec Service\u3001Route\u3001Revision \u548c Configuration)\u7684\u5b58\u50a8\u7248\u672c\u4ece v1alpha1 \u548c v1beta1 \u8fc1\u79fb\u5230 v1 \u3002\u4ece 0.18 \u7248\u672c\u5347\u7ea7\u5230 0.19 \u9700\u8981\u3002 serving-core.yaml","title":"\u670d\u52a1\u5b89\u88c5\u6587\u4ef6"},{"location":"install/yaml-install/serving/serving-installation-files/#knative","text":"\u672c\u6307\u5357\u63d0\u4f9b\u4e86\u5173\u4e8e\u6838\u5fc3 Knative \u670d\u52a1 YAML \u6587\u4ef6\u7684\u53c2\u8003\u4fe1\u606f\uff0c\u5305\u62ec: \u5b89\u88c5 Knative \u670d\u52a1\u6240\u9700\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u548c\u6838\u5fc3\u7ec4\u4ef6\u3002 \u53ef\u7528\u4e8e\u81ea\u5b9a\u4e49\u5b89\u88c5\u7684\u53ef\u9009\u7ec4\u4ef6\u3002 \u6709\u5173\u5b89\u88c5\u8fd9\u4e9b\u6587\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u4f7f\u7528 YAML \u6587\u4ef6\u5b89\u88c5 Knative \u670d\u52a1 . \u4e0b\u8868\u63cf\u8ff0\u4e86 Knative \u670d\u52a1\u4e2d\u5305\u542b\u7684\u5b89\u88c5\u6587\u4ef6: \u6587\u4ef6\u540d \u63cf\u8ff0 \u4f9d\u8d56 serving-core.yaml \u5fc5\u987b:Knative \u670d\u52a1\u6838\u5fc3\u7ec4\u4ef6\u3002 serving-crds.yaml serving-crds.yaml \u5fc5\u987b:Knative \u670d\u52a1\u6838\u5fc3 CRDs\u3002 none serving-default-domain.yaml \u914d\u7f6e Knative \u670d\u52a1\u4f7f\u7528 http://sslip.io \u4f5c\u4e3a\u9ed8\u8ba4\u7684 DNS \u540e\u7f00\u3002 serving-core.yaml serving-hpa.yaml \u901a\u8fc7 Kubernetes \u6c34\u5e73 Pod \u81ea\u52a8\u7f29\u653e\u5668\u81ea\u52a8\u7f29\u653e Knative \u4fee\u8ba2\u7684\u7ec4\u4ef6\u3002 serving-core.yaml serving-post-install-jobs.yaml \u5b89\u88c5 serving-core.yaml \u540e\u7684\u9644\u52a0\u4efb\u52a1\u3002\u76ee\u524d\u5b83\u4e0e serving-storage-version-migration.yaml \u76f8\u540c\u3002 serving-core.yaml serving-storage-version-migration.yaml \u5c06 Knative \u8d44\u6e90(\u5305\u62ec Service\u3001Route\u3001Revision \u548c Configuration)\u7684\u5b58\u50a8\u7248\u672c\u4ece v1alpha1 \u548c v1beta1 \u8fc1\u79fb\u5230 v1 \u3002\u4ece 0.18 \u7248\u672c\u5347\u7ea7\u5230 0.19 \u9700\u8981\u3002 serving-core.yaml","title":"Knative \u670d\u52a1\u5b89\u88c5\u6587\u4ef6"},{"location":"reference/relnotes/","text":"Knative \u53d1\u5e03\u8bf4\u660e \u00b6 \u6709\u5173Knative\u53d1\u884c\u7248\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4ee5\u4e0b\u9875\u9762: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"\u53d1\u5e03\u8bf4\u660e"},{"location":"reference/relnotes/#knative","text":"\u6709\u5173Knative\u53d1\u884c\u7248\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4ee5\u4e0b\u9875\u9762: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"Knative \u53d1\u5e03\u8bf4\u660e"},{"location":"reference/security/","text":"Knative\u5b89\u5168\u548c\u62ab\u9732\u4fe1\u606f \u00b6 \u6b64\u9875\u9762\u63cf\u8ff0Knative\u5b89\u5168\u6027\u548c\u62ab\u9732\u4fe1\u606f\u3002 Knative \u5a01\u80c1\u6a21\u578b \u00b6 \u5a01\u80c1\u6a21\u578b \u62a5\u544a\u4e00\u4e2a\u6f0f\u6d1e \u00b6 \u6211\u4eec\u975e\u5e38\u611f\u8c22\u5411Knative\u5f00\u6e90\u793e\u533a\u62a5\u544a\u6f0f\u6d1e\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u7528\u6237\u3002\u6240\u6709\u7684\u62a5\u544a\u90fd\u7ecf\u8fc7\u4e00\u7ec4\u793e\u533a\u5fd7\u613f\u8005\u7684\u5f7b\u5e95\u8c03\u67e5\u3002 \u8981\u5236\u4f5c\u62a5\u544a\uff0c\u8bf7\u5c06\u5b89\u5168\u7ec6\u8282\u548c\u6240\u6709Knative\u9519\u8bef\u62a5\u544a\u7684\u9884\u671f\u7ec6\u8282\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6\u53d1\u9001\u5230\u79c1\u6709\u7684security@knative.team\u5217\u8868\u3002 \u4f55\u65f6\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e? \u00b6 \u4f60\u8ba4\u4e3a\u4f60\u53d1\u73b0\u4e86Knative\u7684\u4e00\u4e2a\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e \u60a8\u4e0d\u786e\u5b9a\u6f0f\u6d1e\u5982\u4f55\u5f71\u54cdKnative \u4f60\u8ba4\u4e3a\u4f60\u5728Knative\u6240\u4f9d\u8d56\u7684\u53e6\u4e00\u4e2a\u9879\u76ee\u4e2d\u53d1\u73b0\u4e86\u6f0f\u6d1e \u5bf9\u4e8e\u6709\u81ea\u5df1\u6f0f\u6d1e\u62a5\u544a\u548c\u62ab\u9732\u6d41\u7a0b\u7684\u9879\u76ee\uff0c\u8bf7\u76f4\u63a5\u5728\u90a3\u91cc\u62a5\u544a \u4ec0\u4e48\u65f6\u5019\u4e0d\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e? \u00b6 \u60a8\u9700\u8981\u5e2e\u52a9\u8c03\u4f18Knative\u7ec4\u4ef6\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027 \u60a8\u9700\u8981\u5e2e\u52a9\u5e94\u7528\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u66f4\u65b0 \u4f60\u7684\u95ee\u9898\u4e0e\u5b89\u5168\u65e0\u5173 \u6f0f\u6d1e\u54cd\u5e94 \u00b6 \u53ca\u65e9\u62ab\u9732\u5b89\u5168\u6f0f\u6d1e \u6f0f\u6d1e\u62ab\u9732\u54cd\u5e94\u7b56\u7565 \u5b89\u5168\u5de5\u4f5c\u5c0f\u7ec4 \u00b6 \u4e00\u822c\u4fe1\u606f \u4fdd\u5b89\u5de5\u4f5c\u5c0f\u7ec4\u7ae0\u7a0b","title":"\u5b89\u5168"},{"location":"reference/security/#knative","text":"\u6b64\u9875\u9762\u63cf\u8ff0Knative\u5b89\u5168\u6027\u548c\u62ab\u9732\u4fe1\u606f\u3002","title":"Knative\u5b89\u5168\u548c\u62ab\u9732\u4fe1\u606f"},{"location":"reference/security/#knative_1","text":"\u5a01\u80c1\u6a21\u578b","title":"Knative \u5a01\u80c1\u6a21\u578b"},{"location":"reference/security/#_1","text":"\u6211\u4eec\u975e\u5e38\u611f\u8c22\u5411Knative\u5f00\u6e90\u793e\u533a\u62a5\u544a\u6f0f\u6d1e\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u7528\u6237\u3002\u6240\u6709\u7684\u62a5\u544a\u90fd\u7ecf\u8fc7\u4e00\u7ec4\u793e\u533a\u5fd7\u613f\u8005\u7684\u5f7b\u5e95\u8c03\u67e5\u3002 \u8981\u5236\u4f5c\u62a5\u544a\uff0c\u8bf7\u5c06\u5b89\u5168\u7ec6\u8282\u548c\u6240\u6709Knative\u9519\u8bef\u62a5\u544a\u7684\u9884\u671f\u7ec6\u8282\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6\u53d1\u9001\u5230\u79c1\u6709\u7684security@knative.team\u5217\u8868\u3002","title":"\u62a5\u544a\u4e00\u4e2a\u6f0f\u6d1e"},{"location":"reference/security/#_2","text":"\u4f60\u8ba4\u4e3a\u4f60\u53d1\u73b0\u4e86Knative\u7684\u4e00\u4e2a\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e \u60a8\u4e0d\u786e\u5b9a\u6f0f\u6d1e\u5982\u4f55\u5f71\u54cdKnative \u4f60\u8ba4\u4e3a\u4f60\u5728Knative\u6240\u4f9d\u8d56\u7684\u53e6\u4e00\u4e2a\u9879\u76ee\u4e2d\u53d1\u73b0\u4e86\u6f0f\u6d1e \u5bf9\u4e8e\u6709\u81ea\u5df1\u6f0f\u6d1e\u62a5\u544a\u548c\u62ab\u9732\u6d41\u7a0b\u7684\u9879\u76ee\uff0c\u8bf7\u76f4\u63a5\u5728\u90a3\u91cc\u62a5\u544a","title":"\u4f55\u65f6\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e?"},{"location":"reference/security/#_3","text":"\u60a8\u9700\u8981\u5e2e\u52a9\u8c03\u4f18Knative\u7ec4\u4ef6\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027 \u60a8\u9700\u8981\u5e2e\u52a9\u5e94\u7528\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u66f4\u65b0 \u4f60\u7684\u95ee\u9898\u4e0e\u5b89\u5168\u65e0\u5173","title":"\u4ec0\u4e48\u65f6\u5019\u4e0d\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e?"},{"location":"reference/security/#_4","text":"\u53ca\u65e9\u62ab\u9732\u5b89\u5168\u6f0f\u6d1e \u6f0f\u6d1e\u62ab\u9732\u54cd\u5e94\u7b56\u7565","title":"\u6f0f\u6d1e\u54cd\u5e94"},{"location":"reference/security/#_5","text":"\u4e00\u822c\u4fe1\u606f \u4fdd\u5b89\u5de5\u4f5c\u5c0f\u7ec4\u7ae0\u7a0b","title":"\u5b89\u5168\u5de5\u4f5c\u5c0f\u7ec4"},{"location":"samples/","text":"Knative \u4ee3\u7801\u793a\u4f8b \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528Knative\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u5efa\u7acb\u548c\u8fd0\u884c\u5e38\u7528\u7528\u4f8b\u3002 Knative\u6240\u6709\u6837\u54c1 \u00b6 \u7531Knative\u5de5\u4f5c\u7ec4\u79ef\u6781\u6d4b\u8bd5\u548c\u7ef4\u62a4\u7684Knative\u4ee3\u7801\u793a\u4f8b: \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u670d\u52a1\u4ee3\u7801\u793a\u4f8b \u793e\u533a\u62e5\u6709\u7684\u6837\u54c1 \u00b6 \u4f7f\u7528\u4e00\u4e2a\u793e\u533a\u4ee3\u7801\u793a\u4f8b\u542f\u52a8\u5e76\u8fd0\u884c\u3002 \u8fd9\u4e9b\u6837\u672c\u662f\u7531Knative\u793e\u533a\u7684\u6210\u5458\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u3002 \u67e5\u770b\u7531\u793e\u533a\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u4ee3\u7801\u793a\u4f8b . Note \u8fd9\u4e9b\u6837\u672c\u53ef\u80fd\u4f1a\u8fc7\u65f6\uff0c\u6216\u8005\u539f\u59cb\u4f5c\u8005\u53ef\u80fd\u65e0\u6cd5\u7ef4\u6301\u4ed6\u4eec\u7684\u8d21\u732e\u3002\u5982\u679c\u4f60\u53d1\u73b0\u6709\u4ec0\u4e48\u4e1c\u897f\u4e0d\u80fd\u5de5\u4f5c\uff0c\u4f38\u51fa\u63f4\u52a9\u4e4b\u624b\uff0c\u5728\u516c\u5171\u5173\u7cfb\u4e2d\u89e3\u51b3\u5b83\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u6837\u672c\u5bff\u547d\u7684\u4fe1\u606f Sample Name Description Language(s) Hello World \u5feb\u901f\u4ecb\u7ecdKnative\u670d\u52a1\uff0c\u91cd\u70b9\u4ecb\u7ecd\u5982\u4f55\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Clojure , Dart , Elixir , Haskell , Java - Micronaut , Java - Quarkus , R - Go Server , Rust , Swift , Vertx Machine Learning \u5feb\u901f\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u670d\u52a1 Python - BentoML \u5916\u90e8\u4ee3\u7801\u793a\u4f8b \u00b6 \u4f4d\u4e8eKnative repos\u4e4b\u5916\u7684Knative\u4ee3\u7801\u793a\u4f8b\u94fe\u63a5\u5217\u8868: \u4f7f\u7528Knative\u4e8b\u4ef6\u56fe\u50cf\u5904\u7406\uff0c\u4e91\u8fd0\u884c\u5728GKE\u548c\u8c37\u6b4c\u4e91\u89c6\u89c9API Knative\u4e8b\u4ef6\u793a\u4f8b knfun \u5f00\u59cb\u4f7f\u7528Knative 2020 \u56fe\u50cf\u5904\u7406\u7ba1\u9053 BigQuery\u5904\u7406\u7ba1\u9053 \u4f7f\u7528SLOs\u8fdb\u884c\u7b80\u5355\u7684\u6027\u80fd\u6d4b\u8bd5 Tip \u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u94fe\u63a5\u5230\u60a8\u7684\u5916\u90e8\u6258\u7ba1Knative\u4ee3\u7801\u793a\u4f8b\u3002","title":"\u4ee3\u7801\u793a\u4f8b\u6982\u8ff0"},{"location":"samples/#knative","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528Knative\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u5efa\u7acb\u548c\u8fd0\u884c\u5e38\u7528\u7528\u4f8b\u3002","title":"Knative \u4ee3\u7801\u793a\u4f8b"},{"location":"samples/#knative_1","text":"\u7531Knative\u5de5\u4f5c\u7ec4\u79ef\u6781\u6d4b\u8bd5\u548c\u7ef4\u62a4\u7684Knative\u4ee3\u7801\u793a\u4f8b: \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u670d\u52a1\u4ee3\u7801\u793a\u4f8b","title":"Knative\u6240\u6709\u6837\u54c1"},{"location":"samples/#_1","text":"\u4f7f\u7528\u4e00\u4e2a\u793e\u533a\u4ee3\u7801\u793a\u4f8b\u542f\u52a8\u5e76\u8fd0\u884c\u3002 \u8fd9\u4e9b\u6837\u672c\u662f\u7531Knative\u793e\u533a\u7684\u6210\u5458\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u3002 \u67e5\u770b\u7531\u793e\u533a\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u4ee3\u7801\u793a\u4f8b . Note \u8fd9\u4e9b\u6837\u672c\u53ef\u80fd\u4f1a\u8fc7\u65f6\uff0c\u6216\u8005\u539f\u59cb\u4f5c\u8005\u53ef\u80fd\u65e0\u6cd5\u7ef4\u6301\u4ed6\u4eec\u7684\u8d21\u732e\u3002\u5982\u679c\u4f60\u53d1\u73b0\u6709\u4ec0\u4e48\u4e1c\u897f\u4e0d\u80fd\u5de5\u4f5c\uff0c\u4f38\u51fa\u63f4\u52a9\u4e4b\u624b\uff0c\u5728\u516c\u5171\u5173\u7cfb\u4e2d\u89e3\u51b3\u5b83\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u6837\u672c\u5bff\u547d\u7684\u4fe1\u606f Sample Name Description Language(s) Hello World \u5feb\u901f\u4ecb\u7ecdKnative\u670d\u52a1\uff0c\u91cd\u70b9\u4ecb\u7ecd\u5982\u4f55\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Clojure , Dart , Elixir , Haskell , Java - Micronaut , Java - Quarkus , R - Go Server , Rust , Swift , Vertx Machine Learning \u5feb\u901f\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u670d\u52a1 Python - BentoML","title":"\u793e\u533a\u62e5\u6709\u7684\u6837\u54c1"},{"location":"samples/#_2","text":"\u4f4d\u4e8eKnative repos\u4e4b\u5916\u7684Knative\u4ee3\u7801\u793a\u4f8b\u94fe\u63a5\u5217\u8868: \u4f7f\u7528Knative\u4e8b\u4ef6\u56fe\u50cf\u5904\u7406\uff0c\u4e91\u8fd0\u884c\u5728GKE\u548c\u8c37\u6b4c\u4e91\u89c6\u89c9API Knative\u4e8b\u4ef6\u793a\u4f8b knfun \u5f00\u59cb\u4f7f\u7528Knative 2020 \u56fe\u50cf\u5904\u7406\u7ba1\u9053 BigQuery\u5904\u7406\u7ba1\u9053 \u4f7f\u7528SLOs\u8fdb\u884c\u7b80\u5355\u7684\u6027\u80fd\u6d4b\u8bd5 Tip \u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u94fe\u63a5\u5230\u60a8\u7684\u5916\u90e8\u6258\u7ba1Knative\u4ee3\u7801\u793a\u4f8b\u3002","title":"\u5916\u90e8\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/eventing/","text":"Knative \u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3 Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6e90\u7684\u5404\u79cd\u7528\u4f8b\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6765\u6e90 . \u53c2\u89c1 GitHub \u4e2d\u7684 \u6240\u6709 Knative \u4ee3\u7801\u793a\u4f8b \u3002 \u540d\u79f0 \u63cf\u8ff0 \u8bed\u8a00 Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go and Python CloudAuditLogsSource \u914d\u7f6e\u4e00\u4e2a CloudAuditLogsSource \u8d44\u6e90\uff0c\u4ece\u4e91\u5ba1\u8ba1\u65e5\u5fd7\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u4ee5 CloudEvents \u683c\u5f0f\u76f4\u63a5\u53d1\u5e03\u5230\u5e95\u5c42\u4f20\u8f93(Pub/Sub)\u3002 YAML CloudPubSubSource \u914d\u7f6e\u4e00\u4e2a CloudPubSubSource\uff0c\u4f7f\u5176\u6bcf\u6b21\u5728\u4e91\u53d1\u5e03/\u8ba2\u9605\u4e3b\u9898\u4e0a\u53d1\u5e03\u6d88\u606f\u65f6\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u6b64\u6e90\u4f7f\u7528\u4e0e push \u517c\u5bb9\u7684\u683c\u5f0f\u53d1\u9001\u4e8b\u4ef6\u3002 YAML CloudSchedulerSource \u914d\u7f6e CloudSchedulerSource \u8d44\u6e90\uff0c\u7528\u4e8e\u4ece\u8c37\u6b4c CloudScheduler \u63a5\u6536\u9884\u5b9a\u4e8b\u4ef6\u3002 YAML CloudStorageSource \u914d\u7f6e CloudStorageSource \u8d44\u6e90\uff0c\u5f53\u4e00\u4e2a\u65b0\u5bf9\u8c61\u88ab\u6dfb\u52a0\u5230\u8c37\u6b4c\u4e91\u5b58\u50a8(GCS)\u65f6\uff0c\u8be5\u8d44\u6e90\u5c06\u4e0b\u53d1\u5bf9\u8c61\u901a\u77e5\u3002 YAML GitHub source \u5c55\u793a\u5982\u4f55\u8fde\u63a5 GitHub \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML GitLab source \u6f14\u793a\u5982\u4f55\u8fde\u63a5 GitLab \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML Apache Kafka Binding KafkaBinding \u8d1f\u8d23\u5c06 Kafka \u5f15\u5bfc\u8fde\u63a5\u4fe1\u606f\u6ce8\u5165\u5230\u5d4c\u5165 PodSpec(\u5982 spec.template.spec )\u7684 Kubernetes \u8d44\u6e90\u4e2d\u3002\u8fd9\u4f7f\u5f97 Kafka \u5ba2\u6237\u7aef\u53ef\u4ee5\u8f7b\u677e\u5f15\u5bfc\u3002 YAML Apache Kafka Channel \u5b89\u88c5\u5e76\u914d\u7f6e Apache Kafka \u901a\u9053\u4f5c\u4e3a Knative event \u7684\u9ed8\u8ba4\u901a\u9053\u914d\u7f6e\u3002 YAML \u4f7f\u7528JavaScript\u7f16\u5199\u4e8b\u4ef6\u6e90 \u672c\u6559\u7a0b\u63d0\u4f9b\u4e86\u5982\u4f55\u7528 JavaScript \u6784\u5efa\u4e8b\u4ef6\u6e90\u5e76\u4f7f\u7528 ContainerSource \u6216 SinkBinding \u5b9e\u73b0\u5b83\u7684\u6307\u5bfc\u3002 JavaScript Parallel with \u591a\u60c5\u51b5\u4e0b \u7528\u4e24\u4e2a\u5206\u652f\u521b\u5efa\u4e00\u4e2a Parallel\u3002 YAML Parallel with \u4e92\u65a5\u7684\u60c5\u51b5 \u521b\u5efa\u5177\u6709\u4e92\u65a5\u5206\u652f\u7684 Parallel\u3002 YAML","title":"\u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/eventing/#knative","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3 Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6e90\u7684\u5404\u79cd\u7528\u4f8b\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6765\u6e90 . \u53c2\u89c1 GitHub \u4e2d\u7684 \u6240\u6709 Knative \u4ee3\u7801\u793a\u4f8b \u3002 \u540d\u79f0 \u63cf\u8ff0 \u8bed\u8a00 Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go and Python CloudAuditLogsSource \u914d\u7f6e\u4e00\u4e2a CloudAuditLogsSource \u8d44\u6e90\uff0c\u4ece\u4e91\u5ba1\u8ba1\u65e5\u5fd7\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u4ee5 CloudEvents \u683c\u5f0f\u76f4\u63a5\u53d1\u5e03\u5230\u5e95\u5c42\u4f20\u8f93(Pub/Sub)\u3002 YAML CloudPubSubSource \u914d\u7f6e\u4e00\u4e2a CloudPubSubSource\uff0c\u4f7f\u5176\u6bcf\u6b21\u5728\u4e91\u53d1\u5e03/\u8ba2\u9605\u4e3b\u9898\u4e0a\u53d1\u5e03\u6d88\u606f\u65f6\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u6b64\u6e90\u4f7f\u7528\u4e0e push \u517c\u5bb9\u7684\u683c\u5f0f\u53d1\u9001\u4e8b\u4ef6\u3002 YAML CloudSchedulerSource \u914d\u7f6e CloudSchedulerSource \u8d44\u6e90\uff0c\u7528\u4e8e\u4ece\u8c37\u6b4c CloudScheduler \u63a5\u6536\u9884\u5b9a\u4e8b\u4ef6\u3002 YAML CloudStorageSource \u914d\u7f6e CloudStorageSource \u8d44\u6e90\uff0c\u5f53\u4e00\u4e2a\u65b0\u5bf9\u8c61\u88ab\u6dfb\u52a0\u5230\u8c37\u6b4c\u4e91\u5b58\u50a8(GCS)\u65f6\uff0c\u8be5\u8d44\u6e90\u5c06\u4e0b\u53d1\u5bf9\u8c61\u901a\u77e5\u3002 YAML GitHub source \u5c55\u793a\u5982\u4f55\u8fde\u63a5 GitHub \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML GitLab source \u6f14\u793a\u5982\u4f55\u8fde\u63a5 GitLab \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML Apache Kafka Binding KafkaBinding \u8d1f\u8d23\u5c06 Kafka \u5f15\u5bfc\u8fde\u63a5\u4fe1\u606f\u6ce8\u5165\u5230\u5d4c\u5165 PodSpec(\u5982 spec.template.spec )\u7684 Kubernetes \u8d44\u6e90\u4e2d\u3002\u8fd9\u4f7f\u5f97 Kafka \u5ba2\u6237\u7aef\u53ef\u4ee5\u8f7b\u677e\u5f15\u5bfc\u3002 YAML Apache Kafka Channel \u5b89\u88c5\u5e76\u914d\u7f6e Apache Kafka \u901a\u9053\u4f5c\u4e3a Knative event \u7684\u9ed8\u8ba4\u901a\u9053\u914d\u7f6e\u3002 YAML \u4f7f\u7528JavaScript\u7f16\u5199\u4e8b\u4ef6\u6e90 \u672c\u6559\u7a0b\u63d0\u4f9b\u4e86\u5982\u4f55\u7528 JavaScript \u6784\u5efa\u4e8b\u4ef6\u6e90\u5e76\u4f7f\u7528 ContainerSource \u6216 SinkBinding \u5b9e\u73b0\u5b83\u7684\u6307\u5bfc\u3002 JavaScript Parallel with \u591a\u60c5\u51b5\u4e0b \u7528\u4e24\u4e2a\u5206\u652f\u521b\u5efa\u4e00\u4e2a Parallel\u3002 YAML Parallel with \u4e92\u65a5\u7684\u60c5\u51b5 \u521b\u5efa\u5177\u6709\u4e92\u65a5\u5206\u652f\u7684 Parallel\u3002 YAML","title":"Knative \u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/serving/","text":"Knative Serving code samples \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3\u5404\u79cdKnative\u670d\u52a1\u8d44\u6e90\uff0c\u4ee5\u53ca\u5982\u4f55\u8de8\u5e38\u7528\u7528\u4f8b\u5e94\u7528\u5b83\u4eec\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8eKnative\u670d\u52a1\u7684\u4fe1\u606f \u3002 \u53c2\u89c1GitHub\u4e2d\u7684 \u6240\u6709Knative\u4ee3\u7801\u793a\u4f8b \u3002 \u540d\u79f0 \u63cf\u8ff0 \u8bed\u8a00 Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell \u4e91\u4e8b\u4ef6 \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u53d1\u9001\u548c\u63a5\u6536\u4e91\u4e8b\u4ef6\u3002 C# , Go , Node.js , Rust , Java (Vert.x) \u5206\u6d41 \u4e00\u4e2a\u624b\u5de5\u6d41\u91cf\u5206\u5272\u7684\u4f8b\u5b50\u3002 YAML \u9ad8\u7ea7\u90e8\u7f72 \u7b80\u5355\u7684\u84dd\u8272/\u7eff\u8272\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u6a21\u5f0f\u6f14\u793a\u4e86\u5728\u4e0d\u51cf\u5c11\u4efb\u4f55\u6d41\u91cf\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u52a8\u6001\u5e94\u7528\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u3002 YAML Autoscale Knative\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 Go Github Webhook \u4e00\u4e2a\u7b80\u5355\u7684webhook\u5904\u7406\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u4e0eGithub\u7684\u4ea4\u4e92\u3002 Go gRPC \u4e00\u4e2a\u7b80\u5355\u7684gRPC\u670d\u52a1\u5668\u3002 Go Knative \u8def\u7531 \u4f7f\u7528Istio VirtualService\u6982\u5ff5\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e2a\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Kong \u8def\u7531 \u4f7f\u7528Kong API\u7f51\u5173\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e00\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Knative Secrets \u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Kubernetes\u7684Secrets\u4f5c\u4e3aKnative\u4e2d\u7684\u5377\u3002 Go \u591a\u7684\u5bb9\u5668 \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative\u670d\u52a1\u4e3a\u591a\u4e2a\u5bb9\u5668\u6784\u5efa\u548c\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go","title":"\u670d\u52a1\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/serving/#knative-serving-code-samples","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3\u5404\u79cdKnative\u670d\u52a1\u8d44\u6e90\uff0c\u4ee5\u53ca\u5982\u4f55\u8de8\u5e38\u7528\u7528\u4f8b\u5e94\u7528\u5b83\u4eec\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8eKnative\u670d\u52a1\u7684\u4fe1\u606f \u3002 \u53c2\u89c1GitHub\u4e2d\u7684 \u6240\u6709Knative\u4ee3\u7801\u793a\u4f8b \u3002 \u540d\u79f0 \u63cf\u8ff0 \u8bed\u8a00 Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell \u4e91\u4e8b\u4ef6 \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u53d1\u9001\u548c\u63a5\u6536\u4e91\u4e8b\u4ef6\u3002 C# , Go , Node.js , Rust , Java (Vert.x) \u5206\u6d41 \u4e00\u4e2a\u624b\u5de5\u6d41\u91cf\u5206\u5272\u7684\u4f8b\u5b50\u3002 YAML \u9ad8\u7ea7\u90e8\u7f72 \u7b80\u5355\u7684\u84dd\u8272/\u7eff\u8272\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u6a21\u5f0f\u6f14\u793a\u4e86\u5728\u4e0d\u51cf\u5c11\u4efb\u4f55\u6d41\u91cf\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u52a8\u6001\u5e94\u7528\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u3002 YAML Autoscale Knative\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 Go Github Webhook \u4e00\u4e2a\u7b80\u5355\u7684webhook\u5904\u7406\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u4e0eGithub\u7684\u4ea4\u4e92\u3002 Go gRPC \u4e00\u4e2a\u7b80\u5355\u7684gRPC\u670d\u52a1\u5668\u3002 Go Knative \u8def\u7531 \u4f7f\u7528Istio VirtualService\u6982\u5ff5\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e2a\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Kong \u8def\u7531 \u4f7f\u7528Kong API\u7f51\u5173\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e00\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Knative Secrets \u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Kubernetes\u7684Secrets\u4f5c\u4e3aKnative\u4e2d\u7684\u5377\u3002 Go \u591a\u7684\u5bb9\u5668 \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative\u670d\u52a1\u4e3a\u591a\u4e2a\u5bb9\u5668\u6784\u5efa\u548c\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go","title":"Knative Serving code samples"},{"location":"serving/","text":"Knative \u670d\u52a1 \u00b6 Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002 \u5e38\u89c1\u7528\u4f8b \u00b6 \u652f\u6301\u7684Knative\u670d\u52a1\u7528\u4f8b\u793a\u4f8b: \u5feb\u901f\u90e8\u7f72\u65e0\u670d\u52a1\u5668\u5bb9\u5668\u3002 \u81ea\u52a8\u7f29\u653e\uff0c\u5305\u62ec\u5c06\u8c46\u835a\u7f29\u653e\u5230\u96f6\u3002 \u652f\u6301\u591a\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Contour\u3001Kourier\u548cIstio\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 Knative\u670d\u52a1\u540c\u65f6\u652f\u6301HTTP\u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002 \u5b89\u88c5 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u670d\u52a1. \u5f00\u59cb \u00b6 \u8981\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00\u3002 \u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u5b83\u4e3a\u4f60\u7ba1\u7406\u6240\u6709\u7684\u7ec6\u8282\u3002 \u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002 \u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a \u00b6 Knative\u670d\u52a1\u4ee3\u7801\u793a\u4f8b \u8c03\u8bd5Knative\u670d\u52a1\u95ee\u9898 \u00b6 \u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898 \u914d\u7f6e\u548c\u7f51\u7edc \u00b6 \u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df \u4ea4\u901a\u7ba1\u7406 \u53ef\u89c2\u5bdf\u6027 \u00b6 \u670d\u52a1\u6807\u51c6API \u5df2\u77e5\u7684\u95ee\u9898 \u00b6 \u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u89c1 Knative\u670d\u52a1\u95ee\u9898 \u9875\u9762\u3002","title":"\u670d\u52a1\u6982\u8ff0"},{"location":"serving/#knative","text":"Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRDs)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002","title":"Knative \u670d\u52a1"},{"location":"serving/#_1","text":"\u652f\u6301\u7684Knative\u670d\u52a1\u7528\u4f8b\u793a\u4f8b: \u5feb\u901f\u90e8\u7f72\u65e0\u670d\u52a1\u5668\u5bb9\u5668\u3002 \u81ea\u52a8\u7f29\u653e\uff0c\u5305\u62ec\u5c06\u8c46\u835a\u7f29\u653e\u5230\u96f6\u3002 \u652f\u6301\u591a\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Contour\u3001Kourier\u548cIstio\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 Knative\u670d\u52a1\u540c\u65f6\u652f\u6301HTTP\u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002","title":"\u5e38\u89c1\u7528\u4f8b"},{"location":"serving/#_2","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u670d\u52a1.","title":"\u5b89\u88c5"},{"location":"serving/#_3","text":"\u8981\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00\u3002 \u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u5b83\u4e3a\u4f60\u7ba1\u7406\u6240\u6709\u7684\u7ec6\u8282\u3002 \u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002 \u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002","title":"\u5f00\u59cb"},{"location":"serving/#_4","text":"Knative\u670d\u52a1\u4ee3\u7801\u793a\u4f8b","title":"\u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a"},{"location":"serving/#knative_1","text":"\u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898","title":"\u8c03\u8bd5Knative\u670d\u52a1\u95ee\u9898"},{"location":"serving/#_5","text":"\u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df \u4ea4\u901a\u7ba1\u7406","title":"\u914d\u7f6e\u548c\u7f51\u7edc"},{"location":"serving/#_6","text":"\u670d\u52a1\u6807\u51c6API","title":"\u53ef\u89c2\u5bdf\u6027"},{"location":"serving/#_7","text":"\u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u89c1 Knative\u670d\u52a1\u95ee\u9898 \u9875\u9762\u3002","title":"\u5df2\u77e5\u7684\u95ee\u9898"},{"location":"serving/accessing-traces/","text":"Accessing request traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests. Configuring Traces \u00b6 You can update the configuration file for tracing in tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or none). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing Zipkin \u00b6 In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call. Jaeger \u00b6 In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"\u8bbf\u95ee\u8bf7\u6c42\u7684\u75d5\u8ff9"},{"location":"serving/accessing-traces/#accessing-request-traces","text":"Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#configuring-traces","text":"You can update the configuration file for tracing in tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or none). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing","title":"Configuring Traces"},{"location":"serving/accessing-traces/#zipkin","text":"In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.","title":"Zipkin"},{"location":"serving/accessing-traces/#jaeger","text":"In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Jaeger"},{"location":"serving/config-ha/","text":"Configuring high-availability components \u00b6 Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica. Disabling leader election \u00b6 For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\". Scaling the control plane \u00b6 With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions. Scaling the data plane \u00b6 The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"\u9ad8\u53ef\u7528\u6027\u914d\u7f6e\u7ec4\u4ef6"},{"location":"serving/config-ha/#configuring-high-availability-components","text":"Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#disabling-leader-election","text":"For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\".","title":"Disabling leader election"},{"location":"serving/config-ha/#scaling-the-control-plane","text":"With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions.","title":"Scaling the control plane"},{"location":"serving/config-ha/#scaling-the-data-plane","text":"The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Scaling the data plane"},{"location":"serving/convert-deployment-to-knative-service/","text":"\u5c06Kubernetes\u90e8\u7f72\u8f6c\u6362\u4e3aKnative\u670d\u52a1 \u00b6 \u672c\u4e3b\u9898\u5c55\u793a\u5982\u4f55\u5c06Kubernetes\u90e8\u7f72\u8f6c\u6362\u4e3aKnative\u670d\u52a1\u3002 \u597d\u5904 \u00b6 \u8f6c\u6362\u4e3aKnative\u670d\u52a1\u6709\u4ee5\u4e0b\u597d\u5904: \u51cf\u5c11\u670d\u52a1\u5b9e\u4f8b\u7684\u5360\u7528\u7a7a\u95f4\uff0c\u56e0\u4e3a\u5f53\u5b9e\u4f8b\u53d8\u4e3a\u7a7a\u95f2\u65f6\uff0c\u5b83\u4f1a\u6269\u5c55\u52300\u3002 \u7531\u4e8eKnative\u670d\u52a1\u7684\u5185\u7f6e\u81ea\u52a8\u4f38\u7f29\uff0c\u56e0\u6b64\u63d0\u9ad8\u4e86\u6027\u80fd\u3002 \u786e\u5b9a\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u662f\u5426\u9002\u5408Knative \u00b6 \u4e00\u822c\u6765\u8bf4\uff0c\u5982\u679c\u60a8\u7684Kubernetes\u5de5\u4f5c\u8d1f\u8f7d\u975e\u5e38\u9002\u5408Knative\uff0c\u60a8\u53ef\u4ee5\u5220\u9664\u5927\u91cf\u6e05\u5355\u6765\u521b\u5efaKnative Service\u3002 \u4f60\u9700\u8981\u8003\u8651\u4e09\u4e2a\u65b9\u9762: \u6240\u6709\u5b8c\u6210\u7684\u5de5\u4f5c\u90fd\u7531HTTP\u89e6\u53d1\u3002 \u5bb9\u5668\u662f\u65e0\u72b6\u6001\u7684\u3002\u6240\u6709\u72b6\u6001\u90fd\u5b58\u50a8\u5728\u5176\u4ed6\u5730\u65b9\uff0c\u6216\u8005\u53ef\u4ee5\u91cd\u65b0\u521b\u5efa\u3002 \u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u53ea\u4f7f\u7528Secret\u548cConfigMap\u5377\u3002 \u8f6c\u6362\u793a\u4f8b \u00b6 \u4e0b\u9762\u7684\u4f8b\u5b50\u5c55\u793a\u4e86 Kubernetes Nginx\u90e8\u7f72\u548c\u670d\u52a1 , \u5e76\u5c55\u793a\u4e86\u5b83\u5982\u4f55\u8f6c\u6362\u4e3aKnative\u670d\u52a1\u3002 Kubernetes Nginx \u90e8\u7f72\u548c\u670d\u52a1 \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx Knative \u670d\u52a1 \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"\u5c06K8S\u90e8\u7f72\u8f6c\u4e3aKnative\u670d\u52a1"},{"location":"serving/convert-deployment-to-knative-service/#kubernetesknative","text":"\u672c\u4e3b\u9898\u5c55\u793a\u5982\u4f55\u5c06Kubernetes\u90e8\u7f72\u8f6c\u6362\u4e3aKnative\u670d\u52a1\u3002","title":"\u5c06Kubernetes\u90e8\u7f72\u8f6c\u6362\u4e3aKnative\u670d\u52a1"},{"location":"serving/convert-deployment-to-knative-service/#_1","text":"\u8f6c\u6362\u4e3aKnative\u670d\u52a1\u6709\u4ee5\u4e0b\u597d\u5904: \u51cf\u5c11\u670d\u52a1\u5b9e\u4f8b\u7684\u5360\u7528\u7a7a\u95f4\uff0c\u56e0\u4e3a\u5f53\u5b9e\u4f8b\u53d8\u4e3a\u7a7a\u95f2\u65f6\uff0c\u5b83\u4f1a\u6269\u5c55\u52300\u3002 \u7531\u4e8eKnative\u670d\u52a1\u7684\u5185\u7f6e\u81ea\u52a8\u4f38\u7f29\uff0c\u56e0\u6b64\u63d0\u9ad8\u4e86\u6027\u80fd\u3002","title":"\u597d\u5904"},{"location":"serving/convert-deployment-to-knative-service/#knative","text":"\u4e00\u822c\u6765\u8bf4\uff0c\u5982\u679c\u60a8\u7684Kubernetes\u5de5\u4f5c\u8d1f\u8f7d\u975e\u5e38\u9002\u5408Knative\uff0c\u60a8\u53ef\u4ee5\u5220\u9664\u5927\u91cf\u6e05\u5355\u6765\u521b\u5efaKnative Service\u3002 \u4f60\u9700\u8981\u8003\u8651\u4e09\u4e2a\u65b9\u9762: \u6240\u6709\u5b8c\u6210\u7684\u5de5\u4f5c\u90fd\u7531HTTP\u89e6\u53d1\u3002 \u5bb9\u5668\u662f\u65e0\u72b6\u6001\u7684\u3002\u6240\u6709\u72b6\u6001\u90fd\u5b58\u50a8\u5728\u5176\u4ed6\u5730\u65b9\uff0c\u6216\u8005\u53ef\u4ee5\u91cd\u65b0\u521b\u5efa\u3002 \u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u53ea\u4f7f\u7528Secret\u548cConfigMap\u5377\u3002","title":"\u786e\u5b9a\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u662f\u5426\u9002\u5408Knative"},{"location":"serving/convert-deployment-to-knative-service/#_2","text":"\u4e0b\u9762\u7684\u4f8b\u5b50\u5c55\u793a\u4e86 Kubernetes Nginx\u90e8\u7f72\u548c\u670d\u52a1 , \u5e76\u5c55\u793a\u4e86\u5b83\u5982\u4f55\u8f6c\u6362\u4e3aKnative\u670d\u52a1\u3002","title":"\u8f6c\u6362\u793a\u4f8b"},{"location":"serving/convert-deployment-to-knative-service/#kubernetes-nginx","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx","title":"Kubernetes Nginx \u90e8\u7f72\u548c\u670d\u52a1"},{"location":"serving/convert-deployment-to-knative-service/#knative_1","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"Knative \u670d\u52a1"},{"location":"serving/deploying-from-private-registry/","text":"Deploying images from a private container registry \u00b6 You can configure your Knative cluster to deploy images from a private registry across multiple Services and Revisions. To do this, you must create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. You must then add those secrets to the default service account for all Services, or the Revision template for a single Service. Prerequisites \u00b6 You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored. Procedure \u00b6 Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Optional. After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Optional. Add the imagePullSecrets object to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry. Optional. Add the imagePullSecrets object to a Service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : imagePullSecrets : - name : <secret-name> containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\"","title":"\u4ece\u79c1\u6709\u6ce8\u518c\u4e2d\u5fc3\u90e8\u7f72"},{"location":"serving/deploying-from-private-registry/#deploying-images-from-a-private-container-registry","text":"You can configure your Knative cluster to deploy images from a private registry across multiple Services and Revisions. To do this, you must create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. You must then add those secrets to the default service account for all Services, or the Revision template for a single Service.","title":"Deploying images from a private container registry"},{"location":"serving/deploying-from-private-registry/#prerequisites","text":"You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored.","title":"Prerequisites"},{"location":"serving/deploying-from-private-registry/#procedure","text":"Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Optional. After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Optional. Add the imagePullSecrets object to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry. Optional. Add the imagePullSecrets object to a Service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : imagePullSecrets : - name : <secret-name> containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\"","title":"Procedure"},{"location":"serving/istio-authorization/","text":"Enabling requests to Knative services when additional authorization policies are enabled \u00b6 Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods. Before you begin \u00b6 You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation . Mutual TLS in Knative \u00b6 Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ] Health checking and metrics collection \u00b6 In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths. Allowing access from system pods by paths \u00b6 Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u6388\u6743\u7b56\u7565"},{"location":"serving/istio-authorization/#enabling-requests-to-knative-services-when-additional-authorization-policies-are-enabled","text":"Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#before-you-begin","text":"You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation .","title":"Before you begin"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","text":"Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ]","title":"Mutual TLS in Knative"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","text":"In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths.","title":"Health checking and metrics collection"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","text":"Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Allowing access from system pods by paths"},{"location":"serving/knative-kubernetes-services/","text":"Kubernetes services \u00b6 This guide describes the Kubernetes Services that are active when running Knative Serving. Before You Begin \u00b6 This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function. Components \u00b6 Service: activator \u00b6 The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics. Service: autoscaler \u00b6 The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic. Service: controller \u00b6 The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs). Service: webhook \u00b6 The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsistent and invalid objects, and validates and mutates Kubernetes API calls. Deployment: net-certmanager-controller \u00b6 The certmanager reconciles cluster ingresses into cert manager objects. Deployment: net-istio-controller \u00b6 The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service . What's Next \u00b6 For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"Kubernetes \u670d\u52a1"},{"location":"serving/knative-kubernetes-services/#kubernetes-services","text":"This guide describes the Kubernetes Services that are active when running Knative Serving.","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#before-you-begin","text":"This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function.","title":"Before You Begin"},{"location":"serving/knative-kubernetes-services/#components","text":"","title":"Components"},{"location":"serving/knative-kubernetes-services/#service-activator","text":"The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.","title":"Service: activator"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","text":"The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.","title":"Service: autoscaler"},{"location":"serving/knative-kubernetes-services/#service-controller","text":"The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).","title":"Service: controller"},{"location":"serving/knative-kubernetes-services/#service-webhook","text":"The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsistent and invalid objects, and validates and mutates Kubernetes API calls.","title":"Service: webhook"},{"location":"serving/knative-kubernetes-services/#deployment-net-certmanager-controller","text":"The certmanager reconciles cluster ingresses into cert manager objects.","title":"Deployment: net-certmanager-controller"},{"location":"serving/knative-kubernetes-services/#deployment-net-istio-controller","text":"The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service .","title":"Deployment: net-istio-controller"},{"location":"serving/knative-kubernetes-services/#whats-next","text":"For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"What's Next"},{"location":"serving/queue-extensions/","text":"\u7528QPOptions\u6269\u5c55\u961f\u5217\u4ee3\u7406\u6620\u50cf \u00b6 Knative\u670d\u52a1pods\u5305\u62ec\u4e24\u4e2a\u5bb9\u5668: \u7528\u6237\u4e3b\u670d\u52a1\u5bb9\u5668\uff0c\u547d\u540d\u4e3a user-container \u961f\u5217\u4ee3\u7406 - \u4e00\u4e2a\u540d\u4e3a queue-proxy \u7684sidecar\uff0c\u5728 user-container \u524d\u9762\u5145\u5f53\u53cd\u5411\u4ee3\u7406\u3002 \u53ef\u4ee5\u6269\u5c55\u961f\u5217\u4ee3\u7406\u4ee5\u63d0\u4f9b\u5176\u4ed6\u529f\u80fd\u3002\u961f\u5217\u4ee3\u7406\u7684QPOptions\u7279\u6027\u5141\u8bb8\u9644\u52a0\u7684\u8fd0\u884c\u65f6\u5305\u6269\u5c55\u961f\u5217\u4ee3\u7406\u529f\u80fd\u3002 For example, the security-guard repository provides an extension that uses the QPOptions feature. The QPOption package enables users to add additional security features to Queue Proxy. \u53ef\u7528\u7684\u8fd0\u884c\u65f6\u7279\u6027\u5728\u6784\u5efa\u961f\u5217\u4ee3\u7406\u6620\u50cf\u65f6\u786e\u5b9a\u3002 \u961f\u5217\u4ee3\u7406\u5b9a\u4e49\u4e86\u6fc0\u6d3b\u548c\u914d\u7f6e\u6269\u5c55\u7684\u6709\u5e8f\u65b9\u5f0f\u3002 \u989d\u5916\u7684\u4fe1\u606f \u00b6 Enabling Queue Proxy Pod Info - discussing a necessary step to enable the use of extensions. Using extensions enabled by QPOptions - discussing how to configure a service to use features implemented in extensions. \u6dfb\u52a0\u6269\u5c55 \u00b6 You can add extensions by replacing the cmd/queue/main.go file before the Queue Proxy image is built. The following example shows a cmd/queue/main.go file that adds the test-gate extension: package main import \"os\" import \"knative.dev/serving/pkg/queue/sharedmain\" import \"knative.dev/security-guard/pkg/qpoption\" import _ \"knative.dev/security-guard/pkg/test-gate\" func main () { qOpt := qpoption . NewQPSecurityPlugs () defer qOpt . Shutdown () if sharedmain . Main ( qOpt . Setup ) != nil { os . Exit ( 1 ) } }","title":"\u7528QPOptions\u6269\u5c55\u961f\u5217\u4ee3\u7406\u6620\u50cf"},{"location":"serving/queue-extensions/#qpoptions","text":"Knative\u670d\u52a1pods\u5305\u62ec\u4e24\u4e2a\u5bb9\u5668: \u7528\u6237\u4e3b\u670d\u52a1\u5bb9\u5668\uff0c\u547d\u540d\u4e3a user-container \u961f\u5217\u4ee3\u7406 - \u4e00\u4e2a\u540d\u4e3a queue-proxy \u7684sidecar\uff0c\u5728 user-container \u524d\u9762\u5145\u5f53\u53cd\u5411\u4ee3\u7406\u3002 \u53ef\u4ee5\u6269\u5c55\u961f\u5217\u4ee3\u7406\u4ee5\u63d0\u4f9b\u5176\u4ed6\u529f\u80fd\u3002\u961f\u5217\u4ee3\u7406\u7684QPOptions\u7279\u6027\u5141\u8bb8\u9644\u52a0\u7684\u8fd0\u884c\u65f6\u5305\u6269\u5c55\u961f\u5217\u4ee3\u7406\u529f\u80fd\u3002 For example, the security-guard repository provides an extension that uses the QPOptions feature. The QPOption package enables users to add additional security features to Queue Proxy. \u53ef\u7528\u7684\u8fd0\u884c\u65f6\u7279\u6027\u5728\u6784\u5efa\u961f\u5217\u4ee3\u7406\u6620\u50cf\u65f6\u786e\u5b9a\u3002 \u961f\u5217\u4ee3\u7406\u5b9a\u4e49\u4e86\u6fc0\u6d3b\u548c\u914d\u7f6e\u6269\u5c55\u7684\u6709\u5e8f\u65b9\u5f0f\u3002","title":"\u7528QPOptions\u6269\u5c55\u961f\u5217\u4ee3\u7406\u6620\u50cf"},{"location":"serving/queue-extensions/#_1","text":"Enabling Queue Proxy Pod Info - discussing a necessary step to enable the use of extensions. Using extensions enabled by QPOptions - discussing how to configure a service to use features implemented in extensions.","title":"\u989d\u5916\u7684\u4fe1\u606f"},{"location":"serving/queue-extensions/#_2","text":"You can add extensions by replacing the cmd/queue/main.go file before the Queue Proxy image is built. The following example shows a cmd/queue/main.go file that adds the test-gate extension: package main import \"os\" import \"knative.dev/serving/pkg/queue/sharedmain\" import \"knative.dev/security-guard/pkg/qpoption\" import _ \"knative.dev/security-guard/pkg/test-gate\" func main () { qOpt := qpoption . NewQPSecurityPlugs () defer qOpt . Shutdown () if sharedmain . Main ( qOpt . Setup ) != nil { os . Exit ( 1 ) } }","title":"\u6dfb\u52a0\u6269\u5c55"},{"location":"serving/rolling-out-latest-revision/","text":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\" \u8def\u7531\u66f4\u65b0\u72b6\u6001 \u00b6 \u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready \u591a\u4e2a\u5ef6\u5c55 \u00b6 \u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/rolling-out-latest-revision/#_1","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/rolling-out-latest-revision/#procedure","text":"You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/rolling-out-latest-revision/#_2","text":"\u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"\u8def\u7531\u66f4\u65b0\u72b6\u6001"},{"location":"serving/rolling-out-latest-revision/#_3","text":"\u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u591a\u4e2a\u5ef6\u5c55"},{"location":"serving/setting-up-custom-ingress-gateway/","text":"\u914d\u7f6e\u5165\u53e3\u7f51\u5173 \u00b6 Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows. \u66f4\u6362\u9ed8\u8ba4\u7684 istio-ingressgateway \u670d\u52a1 \u00b6 \u6b65\u9aa41: \u521b\u5efa\u7f51\u5173\u670d\u52a1\u548c\u90e8\u7f72\u5b9e\u4f8b \u00b6 You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway \u6b65\u9aa42: \u66f4\u65b0Knative\u7f51\u5173 \u00b6 Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly. \u6b65\u9aa43: \u66f4\u65b0\u7f51\u5173ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ] \u66ff\u6362 knative-ingress-gateway \u7f51\u5173 \u00b6 We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps. \u6b65\u9aa41:\u521b\u5efa\u7f51\u5173 \u00b6 Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u6b65\u9aa42:\u66f4\u65b0\u7f51\u5173ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"\u914d\u7f6e\u5165\u53e3\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#_1","text":"Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows.","title":"\u914d\u7f6e\u5165\u53e3\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#istio-ingressgateway","text":"","title":"\u66f4\u6362\u9ed8\u8ba4\u7684istio-ingressgateway\u670d\u52a1"},{"location":"serving/setting-up-custom-ingress-gateway/#1","text":"You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway","title":"\u6b65\u9aa41: \u521b\u5efa\u7f51\u5173\u670d\u52a1\u548c\u90e8\u7f72\u5b9e\u4f8b"},{"location":"serving/setting-up-custom-ingress-gateway/#2-knative","text":"Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly.","title":"\u6b65\u9aa42: \u66f4\u65b0Knative\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#3-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ]","title":"\u6b65\u9aa43: \u66f4\u65b0\u7f51\u5173ConfigMap"},{"location":"serving/setting-up-custom-ingress-gateway/#knative-ingress-gateway","text":"We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps.","title":"\u66ff\u6362knative-ingress-gateway\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#1_1","text":"Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u6b65\u9aa41:\u521b\u5efa\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#2configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"\u6b65\u9aa42:\u66f4\u65b0\u7f51\u5173ConfigMap"},{"location":"serving/tag-resolution/","text":"Tag resolution \u00b6 Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature. Custom certificates \u00b6 If you are using a registry that has a self-signed certificate, you must configure the default Knative Serving controller Deployment to trust that certificate. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately. Procedure \u00b6 If you are using a custom-certs secret that contains your CA certificates, add the following spec to the default Knative Serving controller Deployment: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. Create a secret in the knative-serving namespace that points to your root CA certificate, and then save the current Knative Serving controller Deployment: kubectl -n knative-serving create secret generic customca --from-file = ca.crt = /root/ca.crt kubectl -n knative-serving get deploy/controller -o yaml > knative-serving-controller.yaml Corporate proxy \u00b6 If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"\u6807\u7b7e\u7684\u51b3\u8bae"},{"location":"serving/tag-resolution/#tag-resolution","text":"Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature.","title":"Tag resolution"},{"location":"serving/tag-resolution/#custom-certificates","text":"If you are using a registry that has a self-signed certificate, you must configure the default Knative Serving controller Deployment to trust that certificate. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately.","title":"Custom certificates"},{"location":"serving/tag-resolution/#procedure","text":"If you are using a custom-certs secret that contains your CA certificates, add the following spec to the default Knative Serving controller Deployment: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. Create a secret in the knative-serving namespace that points to your root CA certificate, and then save the current Knative Serving controller Deployment: kubectl -n knative-serving create secret generic customca --from-file = ca.crt = /root/ca.crt kubectl -n knative-serving get deploy/controller -o yaml > knative-serving-controller.yaml","title":"Procedure"},{"location":"serving/tag-resolution/#corporate-proxy","text":"If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Corporate proxy"},{"location":"serving/traffic-management/","text":"Traffic management \u00b6 You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service. Using tags to create target URLs \u00b6 In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes. Traffic routing examples \u00b6 The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0 Routing and managing traffic by using the Knative CLI \u00b6 You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag revision-0001 = green --tag @latest = blue The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20 Routing and managing traffic with blue/green deployment \u00b6 You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy . Procedure \u00b6 Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"\u4ea4\u901a\u7ba1\u7406"},{"location":"serving/traffic-management/#traffic-management","text":"You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service.","title":"Traffic management"},{"location":"serving/traffic-management/#using-tags-to-create-target-urls","text":"In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes.","title":"Using tags to create target URLs"},{"location":"serving/traffic-management/#traffic-routing-examples","text":"The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0","title":"Traffic routing examples"},{"location":"serving/traffic-management/#routing-and-managing-traffic-by-using-the-knative-cli","text":"You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag revision-0001 = green --tag @latest = blue The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20","title":"Routing and managing traffic by using the Knative CLI"},{"location":"serving/traffic-management/#routing-and-managing-traffic-with-bluegreen-deployment","text":"You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy .","title":"Routing and managing traffic with blue/green deployment"},{"location":"serving/traffic-management/#procedure","text":"Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"Procedure"},{"location":"serving/using-a-custom-domain/","text":"\u914d\u7f6e\u57df\u540d \u00b6 \u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5355\u4e2aKnative\u670d\u52a1\u7684\u57df\uff0c\u6216\u8005\u4e3a\u96c6\u7fa4\u4e0a\u521b\u5efa\u7684\u6240\u6709\u670d\u52a1\u8bbe\u7f6e\u5168\u5c40\u9ed8\u8ba4\u57df\u3002 \u7f3a\u7701\u60c5\u51b5\u4e0b\uff0c\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u4e3a {route}.{namespace}.example.com \u3002 \u4e3a\u5355\u4e2aKnative\u670d\u52a1\u914d\u7f6e\u57df \u00b6 \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49\u5355\u4e2a\u670d\u52a1\u7684\u57df\uff0c\u8bf7\u53c2\u89c1\u6709\u5173 DomainMapping \u7684\u6587\u6863\u3002 \u4e3a\u96c6\u7fa4\u4e2d\u7684\u6240\u6709Knative\u670d\u52a1\u914d\u7f6e\u9ed8\u8ba4\u57df \u00b6 \u901a\u8fc7\u4fee\u6539 config-domain ConfigMap \uff0c\u53ef\u4ee5\u66f4\u6539\u96c6\u7fa4\u4e2d\u6240\u6709Knative\u670d\u52a1\u7684\u9ed8\u8ba4\u57df\u3002 \u8fc7\u7a0b \u00b6 \u5728\u9ed8\u8ba4\u6587\u672c\u7f16\u8f91\u5668\u4e2d\u6253\u5f00 config-domain ConfigMap kubectl edit configmap config-domain -n knative-serving \u7f16\u8f91\u6587\u4ef6\uff0c\u5c06 example.com \u66ff\u6362\u4e3a\u60a8\u60f3\u8981\u4f7f\u7528\u7684\u57df\uff0c\u7136\u540e\u5220\u9664 _example \u952e\u5e76\u4fdd\u5b58\u66f4\u6539\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c knative.dev \u88ab\u914d\u7f6e\u4e3a\u6240\u6709\u8def\u7531\u7684\u57df: apiVersion : v1 data : knative.dev : \"\" kind : ConfigMap [ ... ] \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u90e8\u7f72\uff0cKnative\u4f1a\u534f\u8c03\u5bf9ConfigMap\u6240\u505a\u7684\u66f4\u6539\uff0c\u5e76\u81ea\u52a8\u66f4\u65b0\u6240\u6709\u90e8\u7f72\u7684\u670d\u52a1\u548c\u8def\u7531\u7684\u4e3b\u673a\u540d\u3002 \u9a8c\u8bc1\u6b65\u9aa4 \u00b6 \u5c06\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa4\u3002 \u68c0\u7d22\u8def\u7531\u7684URL: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" \u5176\u4e2d <route-name> \u662f\u8def\u7531\u7684\u540d\u79f0\u3002 \u89c2\u5bdf\u5df2\u914d\u7f6e\u7684\u81ea\u5b9a\u4e49\u57df\u3002 \u53d1\u5e03\u60a8\u7684\u57df \u00b6 \u8981\u4f7f\u60a8\u7684\u57df\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u60a8\u5fc5\u987b\u66f4\u65b0\u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u4ee5\u6307\u5411\u60a8\u7684\u670d\u52a1\u5165\u53e3\u7684IP\u5730\u5740\u3002 \u4e3a\u547d\u540d\u7a7a\u95f4\u521b\u5efa \u901a\u914d\u7b26\u8bb0\u5f55 \uff0c\u5e76\u81ea\u5b9a\u4e49\u5165\u63a5\u53e3IP\u5730\u5740\u7684\u57df\uff0c\u8fd9\u5c06\u4f7f\u540c\u4e00\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u591a\u4e2a\u670d\u52a1\u7684\u4e3b\u673a\u540d\u80fd\u591f\u5de5\u4f5c\uff0c\u800c\u65e0\u9700\u521b\u5efa\u989d\u5916\u7684DNS\u6761\u76ee\u3002 *.default.knative.dev 59 IN A 35.237.28.44 \u521b\u5efa\u4e00\u4e2aA\u8bb0\u5f55\uff0c\u4ece\u5b8c\u5168\u9650\u5b9a\u7684\u57df\u540d\u6307\u5411\u60a8\u7684Knative\u7f51\u5173\u7684IP\u5730\u5740\u3002 \u9700\u8981\u4e3a\u521b\u5efa\u7684\u6bcf\u4e2aKnative\u670d\u52a1\u6216\u8def\u7531\u6267\u884c\u6b64\u6b65\u9aa4\u3002 helloworld-go.default.knative.dev 59 IN A 35.237.28.44 \u5728\u57df\u66f4\u65b0\u4f20\u64ad\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u90e8\u7f72\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u8bbf\u95ee\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u914d\u7f6e\u57df\u540d"},{"location":"serving/using-a-custom-domain/#_1","text":"\u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5355\u4e2aKnative\u670d\u52a1\u7684\u57df\uff0c\u6216\u8005\u4e3a\u96c6\u7fa4\u4e0a\u521b\u5efa\u7684\u6240\u6709\u670d\u52a1\u8bbe\u7f6e\u5168\u5c40\u9ed8\u8ba4\u57df\u3002 \u7f3a\u7701\u60c5\u51b5\u4e0b\uff0c\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u4e3a {route}.{namespace}.example.com \u3002","title":"\u914d\u7f6e\u57df\u540d"},{"location":"serving/using-a-custom-domain/#knative","text":"\u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49\u5355\u4e2a\u670d\u52a1\u7684\u57df\uff0c\u8bf7\u53c2\u89c1\u6709\u5173 DomainMapping \u7684\u6587\u6863\u3002","title":"\u4e3a\u5355\u4e2aKnative\u670d\u52a1\u914d\u7f6e\u57df"},{"location":"serving/using-a-custom-domain/#knative_1","text":"\u901a\u8fc7\u4fee\u6539 config-domain ConfigMap \uff0c\u53ef\u4ee5\u66f4\u6539\u96c6\u7fa4\u4e2d\u6240\u6709Knative\u670d\u52a1\u7684\u9ed8\u8ba4\u57df\u3002","title":"\u4e3a\u96c6\u7fa4\u4e2d\u7684\u6240\u6709Knative\u670d\u52a1\u914d\u7f6e\u9ed8\u8ba4\u57df"},{"location":"serving/using-a-custom-domain/#_2","text":"\u5728\u9ed8\u8ba4\u6587\u672c\u7f16\u8f91\u5668\u4e2d\u6253\u5f00 config-domain ConfigMap kubectl edit configmap config-domain -n knative-serving \u7f16\u8f91\u6587\u4ef6\uff0c\u5c06 example.com \u66ff\u6362\u4e3a\u60a8\u60f3\u8981\u4f7f\u7528\u7684\u57df\uff0c\u7136\u540e\u5220\u9664 _example \u952e\u5e76\u4fdd\u5b58\u66f4\u6539\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c knative.dev \u88ab\u914d\u7f6e\u4e3a\u6240\u6709\u8def\u7531\u7684\u57df: apiVersion : v1 data : knative.dev : \"\" kind : ConfigMap [ ... ] \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u90e8\u7f72\uff0cKnative\u4f1a\u534f\u8c03\u5bf9ConfigMap\u6240\u505a\u7684\u66f4\u6539\uff0c\u5e76\u81ea\u52a8\u66f4\u65b0\u6240\u6709\u90e8\u7f72\u7684\u670d\u52a1\u548c\u8def\u7531\u7684\u4e3b\u673a\u540d\u3002","title":"\u8fc7\u7a0b"},{"location":"serving/using-a-custom-domain/#_3","text":"\u5c06\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa4\u3002 \u68c0\u7d22\u8def\u7531\u7684URL: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" \u5176\u4e2d <route-name> \u662f\u8def\u7531\u7684\u540d\u79f0\u3002 \u89c2\u5bdf\u5df2\u914d\u7f6e\u7684\u81ea\u5b9a\u4e49\u57df\u3002","title":"\u9a8c\u8bc1\u6b65\u9aa4"},{"location":"serving/using-a-custom-domain/#_4","text":"\u8981\u4f7f\u60a8\u7684\u57df\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u60a8\u5fc5\u987b\u66f4\u65b0\u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u4ee5\u6307\u5411\u60a8\u7684\u670d\u52a1\u5165\u53e3\u7684IP\u5730\u5740\u3002 \u4e3a\u547d\u540d\u7a7a\u95f4\u521b\u5efa \u901a\u914d\u7b26\u8bb0\u5f55 \uff0c\u5e76\u81ea\u5b9a\u4e49\u5165\u63a5\u53e3IP\u5730\u5740\u7684\u57df\uff0c\u8fd9\u5c06\u4f7f\u540c\u4e00\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u591a\u4e2a\u670d\u52a1\u7684\u4e3b\u673a\u540d\u80fd\u591f\u5de5\u4f5c\uff0c\u800c\u65e0\u9700\u521b\u5efa\u989d\u5916\u7684DNS\u6761\u76ee\u3002 *.default.knative.dev 59 IN A 35.237.28.44 \u521b\u5efa\u4e00\u4e2aA\u8bb0\u5f55\uff0c\u4ece\u5b8c\u5168\u9650\u5b9a\u7684\u57df\u540d\u6307\u5411\u60a8\u7684Knative\u7f51\u5173\u7684IP\u5730\u5740\u3002 \u9700\u8981\u4e3a\u521b\u5efa\u7684\u6bcf\u4e2aKnative\u670d\u52a1\u6216\u8def\u7531\u6267\u884c\u6b64\u6b65\u9aa4\u3002 helloworld-go.default.knative.dev 59 IN A 35.237.28.44 \u5728\u57df\u66f4\u65b0\u4f20\u64ad\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u90e8\u7f72\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u8bbf\u95ee\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u53d1\u5e03\u60a8\u7684\u57df"},{"location":"serving/using-a-tls-cert/","text":"\u914d\u7f6eHTTPS\u4f7f\u7528TLS\u8bc1\u4e66 \u00b6 \u4e86\u89e3\u5982\u4f55\u5728Knative\u4e2d\u4f7f\u7528TLS\u8bc1\u4e66\u914d\u7f6e\u5b89\u5168HTTPS\u8fde\u63a5( TLS\u53d6\u4ee3SSL ). \u914d\u7f6e\u5b89\u5168HTTPS\u8fde\u63a5\uff0c\u4f7f\u60a8\u7684Knative\u670d\u52a1\u548c\u8def\u7531\u80fd\u591f \u7ec8\u6b62\u5916\u90e8TLS\u8fde\u63a5 . \u53ef\u4ee5\u914d\u7f6eKnative\u6765\u5904\u7406\u624b\u52a8\u6307\u5b9a\u7684\u8bc1\u4e66\uff0c\u4e5f\u53ef\u4ee5\u542f\u7528Knative\u6765\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0\u8bc1\u4e66\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Certbot \u6216 cert-manager \u4e24\u79cd\u65b9\u5f0f\u83b7\u53d6\u8bc1\u4e66\u3002 \u8fd9\u4e24\u4e2a\u5de5\u5177\u90fd\u652f\u6301TLS\u8bc1\u4e66\uff0c\u4f46\u5982\u679c\u60a8\u60f3\u542f\u7528Knative\u6765\u81ea\u52a8\u53d1\u653eTLS\u8bc1\u4e66\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\u548c\u914d\u7f6e\u8bc1\u4e66\u7ba1\u7406\u5668\u5de5\u5177: \u624b\u52a8\u83b7\u53d6\u548c\u66f4\u65b0\u8bc1\u4e66 : \u53ef\u4ee5\u4f7f\u7528Certbot\u548ccert-manager\u5de5\u5177\u624b\u52a8\u83b7\u53d6TLS\u8bc1\u4e66\u3002 \u901a\u5e38\uff0c\u5728\u60a8\u83b7\u5f97\u8bc1\u4e66\u540e\uff0c\u60a8\u5fc5\u987b\u521b\u5efa\u4e00\u4e2aKubernetes\u79d8\u5bc6\u4ee5\u5728\u60a8\u7684\u96c6\u7fa4\u4e2d\u4f7f\u7528\u8be5\u8bc1\u4e66\u3002 \u6709\u5173\u624b\u52a8\u83b7\u53d6\u548c\u914d\u7f6e\u8bc1\u4e66\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u672c\u4e3b\u9898\u540e\u9762\u7684\u6b65\u9aa4\u3002 \u542f\u7528Knative\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0TLS\u8bc1\u4e66 : \u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u914d\u7f6eKnative\u4ee5\u81ea\u52a8\u83b7\u53d6\u65b0\u7684TLS\u8bc1\u4e66\u5e76\u66f4\u65b0\u73b0\u6709\u7684\u8bc1\u4e66\u3002 \u5982\u679c\u60a8\u5e0c\u671b\u542f\u7528Knative\u81ea\u52a8\u53d1\u653eTLS\u8bc1\u4e66\uff0c\u8bf7\u53c2\u89c1 \u542f\u7528TLS\u8bc1\u4e66\u81ea\u52a8\u53d1\u653e \u4e3b\u9898\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c Let's Encrypt\u9881\u53d1\u673a\u6784(CA) \u7528\u4e8e\u6f14\u793a\u5982\u4f55\u542f\u7528HTTPS\u8fde\u63a5\uff0c\u4f46\u662f\u60a8\u53ef\u4ee5\u914d\u7f6eKnative\u4f7f\u7528\u6765\u81ea\u652f\u6301ACME\u534f\u8bae\u7684CA\u7684\u4efb\u4f55\u8bc1\u4e66\u3002\u4f46\u662f\uff0c\u60a8\u5fc5\u987b\u4f7f\u7528\u5e76\u914d\u7f6e\u60a8\u7684\u8bc1\u4e66\u9881\u53d1\u8005\u4f7f\u7528 DNS-01 \u6311\u6218\u7c7b\u578b . Warning Let's Encrypt\u9881\u53d1\u7684\u8bc1\u4e66\u6709\u6548\u671f\u4ec5\u4e3a 90\u5929 \u3002\u56e0\u6b64\uff0c\u5982\u679c\u60a8\u9009\u62e9\u624b\u52a8\u83b7\u53d6\u5e76\u914d\u7f6e\u8bc1\u4e66\uff0c\u8bf7\u786e\u4fdd\u6bcf\u4e2a\u8bc1\u4e66\u90fd\u5728\u8fc7\u671f\u524d\u8fdb\u884c\u4e86\u66f4\u65b0\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u542f\u7528HTTPS\u5b89\u5168\u8fde\u63a5\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42: \u5fc5\u987b\u5b89\u88c5Knative\u670d\u52a1\u3002\u5173\u4e8e\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Knative\u5b89\u88c5\u6307\u5357 \u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . Warning Istio\u53ea\u652f\u6301\u6bcf\u4e2aKubernetes\u96c6\u7fa4\u4e00\u4e2a\u8bc1\u4e66\u3002 \u8981\u4f7f\u7528Knative\u96c6\u7fa4\u4e3a\u591a\u4e2a\u57df\u63d0\u4f9b\u670d\u52a1\uff0c\u5fc5\u987b\u786e\u4fdd\u4e3a\u60f3\u8981\u670d\u52a1\u7684\u6bcf\u4e2a\u57df\u7b7e\u7f72\u4e86\u65b0\u7684\u6216\u73b0\u6709\u7684\u8bc1\u4e66\u3002 \u83b7\u53d6TLS\u8bc1\u4e66 \u00b6 \u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e86\u57df\u7684\u7b7e\u540d\u8bc1\u4e66\uff0c\u8bf7\u53c2\u89c1 \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u4e86\u89e3\u6709\u5173\u914d\u7f6eKnative\u96c6\u7fa4\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u5982\u679c\u60a8\u9700\u8981\u65b0\u7684TLS\u8bc1\u4e66\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u4ee5\u4e0b\u5de5\u5177\u4e4b\u4e00\u4eceLet's Encrypt\u83b7\u53d6\u8bc1\u4e66: \u8bbe\u7f6eCertbot\u624b\u52a8\u83b7\u53d6Let's Encrypt\u8bc1\u4e66 \u5c06cert-manager\u8bbe\u7f6e\u4e3a\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\u6216\u81ea\u52a8\u63d0\u4f9b\u8bc1\u4e66 \u672c\u9875\u6db5\u76d6\u4e86\u8fd9\u4e24\u4e2a\u9009\u9879\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u6709\u5173\u4f7f\u7528\u5176\u4ed6CA\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u8be5\u5de5\u5177\u7684\u53c2\u8003\u6587\u6863: Certbot\u652f\u6301\u63d0\u4f9b\u8005 \u8bc1\u4e66\u7ba1\u7406\u5668\u652f\u6301\u7684\u63d0\u4f9b\u8005 \u4f7f\u7528Certbot\u624b\u52a8\u83b7\u53d6Let 's Encrypt\u8bc1\u4e66 \u00b6 \u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u5b89\u88c5 Certbot \uff0c\u5e76\u4f7f\u7528\u8be5\u5de5\u5177\u4eceLet's Encrypt\u624b\u52a8\u83b7\u53d6TLS\u8bc1\u4e66\u3002 \u6309\u7167 certbot-auto \u5305\u88c5\u811a\u672c \u8bf4\u660e\u5b89\u88c5Certbot\u3002 \u4f7f\u7528Certbot\u5728\u6388\u6743\u8fc7\u7a0b\u4e2d\u901a\u8fc7DNS challenge\u8bf7\u6c42\u8bc1\u4e66: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' \u5176\u4e2d -d \u6307\u5b9a\u4f60\u7684\u57df\u3002\u5982\u679c\u4f60\u60f3\u9a8c\u8bc1\u591a\u4e2a\u57df\uff0c\u4f60\u53ef\u4ee5\u5305\u542b\u591a\u4e2a\u6807\u5fd7: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Cerbot\u547d\u4ee4\u884c \u53c2\u8003\u3002 Certbot\u5de5\u5177\u5c06\u5f15\u5bfc\u60a8\u901a\u8fc7\u5728\u8fd9\u4e9b\u57df\u4e2d\u521b\u5efaTXT\u8bb0\u5f55\u6765\u9a8c\u8bc1\u60a8\u662f\u5426\u62e5\u6709\u6307\u5b9a\u7684\u6bcf\u4e2a\u57df\u3002 \u7ed3\u679c: CertBot\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6: Certificate: fullchain.pem Private key: privkey.pem \u63a5\u4e0b\u6765\u662f\u4ec0\u4e48: \u901a\u8fc7 \u521b\u5efa\u4e00\u4e2aKubernetes secret \u5c06\u8bc1\u4e66\u548c\u79c1\u94a5\u6dfb\u52a0\u5230\u60a8\u7684Knative\u96c6\u7fa4\u4e2d. \u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u83b7\u53d6Let's Encrypt\u8bc1\u4e66 \u00b6 \u60a8\u53ef\u4ee5\u5b89\u88c5\u5e76\u4f7f\u7528 cert-manager \u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u6216\u8005\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u81ea\u52a8\u53d1\u653e\u8bc1\u4e66: \u624b\u52a8\u8bc1\u4e66 : \u5b89\u88c5cert-manager\u540e\uff0c\u4f7f\u7528\u5de5\u5177\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\u3002 \u4f7f\u7528cert-manager\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66: \u5b89\u88c5\u548c\u914d\u7f6ecert-manager . \u901a\u8fc7\u521b\u5efa\u548c\u4f7f\u7528Kubernetes\u79d8\u5bc6\uff0c\u7ee7\u7eed\u6267\u884c\u6709\u5173 \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u7684\u6b65\u9aa4\u3002 \u81ea\u52a8\u8bc1\u4e66 : \u914d\u7f6eKnative\u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0TLS\u8bc1\u4e66\u3002\u4e3a\u8fd9\u79cd\u65b9\u6cd5\u5b89\u88c5\u548c\u914d\u7f6e\u8bc1\u4e66\u7ba1\u7406\u5668\u7684\u6b65\u9aa4\u5728 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u4e3b\u9898\u4e2d\u6709\u8be6\u7ec6\u4ecb\u7ecd\u3002 \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u00b6 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u8bc1\u4e66\uff0c\u6216\u8005\u4f7f\u7528\u4e86Certbot\u6216\u8bc1\u4e66\u7ba1\u7406\u5668\u5de5\u5177\u4e2d\u7684\u4e00\u4e2a\u6765\u624b\u52a8\u83b7\u53d6\u4e00\u4e2a\u65b0\u8bc1\u4e66\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u6b65\u9aa4\u5c06\u8be5\u8bc1\u4e66\u6dfb\u52a0\u5230\u60a8\u7684Knative\u96c6\u7fa4\u4e2d\u3002 \u6709\u5173\u4e3a\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u542f\u7528Knative\u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u89c1 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u3002 \u5426\u5219\uff0c\u8bf7\u6309\u7167\u76f8\u5e94\u9875\u7b7e\u7684\u6b65\u9aa4\u624b\u52a8\u6dfb\u52a0\u8bc1\u4e66: Contour Istio \u8981\u624b\u52a8\u5411Knative\u96c6\u7fa4\u6dfb\u52a0TLS\u8bc1\u4e66\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2aKubernetes secret\uff0c\u7136\u540e\u914d\u7f6eKnative Contour\u63d2\u4ef6\u3002 \u521b\u5efa\u4e00\u4e2aKubernetes secret \u6765\u4fdd\u5b58\u60a8\u7684TLS\u8bc1\u4e66 cert.pem \u548c\u79c1\u94a5 key.pem \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note \u6ce8\u610f\u547d\u540d\u7a7a\u95f4\u548c\u79d8\u5bc6\u540d\u79f0\u3002\u5728\u4ee5\u540e\u7684\u6b65\u9aa4\u4e2d\u60a8\u5c06\u9700\u8981\u8fd9\u4e9b\u3002 \u8981\u5728\u4e0d\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6b64\u8bc1\u4e66\u548c\u79c1\u94a5\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u59d4\u6258\u3002\u4e3a\u6b64\uff0c\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u66f4\u65b0Knative Contour\u63d2\u4ef6\uff0c\u5f53autoTLS\u88ab\u7981\u7528\u65f6\uff0c\u4f7f\u7528\u8bc1\u4e66\u4f5c\u4e3a\u5907\u4efd\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' \u8981\u624b\u52a8\u6dfb\u52a0\u4e00\u4e2aTLS\u8bc1\u4e66\u5230\u60a8\u7684Knative\u96c6\u7fa4\uff0c\u60a8\u9700\u8981\u521b\u5efa\u4e00\u4e2aKubernetes secret\uff0c\u7136\u540e\u914d\u7f6e knative-ingress-gateway : \u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u4e00\u4e2aKubernetes secret\u6765\u4fdd\u5b58\u60a8\u7684TLS\u8bc1\u4e66 cert.pem \u548c\u79c1\u94a5 key.pem : kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem \u914d\u7f6eKnative\u4f7f\u7528\u60a8\u4e3aHTTPS\u8fde\u63a5\u521b\u5efa\u7684\u65b0\u79d8\u5bc6: \u4ee5\u7f16\u8f91\u6a21\u5f0f\u6253\u5f00Knative\u5171\u4eab\u7684 gateway : kubectl edit gateway knative-ingress-gateway --namespace knative-serving \u66f4\u65b0 gateway \u4ee5\u5305\u542b\u4ee5\u4e0b tls: \u90e8\u5206\u548c\u914d\u7f6e: tls : mode : SIMPLE credentialName : tls-cert \u5b9e\u4f8b: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert \u5728\u672c\u4f8b\u4e2d\uff0c TLS_HOSTS \u8868\u793aTLS\u8bc1\u4e66\u7684\u4e3b\u673a\u3002 \u5b83\u53ef\u4ee5\u662f\u5355\u4e2a\u4e3b\u673a\u3001\u591a\u4e2a\u4e3b\u673a\u6216\u901a\u914d\u7b26\u4e3b\u673a\u3002 \u6709\u5173\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605 Istio\u6587\u6863 \u63a5\u4e0b\u6765\u662f\u4ec0\u4e48: \u00b6 \u5728Knative\u96c6\u7fa4\u4e0a\u8fd0\u884c\u66f4\u6539\u4e4b\u540e\uff0c\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528HTTPS\u534f\u8bae\u6765\u5b89\u5168\u8bbf\u95ee\u90e8\u7f72\u7684Knative\u670d\u52a1\u3002","title":"\u914d\u7f6eHTTPS\u8fde\u63a5"},{"location":"serving/using-a-tls-cert/#httpstls","text":"\u4e86\u89e3\u5982\u4f55\u5728Knative\u4e2d\u4f7f\u7528TLS\u8bc1\u4e66\u914d\u7f6e\u5b89\u5168HTTPS\u8fde\u63a5( TLS\u53d6\u4ee3SSL ). \u914d\u7f6e\u5b89\u5168HTTPS\u8fde\u63a5\uff0c\u4f7f\u60a8\u7684Knative\u670d\u52a1\u548c\u8def\u7531\u80fd\u591f \u7ec8\u6b62\u5916\u90e8TLS\u8fde\u63a5 . \u53ef\u4ee5\u914d\u7f6eKnative\u6765\u5904\u7406\u624b\u52a8\u6307\u5b9a\u7684\u8bc1\u4e66\uff0c\u4e5f\u53ef\u4ee5\u542f\u7528Knative\u6765\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0\u8bc1\u4e66\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Certbot \u6216 cert-manager \u4e24\u79cd\u65b9\u5f0f\u83b7\u53d6\u8bc1\u4e66\u3002 \u8fd9\u4e24\u4e2a\u5de5\u5177\u90fd\u652f\u6301TLS\u8bc1\u4e66\uff0c\u4f46\u5982\u679c\u60a8\u60f3\u542f\u7528Knative\u6765\u81ea\u52a8\u53d1\u653eTLS\u8bc1\u4e66\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\u548c\u914d\u7f6e\u8bc1\u4e66\u7ba1\u7406\u5668\u5de5\u5177: \u624b\u52a8\u83b7\u53d6\u548c\u66f4\u65b0\u8bc1\u4e66 : \u53ef\u4ee5\u4f7f\u7528Certbot\u548ccert-manager\u5de5\u5177\u624b\u52a8\u83b7\u53d6TLS\u8bc1\u4e66\u3002 \u901a\u5e38\uff0c\u5728\u60a8\u83b7\u5f97\u8bc1\u4e66\u540e\uff0c\u60a8\u5fc5\u987b\u521b\u5efa\u4e00\u4e2aKubernetes\u79d8\u5bc6\u4ee5\u5728\u60a8\u7684\u96c6\u7fa4\u4e2d\u4f7f\u7528\u8be5\u8bc1\u4e66\u3002 \u6709\u5173\u624b\u52a8\u83b7\u53d6\u548c\u914d\u7f6e\u8bc1\u4e66\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u672c\u4e3b\u9898\u540e\u9762\u7684\u6b65\u9aa4\u3002 \u542f\u7528Knative\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0TLS\u8bc1\u4e66 : \u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u914d\u7f6eKnative\u4ee5\u81ea\u52a8\u83b7\u53d6\u65b0\u7684TLS\u8bc1\u4e66\u5e76\u66f4\u65b0\u73b0\u6709\u7684\u8bc1\u4e66\u3002 \u5982\u679c\u60a8\u5e0c\u671b\u542f\u7528Knative\u81ea\u52a8\u53d1\u653eTLS\u8bc1\u4e66\uff0c\u8bf7\u53c2\u89c1 \u542f\u7528TLS\u8bc1\u4e66\u81ea\u52a8\u53d1\u653e \u4e3b\u9898\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c Let's Encrypt\u9881\u53d1\u673a\u6784(CA) \u7528\u4e8e\u6f14\u793a\u5982\u4f55\u542f\u7528HTTPS\u8fde\u63a5\uff0c\u4f46\u662f\u60a8\u53ef\u4ee5\u914d\u7f6eKnative\u4f7f\u7528\u6765\u81ea\u652f\u6301ACME\u534f\u8bae\u7684CA\u7684\u4efb\u4f55\u8bc1\u4e66\u3002\u4f46\u662f\uff0c\u60a8\u5fc5\u987b\u4f7f\u7528\u5e76\u914d\u7f6e\u60a8\u7684\u8bc1\u4e66\u9881\u53d1\u8005\u4f7f\u7528 DNS-01 \u6311\u6218\u7c7b\u578b . Warning Let's Encrypt\u9881\u53d1\u7684\u8bc1\u4e66\u6709\u6548\u671f\u4ec5\u4e3a 90\u5929 \u3002\u56e0\u6b64\uff0c\u5982\u679c\u60a8\u9009\u62e9\u624b\u52a8\u83b7\u53d6\u5e76\u914d\u7f6e\u8bc1\u4e66\uff0c\u8bf7\u786e\u4fdd\u6bcf\u4e2a\u8bc1\u4e66\u90fd\u5728\u8fc7\u671f\u524d\u8fdb\u884c\u4e86\u66f4\u65b0\u3002","title":"\u914d\u7f6eHTTPS\u4f7f\u7528TLS\u8bc1\u4e66"},{"location":"serving/using-a-tls-cert/#_1","text":"\u542f\u7528HTTPS\u5b89\u5168\u8fde\u63a5\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42: \u5fc5\u987b\u5b89\u88c5Knative\u670d\u52a1\u3002\u5173\u4e8e\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Knative\u5b89\u88c5\u6307\u5357 \u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . Warning Istio\u53ea\u652f\u6301\u6bcf\u4e2aKubernetes\u96c6\u7fa4\u4e00\u4e2a\u8bc1\u4e66\u3002 \u8981\u4f7f\u7528Knative\u96c6\u7fa4\u4e3a\u591a\u4e2a\u57df\u63d0\u4f9b\u670d\u52a1\uff0c\u5fc5\u987b\u786e\u4fdd\u4e3a\u60f3\u8981\u670d\u52a1\u7684\u6bcf\u4e2a\u57df\u7b7e\u7f72\u4e86\u65b0\u7684\u6216\u73b0\u6709\u7684\u8bc1\u4e66\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"serving/using-a-tls-cert/#tls","text":"\u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e86\u57df\u7684\u7b7e\u540d\u8bc1\u4e66\uff0c\u8bf7\u53c2\u89c1 \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u4e86\u89e3\u6709\u5173\u914d\u7f6eKnative\u96c6\u7fa4\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u5982\u679c\u60a8\u9700\u8981\u65b0\u7684TLS\u8bc1\u4e66\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u4ee5\u4e0b\u5de5\u5177\u4e4b\u4e00\u4eceLet's Encrypt\u83b7\u53d6\u8bc1\u4e66: \u8bbe\u7f6eCertbot\u624b\u52a8\u83b7\u53d6Let's Encrypt\u8bc1\u4e66 \u5c06cert-manager\u8bbe\u7f6e\u4e3a\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\u6216\u81ea\u52a8\u63d0\u4f9b\u8bc1\u4e66 \u672c\u9875\u6db5\u76d6\u4e86\u8fd9\u4e24\u4e2a\u9009\u9879\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u6709\u5173\u4f7f\u7528\u5176\u4ed6CA\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u8be5\u5de5\u5177\u7684\u53c2\u8003\u6587\u6863: Certbot\u652f\u6301\u63d0\u4f9b\u8005 \u8bc1\u4e66\u7ba1\u7406\u5668\u652f\u6301\u7684\u63d0\u4f9b\u8005","title":"\u83b7\u53d6TLS\u8bc1\u4e66"},{"location":"serving/using-a-tls-cert/#certbotlet-s-encrypt","text":"\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u5b89\u88c5 Certbot \uff0c\u5e76\u4f7f\u7528\u8be5\u5de5\u5177\u4eceLet's Encrypt\u624b\u52a8\u83b7\u53d6TLS\u8bc1\u4e66\u3002 \u6309\u7167 certbot-auto \u5305\u88c5\u811a\u672c \u8bf4\u660e\u5b89\u88c5Certbot\u3002 \u4f7f\u7528Certbot\u5728\u6388\u6743\u8fc7\u7a0b\u4e2d\u901a\u8fc7DNS challenge\u8bf7\u6c42\u8bc1\u4e66: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' \u5176\u4e2d -d \u6307\u5b9a\u4f60\u7684\u57df\u3002\u5982\u679c\u4f60\u60f3\u9a8c\u8bc1\u591a\u4e2a\u57df\uff0c\u4f60\u53ef\u4ee5\u5305\u542b\u591a\u4e2a\u6807\u5fd7: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 Cerbot\u547d\u4ee4\u884c \u53c2\u8003\u3002 Certbot\u5de5\u5177\u5c06\u5f15\u5bfc\u60a8\u901a\u8fc7\u5728\u8fd9\u4e9b\u57df\u4e2d\u521b\u5efaTXT\u8bb0\u5f55\u6765\u9a8c\u8bc1\u60a8\u662f\u5426\u62e5\u6709\u6307\u5b9a\u7684\u6bcf\u4e2a\u57df\u3002 \u7ed3\u679c: CertBot\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6: Certificate: fullchain.pem Private key: privkey.pem \u63a5\u4e0b\u6765\u662f\u4ec0\u4e48: \u901a\u8fc7 \u521b\u5efa\u4e00\u4e2aKubernetes secret \u5c06\u8bc1\u4e66\u548c\u79c1\u94a5\u6dfb\u52a0\u5230\u60a8\u7684Knative\u96c6\u7fa4\u4e2d.","title":"\u4f7f\u7528Certbot\u624b\u52a8\u83b7\u53d6Let 's Encrypt\u8bc1\u4e66"},{"location":"serving/using-a-tls-cert/#lets-encrypt","text":"\u60a8\u53ef\u4ee5\u5b89\u88c5\u5e76\u4f7f\u7528 cert-manager \u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\uff0c\u6216\u8005\u914d\u7f6e\u60a8\u7684Knative\u96c6\u7fa4\u4ee5\u81ea\u52a8\u53d1\u653e\u8bc1\u4e66: \u624b\u52a8\u8bc1\u4e66 : \u5b89\u88c5cert-manager\u540e\uff0c\u4f7f\u7528\u5de5\u5177\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66\u3002 \u4f7f\u7528cert-manager\u624b\u52a8\u83b7\u53d6\u8bc1\u4e66: \u5b89\u88c5\u548c\u914d\u7f6ecert-manager . \u901a\u8fc7\u521b\u5efa\u548c\u4f7f\u7528Kubernetes\u79d8\u5bc6\uff0c\u7ee7\u7eed\u6267\u884c\u6709\u5173 \u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66 \u7684\u6b65\u9aa4\u3002 \u81ea\u52a8\u8bc1\u4e66 : \u914d\u7f6eKnative\u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u81ea\u52a8\u83b7\u53d6\u548c\u66f4\u65b0TLS\u8bc1\u4e66\u3002\u4e3a\u8fd9\u79cd\u65b9\u6cd5\u5b89\u88c5\u548c\u914d\u7f6e\u8bc1\u4e66\u7ba1\u7406\u5668\u7684\u6b65\u9aa4\u5728 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u4e3b\u9898\u4e2d\u6709\u8be6\u7ec6\u4ecb\u7ecd\u3002","title":"\u4f7f\u7528\u8bc1\u4e66\u7ba1\u7406\u5668\u83b7\u53d6Let's Encrypt\u8bc1\u4e66"},{"location":"serving/using-a-tls-cert/#tls_1","text":"\u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u8bc1\u4e66\uff0c\u6216\u8005\u4f7f\u7528\u4e86Certbot\u6216\u8bc1\u4e66\u7ba1\u7406\u5668\u5de5\u5177\u4e2d\u7684\u4e00\u4e2a\u6765\u624b\u52a8\u83b7\u53d6\u4e00\u4e2a\u65b0\u8bc1\u4e66\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u6b65\u9aa4\u5c06\u8be5\u8bc1\u4e66\u6dfb\u52a0\u5230\u60a8\u7684Knative\u96c6\u7fa4\u4e2d\u3002 \u6709\u5173\u4e3a\u81ea\u52a8\u8bc1\u4e66\u53d1\u653e\u542f\u7528Knative\u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u89c1 \u542f\u7528\u81ea\u52a8TLS\u8bc1\u4e66\u53d1\u653e \u3002 \u5426\u5219\uff0c\u8bf7\u6309\u7167\u76f8\u5e94\u9875\u7b7e\u7684\u6b65\u9aa4\u624b\u52a8\u6dfb\u52a0\u8bc1\u4e66: Contour Istio \u8981\u624b\u52a8\u5411Knative\u96c6\u7fa4\u6dfb\u52a0TLS\u8bc1\u4e66\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2aKubernetes secret\uff0c\u7136\u540e\u914d\u7f6eKnative Contour\u63d2\u4ef6\u3002 \u521b\u5efa\u4e00\u4e2aKubernetes secret \u6765\u4fdd\u5b58\u60a8\u7684TLS\u8bc1\u4e66 cert.pem \u548c\u79c1\u94a5 key.pem \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note \u6ce8\u610f\u547d\u540d\u7a7a\u95f4\u548c\u79d8\u5bc6\u540d\u79f0\u3002\u5728\u4ee5\u540e\u7684\u6b65\u9aa4\u4e2d\u60a8\u5c06\u9700\u8981\u8fd9\u4e9b\u3002 \u8981\u5728\u4e0d\u540c\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6b64\u8bc1\u4e66\u548c\u79c1\u94a5\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u59d4\u6258\u3002\u4e3a\u6b64\uff0c\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2aYAML\u6587\u4ef6: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u66f4\u65b0Knative Contour\u63d2\u4ef6\uff0c\u5f53autoTLS\u88ab\u7981\u7528\u65f6\uff0c\u4f7f\u7528\u8bc1\u4e66\u4f5c\u4e3a\u5907\u4efd\uff0c\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' \u8981\u624b\u52a8\u6dfb\u52a0\u4e00\u4e2aTLS\u8bc1\u4e66\u5230\u60a8\u7684Knative\u96c6\u7fa4\uff0c\u60a8\u9700\u8981\u521b\u5efa\u4e00\u4e2aKubernetes secret\uff0c\u7136\u540e\u914d\u7f6e knative-ingress-gateway : \u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u4e00\u4e2aKubernetes secret\u6765\u4fdd\u5b58\u60a8\u7684TLS\u8bc1\u4e66 cert.pem \u548c\u79c1\u94a5 key.pem : kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem \u914d\u7f6eKnative\u4f7f\u7528\u60a8\u4e3aHTTPS\u8fde\u63a5\u521b\u5efa\u7684\u65b0\u79d8\u5bc6: \u4ee5\u7f16\u8f91\u6a21\u5f0f\u6253\u5f00Knative\u5171\u4eab\u7684 gateway : kubectl edit gateway knative-ingress-gateway --namespace knative-serving \u66f4\u65b0 gateway \u4ee5\u5305\u542b\u4ee5\u4e0b tls: \u90e8\u5206\u548c\u914d\u7f6e: tls : mode : SIMPLE credentialName : tls-cert \u5b9e\u4f8b: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert \u5728\u672c\u4f8b\u4e2d\uff0c TLS_HOSTS \u8868\u793aTLS\u8bc1\u4e66\u7684\u4e3b\u673a\u3002 \u5b83\u53ef\u4ee5\u662f\u5355\u4e2a\u4e3b\u673a\u3001\u591a\u4e2a\u4e3b\u673a\u6216\u901a\u914d\u7b26\u4e3b\u673a\u3002 \u6709\u5173\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605 Istio\u6587\u6863","title":"\u624b\u52a8\u6dfb\u52a0TLS\u8bc1\u4e66"},{"location":"serving/using-a-tls-cert/#_2","text":"\u5728Knative\u96c6\u7fa4\u4e0a\u8fd0\u884c\u66f4\u6539\u4e4b\u540e\uff0c\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528HTTPS\u534f\u8bae\u6765\u5b89\u5168\u8bbf\u95ee\u90e8\u7f72\u7684Knative\u670d\u52a1\u3002","title":"\u63a5\u4e0b\u6765\u662f\u4ec0\u4e48:"},{"location":"serving/using-auto-tls/","text":"\u542f\u7528\u81ea\u52a8tls\u8bc1\u4e66 \u00b6 \u5982\u679c\u5b89\u88c5\u5e76\u914d\u7f6e\u4e86\u8bc1\u4e66\u7ba1\u7406\u5668\uff0c\u5219\u53ef\u4ee5\u914d\u7f6eKnative\u81ea\u52a8\u83b7\u53d6\u65b0\u7684TLS\u8bc1\u4e66\uff0c\u5e76\u4e3aKnative Services\u66f4\u65b0\u73b0\u6709\u7684TLS\u8bc1\u4e66\u3002 \u8981\u4e86\u89e3\u6709\u5173\u5728Knative\u4e2d\u4f7f\u7528\u5b89\u5168\u8fde\u63a5\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u5e26TLS\u8bc1\u4e66\u7684HTTPS . \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u5fc5\u987b\u5728\u60a8\u7684Knative\u96c6\u7fa4\u4e0a\u5b89\u88c5\u4ee5\u4e0b\u7ec4\u4ef6: Knative Serving . \u4e00\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Kourier, Istio\u5177\u6709SDS v1.3\u6216\u66f4\u9ad8\u7248\u672c\uff0c\u6216Contour v1.1\u6216\u66f4\u9ad8\u7248\u672c\u3002 \u53c2\u89c1 \u5b89\u88c5\u7f51\u7edc\u5c42 \u6216 \u5e26\u6709SDS\u7684Istio\uff0c\u7248\u672c1.3\u6216\u66f4\u9ad8 \u3002 cert-manager \u7248\u672c 1.0.0 \u6216\u66f4\u9ad8 . \u60a8\u7684Knative\u96c6\u7fa4\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . \u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u5fc5\u987b\u8bbe\u7f6e\u5e76\u914d\u7f6e\u5230\u60a8\u7684\u57df\u3002 \u5982\u679c\u60a8\u60f3\u4f7f\u7528HTTP-01 challenge\uff0c\u60a8\u9700\u8981\u914d\u7f6e\u60a8\u7684\u81ea\u5b9a\u4e49\u57df\u4ee5\u6620\u5c04\u5230\u5165\u63a5\u53e3\u7684IP\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2aDNS a\u8bb0\u5f55\u6765\u6839\u636e\u60a8\u7684DNS\u63d0\u4f9b\u8005\u7684\u8bf4\u660e\u5c06\u57df\u6620\u5c04\u5230IP\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 TLS\u81ea\u52a8\u53d1\u653e\u6a21\u5f0f \u00b6 Knative\u652f\u6301\u4ee5\u4e0b\u81ea\u52a8TLS\u6a21\u5f0f: \u4f7f\u7528 DNS-01 challenge \u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u7684\u96c6\u7fa4\u9700\u8981\u80fd\u591f\u4e0e\u60a8\u7684DNS\u670d\u52a1\u5668\u901a\u4fe1\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684\u57df\u7684\u6240\u6709\u6743\u3002 \u4f7f\u7528DNS-01\u6311\u6218\u6a21\u5f0f\u65f6\uff0c\u652f\u6301\u6bcf\u4e2a\u547d\u540d\u7a7a\u95f4\u9881\u53d1\u8bc1\u4e66\u3002 This is the recommended mode for faster certificate provision. In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. \u4f7f\u7528DNS-01\u6311\u6218\u6a21\u5f0f\u65f6\uff0c\u652f\u6301\u6839\u636eKnative\u670d\u52a1\u63d0\u4f9b\u8bc1\u4e66\u3002 This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluster ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace. \u542f\u7528\u81ea\u52a8TLS \u00b6 Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : test-email@knative.dev privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True . DNS-01 challenge only: \u914d\u7f6eDNS\u63d0\u4f9b\u7a0b\u5e8f \u00b6 If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com . \u5b89\u88c5net-certmanager-controller\u90e8\u7f72 \u00b6 Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml \u4e3a\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u63d0\u4f9b\u8bc1\u4e66(\u901a\u914d\u7b26\u8bc1\u4e66) \u00b6 Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" . \u914d\u7f6econfig-certmanager ConfigMap \u00b6 Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager -n knative-serving Add the issuerRef within the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-certmanager namespace : knative-serving labels : networking.knative.dev/certificate-provider : cert-manager data : issuerRef : | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer is used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager -n knative-serving -o yaml \u5f00\u542f\u81ea\u52a8TLS \u00b6 Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network -n knative-serving Add the auto-tls: Enabled attribute under the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data : http-protocol : Redirected Example: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled http-protocol : Redirected ... Ensure that the file was updated successfully: kubectl get configmap config-network -n knative-serving -o yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic. \u81ea\u52a8TLS\u9a8c\u8bc1 \u00b6 Run the following command to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default. { custom-domain } autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case. \u7981\u7528\u6bcf\u4e2a\u670d\u52a1\u6216\u8def\u7531\u7684\u81ea\u52a8TLS \u00b6 If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"\u542f\u7528auto-TLS"},{"location":"serving/using-auto-tls/#tls","text":"\u5982\u679c\u5b89\u88c5\u5e76\u914d\u7f6e\u4e86\u8bc1\u4e66\u7ba1\u7406\u5668\uff0c\u5219\u53ef\u4ee5\u914d\u7f6eKnative\u81ea\u52a8\u83b7\u53d6\u65b0\u7684TLS\u8bc1\u4e66\uff0c\u5e76\u4e3aKnative Services\u66f4\u65b0\u73b0\u6709\u7684TLS\u8bc1\u4e66\u3002 \u8981\u4e86\u89e3\u6709\u5173\u5728Knative\u4e2d\u4f7f\u7528\u5b89\u5168\u8fde\u63a5\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u5e26TLS\u8bc1\u4e66\u7684HTTPS .","title":"\u542f\u7528\u81ea\u52a8tls\u8bc1\u4e66"},{"location":"serving/using-auto-tls/#_1","text":"\u5fc5\u987b\u5728\u60a8\u7684Knative\u96c6\u7fa4\u4e0a\u5b89\u88c5\u4ee5\u4e0b\u7ec4\u4ef6: Knative Serving . \u4e00\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Kourier, Istio\u5177\u6709SDS v1.3\u6216\u66f4\u9ad8\u7248\u672c\uff0c\u6216Contour v1.1\u6216\u66f4\u9ad8\u7248\u672c\u3002 \u53c2\u89c1 \u5b89\u88c5\u7f51\u7edc\u5c42 \u6216 \u5e26\u6709SDS\u7684Istio\uff0c\u7248\u672c1.3\u6216\u66f4\u9ad8 \u3002 cert-manager \u7248\u672c 1.0.0 \u6216\u66f4\u9ad8 . \u60a8\u7684Knative\u96c6\u7fa4\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 \u81ea\u5b9a\u4e49\u57df . \u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u5fc5\u987b\u8bbe\u7f6e\u5e76\u914d\u7f6e\u5230\u60a8\u7684\u57df\u3002 \u5982\u679c\u60a8\u60f3\u4f7f\u7528HTTP-01 challenge\uff0c\u60a8\u9700\u8981\u914d\u7f6e\u60a8\u7684\u81ea\u5b9a\u4e49\u57df\u4ee5\u6620\u5c04\u5230\u5165\u63a5\u53e3\u7684IP\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u4e00\u4e2aDNS a\u8bb0\u5f55\u6765\u6839\u636e\u60a8\u7684DNS\u63d0\u4f9b\u8005\u7684\u8bf4\u660e\u5c06\u57df\u6620\u5c04\u5230IP\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"serving/using-auto-tls/#tls_1","text":"Knative\u652f\u6301\u4ee5\u4e0b\u81ea\u52a8TLS\u6a21\u5f0f: \u4f7f\u7528 DNS-01 challenge \u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u7684\u96c6\u7fa4\u9700\u8981\u80fd\u591f\u4e0e\u60a8\u7684DNS\u670d\u52a1\u5668\u901a\u4fe1\uff0c\u4ee5\u9a8c\u8bc1\u60a8\u7684\u57df\u7684\u6240\u6709\u6743\u3002 \u4f7f\u7528DNS-01\u6311\u6218\u6a21\u5f0f\u65f6\uff0c\u652f\u6301\u6bcf\u4e2a\u547d\u540d\u7a7a\u95f4\u9881\u53d1\u8bc1\u4e66\u3002 This is the recommended mode for faster certificate provision. In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. \u4f7f\u7528DNS-01\u6311\u6218\u6a21\u5f0f\u65f6\uff0c\u652f\u6301\u6839\u636eKnative\u670d\u52a1\u63d0\u4f9b\u8bc1\u4e66\u3002 This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluster ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace.","title":"TLS\u81ea\u52a8\u53d1\u653e\u6a21\u5f0f"},{"location":"serving/using-auto-tls/#tls_2","text":"Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : test-email@knative.dev privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True .","title":"\u542f\u7528\u81ea\u52a8TLS"},{"location":"serving/using-auto-tls/#dns-01-challenge-only-dns","text":"If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com .","title":"DNS-01 challenge only: \u914d\u7f6eDNS\u63d0\u4f9b\u7a0b\u5e8f"},{"location":"serving/using-auto-tls/#net-certmanager-controller","text":"Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml","title":"\u5b89\u88c5net-certmanager-controller\u90e8\u7f72"},{"location":"serving/using-auto-tls/#_2","text":"Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" .","title":"\u4e3a\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u63d0\u4f9b\u8bc1\u4e66(\u901a\u914d\u7b26\u8bc1\u4e66)"},{"location":"serving/using-auto-tls/#config-certmanager-configmap","text":"Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager -n knative-serving Add the issuerRef within the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-certmanager namespace : knative-serving labels : networking.knative.dev/certificate-provider : cert-manager data : issuerRef : | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer is used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager -n knative-serving -o yaml","title":"\u914d\u7f6econfig-certmanager ConfigMap"},{"location":"serving/using-auto-tls/#tls_3","text":"Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network -n knative-serving Add the auto-tls: Enabled attribute under the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data : http-protocol : Redirected Example: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled http-protocol : Redirected ... Ensure that the file was updated successfully: kubectl get configmap config-network -n knative-serving -o yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic.","title":"\u5f00\u542f\u81ea\u52a8TLS"},{"location":"serving/using-auto-tls/#tls_4","text":"Run the following command to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default. { custom-domain } autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case.","title":"\u81ea\u52a8TLS\u9a8c\u8bc1"},{"location":"serving/using-auto-tls/#tls_5","text":"If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"\u7981\u7528\u6bcf\u4e2a\u670d\u52a1\u6216\u8def\u7531\u7684\u81ea\u52a8TLS"},{"location":"serving/webhook-customizations/","text":"Exclude namespaces from the Knative webhook \u00b6 The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"\u6392\u9664\u540d\u79f0\u7a7a\u95f4"},{"location":"serving/webhook-customizations/#exclude-namespaces-from-the-knative-webhook","text":"The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/app-security/security-guard-about/","text":"About Security-Guard \u00b6 Security-Guard provides visibility into the security status of deployed Knative Services, by monitoring the behaviors of user containers and events. Security-Guard profile and criteria \u00b6 Security-Guard creates a profile of the user container behavior and of event behavior. The behaviors are then compared to a pre-defined criteria. If the profile does not meet the criteria, Security-Guard can log alerts, block events, or stop a Service instance, depending on user configurations. The criteria that a profile is compared to is composed of a set of micro-rules. These rules describe expected behaviors for events and user containers, including expected responses. You can choose to set micro-rules manually, or use Security-Guard's machine learning feature to automate the creation of micro-rules. Guardians \u00b6 A per-Service set of micro-rules is stored in the Kubernetes system as a Guardian object. Under Knative, Security-Guard store Guardians ny default using the guardians.guard.security.knative.dev CRDs. To list all CRD Guardians use: kubectl get guardians.guard.security.knative.dev Example Output: NAME AGE helloworld-go 10h Using Security-Guard \u00b6 Security-Guard offers situational awareness by writing its alerts to the Service queue proxy log. You may observe the queue-proxy to see alerts. Security alerts appear in the queue proxy log file and start with the string SECURITY ALERT! . The default setup of Security-Guard is to allow any request or response and learn any new pattern after reporting it. When the Service is actively serving requests, it typically takes about 30 min for Security-Guard to learn the patterns of the Service requests and responses and build corresponding micro-rules. After the initial learning period, Security-Guard updates the micro-rules in the Service Guardian, following which, it sends alerts only when a change in behavior is detected. Note that in the default setup, Security-Guard continues to learn any new behavior and therefore avoids reporting alerts repeatedly when the new behavior reoccurs. Correct security procedures should include reviewing any new behavior detected by Security-Guard. Security-Guard can also be configured to operate in other modes of operation, such as: Move from auto learning to manual micro-rules management after the initial learning period Block requests/responses when they do not conform to the micro-rules For more information or for troubleshooting help, see the #security channel in Knative Slack. Security-Guard Use Cases \u00b6 Security-Guard support four different stages in the life of a knative service from a security standpoint. Zero-Day Vulnerable Exploitable Misused We next detail each stage and how Security-Guard is used to manage the security of the service in that stage. Zero-Day \u00b6 Under normal conditions, the Knative user who owns the service is not aware of any known vulnerabilities in the service. Yet, it is reasonable to assume that the service has weaknesses. Security-Guard offers Knative users the ability to detect/block patterns sent as part of incoming events that may be used to exploit unknown, zero-day, service vulnerabilities. Vulnerable \u00b6 Once a CVE that describes a vulnerability in the service is published, the Knative user who owns the service is required to start a process to eliminate the vulnerability by introducing a new revision of the service. This process of removing a known vulnerability may take many weeks to accomplish. Security-Guard enables Knative users to set micro-rules to detect/block incoming events that include patterns that may be used as part of some future exploit targeting the discovered vulnerability. In this way, users are able to continue offering services, although the service has a known vulnerability. Exploitable \u00b6 When a known exploit is found effective in compromising a service, the Knative user who owns the Service needs a way to filter incoming events that contain the specific exploit. This is normally the case during a successful attack, where a working exploit is able to compromise the user-container. Security-Guard enables Knative users a way to set micro-rules to detect/block incoming events that include specific exploits while allowing other events to be served. Misused \u00b6 When an offender has established an attack pattern that is able to take over a service instance, by first exploiting one or more vulnerabilities and then starting to misuse the service instance, stopping the service instance requires the offender to repeat the attack pattern. At any given time, some service instances may be compromised and misused while others behave as designed. Security-Guard enables Knative users a way to detect/remove misused Service instances while allowing other instances to continue serve events. Additional resources \u00b6 See Readme files in the Security-Guard Github Repository .","title":"\u5173\u4e8e\u4fdd\u5b89"},{"location":"serving/app-security/security-guard-about/#about-security-guard","text":"Security-Guard provides visibility into the security status of deployed Knative Services, by monitoring the behaviors of user containers and events.","title":"About Security-Guard"},{"location":"serving/app-security/security-guard-about/#security-guard-profile-and-criteria","text":"Security-Guard creates a profile of the user container behavior and of event behavior. The behaviors are then compared to a pre-defined criteria. If the profile does not meet the criteria, Security-Guard can log alerts, block events, or stop a Service instance, depending on user configurations. The criteria that a profile is compared to is composed of a set of micro-rules. These rules describe expected behaviors for events and user containers, including expected responses. You can choose to set micro-rules manually, or use Security-Guard's machine learning feature to automate the creation of micro-rules.","title":"Security-Guard profile and criteria"},{"location":"serving/app-security/security-guard-about/#guardians","text":"A per-Service set of micro-rules is stored in the Kubernetes system as a Guardian object. Under Knative, Security-Guard store Guardians ny default using the guardians.guard.security.knative.dev CRDs. To list all CRD Guardians use: kubectl get guardians.guard.security.knative.dev Example Output: NAME AGE helloworld-go 10h","title":"Guardians"},{"location":"serving/app-security/security-guard-about/#using-security-guard","text":"Security-Guard offers situational awareness by writing its alerts to the Service queue proxy log. You may observe the queue-proxy to see alerts. Security alerts appear in the queue proxy log file and start with the string SECURITY ALERT! . The default setup of Security-Guard is to allow any request or response and learn any new pattern after reporting it. When the Service is actively serving requests, it typically takes about 30 min for Security-Guard to learn the patterns of the Service requests and responses and build corresponding micro-rules. After the initial learning period, Security-Guard updates the micro-rules in the Service Guardian, following which, it sends alerts only when a change in behavior is detected. Note that in the default setup, Security-Guard continues to learn any new behavior and therefore avoids reporting alerts repeatedly when the new behavior reoccurs. Correct security procedures should include reviewing any new behavior detected by Security-Guard. Security-Guard can also be configured to operate in other modes of operation, such as: Move from auto learning to manual micro-rules management after the initial learning period Block requests/responses when they do not conform to the micro-rules For more information or for troubleshooting help, see the #security channel in Knative Slack.","title":"Using Security-Guard"},{"location":"serving/app-security/security-guard-about/#security-guard-use-cases","text":"Security-Guard support four different stages in the life of a knative service from a security standpoint. Zero-Day Vulnerable Exploitable Misused We next detail each stage and how Security-Guard is used to manage the security of the service in that stage.","title":"Security-Guard Use Cases"},{"location":"serving/app-security/security-guard-about/#zero-day","text":"Under normal conditions, the Knative user who owns the service is not aware of any known vulnerabilities in the service. Yet, it is reasonable to assume that the service has weaknesses. Security-Guard offers Knative users the ability to detect/block patterns sent as part of incoming events that may be used to exploit unknown, zero-day, service vulnerabilities.","title":"Zero-Day"},{"location":"serving/app-security/security-guard-about/#vulnerable","text":"Once a CVE that describes a vulnerability in the service is published, the Knative user who owns the service is required to start a process to eliminate the vulnerability by introducing a new revision of the service. This process of removing a known vulnerability may take many weeks to accomplish. Security-Guard enables Knative users to set micro-rules to detect/block incoming events that include patterns that may be used as part of some future exploit targeting the discovered vulnerability. In this way, users are able to continue offering services, although the service has a known vulnerability.","title":"Vulnerable"},{"location":"serving/app-security/security-guard-about/#exploitable","text":"When a known exploit is found effective in compromising a service, the Knative user who owns the Service needs a way to filter incoming events that contain the specific exploit. This is normally the case during a successful attack, where a working exploit is able to compromise the user-container. Security-Guard enables Knative users a way to set micro-rules to detect/block incoming events that include specific exploits while allowing other events to be served.","title":"Exploitable"},{"location":"serving/app-security/security-guard-about/#misused","text":"When an offender has established an attack pattern that is able to take over a service instance, by first exploiting one or more vulnerabilities and then starting to misuse the service instance, stopping the service instance requires the offender to repeat the attack pattern. At any given time, some service instances may be compromised and misused while others behave as designed. Security-Guard enables Knative users a way to detect/remove misused Service instances while allowing other instances to continue serve events.","title":"Misused"},{"location":"serving/app-security/security-guard-about/#additional-resources","text":"See Readme files in the Security-Guard Github Repository .","title":"Additional resources"},{"location":"serving/app-security/security-guard-example-alerts/","text":"Security-Guard example alerts \u00b6 Send an event with unexpected query string, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io?a=3\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: QueryString: KeyVal: Key a is not known... Send an event with unexpected long url, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io/AAAAAAAAAAAAAAAA\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: Url: KeyVal: Letters: Counter out of Range: 16 ...","title":"\u4fdd\u5b89\u8b66\u62a5\u4f8b\u5b50"},{"location":"serving/app-security/security-guard-example-alerts/#security-guard-example-alerts","text":"Send an event with unexpected query string, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io?a=3\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: QueryString: KeyVal: Key a is not known... Send an event with unexpected long url, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io/AAAAAAAAAAAAAAAA\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: Url: KeyVal: Letters: Counter out of Range: 16 ...","title":"Security-Guard example alerts"},{"location":"serving/app-security/security-guard-install/","text":"Installing Security-Guard \u00b6 Here we show how to install Security-Guard in Knative. Security-Guard is an enhancement to knative-Serving and needs to be installed after the Knative-Serving is successfully installed. Using Security-Guard requires that your cluster will use an enhanced queue-proxy image. In addition, Security-Guard includes automation for auto-learning a per service Guardian. Auto-learning requires you to deploy a guard-service on your kubernetes cluster. guard-service should be installed in any namespace where you deploy knative services that require Security-Guard protection. Before you begin \u00b6 Before installing Security-Guard, learn about Security-Guard Install steps \u00b6 To start this tutorial, after installing Knative Serving, run the following procedure to replace your queue-proxy image and deploy a guard-service in the current namespace. Install from source Install from released images and yamls Clone the Security-Guard repository using git clone git@github.com:knative-sandbox/security-guard.git Do cd security-guard Run ko apply -Rf ./config Use released images to update your system to enable Security-Guard: Set the feature named queueproxy.mount-podinfo to allowed in the config-features ConfigMap. An easy way to do that is using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/deploy/config-features.yaml Set the deployment parameter queue-sidecar-image to gcr.io/knative-releases/knative.dev/security-guard/cmd/queue in the config-deployment ConfigMap. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/queue-proxy.yaml Add the necessary Security-Guard resources to your cluster using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/gateAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/serviceAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/guardiansCrd.yaml Deploy guard-service on your system to enable automated learning of micro-rules. In the current version, it is recommended to deploy guard-service in any namespace where knative services are deployed. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/guard-service.yaml","title":"\u5b89\u88c5\u7684\u4fdd\u5b89"},{"location":"serving/app-security/security-guard-install/#installing-security-guard","text":"Here we show how to install Security-Guard in Knative. Security-Guard is an enhancement to knative-Serving and needs to be installed after the Knative-Serving is successfully installed. Using Security-Guard requires that your cluster will use an enhanced queue-proxy image. In addition, Security-Guard includes automation for auto-learning a per service Guardian. Auto-learning requires you to deploy a guard-service on your kubernetes cluster. guard-service should be installed in any namespace where you deploy knative services that require Security-Guard protection.","title":"Installing Security-Guard"},{"location":"serving/app-security/security-guard-install/#before-you-begin","text":"Before installing Security-Guard, learn about Security-Guard","title":"Before you begin"},{"location":"serving/app-security/security-guard-install/#install-steps","text":"To start this tutorial, after installing Knative Serving, run the following procedure to replace your queue-proxy image and deploy a guard-service in the current namespace. Install from source Install from released images and yamls Clone the Security-Guard repository using git clone git@github.com:knative-sandbox/security-guard.git Do cd security-guard Run ko apply -Rf ./config Use released images to update your system to enable Security-Guard: Set the feature named queueproxy.mount-podinfo to allowed in the config-features ConfigMap. An easy way to do that is using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/deploy/config-features.yaml Set the deployment parameter queue-sidecar-image to gcr.io/knative-releases/knative.dev/security-guard/cmd/queue in the config-deployment ConfigMap. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/queue-proxy.yaml Add the necessary Security-Guard resources to your cluster using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/gateAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/serviceAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/guardiansCrd.yaml Deploy guard-service on your system to enable automated learning of micro-rules. In the current version, it is recommended to deploy guard-service in any namespace where knative services are deployed. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/guard-service.yaml","title":"Install steps"},{"location":"serving/app-security/security-guard-quickstart/","text":"Security-Guard monitoring quickstart \u00b6 This tutorial shows how you can use Security-Guard to protect a deployed Knative Service. Before you begin \u00b6 Before starting the tutorial, make sure to install Security-Guard Creating and deploying a service \u00b6 Tip The following commands create a helloworld-go sample Service while activating and configuring the Security-Guard extension for this Service. You can modify these commands, including changing the Security-Guard configuration for your service using either the kn CLI or changing the service yaml based on this example. Create a sample securedService: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/guard-activate : enable spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Secured World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env \"TARGET=Secured World\" \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/guard-activate=enable After the Service has been created, Guard starts monitoring the Service Pods and all Events sent to the Service. Continue to Security-Guard alert example to test your installation See the Using Security-Guard section to learn about managing the security of the service Cleanup \u00b6 To remove the deployed service use: Apply YAML kn CLI Delete using the YAML file used to create the service by running the command: kubectl delete -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service delete helloworld-go To remove the Guardian of the deployed service use: ```bash kubectl delete guardians.guard.security.knative.dev helloworld-go ```","title":"\u4fdd\u5b89\u5feb\u901f\u5165\u95e8"},{"location":"serving/app-security/security-guard-quickstart/#security-guard-monitoring-quickstart","text":"This tutorial shows how you can use Security-Guard to protect a deployed Knative Service.","title":"Security-Guard monitoring quickstart"},{"location":"serving/app-security/security-guard-quickstart/#before-you-begin","text":"Before starting the tutorial, make sure to install Security-Guard","title":"Before you begin"},{"location":"serving/app-security/security-guard-quickstart/#creating-and-deploying-a-service","text":"Tip The following commands create a helloworld-go sample Service while activating and configuring the Security-Guard extension for this Service. You can modify these commands, including changing the Security-Guard configuration for your service using either the kn CLI or changing the service yaml based on this example. Create a sample securedService: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/guard-activate : enable spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Secured World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env \"TARGET=Secured World\" \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/guard-activate=enable After the Service has been created, Guard starts monitoring the Service Pods and all Events sent to the Service. Continue to Security-Guard alert example to test your installation See the Using Security-Guard section to learn about managing the security of the service","title":"Creating and deploying a service"},{"location":"serving/app-security/security-guard-quickstart/#cleanup","text":"To remove the deployed service use: Apply YAML kn CLI Delete using the YAML file used to create the service by running the command: kubectl delete -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service delete helloworld-go To remove the Guardian of the deployed service use: ```bash kubectl delete guardians.guard.security.knative.dev helloworld-go ```","title":"Cleanup"},{"location":"serving/autoscaling/","text":"\u81ea\u52a8\u7f29\u653e \u00b6 Knative Serving\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u81ea\u52a8\u4f38\u7f29\uff0c\u6216 autoscaling \uff0c\u4ee5\u5339\u914d\u4f20\u5165\u7684\u9700\u6c42\u3002 \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528Knative Pod Autoscaler (KPA)\u9ed8\u8ba4\u63d0\u4f9b\u7684\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u6ca1\u6709\u63a5\u6536\u6d41\u91cf\uff0c\u5e76\u4e14\u542f\u7528\u4e86\u5411\u96f6\u6269\u5c55\uff0cKnative Serving\u5c06\u5e94\u7528\u7a0b\u5e8f\u6269\u5c55\u5230\u96f6\u526f\u672c\u3002 \u5982\u679c\u7981\u7528\u4e86\u5411\u96f6\u4f38\u7f29\uff0c\u5219\u5e94\u7528\u7a0b\u5e8f\u5c06\u88ab\u7f29\u5c0f\u5230\u96c6\u7fa4\u4e0a\u4e3a\u5e94\u7528\u7a0b\u5e8f\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u91cf\u3002 \u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u7684\u6d41\u91cf\u589e\u52a0\uff0c\u526f\u672c\u5c06\u88ab\u6269\u5927\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u4f38\u7f29\u81f3\u96f6\u529f\u80fd\u3002 \u53c2\u89c1 \u914d\u7f6e\u7f29\u653e\u5230\u96f6 \u3002 \u5982\u679c\u5728\u60a8\u7684\u96c6\u7fa4\u4e0a\u542f\u7528\u4e86\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u8981\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u60a8\u5fc5\u987b\u914d\u7f6e \u5e76\u53d1 \u548c \u4f38\u7f29\u8fb9\u754c \u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u8bd5\u8bd5 Go Autoscale Sample App . \u914d\u7f6e\u60a8\u7684Knative\u90e8\u7f72\u4ee5\u4f7f\u7528Kubernetes Horizontal Pod Autoscaler (HPA)\u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA\u3002\u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29\u5668\u4f7f\u7528\u7684 \u5ea6\u91cf\u7c7b\u578b . \u914d\u7f6e\u60a8\u7684Knative\u670d\u52a1\u4f7f\u7528 container-freeze \uff0c\u5b83\u4f1a\u5728Pod\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u6700\u6709\u4ef7\u503c\u7684\u597d\u5904\u662f\u51cf\u5c11\u4e86\u8fd9\u79cd\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u5173\u4e8e\u81ea\u52a8\u7f29\u653e"},{"location":"serving/autoscaling/#_1","text":"Knative Serving\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u81ea\u52a8\u4f38\u7f29\uff0c\u6216 autoscaling \uff0c\u4ee5\u5339\u914d\u4f20\u5165\u7684\u9700\u6c42\u3002 \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528Knative Pod Autoscaler (KPA)\u9ed8\u8ba4\u63d0\u4f9b\u7684\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u6ca1\u6709\u63a5\u6536\u6d41\u91cf\uff0c\u5e76\u4e14\u542f\u7528\u4e86\u5411\u96f6\u6269\u5c55\uff0cKnative Serving\u5c06\u5e94\u7528\u7a0b\u5e8f\u6269\u5c55\u5230\u96f6\u526f\u672c\u3002 \u5982\u679c\u7981\u7528\u4e86\u5411\u96f6\u4f38\u7f29\uff0c\u5219\u5e94\u7528\u7a0b\u5e8f\u5c06\u88ab\u7f29\u5c0f\u5230\u96c6\u7fa4\u4e0a\u4e3a\u5e94\u7528\u7a0b\u5e8f\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u91cf\u3002 \u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u7684\u6d41\u91cf\u589e\u52a0\uff0c\u526f\u672c\u5c06\u88ab\u6269\u5927\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u4f38\u7f29\u81f3\u96f6\u529f\u80fd\u3002 \u53c2\u89c1 \u914d\u7f6e\u7f29\u653e\u5230\u96f6 \u3002 \u5982\u679c\u5728\u60a8\u7684\u96c6\u7fa4\u4e0a\u542f\u7528\u4e86\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u8981\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u60a8\u5fc5\u987b\u914d\u7f6e \u5e76\u53d1 \u548c \u4f38\u7f29\u8fb9\u754c \u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"serving/autoscaling/#_2","text":"\u8bd5\u8bd5 Go Autoscale Sample App . \u914d\u7f6e\u60a8\u7684Knative\u90e8\u7f72\u4ee5\u4f7f\u7528Kubernetes Horizontal Pod Autoscaler (HPA)\u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA\u3002\u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29\u5668\u4f7f\u7528\u7684 \u5ea6\u91cf\u7c7b\u578b . \u914d\u7f6e\u60a8\u7684Knative\u670d\u52a1\u4f7f\u7528 container-freeze \uff0c\u5b83\u4f1a\u5728Pod\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u6700\u6709\u4ef7\u503c\u7684\u597d\u5904\u662f\u51cf\u5c11\u4e86\u8fd9\u79cd\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"serving/autoscaling/autoscaler-types/","text":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u7c7b\u578b \u00b6 Knative\u670d\u52a1\u652f\u6301Knative Pod Autoscaler (KPA)\u548cKubernetes\u7684 Horizontal Pod Autoscaler (HPA)\u7684\u5b9e\u73b0\u3002 \u672c\u4e3b\u9898\u5217\u51fa\u6bcf\u79cd\u81ea\u52a8\u7f29\u653e\u5668\u7684\u7279\u6027\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981\u7684 \u5982\u679c\u60a8\u60f3\u4f7f\u7528Kubernetes\u6c34\u5e73Pod\u81ea\u52a8\u7f29\u653e\u5668(HPA)\uff0c\u60a8\u5fc5\u987b\u5728\u5b89\u88c5Knative\u670d\u52a1\u4e4b\u540e\u5b89\u88c5\u5b83\u3002 \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . Knative Pod Autoscaler (KPA) \u00b6 Knative\u670d\u52a1\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5Knative\u670d\u52a1\u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8ecpu\u7684\u81ea\u52a8\u4f38\u7f29\u3002 Horizontal Pod Autoscaler (HPA) \u00b6 \u4e0d\u662fKnative Serving\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u5fc5\u987b\u5148\u5b89\u88c5Knative Serving\u3002 \u4e0d\u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301CPU-based\u81ea\u52a8\u5b9a\u91cf\u3002 \u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0 \u00b6 \u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0\u7684\u7c7b\u578b(KPA\u6216HPA)\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 class \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u952e: pod-autoscaler-class \u6bcf\u4e2a\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/class \u53ef\u80fd\u7684\u503c: \"kpa.autoscaling.knative.dev\" \u6216 \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" \u5168\u5c40\u548c\u6bcf\u4e2a\u4fee\u8ba2\u8bbe\u7f6e \u00b6 Knative\u4e2d\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u6216\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u8fdb\u884c\u8bbe\u7f6e\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u6bcf\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u8bbe\u7f6e\uff0c\u5219\u5c06\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u6bcf\u4fee\u8ba2\u7684\u8bbe\u7f6e\uff0c\u5f53\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u8bbe\u7f6e\u90fd\u5b58\u5728\u65f6\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u5c06\u8986\u76d6\u5168\u5c40\u8bbe\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e \u00b6 \u4f7f\u7528 config-autoscaler ConfigMap\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u7684\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u8fd0\u8425\u5546\u5b89\u88c5\u4e86Knative\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5728 spec.config.autoscaler ConfigMap \u4e2d\u8bbe\u7f6e\u5168\u5c40\u914d\u7f6e\u8bbe\u7f6e\uff0c\u4f4d\u4e8e KnativeServing \u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4e2d\u3002 \u9ed8\u8ba4\u81ea\u52a8\u4f38\u7f29 ConfigMap \u793a\u4f8b \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\" \u6bcf\u4fee\u8ba2\u8bbe\u7f6e \u00b6 \u81ea\u52a8\u4f38\u7f29\u7684\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u662f\u901a\u8fc7\u5411\u7248\u672c\u6dfb\u52a0 annotations \u6765\u914d\u7f6e\u7684\u3002 Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u670d\u52a1\u6216\u914d\u7f6e\u521b\u5efa\u4fee\u8ba2\uff0c\u5219\u5fc5\u987b\u5728 revision template \u4e2d\u8bbe\u7f6e\u6ce8\u91ca\uff0c\u4ee5\u4fbf\u5728\u521b\u5efa\u6bcf\u4e2a\u4fee\u8ba2\u65f6\u5c06\u4efb\u4f55\u4fee\u6539\u5e94\u7528\u4e8e\u5b83\u4eec\u3002 \u5728\u5355\u4e2a\u4fee\u8ba2\u7684\u9876\u5c42\u5143\u6570\u636e\u4e2d\u8bbe\u7f6e\u6ce8\u91ca\u4e0d\u4f1a\u5c06\u66f4\u6539\u4f20\u64ad\u5230\u5176\u4ed6\u4fee\u8ba2\uff0c\u4e5f\u4e0d\u4f1a\u5c06\u66f4\u6539\u5e94\u7528\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u4e2d\u3002","title":"\u652f\u6301\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#_1","text":"Knative\u670d\u52a1\u652f\u6301Knative Pod Autoscaler (KPA)\u548cKubernetes\u7684 Horizontal Pod Autoscaler (HPA)\u7684\u5b9e\u73b0\u3002 \u672c\u4e3b\u9898\u5217\u51fa\u6bcf\u79cd\u81ea\u52a8\u7f29\u653e\u5668\u7684\u7279\u6027\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981\u7684 \u5982\u679c\u60a8\u60f3\u4f7f\u7528Kubernetes\u6c34\u5e73Pod\u81ea\u52a8\u7f29\u653e\u5668(HPA)\uff0c\u60a8\u5fc5\u987b\u5728\u5b89\u88c5Knative\u670d\u52a1\u4e4b\u540e\u5b89\u88c5\u5b83\u3002 \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 .","title":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#knative-pod-autoscaler-kpa","text":"Knative\u670d\u52a1\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5Knative\u670d\u52a1\u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8ecpu\u7684\u81ea\u52a8\u4f38\u7f29\u3002","title":"Knative Pod Autoscaler (KPA)"},{"location":"serving/autoscaling/autoscaler-types/#horizontal-pod-autoscaler-hpa","text":"\u4e0d\u662fKnative Serving\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u5fc5\u987b\u5148\u5b89\u88c5Knative Serving\u3002 \u4e0d\u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301CPU-based\u81ea\u52a8\u5b9a\u91cf\u3002","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"serving/autoscaling/autoscaler-types/#_2","text":"\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0\u7684\u7c7b\u578b(KPA\u6216HPA)\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 class \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u952e: pod-autoscaler-class \u6bcf\u4e2a\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/class \u53ef\u80fd\u7684\u503c: \"kpa.autoscaling.knative.dev\" \u6216 \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\"","title":"\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0"},{"location":"serving/autoscaling/autoscaler-types/#_3","text":"Knative\u4e2d\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u6216\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u8fdb\u884c\u8bbe\u7f6e\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u6bcf\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u8bbe\u7f6e\uff0c\u5219\u5c06\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u6bcf\u4fee\u8ba2\u7684\u8bbe\u7f6e\uff0c\u5f53\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u8bbe\u7f6e\u90fd\u5b58\u5728\u65f6\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u5c06\u8986\u76d6\u5168\u5c40\u8bbe\u7f6e\u3002","title":"\u5168\u5c40\u548c\u6bcf\u4e2a\u4fee\u8ba2\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaler-types/#_4","text":"\u4f7f\u7528 config-autoscaler ConfigMap\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u7684\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u8fd0\u8425\u5546\u5b89\u88c5\u4e86Knative\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5728 spec.config.autoscaler ConfigMap \u4e2d\u8bbe\u7f6e\u5168\u5c40\u914d\u7f6e\u8bbe\u7f6e\uff0c\u4f4d\u4e8e KnativeServing \u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4e2d\u3002","title":"\u5168\u5c40\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaler-types/#configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\"","title":"\u9ed8\u8ba4\u81ea\u52a8\u4f38\u7f29 ConfigMap \u793a\u4f8b"},{"location":"serving/autoscaling/autoscaler-types/#_5","text":"\u81ea\u52a8\u4f38\u7f29\u7684\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u662f\u901a\u8fc7\u5411\u7248\u672c\u6dfb\u52a0 annotations \u6765\u914d\u7f6e\u7684\u3002 Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u670d\u52a1\u6216\u914d\u7f6e\u521b\u5efa\u4fee\u8ba2\uff0c\u5219\u5fc5\u987b\u5728 revision template \u4e2d\u8bbe\u7f6e\u6ce8\u91ca\uff0c\u4ee5\u4fbf\u5728\u521b\u5efa\u6bcf\u4e2a\u4fee\u8ba2\u65f6\u5c06\u4efb\u4f55\u4fee\u6539\u5e94\u7528\u4e8e\u5b83\u4eec\u3002 \u5728\u5355\u4e2a\u4fee\u8ba2\u7684\u9876\u5c42\u5143\u6570\u636e\u4e2d\u8bbe\u7f6e\u6ce8\u91ca\u4e0d\u4f1a\u5c06\u66f4\u6539\u4f20\u64ad\u5230\u5176\u4ed6\u4fee\u8ba2\uff0c\u4e5f\u4e0d\u4f1a\u5c06\u66f4\u6539\u5e94\u7528\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u4e2d\u3002","title":"\u6bcf\u4fee\u8ba2\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaling-metrics/","text":"\u6307\u6807 \u00b6 \u6307\u6807\u914d\u7f6e\u5b9a\u4e49\u7531Autoscaler\u76d1\u89c6\u7684\u5ea6\u91cf\u7c7b\u578b\u3002 \u8bbe\u7f6e\u6bcf\u4e2a\u4fee\u8ba2\u7684\u6307\u6807 \u00b6 \u5bf9\u4e8e \u6bcf\u4fee\u8ba2 \u914d\u7f6e\uff0c\u8fd9\u662f\u4f7f\u7528 autoscaling.knative.dev/metric \u6ce8\u91ca\u786e\u5b9a\u7684\u3002 \u53ef\u4ee5\u5728\u6bcf\u4e2a\u7248\u672c\u4e2d\u914d\u7f6e\u7684\u53ef\u80fd\u7684\u5ea6\u91cf\u7c7b\u578b\u53d6\u51b3\u4e8e\u60a8\u6b63\u5728\u4f7f\u7528\u7684Autoscaler\u5b9e\u73b0\u7684\u7c7b\u578b: \u9ed8\u8ba4\u7684KPA Autoscaler\u652f\u6301 concurrency \u548c rps \u6307\u6807\u3002 HPA Autoscaler \u652f\u6301 cpu \u6307\u6807\u3002 \u6709\u5173KPA\u548cHPA\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u578b \u7684\u6587\u6863\u3002 \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/metric \u53ef\u80fd\u503c: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" \u6216\u4efb\u4f55\u81ea\u5b9a\u4e49\u6307\u6807\u540d\u79f0\uff0c\u8fd9\u53d6\u51b3\u4e8e\u60a8\u7684Autoscaler\u7c7b\u578b. \"cpu\" , \"memory\" \u548c \"custom\" \u6307\u6807\u4ec5\u5728\u4f7f\u7528HPA\u7c7b\u7684\u7248\u672c\u4e2d\u652f\u6301\u3002 \u9ed8\u8ba4: \"concurrency\" \u6bcf\u4fee\u8ba2\u5e76\u53d1\u914d\u7f6e \u6bcf\u4fee\u8ba2rps\u914d\u7f6e \u6bcf\u4fee\u8ba2cpu\u914d\u7f6e \u6bcf\u4fee\u8ba2\u5185\u5b58\u914d\u7f6e \u6bcf\u4fee\u8ba2\u81ea\u5b9a\u4e49\u6307\u6807\u914d\u7f6e apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" \u60a8\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aHPA\uff0c\u4ee5\u6839\u636e\u60a8\u6307\u5b9a\u7684\u6307\u6807\u6765\u6269\u5c55\u4fee\u8ba2\u3002 HPA\u5c06\u88ab\u914d\u7f6e\u4e3a\u5728\u4fee\u8ba2\u7684\u6240\u6709Pods\u4e2d\u4f7f\u7528\u5ea6\u91cf\u7684 \u5e73\u5747\u503c \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric. \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e \u5e76\u53d1\u76ee\u6807 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u526f\u672c\u914d\u7f6e \u6bcf\u79d2\u8bf7\u6c42\u76ee\u6807","title":"\u914d\u7f6e\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_1","text":"\u6307\u6807\u914d\u7f6e\u5b9a\u4e49\u7531Autoscaler\u76d1\u89c6\u7684\u5ea6\u91cf\u7c7b\u578b\u3002","title":"\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_2","text":"\u5bf9\u4e8e \u6bcf\u4fee\u8ba2 \u914d\u7f6e\uff0c\u8fd9\u662f\u4f7f\u7528 autoscaling.knative.dev/metric \u6ce8\u91ca\u786e\u5b9a\u7684\u3002 \u53ef\u4ee5\u5728\u6bcf\u4e2a\u7248\u672c\u4e2d\u914d\u7f6e\u7684\u53ef\u80fd\u7684\u5ea6\u91cf\u7c7b\u578b\u53d6\u51b3\u4e8e\u60a8\u6b63\u5728\u4f7f\u7528\u7684Autoscaler\u5b9e\u73b0\u7684\u7c7b\u578b: \u9ed8\u8ba4\u7684KPA Autoscaler\u652f\u6301 concurrency \u548c rps \u6307\u6807\u3002 HPA Autoscaler \u652f\u6301 cpu \u6307\u6807\u3002 \u6709\u5173KPA\u548cHPA\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u578b \u7684\u6587\u6863\u3002 \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/metric \u53ef\u80fd\u503c: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" \u6216\u4efb\u4f55\u81ea\u5b9a\u4e49\u6307\u6807\u540d\u79f0\uff0c\u8fd9\u53d6\u51b3\u4e8e\u60a8\u7684Autoscaler\u7c7b\u578b. \"cpu\" , \"memory\" \u548c \"custom\" \u6307\u6807\u4ec5\u5728\u4f7f\u7528HPA\u7c7b\u7684\u7248\u672c\u4e2d\u652f\u6301\u3002 \u9ed8\u8ba4: \"concurrency\" \u6bcf\u4fee\u8ba2\u5e76\u53d1\u914d\u7f6e \u6bcf\u4fee\u8ba2rps\u914d\u7f6e \u6bcf\u4fee\u8ba2cpu\u914d\u7f6e \u6bcf\u4fee\u8ba2\u5185\u5b58\u914d\u7f6e \u6bcf\u4fee\u8ba2\u81ea\u5b9a\u4e49\u6307\u6807\u914d\u7f6e apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" \u60a8\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aHPA\uff0c\u4ee5\u6839\u636e\u60a8\u6307\u5b9a\u7684\u6307\u6807\u6765\u6269\u5c55\u4fee\u8ba2\u3002 HPA\u5c06\u88ab\u914d\u7f6e\u4e3a\u5728\u4fee\u8ba2\u7684\u6240\u6709Pods\u4e2d\u4f7f\u7528\u5ea6\u91cf\u7684 \u5e73\u5747\u503c \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric.","title":"\u8bbe\u7f6e\u6bcf\u4e2a\u4fee\u8ba2\u7684\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_3","text":"\u4e3a\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e \u5e76\u53d1\u76ee\u6807 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u526f\u672c\u914d\u7f6e \u6bcf\u79d2\u8bf7\u6c42\u76ee\u6807","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"serving/autoscaling/autoscaling-targets/","text":"\u76ee\u6807 \u00b6 \u914d\u7f6e\u76ee\u6807\u4e3aAutoscaler\u63d0\u4f9b\u4e00\u4e2a\u503c\uff0c\u5b83\u8bd5\u56fe\u4e3a\u4fee\u8ba2\u7684\u914d\u7f6e\u6307\u6807\u7ef4\u62a4\u8be5\u503c\u3002 \u6709\u5173\u53ef\u914d\u7f6e\u5ea6\u91cf\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u6307\u6807 \u6587\u6863\u3002 \u7528\u4e8e\u914d\u7f6e\u6bcf\u4e2a\u7248\u672c\u76ee\u6807\u7684 target \u6ce8\u91ca\u662f metric agnostic \u3002 \u8fd9\u610f\u5473\u7740\u76ee\u6807\u53ea\u662f\u4e00\u4e2a\u6574\u6570\u503c\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u5ea6\u91cf\u7c7b\u578b\u3002 \u914d\u7f6e\u76ee\u6807 \u00b6 \u5168\u5c40\u8bbe\u7f6e\u952e: container-concurrency-target-default . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u6307\u6807 \u7684\u6587\u6863\u3002. \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u80fd\u503c: An integer (metric agnostic). \u9ed8\u8ba4\u503c: \"100\" for container-concurrency-target-default . \u6ca1\u6709\u4e3a target \u6ce8\u91ca\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u76ee\u6807\u6ce8\u91ca-\u6bcf\u4fee\u8ba2 \u5e76\u53d1\u76ee\u6807-\u5168\u5c40(ConfigMap) \u5e76\u53d1\u76ee\u6807-\u5168\u5c40\u5bb9\u5668(Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u914d\u7f6e\u76ee\u6807"},{"location":"serving/autoscaling/autoscaling-targets/#_1","text":"\u914d\u7f6e\u76ee\u6807\u4e3aAutoscaler\u63d0\u4f9b\u4e00\u4e2a\u503c\uff0c\u5b83\u8bd5\u56fe\u4e3a\u4fee\u8ba2\u7684\u914d\u7f6e\u6307\u6807\u7ef4\u62a4\u8be5\u503c\u3002 \u6709\u5173\u53ef\u914d\u7f6e\u5ea6\u91cf\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u6307\u6807 \u6587\u6863\u3002 \u7528\u4e8e\u914d\u7f6e\u6bcf\u4e2a\u7248\u672c\u76ee\u6807\u7684 target \u6ce8\u91ca\u662f metric agnostic \u3002 \u8fd9\u610f\u5473\u7740\u76ee\u6807\u53ea\u662f\u4e00\u4e2a\u6574\u6570\u503c\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u5ea6\u91cf\u7c7b\u578b\u3002","title":"\u76ee\u6807"},{"location":"serving/autoscaling/autoscaling-targets/#_2","text":"\u5168\u5c40\u8bbe\u7f6e\u952e: container-concurrency-target-default . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u6307\u6807 \u7684\u6587\u6863\u3002. \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u80fd\u503c: An integer (metric agnostic). \u9ed8\u8ba4\u503c: \"100\" for container-concurrency-target-default . \u6ca1\u6709\u4e3a target \u6ce8\u91ca\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u76ee\u6807\u6ce8\u91ca-\u6bcf\u4fee\u8ba2 \u5e76\u53d1\u76ee\u6807-\u5168\u5c40(ConfigMap) \u5e76\u53d1\u76ee\u6807-\u5168\u5c40\u5bb9\u5668(Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u914d\u7f6e\u76ee\u6807"},{"location":"serving/autoscaling/concurrency/","text":"\u914d\u7f6e\u5e76\u53d1 \u00b6 \u5e76\u53d1\u6027\u51b3\u5b9a\u4e86\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u5728\u4efb\u4f55\u7ed9\u5b9a\u65f6\u95f4\u5185\u53ef\u4ee5\u5904\u7406\u7684\u5e76\u53d1\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u7248\u672c\u5e76\u53d1\uff0c\u60a8\u5fc5\u987b\u540c\u65f6\u914d\u7f6e autoscaling.knative.dev/metric \u548c autoscaling.knative.dev/target \u4f5c\u4e3a \u8f6f\u9650\u5236 \uff0c\u6216 containerConcurrency \u4f5c\u4e3a \u786c\u9650\u5236 \u3002 \u5bf9\u4e8e\u5168\u5c40\u5e76\u53d1\u6027\uff0c\u60a8\u53ef\u4ee5\u8bbe\u7f6e container-concurrency-target-default \u503c\u3002 \u8f6f\u4e0e\u786c\u5e76\u53d1\u9650\u5236 \u00b6 \u53ef\u4ee5\u8bbe\u7f6e \u8f6f / \u786c \u5e76\u53d1\u9650\u5236\u3002 Note \u5982\u679c\u540c\u65f6\u6307\u5b9a\u4e86\u8f6f\u9650\u5236\u548c\u786c\u9650\u5236\uff0c\u5219\u5c06\u4f7f\u7528\u4e24\u4e2a\u503c\u4e2d\u8f83\u5c0f\u7684\u503c\u3002 \u8fd9\u53ef\u4ee5\u9632\u6b62Autoscaler\u62e5\u6709\u786c\u9650\u5236\u503c\u4e0d\u5141\u8bb8\u7684\u76ee\u6807\u503c\u3002 \u8f6f\u9650\u989d\u662f\u4e00\u4e2a\u6709\u9488\u5bf9\u6027\u7684\u9650\u989d\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u4e25\u683c\u6267\u884c\u7684\u9650\u989d\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8bf7\u6c42\u7a81\u7136\u7206\u53d1\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u786c\u6027\u9650\u5236\u662f\u5f3a\u5236\u7684\u4e0a\u9650\u3002 \u5982\u679c\u5e76\u53d1\u6027\u8fbe\u5230\u786c\u9650\u5236\uff0c\u591a\u4f59\u7684\u8bf7\u6c42\u5c06\u88ab\u7f13\u51b2\uff0c\u5fc5\u987b\u7b49\u5f85\uff0c\u76f4\u5230\u6709\u8db3\u591f\u7684\u7a7a\u95f2\u5bb9\u91cf\u6765\u6267\u884c\u8bf7\u6c42\u3002 Warning \u53ea\u6709\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6709\u660e\u786e\u7684\u7528\u4f8b\u65f6\uff0c\u624d\u5efa\u8bae\u4f7f\u7528\u786c\u9650\u5236\u914d\u7f6e\u3002 \u6307\u5b9a\u8f83\u4f4e\u7684\u786c\u9650\u5236\u53ef\u80fd\u4f1a\u5bf9\u5e94\u7528\u7a0b\u5e8f\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u989d\u5916\u7684\u51b7\u542f\u52a8\u3002 \u8f6f\u9650\u5236 \u00b6 \u5168\u5c40\u952e: container-concurrency-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"100\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" \u786c\u9650\u5236 \u00b6 \u786c\u9650\u5236\u662f\u5728\u6bcf\u4e2a\u4fee\u8ba2\u4e2d\u4f7f\u7528\u4fee\u8ba2\u89c4\u8303\u4e0a\u7684 containerConcurrency \u5b57\u6bb5\u6307\u5b9a\u7684\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u662f\u6ce8\u91ca\u3002 \u5728\u81ea\u52a8\u4f38\u7f29 ConfigMap \u4e2d\u6ca1\u6709\u786c\u9650\u5236\u7684\u5168\u5c40\u8bbe\u7f6e\uff0c\u56e0\u4e3a containerConcurrency \u5728\u81ea\u52a8\u4f38\u7f29\u4e4b\u5916\u4e5f\u6709\u5f71\u54cd\uff0c\u6bd4\u5982\u5bf9\u8bf7\u6c42\u7684\u7f13\u51b2\u548c\u6392\u961f\u3002 \u4f46\u662f\uff0c\u53ef\u4ee5\u5728 config-defaults.yaml \u4e2d\u4e3a\u4fee\u8ba2\u7248\u7684 containerConcurrency \u5b57\u6bb5\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u9ed8\u8ba4\u503c\u662f 0 \uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9650\u5236\u5141\u8bb8\u6d41\u5165\u4fee\u8ba2\u7248\u7684\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5927\u4e8e 0 \u7684\u503c\u6307\u5b9a\u5728\u4efb\u4f55\u65f6\u5019\u5141\u8bb8\u6d41\u5411\u526f\u672c\u7684\u786e\u5207\u8bf7\u6c42\u6570\u3002 \u5168\u5c40\u952e: container-concurrency (in config-defaults.yaml ) \u6bcf\u4fee\u8ba2\u89c4\u8303\u952e: containerConcurrency \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 , \u610f\u601d\u662f\u6ca1\u6709\u9650\u5236 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (\u9ed8\u8ba4 ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\" \u76ee\u6807\u5229\u7528\u7387 \u00b6 \u9664\u4e86\u524d\u9762\u89e3\u91ca\u7684\u6587\u5b57\u8bbe\u7f6e\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u76ee\u6807\u5229\u7528\u7387\u503c \u8fdb\u4e00\u6b65\u8c03\u6574\u5e76\u53d1\u503c\u3002 \u8be5\u503c\u6307\u5b9aAutoscaler\u5b9e\u9645\u9488\u5bf9\u524d\u9762\u6307\u5b9a\u7684\u76ee\u6807\u7684\u767e\u5206\u6bd4\u3002 \u8fd9\u4e5f\u88ab\u79f0\u4e3a\u6307\u5b9a\u526f\u672c\u8fd0\u884c\u65f6\u7684 hotness \uff0c\u8fd9\u5c06\u5bfc\u81f4Autoscaler\u5728\u8fbe\u5230\u5b9a\u4e49\u7684\u786c\u9650\u5236\u4e4b\u524d\u6269\u5927\u3002 \u4f8b\u5982\uff0c\u5982\u679c containerConcurrency \u8bbe\u7f6e\u4e3a10\uff0c\u76ee\u6807\u5229\u7528\u7387\u8bbe\u7f6e\u4e3a70%(\u767e\u5206\u4e4b\u4e03\u5341)\uff0c\u5f53\u6240\u6709\u73b0\u6709\u526f\u672c\u7684\u5e73\u5747\u5e76\u53d1\u8bf7\u6c42\u6570\u8fbe\u52307\u65f6\uff0cAutoscaler\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u526f\u672c\u3002 \u7f16\u53f7\u4e3a7\u523010\u7684\u8bf7\u6c42\u4ecd\u7136\u4f1a\u88ab\u53d1\u9001\u5230\u73b0\u6709\u7684\u526f\u672c\uff0c\u4f46\u8fd9\u5141\u8bb8\u5728\u8fbe\u5230 containerConcurrency \u9650\u5236\u65f6\u542f\u52a8\u989d\u5916\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: container-concurrency-target-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target-utilization-percentage \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 70 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"\u914d\u7f6e\u5e76\u53d1"},{"location":"serving/autoscaling/concurrency/#_1","text":"\u5e76\u53d1\u6027\u51b3\u5b9a\u4e86\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u5728\u4efb\u4f55\u7ed9\u5b9a\u65f6\u95f4\u5185\u53ef\u4ee5\u5904\u7406\u7684\u5e76\u53d1\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u7248\u672c\u5e76\u53d1\uff0c\u60a8\u5fc5\u987b\u540c\u65f6\u914d\u7f6e autoscaling.knative.dev/metric \u548c autoscaling.knative.dev/target \u4f5c\u4e3a \u8f6f\u9650\u5236 \uff0c\u6216 containerConcurrency \u4f5c\u4e3a \u786c\u9650\u5236 \u3002 \u5bf9\u4e8e\u5168\u5c40\u5e76\u53d1\u6027\uff0c\u60a8\u53ef\u4ee5\u8bbe\u7f6e container-concurrency-target-default \u503c\u3002","title":"\u914d\u7f6e\u5e76\u53d1"},{"location":"serving/autoscaling/concurrency/#_2","text":"\u53ef\u4ee5\u8bbe\u7f6e \u8f6f / \u786c \u5e76\u53d1\u9650\u5236\u3002 Note \u5982\u679c\u540c\u65f6\u6307\u5b9a\u4e86\u8f6f\u9650\u5236\u548c\u786c\u9650\u5236\uff0c\u5219\u5c06\u4f7f\u7528\u4e24\u4e2a\u503c\u4e2d\u8f83\u5c0f\u7684\u503c\u3002 \u8fd9\u53ef\u4ee5\u9632\u6b62Autoscaler\u62e5\u6709\u786c\u9650\u5236\u503c\u4e0d\u5141\u8bb8\u7684\u76ee\u6807\u503c\u3002 \u8f6f\u9650\u989d\u662f\u4e00\u4e2a\u6709\u9488\u5bf9\u6027\u7684\u9650\u989d\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u4e25\u683c\u6267\u884c\u7684\u9650\u989d\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8bf7\u6c42\u7a81\u7136\u7206\u53d1\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u786c\u6027\u9650\u5236\u662f\u5f3a\u5236\u7684\u4e0a\u9650\u3002 \u5982\u679c\u5e76\u53d1\u6027\u8fbe\u5230\u786c\u9650\u5236\uff0c\u591a\u4f59\u7684\u8bf7\u6c42\u5c06\u88ab\u7f13\u51b2\uff0c\u5fc5\u987b\u7b49\u5f85\uff0c\u76f4\u5230\u6709\u8db3\u591f\u7684\u7a7a\u95f2\u5bb9\u91cf\u6765\u6267\u884c\u8bf7\u6c42\u3002 Warning \u53ea\u6709\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6709\u660e\u786e\u7684\u7528\u4f8b\u65f6\uff0c\u624d\u5efa\u8bae\u4f7f\u7528\u786c\u9650\u5236\u914d\u7f6e\u3002 \u6307\u5b9a\u8f83\u4f4e\u7684\u786c\u9650\u5236\u53ef\u80fd\u4f1a\u5bf9\u5e94\u7528\u7a0b\u5e8f\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u989d\u5916\u7684\u51b7\u542f\u52a8\u3002","title":"\u8f6f\u4e0e\u786c\u5e76\u53d1\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_3","text":"\u5168\u5c40\u952e: container-concurrency-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"100\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u8f6f\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_4","text":"\u786c\u9650\u5236\u662f\u5728\u6bcf\u4e2a\u4fee\u8ba2\u4e2d\u4f7f\u7528\u4fee\u8ba2\u89c4\u8303\u4e0a\u7684 containerConcurrency \u5b57\u6bb5\u6307\u5b9a\u7684\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u662f\u6ce8\u91ca\u3002 \u5728\u81ea\u52a8\u4f38\u7f29 ConfigMap \u4e2d\u6ca1\u6709\u786c\u9650\u5236\u7684\u5168\u5c40\u8bbe\u7f6e\uff0c\u56e0\u4e3a containerConcurrency \u5728\u81ea\u52a8\u4f38\u7f29\u4e4b\u5916\u4e5f\u6709\u5f71\u54cd\uff0c\u6bd4\u5982\u5bf9\u8bf7\u6c42\u7684\u7f13\u51b2\u548c\u6392\u961f\u3002 \u4f46\u662f\uff0c\u53ef\u4ee5\u5728 config-defaults.yaml \u4e2d\u4e3a\u4fee\u8ba2\u7248\u7684 containerConcurrency \u5b57\u6bb5\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u9ed8\u8ba4\u503c\u662f 0 \uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9650\u5236\u5141\u8bb8\u6d41\u5165\u4fee\u8ba2\u7248\u7684\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5927\u4e8e 0 \u7684\u503c\u6307\u5b9a\u5728\u4efb\u4f55\u65f6\u5019\u5141\u8bb8\u6d41\u5411\u526f\u672c\u7684\u786e\u5207\u8bf7\u6c42\u6570\u3002 \u5168\u5c40\u952e: container-concurrency (in config-defaults.yaml ) \u6bcf\u4fee\u8ba2\u89c4\u8303\u952e: containerConcurrency \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 , \u610f\u601d\u662f\u6ca1\u6709\u9650\u5236 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (\u9ed8\u8ba4 ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\"","title":"\u786c\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_5","text":"\u9664\u4e86\u524d\u9762\u89e3\u91ca\u7684\u6587\u5b57\u8bbe\u7f6e\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u76ee\u6807\u5229\u7528\u7387\u503c \u8fdb\u4e00\u6b65\u8c03\u6574\u5e76\u53d1\u503c\u3002 \u8be5\u503c\u6307\u5b9aAutoscaler\u5b9e\u9645\u9488\u5bf9\u524d\u9762\u6307\u5b9a\u7684\u76ee\u6807\u7684\u767e\u5206\u6bd4\u3002 \u8fd9\u4e5f\u88ab\u79f0\u4e3a\u6307\u5b9a\u526f\u672c\u8fd0\u884c\u65f6\u7684 hotness \uff0c\u8fd9\u5c06\u5bfc\u81f4Autoscaler\u5728\u8fbe\u5230\u5b9a\u4e49\u7684\u786c\u9650\u5236\u4e4b\u524d\u6269\u5927\u3002 \u4f8b\u5982\uff0c\u5982\u679c containerConcurrency \u8bbe\u7f6e\u4e3a10\uff0c\u76ee\u6807\u5229\u7528\u7387\u8bbe\u7f6e\u4e3a70%(\u767e\u5206\u4e4b\u4e03\u5341)\uff0c\u5f53\u6240\u6709\u73b0\u6709\u526f\u672c\u7684\u5e73\u5747\u5e76\u53d1\u8bf7\u6c42\u6570\u8fbe\u52307\u65f6\uff0cAutoscaler\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u526f\u672c\u3002 \u7f16\u53f7\u4e3a7\u523010\u7684\u8bf7\u6c42\u4ecd\u7136\u4f1a\u88ab\u53d1\u9001\u5230\u73b0\u6709\u7684\u526f\u672c\uff0c\u4f46\u8fd9\u5141\u8bb8\u5728\u8fbe\u5230 containerConcurrency \u9650\u5236\u65f6\u542f\u52a8\u989d\u5916\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: container-concurrency-target-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target-utilization-percentage \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 70 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"\u76ee\u6807\u5229\u7528\u7387"},{"location":"serving/autoscaling/container-freezer/","text":"\u914d\u7f6e\u5bb9\u5668\u51b0\u7bb1 \u00b6 \u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u5728\u5176\u6d41\u91cf\u964d\u81f3\u96f6\u6216\u4ece\u96f6\u6269\u5c55\u65f6\u8c03\u7528\u7aef\u70b9API\u3002 \u5728\u793e\u533a\u7ef4\u62a4\u7684\u7aef\u70b9API\u5b9e\u73b0\u5bb9\u5668-\u51b7\u51bb\u4e2d\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u51bb\u7ed3\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u4ece\u96f6\u4e0a\u5347\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u6062\u590d\u3002 \u4f46\u662f\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u8fd0\u884c\u81ea\u5df1\u7684\u5b9e\u73b0(\u4f8b\u5982\uff0c\u4f5c\u4e3a\u8ba1\u8d39\u7ec4\u4ef6\uff0c\u5728\u5904\u7406\u8bf7\u6c42\u65f6\u8bb0\u5f55\u65e5\u5fd7)\u3002 \u914d\u7f6emin-scale \u00b6 \u8981\u4f7f\u7528\u5bb9\u5668\u51b7\u51bb\uff0c\u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e autoscaling.knative.dev/min-scale \u7684\u503c\u5fc5\u987b\u5927\u4e8e\u96f6\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go \u914d\u7f6e\u7aef\u70b9API\u5730\u5740 \u00b6 \u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u8c03\u7528\u7aef\u70b9API\u5730\u5740\uff0c\u56e0\u6b64\u9700\u8981\u914d\u7f6eAPI\u5730\u5740\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6253\u5f00 config-deployment ConfigMap: kubectl edit configmap config-deployment -n knative-serving \u4f8b\u5982\uff0c\u7f16\u8f91\u8be5\u6587\u4ef6\u4ee5\u914d\u7f6e\u7aef\u70b9API\u5730\u5740: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note \u5982\u679c\u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0, \u4f7f\u7528 http://$HOST_IP:9696 \u4f5c\u4e3a concurrency-state-endpoint \u7684\u503c, \u7531\u4e8e\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\uff0c\u76f8\u5e94\u7684\u503c\u5c06\u5728\u8fd0\u884c\u65f6\u7531\u961f\u5217\u4ee3\u7406\u63d2\u5165. \u5982\u679c\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\u5b9e\u73b0\u90e8\u7f72\u5728\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u4e2d\uff0c\u5219\u4f7f\u7528\u7279\u5b9a\u7684\u670d\u52a1\u5730\u5740\uff0c\u4f8b\u5982 http://billing.default.svc:9696 . \u4e0b\u4e00\u6b65 \u00b6 \u5b9e\u73b0\u60a8\u81ea\u5df1\u7684\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684 container-freeze \u5b9e\u73b0\u3002","title":"container-freezer"},{"location":"serving/autoscaling/container-freezer/#_1","text":"\u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u5728\u5176\u6d41\u91cf\u964d\u81f3\u96f6\u6216\u4ece\u96f6\u6269\u5c55\u65f6\u8c03\u7528\u7aef\u70b9API\u3002 \u5728\u793e\u533a\u7ef4\u62a4\u7684\u7aef\u70b9API\u5b9e\u73b0\u5bb9\u5668-\u51b7\u51bb\u4e2d\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u51bb\u7ed3\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u4ece\u96f6\u4e0a\u5347\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u6062\u590d\u3002 \u4f46\u662f\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u8fd0\u884c\u81ea\u5df1\u7684\u5b9e\u73b0(\u4f8b\u5982\uff0c\u4f5c\u4e3a\u8ba1\u8d39\u7ec4\u4ef6\uff0c\u5728\u5904\u7406\u8bf7\u6c42\u65f6\u8bb0\u5f55\u65e5\u5fd7)\u3002","title":"\u914d\u7f6e\u5bb9\u5668\u51b0\u7bb1"},{"location":"serving/autoscaling/container-freezer/#min-scale","text":"\u8981\u4f7f\u7528\u5bb9\u5668\u51b7\u51bb\uff0c\u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e autoscaling.knative.dev/min-scale \u7684\u503c\u5fc5\u987b\u5927\u4e8e\u96f6\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u914d\u7f6emin-scale"},{"location":"serving/autoscaling/container-freezer/#api","text":"\u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u8c03\u7528\u7aef\u70b9API\u5730\u5740\uff0c\u56e0\u6b64\u9700\u8981\u914d\u7f6eAPI\u5730\u5740\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6253\u5f00 config-deployment ConfigMap: kubectl edit configmap config-deployment -n knative-serving \u4f8b\u5982\uff0c\u7f16\u8f91\u8be5\u6587\u4ef6\u4ee5\u914d\u7f6e\u7aef\u70b9API\u5730\u5740: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note \u5982\u679c\u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0, \u4f7f\u7528 http://$HOST_IP:9696 \u4f5c\u4e3a concurrency-state-endpoint \u7684\u503c, \u7531\u4e8e\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\uff0c\u76f8\u5e94\u7684\u503c\u5c06\u5728\u8fd0\u884c\u65f6\u7531\u961f\u5217\u4ee3\u7406\u63d2\u5165. \u5982\u679c\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\u5b9e\u73b0\u90e8\u7f72\u5728\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u4e2d\uff0c\u5219\u4f7f\u7528\u7279\u5b9a\u7684\u670d\u52a1\u5730\u5740\uff0c\u4f8b\u5982 http://billing.default.svc:9696 .","title":"\u914d\u7f6e\u7aef\u70b9API\u5730\u5740"},{"location":"serving/autoscaling/container-freezer/#_2","text":"\u5b9e\u73b0\u60a8\u81ea\u5df1\u7684\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684 container-freeze \u5b9e\u73b0\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"serving/autoscaling/kpa-specific/","text":"Knative Pod Autoscaler\u7684\u989d\u5916\u81ea\u52a8\u7f29\u653e\u914d\u7f6e \u00b6 \u4ee5\u4e0b\u8bbe\u7f6e\u662f\u9488\u5bf9Knative Pod Autoscaler (KPA)\u7684\u3002 \u6a21\u5f0f \u00b6 KPA\u5bf9\u57fa\u4e8e\u65f6\u95f4\u7684\u7a97\u53e3\u4e2d\u805a\u5408\u7684 \u6307\u6807 \u8d77\u4f5c\u7528\u3002 \u8fd9\u4e9b\u7a97\u53e3\u5b9a\u4e49Autoscaler\u8003\u8651\u7684\u5386\u53f2\u6570\u636e\u91cf\uff0c\u5e76\u7528\u4e8e\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u5e73\u6ed1\u6570\u636e\u3002 \u8fd9\u4e9b\u7a97\u53e3\u8d8a\u77ed\uff0cAutoscaler\u7684\u53cd\u5e94\u5c31\u8d8a\u5feb\u3002 KPA\u7684\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f: stable and panic \u3002 \u6bcf\u4e2a\u6a21\u5f0f\u90fd\u6709\u5355\u72ec\u7684\u805a\u5408\u7a97\u53e3: stable-window and panic-window \u3002 \u7a33\u5b9a\u6a21\u5f0f\u7528\u4e8e\u4e00\u822c\u64cd\u4f5c\uff0c\u800c\u6050\u614c\u6a21\u5f0f\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6709\u4e00\u4e2a\u66f4\u77ed\u7684\u7a97\u53e3\uff0c\u5982\u679c\u6d41\u91cf\u6fc0\u589e\uff0c\u5c06\u88ab\u7528\u4e8e\u5feb\u901f\u6269\u5c55\u4fee\u8ba2\u3002 \u6ce8\u91ca \u5f53\u4f7f\u7528\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u7248\u672c\u5c06\u4e0d\u4f1a\u7f29\u5c0f\u4ee5\u907f\u514d\u6d41\u5931\u3002 \u5982\u679c\u5728\u7a33\u5b9a\u7a97\u53e3\u65f6\u95f4\u5185\u6ca1\u6709\u5feb\u901f\u53cd\u5e94\u7684\u7406\u7531\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u79bb\u5f00\u6050\u614c\u6a21\u5f0f\u3002 \u7a33\u5b9a\u7a97\u53e3 \u00b6 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5f53\u526f\u672c\u5f52\u96f6\u65f6\uff0c\u53ea\u6709\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u4efb\u4f55\u8bbf\u95ee\u4fee\u8ba2\u7248\u672c\u7684\u6d41\u91cf\u65f6\uff0c\u624d\u4f1a\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\" \u6050\u614c\u7a97\u53e3 \u00b6 \u6050\u614c\u7a97\u53e3\u88ab\u5b9a\u4e49\u4e3a\u7a33\u5b9a\u7a97\u53e3\u7684\u767e\u5206\u6bd4\uff0c\u786e\u4fdd\u4e24\u8005\u5728\u5de5\u4f5c\u65b9\u5f0f\u4e0a\u5f7c\u6b64\u76f8\u5bf9\u3002 \u6b64\u503c\u6307\u793a\u5728\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u5bf9\u5386\u53f2\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u7684\u7a97\u53e3\u5c06\u5982\u4f55\u6536\u7f29\u3002 \u4f8b\u5982\uff0c\u503c 10.0 \u610f\u5473\u7740\u5728\u7d27\u6025\u6a21\u5f0f\u4e0b\uff0c\u7a97\u53e3\u5c06\u662f\u7a33\u5b9a\u7a97\u53e3\u5927\u5c0f\u768410%\u3002 \u5168\u5c40\u952e: panic-window-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-window-percentage \u53ef\u7528\u503c: float, 1.0 <= value <= 100.0 \u9ed8\u8ba4\u503c: 10.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\" \u6050\u614c\u7684\u9608\u503c \u00b6 \u8fd9\u4e2a\u9608\u503c\u5b9a\u4e49Autoscaler\u4f55\u65f6\u4ece\u7a33\u5b9a\u6a21\u5f0f\u8f6c\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u8be5\u503c\u662f\u5f53\u524d\u526f\u672c\u6570\u91cf\u6240\u80fd\u5904\u7406\u7684\u6d41\u91cf\u7684\u767e\u5206\u6bd4\u3002 Note \u503c 100.0 (100%)\u610f\u5473\u7740Autoscaler\u603b\u662f\u5904\u4e8e\u7d27\u6025\u6a21\u5f0f\uff0c\u56e0\u6b64\u6700\u5c0f\u503c\u5e94\u8be5\u9ad8\u4e8e 100.0 \u3002 \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a 200.0 \u610f\u5473\u7740\u5982\u679c\u6d41\u91cf\u662f\u5f53\u524d\u526f\u672c\u603b\u4f53\u53ef\u4ee5\u5904\u7406\u7684\u4e24\u500d\uff0c\u5c06\u542f\u52a8\u6050\u614c\u6a21\u5f0f\u3002 \u5168\u5c40\u952e: panic-threshold-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-threshold-percentage \u53ef\u7528\u503c: float, 110.0 <= value <= 1000.0 \u9ed8\u8ba4\u503c: 200.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\" \u4f38\u7f29\u6bd4\u7387 \u00b6 \u8fd9\u4e9b\u8bbe\u7f6e\u901a\u8fc7\u5728\u5355\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u590d\u5236\u96c6\u7fa4\u53ef\u4ee5\u6269\u5927\u6216\u7f29\u5c0f\u591a\u5c11\u6765\u63a7\u5236\u3002 \u5728\u6bcf\u4e2a\u65b9\u5411\u4e0a\u603b\u662f\u5141\u8bb8\u4e00\u4e2a\u526f\u672c\u7684\u6700\u5c0f\u53d8\u5316\uff0c\u56e0\u6b64Autoscaler\u53ef\u4ee5\u968f\u65f6\u6269\u5c55\u5230+/- 1\u526f\u672c\uff0c\u800c\u4e0d\u7ba1\u8bbe\u7f6e\u7684\u7f29\u653e\u7387\u5982\u4f55\u3002 \u6269\u5927\u7387 \u00b6 \u6b64\u8bbe\u7f6e\u786e\u5b9a\u6240\u9700\u4e0e\u73b0\u6709Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u503c\u4e3a 2.0 \uff0c\u5219\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 2*N \u4e2aPods\u3002 \u5168\u5c40\u952e: max-scale-up-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 1000.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\" \u964d\u4f4e\u7387 \u00b6 \u6b64\u8bbe\u7f6e\u786e\u5b9a\u73b0\u6709Pod\u4e0e\u6240\u9700Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5f53\u503c\u4e3a 2.0 \u65f6\uff0c\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u5185\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 N/2 \u4e2apod\u3002 \u5168\u5c40\u952e: max-scale-down-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 2.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"\u989d\u5916\u914d\u7f6e"},{"location":"serving/autoscaling/kpa-specific/#knative-pod-autoscaler","text":"\u4ee5\u4e0b\u8bbe\u7f6e\u662f\u9488\u5bf9Knative Pod Autoscaler (KPA)\u7684\u3002","title":"Knative Pod Autoscaler\u7684\u989d\u5916\u81ea\u52a8\u7f29\u653e\u914d\u7f6e"},{"location":"serving/autoscaling/kpa-specific/#_1","text":"KPA\u5bf9\u57fa\u4e8e\u65f6\u95f4\u7684\u7a97\u53e3\u4e2d\u805a\u5408\u7684 \u6307\u6807 \u8d77\u4f5c\u7528\u3002 \u8fd9\u4e9b\u7a97\u53e3\u5b9a\u4e49Autoscaler\u8003\u8651\u7684\u5386\u53f2\u6570\u636e\u91cf\uff0c\u5e76\u7528\u4e8e\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u5e73\u6ed1\u6570\u636e\u3002 \u8fd9\u4e9b\u7a97\u53e3\u8d8a\u77ed\uff0cAutoscaler\u7684\u53cd\u5e94\u5c31\u8d8a\u5feb\u3002 KPA\u7684\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f: stable and panic \u3002 \u6bcf\u4e2a\u6a21\u5f0f\u90fd\u6709\u5355\u72ec\u7684\u805a\u5408\u7a97\u53e3: stable-window and panic-window \u3002 \u7a33\u5b9a\u6a21\u5f0f\u7528\u4e8e\u4e00\u822c\u64cd\u4f5c\uff0c\u800c\u6050\u614c\u6a21\u5f0f\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6709\u4e00\u4e2a\u66f4\u77ed\u7684\u7a97\u53e3\uff0c\u5982\u679c\u6d41\u91cf\u6fc0\u589e\uff0c\u5c06\u88ab\u7528\u4e8e\u5feb\u901f\u6269\u5c55\u4fee\u8ba2\u3002 \u6ce8\u91ca \u5f53\u4f7f\u7528\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u7248\u672c\u5c06\u4e0d\u4f1a\u7f29\u5c0f\u4ee5\u907f\u514d\u6d41\u5931\u3002 \u5982\u679c\u5728\u7a33\u5b9a\u7a97\u53e3\u65f6\u95f4\u5185\u6ca1\u6709\u5feb\u901f\u53cd\u5e94\u7684\u7406\u7531\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u79bb\u5f00\u6050\u614c\u6a21\u5f0f\u3002","title":"\u6a21\u5f0f"},{"location":"serving/autoscaling/kpa-specific/#_2","text":"\u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5f53\u526f\u672c\u5f52\u96f6\u65f6\uff0c\u53ea\u6709\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u4efb\u4f55\u8bbf\u95ee\u4fee\u8ba2\u7248\u672c\u7684\u6d41\u91cf\u65f6\uff0c\u624d\u4f1a\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u7a33\u5b9a\u7a97\u53e3"},{"location":"serving/autoscaling/kpa-specific/#_3","text":"\u6050\u614c\u7a97\u53e3\u88ab\u5b9a\u4e49\u4e3a\u7a33\u5b9a\u7a97\u53e3\u7684\u767e\u5206\u6bd4\uff0c\u786e\u4fdd\u4e24\u8005\u5728\u5de5\u4f5c\u65b9\u5f0f\u4e0a\u5f7c\u6b64\u76f8\u5bf9\u3002 \u6b64\u503c\u6307\u793a\u5728\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u5bf9\u5386\u53f2\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u7684\u7a97\u53e3\u5c06\u5982\u4f55\u6536\u7f29\u3002 \u4f8b\u5982\uff0c\u503c 10.0 \u610f\u5473\u7740\u5728\u7d27\u6025\u6a21\u5f0f\u4e0b\uff0c\u7a97\u53e3\u5c06\u662f\u7a33\u5b9a\u7a97\u53e3\u5927\u5c0f\u768410%\u3002 \u5168\u5c40\u952e: panic-window-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-window-percentage \u53ef\u7528\u503c: float, 1.0 <= value <= 100.0 \u9ed8\u8ba4\u503c: 10.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\"","title":"\u6050\u614c\u7a97\u53e3"},{"location":"serving/autoscaling/kpa-specific/#_4","text":"\u8fd9\u4e2a\u9608\u503c\u5b9a\u4e49Autoscaler\u4f55\u65f6\u4ece\u7a33\u5b9a\u6a21\u5f0f\u8f6c\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u8be5\u503c\u662f\u5f53\u524d\u526f\u672c\u6570\u91cf\u6240\u80fd\u5904\u7406\u7684\u6d41\u91cf\u7684\u767e\u5206\u6bd4\u3002 Note \u503c 100.0 (100%)\u610f\u5473\u7740Autoscaler\u603b\u662f\u5904\u4e8e\u7d27\u6025\u6a21\u5f0f\uff0c\u56e0\u6b64\u6700\u5c0f\u503c\u5e94\u8be5\u9ad8\u4e8e 100.0 \u3002 \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a 200.0 \u610f\u5473\u7740\u5982\u679c\u6d41\u91cf\u662f\u5f53\u524d\u526f\u672c\u603b\u4f53\u53ef\u4ee5\u5904\u7406\u7684\u4e24\u500d\uff0c\u5c06\u542f\u52a8\u6050\u614c\u6a21\u5f0f\u3002 \u5168\u5c40\u952e: panic-threshold-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-threshold-percentage \u53ef\u7528\u503c: float, 110.0 <= value <= 1000.0 \u9ed8\u8ba4\u503c: 200.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\"","title":"\u6050\u614c\u7684\u9608\u503c"},{"location":"serving/autoscaling/kpa-specific/#_5","text":"\u8fd9\u4e9b\u8bbe\u7f6e\u901a\u8fc7\u5728\u5355\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u590d\u5236\u96c6\u7fa4\u53ef\u4ee5\u6269\u5927\u6216\u7f29\u5c0f\u591a\u5c11\u6765\u63a7\u5236\u3002 \u5728\u6bcf\u4e2a\u65b9\u5411\u4e0a\u603b\u662f\u5141\u8bb8\u4e00\u4e2a\u526f\u672c\u7684\u6700\u5c0f\u53d8\u5316\uff0c\u56e0\u6b64Autoscaler\u53ef\u4ee5\u968f\u65f6\u6269\u5c55\u5230+/- 1\u526f\u672c\uff0c\u800c\u4e0d\u7ba1\u8bbe\u7f6e\u7684\u7f29\u653e\u7387\u5982\u4f55\u3002","title":"\u4f38\u7f29\u6bd4\u7387"},{"location":"serving/autoscaling/kpa-specific/#_6","text":"\u6b64\u8bbe\u7f6e\u786e\u5b9a\u6240\u9700\u4e0e\u73b0\u6709Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u503c\u4e3a 2.0 \uff0c\u5219\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 2*N \u4e2aPods\u3002 \u5168\u5c40\u952e: max-scale-up-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 1000.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\"","title":"\u6269\u5927\u7387"},{"location":"serving/autoscaling/kpa-specific/#_7","text":"\u6b64\u8bbe\u7f6e\u786e\u5b9a\u73b0\u6709Pod\u4e0e\u6240\u9700Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5f53\u503c\u4e3a 2.0 \u65f6\uff0c\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u5185\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 N/2 \u4e2apod\u3002 \u5168\u5c40\u952e: max-scale-down-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 2.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"\u964d\u4f4e\u7387"},{"location":"serving/autoscaling/rps-target/","text":"\u914d\u7f6e\u6bcf\u79d2\u8bf7\u6c42\u6570(RPS)\u76ee\u6807 \u00b6 \u6b64\u8bbe\u7f6e\u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u6307\u5b9a\u6bcf\u79d2\u8bf7\u6c42\u6570\u7684\u76ee\u6807\u3002 \u60a8\u7684\u4fee\u8ba2\u8fd8\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 rps \u6307\u6807\u6ce8\u91ca \u3002 \u5168\u5c40\u503c: requests-per-second-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u503c: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"200\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"\u914d\u7f6eRPS"},{"location":"serving/autoscaling/rps-target/#rps","text":"\u6b64\u8bbe\u7f6e\u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u6307\u5b9a\u6bcf\u79d2\u8bf7\u6c42\u6570\u7684\u76ee\u6807\u3002 \u60a8\u7684\u4fee\u8ba2\u8fd8\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 rps \u6307\u6807\u6ce8\u91ca \u3002 \u5168\u5c40\u503c: requests-per-second-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u503c: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"200\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"\u914d\u7f6e\u6bcf\u79d2\u8bf7\u6c42\u6570(RPS)\u76ee\u6807"},{"location":"serving/autoscaling/scale-bounds/","text":"\u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u00b6 \u60a8\u53ef\u4ee5\u914d\u7f6e\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u63a7\u5236\u81ea\u52a8\u7f29\u653e\u884c\u4e3a\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5728\u521b\u5efa\u4e4b\u540e\u7acb\u5373\u5c06\u4fee\u8ba2\u6269\u5c55\u5230\u7684\u521d\u59cb\u89c4\u6a21\u3002 \u8fd9\u53ef\u4ee5\u662f\u6240\u6709\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u662f\u4f7f\u7528\u6ce8\u91ca\u7684\u7279\u5b9a\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002 \u4e0b\u754c \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 Knative\u5c06\u5c1d\u8bd5\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u90fd\u4e0d\u5c11\u4e8e\u8fd9\u4e2a\u6570\u91cf\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: min-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/min-scale \u53ef\u80fd\u503c: integer \u9ed8\u8ba4\u503c: 0 \u5982\u679c\u542f\u7528\u4e86scale-to-zero\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86KPA\u7c7b\uff0c\u5219\u4e3a1 Note \u6709\u5173\u7f29\u96f6\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7f29\u96f6\u914d\u7f6e \u6587\u6863\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\" \u4e0a\u5c4a \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5927\u6570\u91cf\u3002 Knative\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u8fd0\u884c\u7684\u526f\u672c\u6216\u6b63\u5728\u521b\u5efa\u7684\u526f\u672c\u7684\u6570\u91cf\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u6570\u76ee\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 max-scale-limit \u5168\u5c40\u952e\uff0cKnative\u5c06\u786e\u4fdd\u5168\u5c40\u6700\u5927\u523b\u5ea6\u548c\u6bcf\u4e2a\u65b0\u4fee\u8ba2\u7248\u672c\u7684\u6700\u5927\u523b\u5ea6\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u5f53 max-scale-limit \u8bbe\u7f6e\u4e3a\u6b63\u503c\u65f6\uff0c\u4e0d\u5141\u8bb8\u6700\u5927\u523b\u5ea6\u9ad8\u4e8e\u8be5\u503c(\u5305\u62ec0\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9650)\u7684\u4fee\u8ba2\u3002 \u5168\u5c40\u952e: max-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/max-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 \u8fd9\u610f\u5473\u7740\u65e0\u9650\u7684 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\" \u521d\u59cb\u7f29\u653e \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u4e86\u4e00\u4e2a\u4fee\u8ba2\u5728\u88ab\u6807\u8bb0\u4e3a Ready \u4e4b\u524d\u5fc5\u987b\u7acb\u5373\u8fbe\u5230\u7684\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u3002 \u5728\u4fee\u8ba2\u4e00\u6b21\u8fbe\u5230\u6b64\u89c4\u6a21\u540e\uff0c\u8be5\u503c\u5c06\u88ab\u5ffd\u7565\u3002 \u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u5b9e\u9645\u63a5\u6536\u7684\u6d41\u91cf\u53ea\u9700\u8981\u8f83\u5c0f\u7684\u89c4\u6a21\uff0c\u5219\u5728\u8fbe\u5230\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u540e\uff0c\u4fee\u8ba2\u5c06\u9010\u6b65\u7f29\u5c0f\u3002 \u5728\u521b\u5efa\u4fee\u8ba2\u65f6\uff0c\u81ea\u52a8\u9009\u62e9\u521d\u59cb\u6807\u5ea6\u548c\u4e0b\u9650\u4e2d\u8f83\u5927\u7684\u4f5c\u4e3a\u521d\u59cb\u76ee\u6807\u6807\u5ea6\u3002 \u5168\u5c40\u952e: initial-scale \u7ed3\u5408 allow-zero-initial-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/initial-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\" \u6269\u5927\u6700\u4f4e \u00b6 \u6b64\u503c\u63a7\u5236\u5f53\u4fee\u8ba2\u4ece\u96f6\u6269\u5c55\u65f6\u5c06\u521b\u5efa\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 \u5168\u5c40\u952e: n/a \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/activation-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/activation-scale : \"5\" spec : containers : - image : gcr.io/knative-samples/helloworld-go \u7f29\u5ef6\u8fdf \u00b6 \u7f29\u5ef6\u8fdf\u6307\u5b9a\u5728\u5e94\u7528\u7f29\u5c0f\u51b3\u7b56\u4e4b\u524d\uff0c\u5728\u5e76\u53d1\u51cf\u5c11\u65f6\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u7a97\u53e3\u3002 \u8fd9\u53ef\u80fd\u5f88\u6709\u7528\uff0c\u4f8b\u5982\uff0c\u5728\u53ef\u914d\u7f6e\u7684\u6301\u7eed\u65f6\u95f4\u5185\u4fdd\u7559\u5bb9\u5668\uff0c\u4ee5\u907f\u514d\u5728\u65b0\u8bf7\u6c42\u8fdb\u5165\u65f6\u51fa\u73b0\u51b7\u542f\u52a8\u60e9\u7f5a\u3002 \u4e0e\u8bbe\u7f6e\u4e0b\u9650\u4e0d\u540c\u7684\u662f\uff0c\u5982\u679c\u5728\u5ef6\u8fdf\u671f\u95f4\u7ef4\u6301\u8f83\u4f4e\u7684\u5e76\u53d1\u6027\uff0c\u5219\u4fee\u8ba2\u6700\u7ec8\u5c06\u88ab\u7f29\u5c0f\u3002 Note \u53ea\u652f\u6301\u9ed8\u8ba4\u7684KPA\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u3002 \u5168\u5c40\u952e: scale-down-delay \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/scale-down-delay \u53ef\u7528\u503c: Duration, 0s <= value <= 1h \u9ed8\u8ba4\u503c: 0s (no delay) \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\" \u7a33\u5b9a\u7a97\u53e3 \u00b6 \u7a33\u5b9a\u7a97\u53e3\u5b9a\u4e49\u6ed1\u52a8\u65f6\u95f4\u7a97\u53e3\uff0c\u5f53\u81ea\u52a8\u7f29\u653e\u5668\u4e0d\u5904\u4e8e Panic mode \u65f6\uff0c\u5728\u8be5\u65f6\u95f4\u7a97\u53e3\u4e0a\u5bf9\u6307\u6807\u8fdb\u884c\u5e73\u5747\uff0c\u4ee5\u63d0\u4f9b\u7f29\u653e\u51b3\u7b56\u7684\u8f93\u5165\u3002 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5728\u7f29\u5c0f\u671f\u95f4\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u8bbf\u95ee\u4fee\u8ba2\u7248\u7684\u6d41\u91cf\u540e\uff0c\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u88ab\u5220\u9664\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u914d\u7f6e\u8fb9\u754c"},{"location":"serving/autoscaling/scale-bounds/#_1","text":"\u60a8\u53ef\u4ee5\u914d\u7f6e\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u63a7\u5236\u81ea\u52a8\u7f29\u653e\u884c\u4e3a\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5728\u521b\u5efa\u4e4b\u540e\u7acb\u5373\u5c06\u4fee\u8ba2\u6269\u5c55\u5230\u7684\u521d\u59cb\u89c4\u6a21\u3002 \u8fd9\u53ef\u4ee5\u662f\u6240\u6709\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u662f\u4f7f\u7528\u6ce8\u91ca\u7684\u7279\u5b9a\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u7f29\u653e\u8fb9\u754c"},{"location":"serving/autoscaling/scale-bounds/#_2","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 Knative\u5c06\u5c1d\u8bd5\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u90fd\u4e0d\u5c11\u4e8e\u8fd9\u4e2a\u6570\u91cf\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: min-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/min-scale \u53ef\u80fd\u503c: integer \u9ed8\u8ba4\u503c: 0 \u5982\u679c\u542f\u7528\u4e86scale-to-zero\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86KPA\u7c7b\uff0c\u5219\u4e3a1 Note \u6709\u5173\u7f29\u96f6\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7f29\u96f6\u914d\u7f6e \u6587\u6863\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\"","title":"\u4e0b\u754c"},{"location":"serving/autoscaling/scale-bounds/#_3","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5927\u6570\u91cf\u3002 Knative\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u8fd0\u884c\u7684\u526f\u672c\u6216\u6b63\u5728\u521b\u5efa\u7684\u526f\u672c\u7684\u6570\u91cf\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u6570\u76ee\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 max-scale-limit \u5168\u5c40\u952e\uff0cKnative\u5c06\u786e\u4fdd\u5168\u5c40\u6700\u5927\u523b\u5ea6\u548c\u6bcf\u4e2a\u65b0\u4fee\u8ba2\u7248\u672c\u7684\u6700\u5927\u523b\u5ea6\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u5f53 max-scale-limit \u8bbe\u7f6e\u4e3a\u6b63\u503c\u65f6\uff0c\u4e0d\u5141\u8bb8\u6700\u5927\u523b\u5ea6\u9ad8\u4e8e\u8be5\u503c(\u5305\u62ec0\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9650)\u7684\u4fee\u8ba2\u3002 \u5168\u5c40\u952e: max-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/max-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 \u8fd9\u610f\u5473\u7740\u65e0\u9650\u7684 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\"","title":"\u4e0a\u5c4a"},{"location":"serving/autoscaling/scale-bounds/#_4","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u4e86\u4e00\u4e2a\u4fee\u8ba2\u5728\u88ab\u6807\u8bb0\u4e3a Ready \u4e4b\u524d\u5fc5\u987b\u7acb\u5373\u8fbe\u5230\u7684\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u3002 \u5728\u4fee\u8ba2\u4e00\u6b21\u8fbe\u5230\u6b64\u89c4\u6a21\u540e\uff0c\u8be5\u503c\u5c06\u88ab\u5ffd\u7565\u3002 \u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u5b9e\u9645\u63a5\u6536\u7684\u6d41\u91cf\u53ea\u9700\u8981\u8f83\u5c0f\u7684\u89c4\u6a21\uff0c\u5219\u5728\u8fbe\u5230\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u540e\uff0c\u4fee\u8ba2\u5c06\u9010\u6b65\u7f29\u5c0f\u3002 \u5728\u521b\u5efa\u4fee\u8ba2\u65f6\uff0c\u81ea\u52a8\u9009\u62e9\u521d\u59cb\u6807\u5ea6\u548c\u4e0b\u9650\u4e2d\u8f83\u5927\u7684\u4f5c\u4e3a\u521d\u59cb\u76ee\u6807\u6807\u5ea6\u3002 \u5168\u5c40\u952e: initial-scale \u7ed3\u5408 allow-zero-initial-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/initial-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\"","title":"\u521d\u59cb\u7f29\u653e"},{"location":"serving/autoscaling/scale-bounds/#_5","text":"\u6b64\u503c\u63a7\u5236\u5f53\u4fee\u8ba2\u4ece\u96f6\u6269\u5c55\u65f6\u5c06\u521b\u5efa\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 \u5168\u5c40\u952e: n/a \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/activation-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/activation-scale : \"5\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u6269\u5927\u6700\u4f4e"},{"location":"serving/autoscaling/scale-bounds/#_6","text":"\u7f29\u5ef6\u8fdf\u6307\u5b9a\u5728\u5e94\u7528\u7f29\u5c0f\u51b3\u7b56\u4e4b\u524d\uff0c\u5728\u5e76\u53d1\u51cf\u5c11\u65f6\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u7a97\u53e3\u3002 \u8fd9\u53ef\u80fd\u5f88\u6709\u7528\uff0c\u4f8b\u5982\uff0c\u5728\u53ef\u914d\u7f6e\u7684\u6301\u7eed\u65f6\u95f4\u5185\u4fdd\u7559\u5bb9\u5668\uff0c\u4ee5\u907f\u514d\u5728\u65b0\u8bf7\u6c42\u8fdb\u5165\u65f6\u51fa\u73b0\u51b7\u542f\u52a8\u60e9\u7f5a\u3002 \u4e0e\u8bbe\u7f6e\u4e0b\u9650\u4e0d\u540c\u7684\u662f\uff0c\u5982\u679c\u5728\u5ef6\u8fdf\u671f\u95f4\u7ef4\u6301\u8f83\u4f4e\u7684\u5e76\u53d1\u6027\uff0c\u5219\u4fee\u8ba2\u6700\u7ec8\u5c06\u88ab\u7f29\u5c0f\u3002 Note \u53ea\u652f\u6301\u9ed8\u8ba4\u7684KPA\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u3002 \u5168\u5c40\u952e: scale-down-delay \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/scale-down-delay \u53ef\u7528\u503c: Duration, 0s <= value <= 1h \u9ed8\u8ba4\u503c: 0s (no delay) \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"\u7f29\u5ef6\u8fdf"},{"location":"serving/autoscaling/scale-bounds/#_7","text":"\u7a33\u5b9a\u7a97\u53e3\u5b9a\u4e49\u6ed1\u52a8\u65f6\u95f4\u7a97\u53e3\uff0c\u5f53\u81ea\u52a8\u7f29\u653e\u5668\u4e0d\u5904\u4e8e Panic mode \u65f6\uff0c\u5728\u8be5\u65f6\u95f4\u7a97\u53e3\u4e0a\u5bf9\u6307\u6807\u8fdb\u884c\u5e73\u5747\uff0c\u4ee5\u63d0\u4f9b\u7f29\u653e\u51b3\u7b56\u7684\u8f93\u5165\u3002 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5728\u7f29\u5c0f\u671f\u95f4\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u8bbf\u95ee\u4fee\u8ba2\u7248\u7684\u6d41\u91cf\u540e\uff0c\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u88ab\u5220\u9664\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u7a33\u5b9a\u7a97\u53e3"},{"location":"serving/autoscaling/scale-to-zero/","text":"\u914d\u7f6e\u7f29\u96f6 \u00b6 Warning \u53ea\u6709\u5728\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\uff0c\u624d\u80fd\u542f\u7528\u7f29\u653e\u5230\u96f6\uff0c\u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002\u6709\u5173\u4f7f\u7528KPA\u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684Autoscaler\u7c7b\u578b \u7684\u6587\u6863\u3002 \u7f29\u653e\u5230\u96f6 \u00b6 \u7f29\u653e\u5230\u96f6\u7684\u503c\u63a7\u5236Knative\u662f\u5426\u5141\u8bb8\u526f\u672c\u7f29\u5c0f\u5230\u96f6(\u5982\u679c\u8bbe\u7f6e\u4e3a true )\uff0c\u6216\u5982\u679c\u8bbe\u7f6e\u4e3a false \uff0c\u57281\u4e2a\u526f\u672c\u65f6\u505c\u6b62\u3002 Note \u6709\u5173\u6bcf\u4fee\u8ba2\u7f29\u653e\u8fb9\u754c\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u7684\u6587\u6863\u3002 \u5168\u5c40\u952e: enable-scale-to-zero \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: No per-revision setting. \u53ef\u80fd\u503c: boolean \u9ed8\u8ba4\u7684: true \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\" \u7f29\u96f6\u5bbd\u9650\u671f \u00b6 \u6b64\u8bbe\u7f6e\u6307\u5b9a\u4e86\u4e00\u4e2a\u4e0a\u9650\u65f6\u95f4\u9650\u5236\uff0c\u5728\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u4e4b\u524d\uff0c\u7cfb\u7edf\u5c06\u5728\u5185\u90e8\u7b49\u5f85\u4ece\u96f6\u5f00\u59cb\u6269\u5c55\u673a\u5236\u5230\u4f4d\u3002 Warning \u8fd9\u662f\u4e00\u4e2a\u63a7\u5236\u5141\u8bb8\u5185\u90e8\u7f51\u7edc\u7f16\u7a0b\u7684\u65f6\u95f4\u7684\u503c\uff0c\u53ea\u6709\u5f53\u60a8\u9047\u5230\u5728\u4fee\u8ba2\u7f29\u5230\u96f6\u526f\u672c\u65f6\u8bf7\u6c42\u88ab\u4e22\u5f03\u7684\u95ee\u9898\u65f6\uff0c\u624d\u5e94\u8be5\u8c03\u6574\u8be5\u503c\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u4f1a\u8c03\u6574\u6d41\u91cf\u7ed3\u675f\u540e\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u4fdd\u7559\u591a\u957f\u65f6\u95f4\uff0c\u4e5f\u4e0d\u4fdd\u8bc1\u5728\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u5b9e\u9645\u4fdd\u7559\u8be5\u526f\u672c\u3002 \u5168\u5c40\u952e: scale-to-zero-grace-period \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u80fd\u503c: Duration \u9ed8\u8ba4\u503c: 30s \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\" \u7f29\u96f6\u4fdd\u7559\u671f \u00b6 scale-to-zero-pod-retention-period \u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u5c06\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u5c0f\u65f6\u95f4\u3002 \u8fd9\u4e0e scale-to-zero-grace-period \u6807\u5fd7\u5f62\u6210\u5bf9\u6bd4\uff0c\u8be5\u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u957f\u65f6\u95f4\u3002 Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: \u975e\u8d1f\u65f6\u95f4\u5b57\u7b26\u4e32 Default: 0s Example: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"\u914d\u7f6e\u7f29\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_1","text":"Warning \u53ea\u6709\u5728\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\uff0c\u624d\u80fd\u542f\u7528\u7f29\u653e\u5230\u96f6\uff0c\u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002\u6709\u5173\u4f7f\u7528KPA\u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684Autoscaler\u7c7b\u578b \u7684\u6587\u6863\u3002","title":"\u914d\u7f6e\u7f29\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_2","text":"\u7f29\u653e\u5230\u96f6\u7684\u503c\u63a7\u5236Knative\u662f\u5426\u5141\u8bb8\u526f\u672c\u7f29\u5c0f\u5230\u96f6(\u5982\u679c\u8bbe\u7f6e\u4e3a true )\uff0c\u6216\u5982\u679c\u8bbe\u7f6e\u4e3a false \uff0c\u57281\u4e2a\u526f\u672c\u65f6\u505c\u6b62\u3002 Note \u6709\u5173\u6bcf\u4fee\u8ba2\u7f29\u653e\u8fb9\u754c\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u7684\u6587\u6863\u3002 \u5168\u5c40\u952e: enable-scale-to-zero \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: No per-revision setting. \u53ef\u80fd\u503c: boolean \u9ed8\u8ba4\u7684: true \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\"","title":"\u7f29\u653e\u5230\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_3","text":"\u6b64\u8bbe\u7f6e\u6307\u5b9a\u4e86\u4e00\u4e2a\u4e0a\u9650\u65f6\u95f4\u9650\u5236\uff0c\u5728\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u4e4b\u524d\uff0c\u7cfb\u7edf\u5c06\u5728\u5185\u90e8\u7b49\u5f85\u4ece\u96f6\u5f00\u59cb\u6269\u5c55\u673a\u5236\u5230\u4f4d\u3002 Warning \u8fd9\u662f\u4e00\u4e2a\u63a7\u5236\u5141\u8bb8\u5185\u90e8\u7f51\u7edc\u7f16\u7a0b\u7684\u65f6\u95f4\u7684\u503c\uff0c\u53ea\u6709\u5f53\u60a8\u9047\u5230\u5728\u4fee\u8ba2\u7f29\u5230\u96f6\u526f\u672c\u65f6\u8bf7\u6c42\u88ab\u4e22\u5f03\u7684\u95ee\u9898\u65f6\uff0c\u624d\u5e94\u8be5\u8c03\u6574\u8be5\u503c\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u4f1a\u8c03\u6574\u6d41\u91cf\u7ed3\u675f\u540e\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u4fdd\u7559\u591a\u957f\u65f6\u95f4\uff0c\u4e5f\u4e0d\u4fdd\u8bc1\u5728\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u5b9e\u9645\u4fdd\u7559\u8be5\u526f\u672c\u3002 \u5168\u5c40\u952e: scale-to-zero-grace-period \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u80fd\u503c: Duration \u9ed8\u8ba4\u503c: 30s \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\"","title":"\u7f29\u96f6\u5bbd\u9650\u671f"},{"location":"serving/autoscaling/scale-to-zero/#_4","text":"scale-to-zero-pod-retention-period \u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u5c06\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u5c0f\u65f6\u95f4\u3002 \u8fd9\u4e0e scale-to-zero-grace-period \u6807\u5fd7\u5f62\u6210\u5bf9\u6bd4\uff0c\u8be5\u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u957f\u65f6\u95f4\u3002 Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: \u975e\u8d1f\u65f6\u95f4\u5b57\u7b26\u4e32 Default: 0s Example: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"\u7f29\u96f6\u4fdd\u7559\u671f"},{"location":"serving/autoscaling/autoscale-go/","text":"\u81ea\u52a8\u7f29\u653e\u6837\u672c\u5e94\u7528\u7a0b\u5e8f-Go \u00b6 Knative\u670d\u52a1\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u5b89\u88c5\u4e86 Knative\u670d\u52a1 \u7684Kubernetes\u96c6\u7fa4 . \u5b89\u88c5\u4e86 hey \u8d1f\u8f7d\u751f\u6210\u5668 ( go get -u github.com/rakyll/hey ). \u514b\u9686\u8fd9\u4e2a\u5b58\u50a8\u5e93\uff0c\u5e76\u79fb\u52a8\u5230\u793a\u4f8b\u76ee\u5f55: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs \u90e8\u7f72\u670d\u52a1 \u00b6 \u90e8\u7f72 \u6837\u4f8b Knative\u670d\u52a1: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml \u83b7\u53d6\u670d\u52a1\u7684URL(\u4e00\u65e6\u4e3a Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True \u52a0\u8f7d\u670d\u52a1 \u00b6 \u5411\u81ea\u52a8\u4f38\u7f29\u5e94\u7528\u7a0b\u5e8f\u53d1\u51fa\u4e00\u4e2a\u8bf7\u6c42\uff0c\u4ee5\u67e5\u770b\u5b83\u6d88\u8017\u4e00\u4e9b\u8d44\u6e90\u3002 curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. \u53d1\u900130\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u630150\u4e2a\u98de\u884c\u8bf7\u6c42\u3002 hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s \u5206\u6790 \u00b6 \u7b97\u6cd5 \u00b6 Knative\u670d\u52a1\u81ea\u52a8\u4f38\u7f29\u662f\u57fa\u4e8e\u6bcf\u4e2aPod\u7684\u98de\u884c\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf(\u5e76\u53d1\u6027)\u3002 \u7cfb\u7edf\u9ed8\u8ba4 \u76ee\u6807\u5e76\u53d1\u6570\u4e3a100 (\u641c\u7d22 container-concurrency-target-default),\u4f46\u6211\u4eec\u7684\u670d\u52a1 \u7528\u4e8610 \u3002 \u6211\u4eec\u52a0\u8f7d\u4e8650\u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u670d\u52a1\uff0c\u56e0\u6b64\u81ea\u52a8\u7f29\u653e\u5668\u521b\u5efa\u4e865\u4e2aPod( 50 concurrent requests / target of 10 = 5 pods ) \u6050\u614c \u00b6 \u81ea\u52a8\u8ba1\u7b97\u5668\u572860\u79d2\u7684\u7a97\u53e3\u5185\u8ba1\u7b97\u5e73\u5747\u5e76\u53d1\u6027\uff0c\u56e0\u6b64\u7cfb\u7edf\u9700\u8981\u4e00\u5206\u949f\u65f6\u95f4\u624d\u80fd\u7a33\u5b9a\u5728\u6240\u9700\u7684\u5e76\u53d1\u6027\u6c34\u5e73\u4e0a\u3002 \u7136\u800c\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u4e5f\u4f1a\u8ba1\u7b97\u4e00\u4e2a6\u79d2\u7684\u201c\u6050\u614c\u201d\u7a97\u53e3\uff0c\u5982\u679c\u8be5\u7a97\u53e3\u8fbe\u5230\u76ee\u6807\u5e76\u53d1\u6570\u76842\u500d\uff0c\u5c31\u4f1a\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u5728\u6050\u614c\u6a21\u5f0f\u4e0b\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5728\u66f4\u77ed\u3001\u66f4\u654f\u611f\u7684\u6050\u614c\u7a97\u53e3\u4e0a\u64cd\u4f5c\u3002 \u4e00\u65e6\u6050\u614c\u6761\u4ef6\u572860\u79d2\u5185\u4e0d\u518d\u6ee1\u8db3\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u8fd4\u56de\u5230\u6700\u521d\u768460\u79d2\u201c\u7a33\u5b9a\u201d\u7a97\u53e3\u3002 | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME \u5b9a\u5236 \u00b6 \u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u901a\u8fc7\u6ce8\u91ca\u8fdb\u884c\u5b9a\u5236\u3002Knative\u4e2d\u5185\u7f6e\u4e86\u4e24\u4e2a\u81ea\u52a8\u7f29\u653e\u7c7b: kpa.autoscaling.knative.dev \u5b83\u662f\u524d\u9762\u63cf\u8ff0\u7684\u57fa\u4e8e\u5e76\u53d1\u7684\u81ea\u52a8\u7f29\u653e\u5668(\u9ed8\u8ba4\u503c), \u548c hpa.autoscaling.knative.dev \u5b83\u59d4\u6258\u7ed9Kubernetes HPA\uff0c\u5b83\u4f1a\u6839\u636eCPU\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29. \u5728CPU\u4e0a\u6269\u5c55\u670d\u52a1\u7684\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 \u6b64\u5916\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u76ee\u6807\u548c\u7f29\u653e\u8fb9\u754c\u53ef\u4ee5\u5728\u6ce8\u91ca\u4e2d\u6307\u5b9a\u3002\u5177\u6709\u81ea\u5b9a\u4e49\u76ee\u6807\u548c\u89c4\u6a21\u8fb9\u754c\u7684\u670d\u52a1\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note \u5bf9\u4e8e hpa.autoscaling.knative.dev \u7c7b\u670d\u52a1\uff0c autoscaling.knative.dev/target \u6307\u5b9aCPU\u767e\u5206\u6bd4\u76ee\u6807(\u9ed8\u8ba4\u4e3a \"80\" )\u3002 \u6f14\u793a \u00b6 \u67e5\u770bKnative\u81ea\u52a8\u7f29\u653e\u81ea\u5b9a\u4e49\u7684 Kubecon\u6f14\u793a (32\u5206\u949f)\u3002 \u5176\u4ed6\u7684\u5b9e\u9a8c \u00b6 \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u6301100\u4e2a\u5e76\u53d1\u8bf7\u6c42\u3002 hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u77ed\u8bf7\u6c42(10\u6beb\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u957f\u8bf7\u6c42(1\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" \u53d1\u900160\u79d2\u7684\u9ad8CPU\u4f7f\u7528\u7387\u7684\u6d41\u91cf(~1 cpu/sec/request\uff0c\u603b\u5171100\u4e2aCPU)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" \u53d1\u900160\u79d2\u7684\u9ad8\u5185\u5b58\u4f7f\u7528\u6d41\u91cf(\u6bcf\u8bf7\u6c421gb\uff0c\u51715gb)\u3002 hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\" \u6e05\u7406 \u00b6 kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml \u8fdb\u4e00\u6b65\u7684\u9605\u8bfb \u00b6 \u81ea\u52a8\u5b9a\u91cf\u5f00\u53d1\u4eba\u5458\u6587\u6863","title":"\u6837\u672c\u5e94\u7528"},{"location":"serving/autoscaling/autoscale-go/#-go","text":"Knative\u670d\u52a1\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002","title":"\u81ea\u52a8\u7f29\u653e\u6837\u672c\u5e94\u7528\u7a0b\u5e8f-Go"},{"location":"serving/autoscaling/autoscale-go/#_1","text":"\u5b89\u88c5\u4e86 Knative\u670d\u52a1 \u7684Kubernetes\u96c6\u7fa4 . \u5b89\u88c5\u4e86 hey \u8d1f\u8f7d\u751f\u6210\u5668 ( go get -u github.com/rakyll/hey ). \u514b\u9686\u8fd9\u4e2a\u5b58\u50a8\u5e93\uff0c\u5e76\u79fb\u52a8\u5230\u793a\u4f8b\u76ee\u5f55: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"serving/autoscaling/autoscale-go/#_2","text":"\u90e8\u7f72 \u6837\u4f8b Knative\u670d\u52a1: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml \u83b7\u53d6\u670d\u52a1\u7684URL(\u4e00\u65e6\u4e3a Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True","title":"\u90e8\u7f72\u670d\u52a1"},{"location":"serving/autoscaling/autoscale-go/#_3","text":"\u5411\u81ea\u52a8\u4f38\u7f29\u5e94\u7528\u7a0b\u5e8f\u53d1\u51fa\u4e00\u4e2a\u8bf7\u6c42\uff0c\u4ee5\u67e5\u770b\u5b83\u6d88\u8017\u4e00\u4e9b\u8d44\u6e90\u3002 curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. \u53d1\u900130\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u630150\u4e2a\u98de\u884c\u8bf7\u6c42\u3002 hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s","title":"\u52a0\u8f7d\u670d\u52a1"},{"location":"serving/autoscaling/autoscale-go/#_4","text":"","title":"\u5206\u6790"},{"location":"serving/autoscaling/autoscale-go/#_5","text":"Knative\u670d\u52a1\u81ea\u52a8\u4f38\u7f29\u662f\u57fa\u4e8e\u6bcf\u4e2aPod\u7684\u98de\u884c\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf(\u5e76\u53d1\u6027)\u3002 \u7cfb\u7edf\u9ed8\u8ba4 \u76ee\u6807\u5e76\u53d1\u6570\u4e3a100 (\u641c\u7d22 container-concurrency-target-default),\u4f46\u6211\u4eec\u7684\u670d\u52a1 \u7528\u4e8610 \u3002 \u6211\u4eec\u52a0\u8f7d\u4e8650\u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u670d\u52a1\uff0c\u56e0\u6b64\u81ea\u52a8\u7f29\u653e\u5668\u521b\u5efa\u4e865\u4e2aPod( 50 concurrent requests / target of 10 = 5 pods )","title":"\u7b97\u6cd5"},{"location":"serving/autoscaling/autoscale-go/#_6","text":"\u81ea\u52a8\u8ba1\u7b97\u5668\u572860\u79d2\u7684\u7a97\u53e3\u5185\u8ba1\u7b97\u5e73\u5747\u5e76\u53d1\u6027\uff0c\u56e0\u6b64\u7cfb\u7edf\u9700\u8981\u4e00\u5206\u949f\u65f6\u95f4\u624d\u80fd\u7a33\u5b9a\u5728\u6240\u9700\u7684\u5e76\u53d1\u6027\u6c34\u5e73\u4e0a\u3002 \u7136\u800c\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u4e5f\u4f1a\u8ba1\u7b97\u4e00\u4e2a6\u79d2\u7684\u201c\u6050\u614c\u201d\u7a97\u53e3\uff0c\u5982\u679c\u8be5\u7a97\u53e3\u8fbe\u5230\u76ee\u6807\u5e76\u53d1\u6570\u76842\u500d\uff0c\u5c31\u4f1a\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u5728\u6050\u614c\u6a21\u5f0f\u4e0b\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5728\u66f4\u77ed\u3001\u66f4\u654f\u611f\u7684\u6050\u614c\u7a97\u53e3\u4e0a\u64cd\u4f5c\u3002 \u4e00\u65e6\u6050\u614c\u6761\u4ef6\u572860\u79d2\u5185\u4e0d\u518d\u6ee1\u8db3\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u8fd4\u56de\u5230\u6700\u521d\u768460\u79d2\u201c\u7a33\u5b9a\u201d\u7a97\u53e3\u3002 | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME","title":"\u6050\u614c"},{"location":"serving/autoscaling/autoscale-go/#_7","text":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u901a\u8fc7\u6ce8\u91ca\u8fdb\u884c\u5b9a\u5236\u3002Knative\u4e2d\u5185\u7f6e\u4e86\u4e24\u4e2a\u81ea\u52a8\u7f29\u653e\u7c7b: kpa.autoscaling.knative.dev \u5b83\u662f\u524d\u9762\u63cf\u8ff0\u7684\u57fa\u4e8e\u5e76\u53d1\u7684\u81ea\u52a8\u7f29\u653e\u5668(\u9ed8\u8ba4\u503c), \u548c hpa.autoscaling.knative.dev \u5b83\u59d4\u6258\u7ed9Kubernetes HPA\uff0c\u5b83\u4f1a\u6839\u636eCPU\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29. \u5728CPU\u4e0a\u6269\u5c55\u670d\u52a1\u7684\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 \u6b64\u5916\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u76ee\u6807\u548c\u7f29\u653e\u8fb9\u754c\u53ef\u4ee5\u5728\u6ce8\u91ca\u4e2d\u6307\u5b9a\u3002\u5177\u6709\u81ea\u5b9a\u4e49\u76ee\u6807\u548c\u89c4\u6a21\u8fb9\u754c\u7684\u670d\u52a1\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note \u5bf9\u4e8e hpa.autoscaling.knative.dev \u7c7b\u670d\u52a1\uff0c autoscaling.knative.dev/target \u6307\u5b9aCPU\u767e\u5206\u6bd4\u76ee\u6807(\u9ed8\u8ba4\u4e3a \"80\" )\u3002","title":"\u5b9a\u5236"},{"location":"serving/autoscaling/autoscale-go/#_8","text":"\u67e5\u770bKnative\u81ea\u52a8\u7f29\u653e\u81ea\u5b9a\u4e49\u7684 Kubecon\u6f14\u793a (32\u5206\u949f)\u3002","title":"\u6f14\u793a"},{"location":"serving/autoscaling/autoscale-go/#_9","text":"\u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u6301100\u4e2a\u5e76\u53d1\u8bf7\u6c42\u3002 hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u77ed\u8bf7\u6c42(10\u6beb\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u957f\u8bf7\u6c42(1\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" \u53d1\u900160\u79d2\u7684\u9ad8CPU\u4f7f\u7528\u7387\u7684\u6d41\u91cf(~1 cpu/sec/request\uff0c\u603b\u5171100\u4e2aCPU)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" \u53d1\u900160\u79d2\u7684\u9ad8\u5185\u5b58\u4f7f\u7528\u6d41\u91cf(\u6bcf\u8bf7\u6c421gb\uff0c\u51715gb)\u3002 hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\"","title":"\u5176\u4ed6\u7684\u5b9e\u9a8c"},{"location":"serving/autoscaling/autoscale-go/#_10","text":"kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml","title":"\u6e05\u7406"},{"location":"serving/autoscaling/autoscale-go/#_11","text":"\u81ea\u52a8\u5b9a\u91cf\u5f00\u53d1\u4eba\u5458\u6587\u6863","title":"\u8fdb\u4e00\u6b65\u7684\u9605\u8bfb"},{"location":"serving/configuration/config-defaults/","text":"\u914d\u7f6e\u9ed8\u8ba4ConfigMap \u00b6 The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml \u4f8b\u5b50 config-defaults ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property. \u5c5e\u6027 \u00b6 \u4fee\u8ba2\u7684\u8d85\u65f6\u79d2 \u00b6 The revision timeout value determines the default number of seconds to use for the revision's per-request timeout if none is specified. Global key: revision-timeout-seconds Per-revision spec key: timeoutSeconds Possible values: integer Default: \"300\" (5 minutes) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : timeoutSeconds : 300 containers : - image : gcr.io/knative-samples/helloworld-go \u6700\u5927\u4fee\u8ba2\u8d85\u65f6\u79d2\u6570 \u00b6 The max-revision-timeout-seconds value determines the maximum number of seconds that can be used for revision-timeout-seconds . This value must be greater than or equal to revision-timeout-seconds . If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests from being disrupted. Global key: max-revision-timeout-seconds Per-revision annotation key: N/A Possible values: integer Default: \"600\" (10 minutes) Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\" \u4fee\u8ba2\u7684CPU\u8bf7\u6c42 \u00b6 The revision-cpu-request value determines the CPU allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-request Per-revision annotation key: cpu Possible values: integer Default: \"400m\" (0.4 of a CPU, or 400 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"400m\" \u4fee\u8ba2\u7684\u5185\u5b58\u8bf7\u6c42 \u00b6 The revision-memory-request value determines the memory allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-request Per-revision annotation key: memory Possible values: integer Default: \"100M\" (100 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"100M\" \u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u8bf7\u6c42 \u00b6 The revision-ephemeral-storage-request value determines the ephemeral storage allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-request Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"500M\" (500 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"500M\" \u4fee\u8ba2 CPU \u9650\u5236 \u00b6 The revision-cpu-limit value determines the default CPU allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-limit Per-revision annotation key: cpu Possible values: integer Default: \"1000m\" (1 CPU, or 1000 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"1000m\" \u4fee\u8ba2\u5185\u5b58\u9650\u5236 \u00b6 The revision-memory-limit value determines the default memory allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-limit Per-revision annotation key: memory Possible values: integer Default: \"200M\" (200 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"200M\" \u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u9650\u5236 \u00b6 The revision-ephemeral-storage-limit value determines the default ephemeral storage limit allocated to revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-limit Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"750M\" (750 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"750M\" \u5bb9\u5668\u540d\u79f0\u6a21\u677f \u00b6 The container-name-template value provides a template for the default container name if no container name is specified. This field supports Go templating and is supplied by the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Global key: container-name-template Per-revision annotation key: name Possible values: string Default: \"user-container\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go \u5bb9\u5668\u7684\u5e76\u53d1 \u00b6 The container-concurrency value specifies the maximum number of requests the container can handle at once. Requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Global key: container-concurrency Per-revision spec key: containerConcurrency Possible values: integer Default: \"0\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 0 \u5bb9\u5668\u5e76\u53d1\u6700\u5927\u9650\u5236 \u00b6 The container-concurrency-max-limit setting disables arbitrary large concurrency values, or autoscaling targets, for individual revisions. The container-concurrency default setting must be at or below this value. The value of the container-concurrency-max-limit setting must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of zero (unbounded), unless allow-container-concurrency-zero is set to \"false\" . Global key: container-concurrency-max-limit Per-revision annotation key: N/A Possible values: integer Default: \"1000\" Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\" apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : defaults : container-concurrency-max-limit : \"1000\" \u5141\u8bb8\u5bb9\u5668\u5e76\u53d1\u4e3a\u96f6 \u00b6 The allow-container-concurrency-zero value determines whether users can specify 0 (unbounded) for containerConcurrency . Global key: allow-container-concurrency-zero Per-revision annotation key: N/A Possible values: boolean Default: \"true\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\" \u4f7f\u670d\u52a1\u94fe\u63a5 \u00b6 The enable-service-links value specifies the default value used for the enableServiceLinks field of the PodSpec when it is omitted by the user. See the Kubernetes documentation about the enableServiceLinks feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of Services, it is suggested to set this value to false . See serving#8498 . Global key: enable-service-links Per-revision annotation key: N/A Possible values: true|false|default Default: \"false\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"\u914d\u7f6eConfigMap\u9ed8\u8ba4\u503c"},{"location":"serving/configuration/config-defaults/#configmap","text":"The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml","title":"\u914d\u7f6e\u9ed8\u8ba4ConfigMap"},{"location":"serving/configuration/config-defaults/#config-defaults-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property.","title":"\u4f8b\u5b50 config-defaults ConfigMap"},{"location":"serving/configuration/config-defaults/#_1","text":"","title":"\u5c5e\u6027"},{"location":"serving/configuration/config-defaults/#_2","text":"The revision timeout value determines the default number of seconds to use for the revision's per-request timeout if none is specified. Global key: revision-timeout-seconds Per-revision spec key: timeoutSeconds Possible values: integer Default: \"300\" (5 minutes) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : timeoutSeconds : 300 containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u4fee\u8ba2\u7684\u8d85\u65f6\u79d2"},{"location":"serving/configuration/config-defaults/#_3","text":"The max-revision-timeout-seconds value determines the maximum number of seconds that can be used for revision-timeout-seconds . This value must be greater than or equal to revision-timeout-seconds . If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests from being disrupted. Global key: max-revision-timeout-seconds Per-revision annotation key: N/A Possible values: integer Default: \"600\" (10 minutes) Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\"","title":"\u6700\u5927\u4fee\u8ba2\u8d85\u65f6\u79d2\u6570"},{"location":"serving/configuration/config-defaults/#cpu","text":"The revision-cpu-request value determines the CPU allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-request Per-revision annotation key: cpu Possible values: integer Default: \"400m\" (0.4 of a CPU, or 400 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"400m\"","title":"\u4fee\u8ba2\u7684CPU\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#_4","text":"The revision-memory-request value determines the memory allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-request Per-revision annotation key: memory Possible values: integer Default: \"100M\" (100 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"100M\"","title":"\u4fee\u8ba2\u7684\u5185\u5b58\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#_5","text":"The revision-ephemeral-storage-request value determines the ephemeral storage allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-request Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"500M\" (500 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"500M\"","title":"\u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#cpu_1","text":"The revision-cpu-limit value determines the default CPU allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-limit Per-revision annotation key: cpu Possible values: integer Default: \"1000m\" (1 CPU, or 1000 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"1000m\"","title":"\u4fee\u8ba2 CPU \u9650\u5236"},{"location":"serving/configuration/config-defaults/#_6","text":"The revision-memory-limit value determines the default memory allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-limit Per-revision annotation key: memory Possible values: integer Default: \"200M\" (200 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"200M\"","title":"\u4fee\u8ba2\u5185\u5b58\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_7","text":"The revision-ephemeral-storage-limit value determines the default ephemeral storage limit allocated to revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-limit Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"750M\" (750 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"750M\"","title":"\u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_8","text":"The container-name-template value provides a template for the default container name if no container name is specified. This field supports Go templating and is supplied by the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Global key: container-name-template Per-revision annotation key: name Possible values: string Default: \"user-container\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go","title":"\u5bb9\u5668\u540d\u79f0\u6a21\u677f"},{"location":"serving/configuration/config-defaults/#_9","text":"The container-concurrency value specifies the maximum number of requests the container can handle at once. Requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Global key: container-concurrency Per-revision spec key: containerConcurrency Possible values: integer Default: \"0\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 0","title":"\u5bb9\u5668\u7684\u5e76\u53d1"},{"location":"serving/configuration/config-defaults/#_10","text":"The container-concurrency-max-limit setting disables arbitrary large concurrency values, or autoscaling targets, for individual revisions. The container-concurrency default setting must be at or below this value. The value of the container-concurrency-max-limit setting must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of zero (unbounded), unless allow-container-concurrency-zero is set to \"false\" . Global key: container-concurrency-max-limit Per-revision annotation key: N/A Possible values: integer Default: \"1000\" Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\" apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : defaults : container-concurrency-max-limit : \"1000\"","title":"\u5bb9\u5668\u5e76\u53d1\u6700\u5927\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_11","text":"The allow-container-concurrency-zero value determines whether users can specify 0 (unbounded) for containerConcurrency . Global key: allow-container-concurrency-zero Per-revision annotation key: N/A Possible values: boolean Default: \"true\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\"","title":"\u5141\u8bb8\u5bb9\u5668\u5e76\u53d1\u4e3a\u96f6"},{"location":"serving/configuration/config-defaults/#_12","text":"The enable-service-links value specifies the default value used for the enableServiceLinks field of the PodSpec when it is omitted by the user. See the Kubernetes documentation about the enableServiceLinks feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of Services, it is suggested to set this value to false . See serving#8498 . Global key: enable-service-links Per-revision annotation key: N/A Possible values: true|false|default Default: \"false\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"\u4f7f\u670d\u52a1\u94fe\u63a5"},{"location":"serving/configuration/deployment/","text":"Configure Deployment resources \u00b6 The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml Example config-deployment ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\" Configuring progress deadlines \u00b6 Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\" Skipping tag resolution \u00b6 You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com Enable container-freezer service \u00b6 You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"\u914d\u7f6e\u90e8\u7f72\u8d44\u6e90"},{"location":"serving/configuration/deployment/#configure-deployment-resources","text":"The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml","title":"Configure Deployment resources"},{"location":"serving/configuration/deployment/#example-config-deployment-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\"","title":"Example config-deployment ConfigMap"},{"location":"serving/configuration/deployment/#configuring-progress-deadlines","text":"Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\"","title":"Configuring progress deadlines"},{"location":"serving/configuration/deployment/#skipping-tag-resolution","text":"You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com","title":"Skipping tag resolution"},{"location":"serving/configuration/deployment/#enable-container-freezer-service","text":"You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"Enable container-freezer service"},{"location":"serving/configuration/feature-flags/","text":"Feature and extension flags \u00b6 The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API. Configuring flags \u00b6 Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used. Lifecycle \u00b6 When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases. Feature lifecycle stages \u00b6 Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative. Extension lifecycle stages \u00b6 An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default. Available Flags \u00b6 Multiple containers \u00b6 Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java Kubernetes EmptyDir Volume \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {} Kubernetes PersistentVolumeClaim (PVC) \u00b6 Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true Kubernetes node affinity \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2 Kubernetes host aliases \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" Kubernetes node selector \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue Kubernetes toleration \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Kubernetes Downward API \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName Kubernetes priority class name \u00b6 Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ... Kubernetes dry run \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun: enabled annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun : enabled ... Kubernetes runtime class \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-runtimeclassname This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ... Kubernetes security context \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ... Kubernetes security context capabilities \u00b6 Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE Tag header based routing \u00b6 Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled. Kubernetes init containers \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ... Queue Proxy Pod Info \u00b6 Type : Extension ConfigMap key: queueproxy.mount-podinfo You must set this feature to either \"enabled or \"allowed\" when using QPOptions. The flag controls whether Knative mounts the pod-info volume to the queue-proxy container. Mounting the pod-info volume allows extensions that use QPOptions to access the Service annotations, by reading the /etc/podinfo/annnotations file. See Extending Queue Proxy image with QPOptions for more details. When this feature is enabled , the pod-info volume is always mounted. This is helpful in case where all or most of the cluster Services are required to use extensions that rely on QPOptions. When this feature is allowed , the pod-info volume is not mounted by default. Instead, the volume is mounted only for Services that add the features.knative.dev/queueproxy-podinfo: enabled annotation as shown below: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled ... Kubernetes Topology Spread Constraints \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-topologyspreadconstraints This flag controls whether topology spread constraints can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... topologySpreadConstraints : - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar ... Kubernetes DNS Policy \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-dnspolicy This flag controls whether a DNS policy can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : dnsPolicy : ClusterFirstWithHostNet ... Kubernetes Scheduler Name \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-schedulername This flag controls whether a scheduler name can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... schedulerName : custom-scheduler-example ...","title":"\u7279\u6027\u548c\u6269\u5c55\u6807\u5fd7"},{"location":"serving/configuration/feature-flags/#feature-and-extension-flags","text":"The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API.","title":"Feature and extension flags"},{"location":"serving/configuration/feature-flags/#configuring-flags","text":"Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used.","title":"Configuring flags"},{"location":"serving/configuration/feature-flags/#lifecycle","text":"When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases.","title":"Lifecycle"},{"location":"serving/configuration/feature-flags/#feature-lifecycle-stages","text":"Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative.","title":"Feature lifecycle stages"},{"location":"serving/configuration/feature-flags/#extension-lifecycle-stages","text":"An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default.","title":"Extension lifecycle stages"},{"location":"serving/configuration/feature-flags/#available-flags","text":"","title":"Available Flags"},{"location":"serving/configuration/feature-flags/#multiple-containers","text":"Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java","title":"Multiple containers"},{"location":"serving/configuration/feature-flags/#kubernetes-emptydir-volume","text":"Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {}","title":"Kubernetes EmptyDir Volume"},{"location":"serving/configuration/feature-flags/#kubernetes-persistentvolumeclaim-pvc","text":"Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true","title":"Kubernetes PersistentVolumeClaim (PVC)"},{"location":"serving/configuration/feature-flags/#kubernetes-node-affinity","text":"Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2","title":"Kubernetes node affinity"},{"location":"serving/configuration/feature-flags/#kubernetes-host-aliases","text":"Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\"","title":"Kubernetes host aliases"},{"location":"serving/configuration/feature-flags/#kubernetes-node-selector","text":"Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue","title":"Kubernetes node selector"},{"location":"serving/configuration/feature-flags/#kubernetes-toleration","text":"Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\"","title":"Kubernetes toleration"},{"location":"serving/configuration/feature-flags/#kubernetes-downward-api","text":"Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName","title":"Kubernetes Downward API"},{"location":"serving/configuration/feature-flags/#kubernetes-priority-class-name","text":"Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ...","title":"Kubernetes priority class name"},{"location":"serving/configuration/feature-flags/#kubernetes-dry-run","text":"Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun: enabled annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun : enabled ...","title":"Kubernetes dry run"},{"location":"serving/configuration/feature-flags/#kubernetes-runtime-class","text":"Type : Extension ConfigMap key: kubernetes.podspec-runtimeclassname This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ...","title":"Kubernetes runtime class"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context","text":"Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ...","title":"Kubernetes security context"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context-capabilities","text":"Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE","title":"Kubernetes security context capabilities"},{"location":"serving/configuration/feature-flags/#tag-header-based-routing","text":"Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Tag header based routing"},{"location":"serving/configuration/feature-flags/#kubernetes-init-containers","text":"Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ...","title":"Kubernetes init containers"},{"location":"serving/configuration/feature-flags/#queue-proxy-pod-info","text":"Type : Extension ConfigMap key: queueproxy.mount-podinfo You must set this feature to either \"enabled or \"allowed\" when using QPOptions. The flag controls whether Knative mounts the pod-info volume to the queue-proxy container. Mounting the pod-info volume allows extensions that use QPOptions to access the Service annotations, by reading the /etc/podinfo/annnotations file. See Extending Queue Proxy image with QPOptions for more details. When this feature is enabled , the pod-info volume is always mounted. This is helpful in case where all or most of the cluster Services are required to use extensions that rely on QPOptions. When this feature is allowed , the pod-info volume is not mounted by default. Instead, the volume is mounted only for Services that add the features.knative.dev/queueproxy-podinfo: enabled annotation as shown below: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled ...","title":"Queue Proxy Pod Info"},{"location":"serving/configuration/feature-flags/#kubernetes-topology-spread-constraints","text":"Type : Extension ConfigMap key: kubernetes.podspec-topologyspreadconstraints This flag controls whether topology spread constraints can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... topologySpreadConstraints : - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar ...","title":"Kubernetes Topology Spread Constraints"},{"location":"serving/configuration/feature-flags/#kubernetes-dns-policy","text":"Type : Extension ConfigMap key: kubernetes.podspec-dnspolicy This flag controls whether a DNS policy can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : dnsPolicy : ClusterFirstWithHostNet ...","title":"Kubernetes DNS Policy"},{"location":"serving/configuration/feature-flags/#kubernetes-scheduler-name","text":"Type : Extension ConfigMap key: kubernetes.podspec-schedulername This flag controls whether a scheduler name can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... schedulerName : custom-scheduler-example ...","title":"Kubernetes Scheduler Name"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/","text":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\" \u8def\u7531\u66f4\u65b0\u72b6\u6001 \u00b6 \u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready \u591a\u4e2a\u5ef6\u5c55 \u00b6 \u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_1","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#procedure","text":"You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_2","text":"\u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"\u8def\u7531\u66f4\u65b0\u72b6\u6001"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_3","text":"\u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u591a\u4e2a\u5ef6\u5c55"},{"location":"serving/load-balancing/","text":"Load balancing \u00b6 You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled. Activator pod selection \u00b6 Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency . Configuring target burst capacity \u00b6 Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"\u5bf9\u8d1f\u8f7d\u5e73\u8861"},{"location":"serving/load-balancing/#load-balancing","text":"You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled.","title":"Load balancing"},{"location":"serving/load-balancing/#activator-pod-selection","text":"Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency .","title":"Activator pod selection"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","text":"Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/activator-capacity/","text":"Configuring Activator capacity \u00b6 If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity. Setting the Activator capacity \u00b6 Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"\u914d\u7f6e\u6fc0\u6d3b\u80fd\u529b"},{"location":"serving/load-balancing/activator-capacity/#configuring-activator-capacity","text":"If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity.","title":"Configuring Activator capacity"},{"location":"serving/load-balancing/activator-capacity/#setting-the-activator-capacity","text":"Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"Setting the Activator capacity"},{"location":"serving/load-balancing/target-burst-capacity/","text":"Configuring target burst capacity \u00b6 Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the target burst capacity \u00b6 Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"\u914d\u7f6e\u76ee\u6807\u7a81\u53d1\u5bb9\u91cf"},{"location":"serving/load-balancing/target-burst-capacity/#configuring-target-burst-capacity","text":"Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","text":"Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Setting the target burst capacity"},{"location":"serving/observability/logging/collecting-logs/","text":"\u65e5\u5fd7 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528\u65e5\u5fd7\u5904\u7406\u5668\u548c\u8f6c\u53d1\u5668 Fluent Bit \u5728\u4e2d\u5fc3\u76ee\u5f55\u4e2d\u6536\u96c6Kubernetes\u65e5\u5fd7\u3002 \u8fd0\u884cKnative\u65f6\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff0c\u4f46\u662f\u4f7f\u7528 Knative\u670d\u52a1 \u4f1a\u5f88\u6709\u5e2e\u52a9\uff0c\u5b83\u4f1a\u5728\u4e0d\u518d\u9700\u8981Pod\u548c\u76f8\u5173\u65e5\u5fd7\u65f6\u81ea\u52a8\u5220\u9664\u5b83\u4eec\u3002 Fluent Bit\u652f\u6301\u5bfc\u51fa\u5230\u8bb8\u591a\u5176\u4ed6\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\u3002 \u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u4f8b\u5982Splunk\u3001datdog\u3001ElasticSearch\u6216Stackdriver\uff0c\u60a8\u53ef\u4ee5\u6309\u7167 FluentBit\u6587\u6863 \u914d\u7f6e\u65e5\u5fd7\u8f6c\u53d1\u5668\u3002 \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6 \u00b6 \u8bbe\u7f6e\u65e5\u5fd7\u6536\u96c6\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4: \u5728\u5404\u8282\u70b9\u4e0a\u8fd0\u884c\u65e5\u5fd7\u8f6c\u53d1DaemonSet\u3002 \u5728\u96c6\u7fa4\u7684\u67d0\u4e2a\u5730\u65b9\u8fd0\u884c\u6536\u96c6\u5668\u3002 Tip \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u7684\u662f StatefulSet\uff0c\u5b83\u5c06\u65e5\u5fd7\u5b58\u50a8\u5728Kubernetes PersistentVolumeClaim\u4e0a\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528HostPath\u3002 \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 fluent-bit-collector.yaml \u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aStatefulSet\uff0c\u4ee5\u53ca\u4e00\u4e2aKubernetes\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u5141\u8bb8\u4ece\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u548c\u8bfb\u53d6\u65e5\u5fd7\u3002 \u63d0\u4f9b\u7684\u914d\u7f6e\u5c06\u5728\u540d\u4e3a logging \u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u76d1\u89c6\u914d\u7f6e\u3002 Important \u5728\u8f6c\u53d1\u4e4b\u524d\u8bbe\u7acb\u6536\u8d27\u4eba\u3002\u5728\u914d\u7f6e\u8f6c\u53d1\u5668\u65f6\uff0c\u60a8\u5c06\u9700\u8981\u6536\u96c6\u5668\u7684\u5730\u5740\uff0c\u5e76\u4e14\u8f6c\u53d1\u5668\u53ef\u80fd\u4f1a\u5c06\u65e5\u5fd7\u6392\u961f\uff0c\u76f4\u5230\u6536\u96c6\u5668\u51c6\u5907\u597d\u4e3a\u6b62\u3002 \u8fc7\u7a0b \u00b6 \u8f93\u5165\u547d\u4ee4\u5e94\u7528\u914d\u7f6e: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml \u9ed8\u8ba4\u914d\u7f6e\u5c06\u65e5\u5fd7\u5206\u4e3a: Knative\u670d\u52a1\uff0c\u6216\u5e26\u6709 app=Knative \u6807\u7b7e\u7684Pod\u3002 Non-Knative apps. Note \u65e5\u5fd7\u9ed8\u8ba4\u4f7f\u7528Pod\u540d\u79f0\u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55;\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5728\u5b89\u88c5\u4e4b\u524d\u6216\u4e4b\u540e\u66f4\u65b0 log-collector-config ConfigMap\u6765\u66f4\u6539\u3002 Warning ConfigMap\u66f4\u65b0\u540e\uff0c\u5fc5\u987b\u91cd\u65b0\u542f\u52a8Fluent Bit\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5220\u9664Pod\u5e76\u8ba9StatefulSet\u91cd\u65b0\u521b\u5efa\u5b83\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 \u8981\u901a\u8fc7web\u6d4f\u89c8\u5668\u8bbf\u95ee\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl port-forward --namespace logging service/log-collector 8080 :80 \u5bfc\u822a\u5230 http://localhost:8080/ . \u53ef\u9009:\u60a8\u53ef\u4ee5\u5728 nginx Pod\u4e2d\u6253\u5f00\u4e00\u4e2ashell\uff0c\u5e76\u4f7f\u7528Unix\u5de5\u5177\u641c\u7d22\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 \u5efa\u7acb\u8f6c\u53d1 \u00b6 \u8bf7\u53c2\u9605 Fluent Bit \u6587\u6863\uff0c\u8bbe\u7f6e\u4e00\u4e2a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c06\u65e5\u5fd7\u8f6c\u53d1\u5230ElasticSearch\u7684Fluent Bit DaemonSet\u3002 \u5f53\u60a8\u5728\u5b89\u88c5\u6b65\u9aa4\u4e2d\u521b\u5efaConfigMap\u65f6\uff0c\u5fc5\u987b: \u5c06ElasticSearch\u914d\u7f6e\u66ff\u6362\u4e3a fluent-bit-configmap.yaml , or \u5c06\u4e0b\u9762\u7684\u5757\u6dfb\u52a0\u5230ConfigMap\u4e2d\uff0c\u5e76\u5c06 @INCLUDE output-elasticsearch.conf \u66f4\u65b0\u4e3a @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True \u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668 \u00b6 Warning \u6b64\u8fc7\u7a0b\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\u8bbe\u7f6e\uff0c\u4e0d\u9002\u5408\u751f\u4ea7\u4f7f\u7528\u3002 \u5982\u679c\u4f7f\u7528\u672c\u5730Kubernetes\u96c6\u7fa4\u8fdb\u884c\u5f00\u53d1\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a hostPath PersistentVolume \u5728\u684c\u9762\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b58\u50a8\u65e5\u5fd7\u3002 \u8fd9\u5141\u8bb8\u60a8\u5728\u6587\u4ef6\u4e0a\u4f7f\u7528\u5e38\u7528\u7684\u684c\u9762\u5de5\u5177\uff0c\u800c\u4e0d\u9700\u8981\u7279\u5b9a\u4e8ekubernetes\u7684\u5de5\u5177\u3002 PersistentVolumeClaim \u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note hostPath \u5c06\u6839\u636e\u60a8\u7684Kubernetes\u8f6f\u4ef6\u548c\u4e3b\u673a\u64cd\u4f5c\u7cfb\u7edf\u800c\u6709\u6240\u4e0d\u540c\u3002 \u60a8\u5fc5\u987b\u66f4\u65b0StatefulSet volumeClaimTemplates \u4ee5\u5f15\u7528 shared-logs \u5377\uff0c\u793a\u4f8b\u5982\u4e0b: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 \u5f53\u521b\u5efa\u96c6\u7fa4\u65f6\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 kind-config.yaml \u5e76\u4e3a\u6bcf\u4e2a\u8282\u70b9\u6307\u5b9a extraMounts \uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs \u7136\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528 /shared/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 spec.hostPath.path \u3002 \u6ce8\u610f\u76ee\u5f55\u8def\u5f84 ./logs \u662f\u76f8\u5bf9\u4e8e\u521b\u5efaKind\u96c6\u7fa4\u7684\u76ee\u5f55\u7684\u3002 Docker \u684c\u9762 \u00b6 Docker\u684c\u9762\u81ea\u52a8\u5728\u4e3b\u673a\u548c\u5ba2\u6237\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u95f4\u521b\u5efa\u4e00\u4e9b\u5171\u4eab\u6302\u8f7d\uff0c\u56e0\u6b64\u60a8\u53ea\u9700\u8981\u77e5\u9053\u5230\u60a8\u7684\u4e3b\u76ee\u5f55\u7684\u8def\u5f84\u3002 \u4ee5\u4e0b\u662f\u4e0d\u540c\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e00\u4e9b\u4f8b\u5b50: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube\u9700\u8981\u4e00\u4e2a\u663e\u5f0f\u547d\u4ee4\u5c06\u4e00\u4e2a \u76ee\u5f55\u6302\u8f7d \u5230\u8fd0\u884cKubernetes\u7684\u865a\u62df\u673a\u4e2d\u3002 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 logs \u76ee\u5f55\u6302\u8f7d\u5230\u865a\u62df\u673a\u7684 /mnt/logs \u76ee\u5f55\u4e2d: minikube mount ./logs:/mnt/logs \u60a8\u8fd8\u5fc5\u987b\u5f15\u7528 /mnt/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 hostPath.path \u3002","title":"\u6536\u96c6\u65e5\u5fd7"},{"location":"serving/observability/logging/collecting-logs/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528\u65e5\u5fd7\u5904\u7406\u5668\u548c\u8f6c\u53d1\u5668 Fluent Bit \u5728\u4e2d\u5fc3\u76ee\u5f55\u4e2d\u6536\u96c6Kubernetes\u65e5\u5fd7\u3002 \u8fd0\u884cKnative\u65f6\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff0c\u4f46\u662f\u4f7f\u7528 Knative\u670d\u52a1 \u4f1a\u5f88\u6709\u5e2e\u52a9\uff0c\u5b83\u4f1a\u5728\u4e0d\u518d\u9700\u8981Pod\u548c\u76f8\u5173\u65e5\u5fd7\u65f6\u81ea\u52a8\u5220\u9664\u5b83\u4eec\u3002 Fluent Bit\u652f\u6301\u5bfc\u51fa\u5230\u8bb8\u591a\u5176\u4ed6\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\u3002 \u5982\u679c\u60a8\u5df2\u7ecf\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u65e5\u5fd7\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u4f8b\u5982Splunk\u3001datdog\u3001ElasticSearch\u6216Stackdriver\uff0c\u60a8\u53ef\u4ee5\u6309\u7167 FluentBit\u6587\u6863 \u914d\u7f6e\u65e5\u5fd7\u8f6c\u53d1\u5668\u3002","title":"\u65e5\u5fd7"},{"location":"serving/observability/logging/collecting-logs/#_2","text":"\u8bbe\u7f6e\u65e5\u5fd7\u6536\u96c6\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4: \u5728\u5404\u8282\u70b9\u4e0a\u8fd0\u884c\u65e5\u5fd7\u8f6c\u53d1DaemonSet\u3002 \u5728\u96c6\u7fa4\u7684\u67d0\u4e2a\u5730\u65b9\u8fd0\u884c\u6536\u96c6\u5668\u3002 Tip \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u4f7f\u7528\u7684\u662f StatefulSet\uff0c\u5b83\u5c06\u65e5\u5fd7\u5b58\u50a8\u5728Kubernetes PersistentVolumeClaim\u4e0a\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528HostPath\u3002","title":"\u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6"},{"location":"serving/observability/logging/collecting-logs/#_3","text":"fluent-bit-collector.yaml \u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aStatefulSet\uff0c\u4ee5\u53ca\u4e00\u4e2aKubernetes\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u5141\u8bb8\u4ece\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u548c\u8bfb\u53d6\u65e5\u5fd7\u3002 \u63d0\u4f9b\u7684\u914d\u7f6e\u5c06\u5728\u540d\u4e3a logging \u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u76d1\u89c6\u914d\u7f6e\u3002 Important \u5728\u8f6c\u53d1\u4e4b\u524d\u8bbe\u7acb\u6536\u8d27\u4eba\u3002\u5728\u914d\u7f6e\u8f6c\u53d1\u5668\u65f6\uff0c\u60a8\u5c06\u9700\u8981\u6536\u96c6\u5668\u7684\u5730\u5740\uff0c\u5e76\u4e14\u8f6c\u53d1\u5668\u53ef\u80fd\u4f1a\u5c06\u65e5\u5fd7\u6392\u961f\uff0c\u76f4\u5230\u6536\u96c6\u5668\u51c6\u5907\u597d\u4e3a\u6b62\u3002","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"serving/observability/logging/collecting-logs/#_4","text":"\u8f93\u5165\u547d\u4ee4\u5e94\u7528\u914d\u7f6e: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml \u9ed8\u8ba4\u914d\u7f6e\u5c06\u65e5\u5fd7\u5206\u4e3a: Knative\u670d\u52a1\uff0c\u6216\u5e26\u6709 app=Knative \u6807\u7b7e\u7684Pod\u3002 Non-Knative apps. Note \u65e5\u5fd7\u9ed8\u8ba4\u4f7f\u7528Pod\u540d\u79f0\u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55;\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5728\u5b89\u88c5\u4e4b\u524d\u6216\u4e4b\u540e\u66f4\u65b0 log-collector-config ConfigMap\u6765\u66f4\u6539\u3002 Warning ConfigMap\u66f4\u65b0\u540e\uff0c\u5fc5\u987b\u91cd\u65b0\u542f\u52a8Fluent Bit\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5220\u9664Pod\u5e76\u8ba9StatefulSet\u91cd\u65b0\u521b\u5efa\u5b83\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002 \u8981\u901a\u8fc7web\u6d4f\u89c8\u5668\u8bbf\u95ee\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl port-forward --namespace logging service/log-collector 8080 :80 \u5bfc\u822a\u5230 http://localhost:8080/ . \u53ef\u9009:\u60a8\u53ef\u4ee5\u5728 nginx Pod\u4e2d\u6253\u5f00\u4e00\u4e2ashell\uff0c\u5e76\u4f7f\u7528Unix\u5de5\u5177\u641c\u7d22\u65e5\u5fd7\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"\u8fc7\u7a0b"},{"location":"serving/observability/logging/collecting-logs/#_5","text":"\u8bf7\u53c2\u9605 Fluent Bit \u6587\u6863\uff0c\u8bbe\u7f6e\u4e00\u4e2a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c06\u65e5\u5fd7\u8f6c\u53d1\u5230ElasticSearch\u7684Fluent Bit DaemonSet\u3002 \u5f53\u60a8\u5728\u5b89\u88c5\u6b65\u9aa4\u4e2d\u521b\u5efaConfigMap\u65f6\uff0c\u5fc5\u987b: \u5c06ElasticSearch\u914d\u7f6e\u66ff\u6362\u4e3a fluent-bit-configmap.yaml , or \u5c06\u4e0b\u9762\u7684\u5757\u6dfb\u52a0\u5230ConfigMap\u4e2d\uff0c\u5e76\u5c06 @INCLUDE output-elasticsearch.conf \u66f4\u65b0\u4e3a @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"\u5efa\u7acb\u8f6c\u53d1"},{"location":"serving/observability/logging/collecting-logs/#_6","text":"Warning \u6b64\u8fc7\u7a0b\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\u8bbe\u7f6e\uff0c\u4e0d\u9002\u5408\u751f\u4ea7\u4f7f\u7528\u3002 \u5982\u679c\u4f7f\u7528\u672c\u5730Kubernetes\u96c6\u7fa4\u8fdb\u884c\u5f00\u53d1\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a hostPath PersistentVolume \u5728\u684c\u9762\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b58\u50a8\u65e5\u5fd7\u3002 \u8fd9\u5141\u8bb8\u60a8\u5728\u6587\u4ef6\u4e0a\u4f7f\u7528\u5e38\u7528\u7684\u684c\u9762\u5de5\u5177\uff0c\u800c\u4e0d\u9700\u8981\u7279\u5b9a\u4e8ekubernetes\u7684\u5de5\u5177\u3002 PersistentVolumeClaim \u770b\u8d77\u6765\u7c7b\u4f3c\u5982\u4e0b: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note hostPath \u5c06\u6839\u636e\u60a8\u7684Kubernetes\u8f6f\u4ef6\u548c\u4e3b\u673a\u64cd\u4f5c\u7cfb\u7edf\u800c\u6709\u6240\u4e0d\u540c\u3002 \u60a8\u5fc5\u987b\u66f4\u65b0StatefulSet volumeClaimTemplates \u4ee5\u5f15\u7528 shared-logs \u5377\uff0c\u793a\u4f8b\u5982\u4e0b: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"\u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668"},{"location":"serving/observability/logging/collecting-logs/#kind","text":"\u5f53\u521b\u5efa\u96c6\u7fa4\u65f6\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528 kind-config.yaml \u5e76\u4e3a\u6bcf\u4e2a\u8282\u70b9\u6307\u5b9a extraMounts \uff0c\u5982\u4e0b\u4f8b\u6240\u793a: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs \u7136\u540e\u4f60\u53ef\u4ee5\u4f7f\u7528 /shared/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 spec.hostPath.path \u3002 \u6ce8\u610f\u76ee\u5f55\u8def\u5f84 ./logs \u662f\u76f8\u5bf9\u4e8e\u521b\u5efaKind\u96c6\u7fa4\u7684\u76ee\u5f55\u7684\u3002","title":"Kind"},{"location":"serving/observability/logging/collecting-logs/#docker","text":"Docker\u684c\u9762\u81ea\u52a8\u5728\u4e3b\u673a\u548c\u5ba2\u6237\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u95f4\u521b\u5efa\u4e00\u4e9b\u5171\u4eab\u6302\u8f7d\uff0c\u56e0\u6b64\u60a8\u53ea\u9700\u8981\u77e5\u9053\u5230\u60a8\u7684\u4e3b\u76ee\u5f55\u7684\u8def\u5f84\u3002 \u4ee5\u4e0b\u662f\u4e0d\u540c\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e00\u4e9b\u4f8b\u5b50: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker \u684c\u9762"},{"location":"serving/observability/logging/collecting-logs/#minikube","text":"Minikube\u9700\u8981\u4e00\u4e2a\u663e\u5f0f\u547d\u4ee4\u5c06\u4e00\u4e2a \u76ee\u5f55\u6302\u8f7d \u5230\u8fd0\u884cKubernetes\u7684\u865a\u62df\u673a\u4e2d\u3002 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 logs \u76ee\u5f55\u6302\u8f7d\u5230\u865a\u62df\u673a\u7684 /mnt/logs \u76ee\u5f55\u4e2d: minikube mount ./logs:/mnt/logs \u60a8\u8fd8\u5fc5\u987b\u5f15\u7528 /mnt/logs \u4f5c\u4e3aPersistentVolume\u4e2d\u7684 hostPath.path \u3002","title":"Minikube"},{"location":"serving/observability/logging/config-logging/","text":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e \u00b6 \u6240\u6709Knative\u7ec4\u4ef6\u7684\u65e5\u5fd7\u914d\u7f6e\u90fd\u901a\u8fc7\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging ConfigMap\u8fdb\u884c\u7ba1\u7406\u3002 \u4f8b\u5982\uff0c\u670d\u52a1\u7ec4\u4ef6\u901a\u8fc7 knative-serving \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u4e8b\u4ef6\u7ec4\u4ef6\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u7b49\u7b49\u3002 Knative\u7ec4\u4ef6\u4f7f\u7528 zap \u65e5\u5fd7\u5e93;\u9009\u9879 \u5728\u8be5\u9879\u76ee\u4e2d\u6709\u66f4\u8be6\u7ec6\u7684\u6587\u6863 \u3002 \u9664\u4e86 zap-logger-config \uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u952e\uff0c\u9002\u7528\u4e8e\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u7ec4\u4ef6\uff0c config-logging ConfigMap\u652f\u6301\u8986\u76d6\u5355\u4e2a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u7ea7\u522b\u3002 ConfigMap key Description zap-logger-config \u7528\u4e8ezap\u8bb0\u5f55\u5668\u914d\u7f6e\u7684JSON\u5bf9\u8c61\u5bb9\u5668\u3002\u5173\u952e\u5b57\u6bb5\u5728\u4e0b\u9762\u7a81\u51fa\u663e\u793a\u3002 zap-logger-config.level \u7ec4\u4ef6\u7684\u9ed8\u8ba4\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 zap-logger-config.encoding \u7ec4\u4ef6\u65e5\u5fd7\u7684\u65e5\u5fd7\u7f16\u7801\u683c\u5f0f(\u9ed8\u8ba4\u4e3aJSON)\u3002 zap-logger-config.encoderConfig \u7528\u4e8e\u81ea\u5b9a\u4e49\u8bb0\u5f55\u5185\u5bb9\u7684 zap EncoderConfig \u3002 loglevel.<component> \u4ec5\u8986\u76d6\u7ed9\u5b9a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 Zap\u652f\u6301\u7684\u65e5\u5fd7\u7ea7\u522b\u6709: debug - \u7ec6\u7c92\u5ea6\u7684\u8c03\u8bd5 info - \u6b63\u5e38\u7684\u65e5\u5fd7 warn - \u610f\u5916\u4f46\u975e\u5173\u952e\u7684\u9519\u8bef error - \u5173\u952e\u7684\u9519\u8bef;\u6b63\u5e38\u64cd\u4f5c\u65f6\u51fa\u73b0\u610f\u5916 dpanic - \u5728\u8c03\u8bd5\u6a21\u5f0f\u4e0b\uff0c\u89e6\u53d1\u6050\u614c(\u5d29\u6e83) panic - \u5f15\u53d1\u6050\u614c(\u5d29\u6e83) fatal - \u7acb\u5373\u9000\u51fa\uff0c\u9000\u51fa\u72b6\u6001\u4e3a1(\u5931\u8d25)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bb0\u5f55"},{"location":"serving/observability/logging/config-logging/#_1","text":"\u6240\u6709Knative\u7ec4\u4ef6\u7684\u65e5\u5fd7\u914d\u7f6e\u90fd\u901a\u8fc7\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging ConfigMap\u8fdb\u884c\u7ba1\u7406\u3002 \u4f8b\u5982\uff0c\u670d\u52a1\u7ec4\u4ef6\u901a\u8fc7 knative-serving \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u4e8b\u4ef6\u7ec4\u4ef6\u901a\u8fc7 knative-eventing \u547d\u540d\u7a7a\u95f4\u4e2d\u7684 config-logging \u914d\u7f6e\uff0c\u7b49\u7b49\u3002 Knative\u7ec4\u4ef6\u4f7f\u7528 zap \u65e5\u5fd7\u5e93;\u9009\u9879 \u5728\u8be5\u9879\u76ee\u4e2d\u6709\u66f4\u8be6\u7ec6\u7684\u6587\u6863 \u3002 \u9664\u4e86 zap-logger-config \uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u952e\uff0c\u9002\u7528\u4e8e\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u7ec4\u4ef6\uff0c config-logging ConfigMap\u652f\u6301\u8986\u76d6\u5355\u4e2a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u7ea7\u522b\u3002 ConfigMap key Description zap-logger-config \u7528\u4e8ezap\u8bb0\u5f55\u5668\u914d\u7f6e\u7684JSON\u5bf9\u8c61\u5bb9\u5668\u3002\u5173\u952e\u5b57\u6bb5\u5728\u4e0b\u9762\u7a81\u51fa\u663e\u793a\u3002 zap-logger-config.level \u7ec4\u4ef6\u7684\u9ed8\u8ba4\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 zap-logger-config.encoding \u7ec4\u4ef6\u65e5\u5fd7\u7684\u65e5\u5fd7\u7f16\u7801\u683c\u5f0f(\u9ed8\u8ba4\u4e3aJSON)\u3002 zap-logger-config.encoderConfig \u7528\u4e8e\u81ea\u5b9a\u4e49\u8bb0\u5f55\u5185\u5bb9\u7684 zap EncoderConfig \u3002 loglevel.<component> \u4ec5\u8986\u76d6\u7ed9\u5b9a\u7ec4\u4ef6\u7684\u65e5\u5fd7\u8bb0\u5f55\u7ea7\u522b\u3002\u5728\u6b64\u7ea7\u522b\u6216\u4ee5\u4e0a\u7684\u6d88\u606f\u5c06\u88ab\u8bb0\u5f55\u3002 Zap\u652f\u6301\u7684\u65e5\u5fd7\u7ea7\u522b\u6709: debug - \u7ec6\u7c92\u5ea6\u7684\u8c03\u8bd5 info - \u6b63\u5e38\u7684\u65e5\u5fd7 warn - \u610f\u5916\u4f46\u975e\u5173\u952e\u7684\u9519\u8bef error - \u5173\u952e\u7684\u9519\u8bef;\u6b63\u5e38\u64cd\u4f5c\u65f6\u51fa\u73b0\u610f\u5916 dpanic - \u5728\u8c03\u8bd5\u6a21\u5f0f\u4e0b\uff0c\u89e6\u53d1\u6050\u614c(\u5d29\u6e83) panic - \u5f15\u53d1\u6050\u614c(\u5d29\u6e83) fatal - \u7acb\u5373\u9000\u51fa\uff0c\u9000\u51fa\u72b6\u6001\u4e3a1(\u5931\u8d25)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e"},{"location":"serving/observability/metrics/collecting-metrics/","text":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf \u00b6 Knative\u652f\u6301\u6536\u96c6\u6307\u6807\u7684\u4e0d\u540c\u6d41\u884c\u5de5\u5177: Prometheus OpenTelemetry Collector Grafana \u4eea\u8868\u677f\u53ef\u7528\u4e8e\u76f4\u63a5\u4ecePrometheus\u6536\u96c6\u7684\u6307\u6807\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6eOpenTelemetry Collector\uff0c\u4ee5\u4fbf\u4eceKnative\u7ec4\u4ef6\u63a5\u6536\u5ea6\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u53d1\u7ed9\u652f\u6301OpenTelemetry\u7684\u5176\u4ed6\u5ea6\u91cf\u63d0\u4f9b\u7a0b\u5e8f\u3002 Warning \u60a8\u4e0d\u80fd\u540c\u65f6\u4f7f\u7528OpenTelemetry Collector\u548cPrometheus\u3002 \u9ed8\u8ba4\u7684\u5ea6\u91cf\u540e\u7aef\u662fPrometheus\u3002 \u4f60\u9700\u8981\u4ece config-observability Configmap\u4e2d\u5220\u9664 metrics.backend-destination \u548c metrics.request-metrics-backend-destination \u952e\u6765\u542f\u7528Prometheus\u5ea6\u91cf\u3002 \u5173\u4e8e Prometheus \u00b6 Prometheus \u662f\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u3001\u805a\u5408\u65f6\u95f4\u5e8f\u5217\u5ea6\u91cf\u548c\u8b66\u62a5\u7684\u5f00\u6e90\u5de5\u5177\u3002 \u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u522e\u9664OpenTelemetry Collector\uff0c\u4e0b\u9762\u5c06\u5728\u4f7f\u7528Prometheus\u65f6\u6f14\u793a\u8fd9\u4e00\u70b9\u3002 \u914d\u7f6e Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml \u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . \u5173\u4e8e OpenTelemetry \u00b6 OpenTelemetry\u662f\u4e00\u4e2a\u9488\u5bf9\u4e91\u539f\u751f\u8f6f\u4ef6\u7684CNCF\u53ef\u89c2\u5bdf\u6027\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u7ec4\u5de5\u5177\u3001api\u548csdk\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528OpenTelemetry\u6765\u6d4b\u91cf\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u3002 \u8fd9\u4e9b\u6570\u636e\u5305\u62ec\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u6765\u4e86\u89e3Knative\u7ec4\u4ef6\u7684\u6027\u80fd\u548c\u884c\u4e3a\u3002 OpenTelemetry\u5141\u8bb8\u60a8\u8f7b\u677e\u5730\u5c06\u6307\u6807\u5bfc\u51fa\u5230\u591a\u4e2a\u76d1\u89c6\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u6784\u5efa\u6216\u91cd\u65b0\u914d\u7f6eKnative\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u7406\u89e3\u6536\u96c6\u5668 \u00b6 \u6536\u96c6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u5404\u79cdKnative\u7ec4\u4ef6\u53ef\u4ee5\u5728\u5176\u4e2d\u63a8\u9001\u7531\u76d1\u89c6\u670d\u52a1\u4fdd\u7559\u548c\u6536\u96c6\u7684\u6307\u6807\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528ConfigMap\u548cDeployment\u914d\u7f6e\u5355\u4e2a\u6536\u96c6\u5668\u5b9e\u4f8b\u3002 Tip \u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u90e8\u7f72\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 OpenTelemetry Operator \u81ea\u52a8\u5316\u8fd9\u4e9b\u6b65\u9aa4\u4e2d\u7684\u4e00\u4e9b\u3002 Caution https://github.com/knative-sandbox/monitoring/tree/main/grafana\u4e0a\u7684Grafana\u4eea\u8868\u677f\u4e0d\u80fd\u4f7f\u7528\u4eceOpenTelemetry Collector\u4e2d\u63d0\u53d6\u7684\u6307\u6807\u3002 \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' \u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u6536\u96c6\u5ea6\u91cf\u6807\u51c6"},{"location":"serving/observability/metrics/collecting-metrics/#knative","text":"Knative\u652f\u6301\u6536\u96c6\u6307\u6807\u7684\u4e0d\u540c\u6d41\u884c\u5de5\u5177: Prometheus OpenTelemetry Collector Grafana \u4eea\u8868\u677f\u53ef\u7528\u4e8e\u76f4\u63a5\u4ecePrometheus\u6536\u96c6\u7684\u6307\u6807\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6eOpenTelemetry Collector\uff0c\u4ee5\u4fbf\u4eceKnative\u7ec4\u4ef6\u63a5\u6536\u5ea6\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u53d1\u7ed9\u652f\u6301OpenTelemetry\u7684\u5176\u4ed6\u5ea6\u91cf\u63d0\u4f9b\u7a0b\u5e8f\u3002 Warning \u60a8\u4e0d\u80fd\u540c\u65f6\u4f7f\u7528OpenTelemetry Collector\u548cPrometheus\u3002 \u9ed8\u8ba4\u7684\u5ea6\u91cf\u540e\u7aef\u662fPrometheus\u3002 \u4f60\u9700\u8981\u4ece config-observability Configmap\u4e2d\u5220\u9664 metrics.backend-destination \u548c metrics.request-metrics-backend-destination \u952e\u6765\u542f\u7528Prometheus\u5ea6\u91cf\u3002","title":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus","text":"Prometheus \u662f\u4e00\u4e2a\u7528\u4e8e\u6536\u96c6\u3001\u805a\u5408\u65f6\u95f4\u5e8f\u5217\u5ea6\u91cf\u548c\u8b66\u62a5\u7684\u5f00\u6e90\u5de5\u5177\u3002 \u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u522e\u9664OpenTelemetry Collector\uff0c\u4e0b\u9762\u5c06\u5728\u4f7f\u7528Prometheus\u65f6\u6f14\u793a\u8fd9\u4e00\u70b9\u3002","title":"\u5173\u4e8e Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus_1","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"\u914d\u7f6e Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus_2","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"\u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b"},{"location":"serving/observability/metrics/collecting-metrics/#opentelemetry","text":"OpenTelemetry\u662f\u4e00\u4e2a\u9488\u5bf9\u4e91\u539f\u751f\u8f6f\u4ef6\u7684CNCF\u53ef\u89c2\u5bdf\u6027\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u7ec4\u5de5\u5177\u3001api\u548csdk\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528OpenTelemetry\u6765\u6d4b\u91cf\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u3002 \u8fd9\u4e9b\u6570\u636e\u5305\u62ec\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u6765\u4e86\u89e3Knative\u7ec4\u4ef6\u7684\u6027\u80fd\u548c\u884c\u4e3a\u3002 OpenTelemetry\u5141\u8bb8\u60a8\u8f7b\u677e\u5730\u5c06\u6307\u6807\u5bfc\u51fa\u5230\u591a\u4e2a\u76d1\u89c6\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u6784\u5efa\u6216\u91cd\u65b0\u914d\u7f6eKnative\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002","title":"\u5173\u4e8e OpenTelemetry"},{"location":"serving/observability/metrics/collecting-metrics/#_1","text":"\u6536\u96c6\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u5404\u79cdKnative\u7ec4\u4ef6\u53ef\u4ee5\u5728\u5176\u4e2d\u63a8\u9001\u7531\u76d1\u89c6\u670d\u52a1\u4fdd\u7559\u548c\u6536\u96c6\u7684\u6307\u6807\u3002 \u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528ConfigMap\u548cDeployment\u914d\u7f6e\u5355\u4e2a\u6536\u96c6\u5668\u5b9e\u4f8b\u3002 Tip \u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u90e8\u7f72\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 OpenTelemetry Operator \u81ea\u52a8\u5316\u8fd9\u4e9b\u6b65\u9aa4\u4e2d\u7684\u4e00\u4e9b\u3002 Caution https://github.com/knative-sandbox/monitoring/tree/main/grafana\u4e0a\u7684Grafana\u4eea\u8868\u677f\u4e0d\u80fd\u4f7f\u7528\u4eceOpenTelemetry Collector\u4e2d\u63d0\u53d6\u7684\u6307\u6807\u3002","title":"\u7406\u89e3\u6536\u96c6\u5668"},{"location":"serving/observability/metrics/collecting-metrics/#_2","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"serving/observability/metrics/collecting-metrics/#_3","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e"},{"location":"serving/observability/metrics/serving-metrics/","text":"Knative Serving metrics \u00b6 Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next. Activator \u00b6 The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable Autoscaler \u00b6 Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable Controller \u00b6 The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable Webhook \u00b6 Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable Go Runtime - memstats \u00b6 Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Knative\u670d\u52a1\u6307\u6807"},{"location":"serving/observability/metrics/serving-metrics/#knative-serving-metrics","text":"Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next.","title":"Knative Serving metrics"},{"location":"serving/observability/metrics/serving-metrics/#activator","text":"The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable","title":"Activator"},{"location":"serving/observability/metrics/serving-metrics/#autoscaler","text":"Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable","title":"Autoscaler"},{"location":"serving/observability/metrics/serving-metrics/#controller","text":"The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable","title":"Controller"},{"location":"serving/observability/metrics/serving-metrics/#webhook","text":"Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable","title":"Webhook"},{"location":"serving/observability/metrics/serving-metrics/#go-runtime-memstats","text":"Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Go Runtime - memstats"},{"location":"serving/reference/serving-api/","text":"This file is updated to the correct version from the serving repo (docs/serving-api.md) during the build.","title":"\u670d\u52a1API"},{"location":"serving/revisions/","text":"\u5173\u4e8e\u4fee\u8ba2 \u00b6 \u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002 \u989d\u5916\u8d44\u6e90 \u00b6 \u4fee\u8ba2\u6982\u5ff5\u6587\u6863","title":"\u5173\u4e8e\u4fee\u8ba2"},{"location":"serving/revisions/#_1","text":"\u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002","title":"\u5173\u4e8e\u4fee\u8ba2"},{"location":"serving/revisions/#_2","text":"\u4fee\u8ba2\u6982\u5ff5\u6587\u6863","title":"\u989d\u5916\u8d44\u6e90"},{"location":"serving/revisions/revision-admin-config-options/","text":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a Knative Services \u7684 revision \u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 config-gc ConfigMap \u6765\u8bbe\u7f6e\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u914d\u7f6e\u3002 \u53ef\u4ee5\u4fee\u6539\u4ee5\u4e0b\u5783\u573e\u6536\u96c6\u8bbe\u7f6e: Name Description retain-since-create-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u521b\u5efa\u4fee\u8ba2\u5f00\u59cb\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 retain-since-last-active-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u4fee\u8ba2\u6700\u540e\u4e00\u6b21\u6d3b\u52a8\u5230\u4fee\u8ba2\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 min-non-active-revisions \u8981\u4fdd\u7559\u7684\u975e\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5c0f\u6570\u91cf\u3002 max-non-active-revisions \u8981\u4fdd\u7559\u7684\u672a\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5927\u6570\u91cf\u3002 \u5982\u679c\u4fee\u8ba2\u7248\u5c5e\u4e8e\u4e0b\u5217\u4efb\u4f55\u4e00\u7c7b\uff0c\u5219\u59cb\u7ec8\u4fdd\u7559: \u4fee\u8ba2\u662f\u6d3b\u52a8\u7684\uff0c\u5e76\u7531\u8def\u7531\u5f15\u7528\u3002 \u5728 retain-since-create-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u521b\u5efa\u4fee\u8ba2\u3002 \u5728 retain-since-last-active-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\uff0c\u8def\u7531\u6700\u540e\u4e00\u6b21\u5f15\u7528\u4fee\u8ba2\u3002 \u73b0\u6709\u4fee\u8ba2\u7684\u6570\u91cf\u5c11\u4e8e min-non-active-revisions \u8bbe\u7f6e\u6240\u6307\u5b9a\u7684\u6570\u91cf\u3002 \u4f8b\u5b50 \u00b6 \u7acb\u5373\u6e05\u7406\u4efb\u4f55\u4e0d\u6d3b\u8dc3\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" \u4fdd\u7559\u6700\u540e 10 \u4e2a\u672a\u6fc0\u6d3b\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\" \u5728\u96c6\u7fa4\u4e0a\u7981\u7528\u5783\u573e\u6536\u96c6: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\"","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-admin-config-options/#_1","text":"\u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a Knative Services \u7684 revision \u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-admin-config-options/#_2","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 config-gc ConfigMap \u6765\u8bbe\u7f6e\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u914d\u7f6e\u3002 \u53ef\u4ee5\u4fee\u6539\u4ee5\u4e0b\u5783\u573e\u6536\u96c6\u8bbe\u7f6e: Name Description retain-since-create-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u521b\u5efa\u4fee\u8ba2\u5f00\u59cb\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 retain-since-last-active-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u4fee\u8ba2\u6700\u540e\u4e00\u6b21\u6d3b\u52a8\u5230\u4fee\u8ba2\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 min-non-active-revisions \u8981\u4fdd\u7559\u7684\u975e\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5c0f\u6570\u91cf\u3002 max-non-active-revisions \u8981\u4fdd\u7559\u7684\u672a\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5927\u6570\u91cf\u3002 \u5982\u679c\u4fee\u8ba2\u7248\u5c5e\u4e8e\u4e0b\u5217\u4efb\u4f55\u4e00\u7c7b\uff0c\u5219\u59cb\u7ec8\u4fdd\u7559: \u4fee\u8ba2\u662f\u6d3b\u52a8\u7684\uff0c\u5e76\u7531\u8def\u7531\u5f15\u7528\u3002 \u5728 retain-since-create-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u521b\u5efa\u4fee\u8ba2\u3002 \u5728 retain-since-last-active-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\uff0c\u8def\u7531\u6700\u540e\u4e00\u6b21\u5f15\u7528\u4fee\u8ba2\u3002 \u73b0\u6709\u4fee\u8ba2\u7684\u6570\u91cf\u5c11\u4e8e min-non-active-revisions \u8bbe\u7f6e\u6240\u6307\u5b9a\u7684\u6570\u91cf\u3002","title":"\u5783\u573e\u6536\u96c6"},{"location":"serving/revisions/revision-admin-config-options/#_3","text":"\u7acb\u5373\u6e05\u7406\u4efb\u4f55\u4e0d\u6d3b\u8dc3\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" \u4fdd\u7559\u6700\u540e 10 \u4e2a\u672a\u6fc0\u6d3b\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\" \u5728\u96c6\u7fa4\u4e0a\u7981\u7528\u5783\u573e\u6536\u96c6: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\"","title":"\u4f8b\u5b50"},{"location":"serving/revisions/revision-developer-config-options/","text":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u867d\u7136\u5728\u4e0d\u4fee\u6539 Knative \u670d\u52a1\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u624b\u52a8\u521b\u5efa\u4fee\u8ba2\uff0c\u4f46\u60a8\u53ef\u4ee5\u4fee\u6539\u73b0\u6709\u4fee\u8ba2\u7684\u89c4\u8303\u4ee5\u66f4\u6539\u5176\u884c\u4e3a\u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u4e3a\u4fee\u8ba2\u7248\u7981\u7528\u5783\u573e\u6536\u96c6 \u00b6 \u4f60\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0 serving.knative.dev/no-gc: \"true\" \u6ce8\u91ca\u6765\u914d\u7f6e Revision\uff0c\u4f7f\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u5783\u573e\u56de\u6536: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\"","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-developer-config-options/#_1","text":"\u867d\u7136\u5728\u4e0d\u4fee\u6539 Knative \u670d\u52a1\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u624b\u52a8\u521b\u5efa\u4fee\u8ba2\uff0c\u4f46\u60a8\u53ef\u4ee5\u4fee\u6539\u73b0\u6709\u4fee\u8ba2\u7684\u89c4\u8303\u4ee5\u66f4\u6539\u5176\u884c\u4e3a\u3002","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-developer-config-options/#_2","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002","title":"\u5783\u573e\u6536\u96c6"},{"location":"serving/revisions/revision-developer-config-options/#_3","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0 serving.knative.dev/no-gc: \"true\" \u6ce8\u91ca\u6765\u914d\u7f6e Revision\uff0c\u4f7f\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u5783\u573e\u56de\u6536: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\"","title":"\u4e3a\u4fee\u8ba2\u7248\u7981\u7528\u5783\u573e\u6536\u96c6"},{"location":"serving/services/","text":"About Knative Services \u00b6 Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic. Modifying Knative services \u00b6 Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present. Additional resources \u00b6 For more information about the Knative Service object, see the Resource Types documentation.","title":"\u5173\u4e8e\u670d\u52a1"},{"location":"serving/services/#about-knative-services","text":"Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.","title":"About Knative Services"},{"location":"serving/services/#modifying-knative-services","text":"Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present.","title":"Modifying Knative services"},{"location":"serving/services/#additional-resources","text":"For more information about the Knative Service object, see the Resource Types documentation.","title":"Additional resources"},{"location":"serving/services/byo-certificate/","text":"Using a custom TLS certificate for DomainMapping \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation. Prerequisites \u00b6 You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed. Procedure \u00b6 Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"\u4e3aDomainMapping\u4f7f\u7528\u81ea\u5b9a\u4e49TLS\u8bc1\u4e66"},{"location":"serving/services/byo-certificate/#using-a-custom-tls-certificate-for-domainmapping","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation.","title":"Using a custom TLS certificate for DomainMapping"},{"location":"serving/services/byo-certificate/#prerequisites","text":"You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed.","title":"Prerequisites"},{"location":"serving/services/byo-certificate/#procedure","text":"Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"Procedure"},{"location":"serving/services/certificate-class/","text":"Configuring a custom certificate class for a Service \u00b6 When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation. Using the certificate class annotation \u00b6 Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"\u914d\u7f6e\u8bc1\u4e66\u7c7b"},{"location":"serving/services/certificate-class/#configuring-a-custom-certificate-class-for-a-service","text":"When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation.","title":"Configuring a custom certificate class for a Service"},{"location":"serving/services/certificate-class/#using-the-certificate-class-annotation","text":"Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"Using the certificate class annotation"},{"location":"serving/services/configure-requests-limits-services/","text":"Configure resource requests and limits \u00b6 You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1 Additional resources \u00b6 For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"\u914d\u7f6e\u8d44\u6e90\u8bf7\u6c42\u548c\u9650\u5236"},{"location":"serving/services/configure-requests-limits-services/#configure-resource-requests-and-limits","text":"You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1","title":"Configure resource requests and limits"},{"location":"serving/services/configure-requests-limits-services/#additional-resources","text":"For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"Additional resources"},{"location":"serving/services/creating-services/","text":"Creating a Service \u00b6 You can create a Knative service by applying a YAML file or using the kn service create CLI command. Prerequisites \u00b6 To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI . Procedure \u00b6 Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"\u521b\u5efa\u670d\u52a1"},{"location":"serving/services/creating-services/#creating-a-service","text":"You can create a Knative service by applying a YAML file or using the kn service create CLI command.","title":"Creating a Service"},{"location":"serving/services/creating-services/#prerequisites","text":"To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI .","title":"Prerequisites"},{"location":"serving/services/creating-services/#procedure","text":"Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"Procedure"},{"location":"serving/services/custom-domains/","text":"Configuring custom domains \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain . Prerequisites \u00b6 You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar. Procedure \u00b6 To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u7684\u57df"},{"location":"serving/services/custom-domains/#configuring-custom-domains","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain .","title":"Configuring custom domains"},{"location":"serving/services/custom-domains/#prerequisites","text":"You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar.","title":"Prerequisites"},{"location":"serving/services/custom-domains/#procedure","text":"To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Procedure"},{"location":"serving/services/http-protocol/","text":"HTTPS redirection \u00b6 Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details. Overriding the default HTTP behavior \u00b6 You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"HTTPS\u91cd\u5b9a\u5411"},{"location":"serving/services/http-protocol/#https-redirection","text":"Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details.","title":"HTTPS redirection"},{"location":"serving/services/http-protocol/#overriding-the-default-http-behavior","text":"You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"Overriding the default HTTP behavior"},{"location":"serving/services/ingress-class/","text":"Configuring Services custom ingress class \u00b6 When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services Using the ingress class annotation \u00b6 Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"\u914d\u7f6e\u5bfc\u5165\u7c7b"},{"location":"serving/services/ingress-class/#configuring-services-custom-ingress-class","text":"When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services","title":"Configuring Services custom ingress class"},{"location":"serving/services/ingress-class/#using-the-ingress-class-annotation","text":"Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"Using the ingress class annotation"},{"location":"serving/services/private-services/","text":"Configuring private Services \u00b6 By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway. Using the cluster-local label \u00b6 To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local Example \u00b6 You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"\u914d\u7f6e\u79c1\u4eba\u670d\u52a1"},{"location":"serving/services/private-services/#configuring-private-services","text":"By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway.","title":"Configuring private Services"},{"location":"serving/services/private-services/#using-the-cluster-local-label","text":"To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local","title":"Using the cluster-local label"},{"location":"serving/services/private-services/#example","text":"You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"Example"},{"location":"serving/services/service-metrics/","text":"Service metrics \u00b6 Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side. Queue proxy metrics \u00b6 Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"\u670d\u52a1\u6307\u6807"},{"location":"serving/services/service-metrics/#service-metrics","text":"Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side.","title":"Service metrics"},{"location":"serving/services/service-metrics/#queue-proxy-metrics","text":"Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Queue proxy metrics"},{"location":"serving/services/using-queue-extensions/","text":"Using extensions enabled by QPOptions \u00b6 QPOptions is a Queue Proxy feature that enables extending Queue Proxy with additional Go packages. For example, the security-guard repository extends Queue Proxy by adding runtime security features to protect user services. Once your cluster is setup with extensions enabled by QPOptions, a Service can decide which extensions it wish to use and how to configure such extensions. Activating and configuring extensions is described here. Overview \u00b6 A Service can activate and configure extensions by adding qpoption.knative.dev/* annotations under the: spec.template.metadata of the Service Custom Resource Definition (CRD). Setting a value of: qpoption.knative.dev/<ExtensionName>-activate: \"enable\" activates the extension. Setting a value of: qpoption.knative.dev/<extension-name>-config-<key>: \"<value>\" adds a configuration of key: value to the extension. In addition, the Service must ensure that the Pod Info volume is mounted by adding the features.knative.dev/queueproxy-podinfo: enabled annotation under the: spec.template.metadata of the Service CRD. You can create a Knative Service by applying a YAML file or by using the kn service create CLI command. Prerequisites \u00b6 Before you can use extensions enabled by QPOptions, you must: Prepare your cluster: Make sure you are using a Queue Proxy image that was built with the extensions that you wish to use - See Extending Queue Proxy image with QPOptions . Make sure that the cluster config-features is set with queueproxy.mount-podinfo: allowed . See Enabling Queue Proxy Pod Info for more details. Meet the prerequisites in Creating a Service Procedure \u00b6 Tip The following commands create a helloworld-go sample Service while activating and configuring the test-gate extension for this Service. You can modify these commands, including the extension(s) to be activated and the extension configuration. Create a sample Service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/testgate-activate : enable qpoption.knative.dev/testgate-config-response : CU qpoption.knative.dev/testgate-config-sender : Joe spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env TARGET=World \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/testgate-activate=enable \\ --annotation qpoption.knative.dev/testgate-config-response=Goodbye \\ --annotation qpoption.knative.dev/testgate-config-sender=Joe After the Service has been created, Knative propagates the annotations to the podSpec of the Service deployment. When a Service pod is created, the Queue Proxy sidecar will mount a volume that contains the pod annotations and activate the testgate extension. This occurs if the testgate extension is available in the Queue Proxy image. The testgate extension will then be configured with the configuration: { sender: \"Joe\", response: \"CU\"} .","title":"\u4f7f\u7528QPOptions\u542f\u7528\u7684\u6269\u5c55"},{"location":"serving/services/using-queue-extensions/#using-extensions-enabled-by-qpoptions","text":"QPOptions is a Queue Proxy feature that enables extending Queue Proxy with additional Go packages. For example, the security-guard repository extends Queue Proxy by adding runtime security features to protect user services. Once your cluster is setup with extensions enabled by QPOptions, a Service can decide which extensions it wish to use and how to configure such extensions. Activating and configuring extensions is described here.","title":"Using extensions enabled by QPOptions"},{"location":"serving/services/using-queue-extensions/#overview","text":"A Service can activate and configure extensions by adding qpoption.knative.dev/* annotations under the: spec.template.metadata of the Service Custom Resource Definition (CRD). Setting a value of: qpoption.knative.dev/<ExtensionName>-activate: \"enable\" activates the extension. Setting a value of: qpoption.knative.dev/<extension-name>-config-<key>: \"<value>\" adds a configuration of key: value to the extension. In addition, the Service must ensure that the Pod Info volume is mounted by adding the features.knative.dev/queueproxy-podinfo: enabled annotation under the: spec.template.metadata of the Service CRD. You can create a Knative Service by applying a YAML file or by using the kn service create CLI command.","title":"Overview"},{"location":"serving/services/using-queue-extensions/#prerequisites","text":"Before you can use extensions enabled by QPOptions, you must: Prepare your cluster: Make sure you are using a Queue Proxy image that was built with the extensions that you wish to use - See Extending Queue Proxy image with QPOptions . Make sure that the cluster config-features is set with queueproxy.mount-podinfo: allowed . See Enabling Queue Proxy Pod Info for more details. Meet the prerequisites in Creating a Service","title":"Prerequisites"},{"location":"serving/services/using-queue-extensions/#procedure","text":"Tip The following commands create a helloworld-go sample Service while activating and configuring the test-gate extension for this Service. You can modify these commands, including the extension(s) to be activated and the extension configuration. Create a sample Service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/testgate-activate : enable qpoption.knative.dev/testgate-config-response : CU qpoption.knative.dev/testgate-config-sender : Joe spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env TARGET=World \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/testgate-activate=enable \\ --annotation qpoption.knative.dev/testgate-config-response=Goodbye \\ --annotation qpoption.knative.dev/testgate-config-sender=Joe After the Service has been created, Knative propagates the annotations to the podSpec of the Service deployment. When a Service pod is created, the Queue Proxy sidecar will mount a volume that contains the pod annotations and activate the testgate extension. This occurs if the testgate extension is available in the Queue Proxy image. The testgate extension will then be configured with the configuration: { sender: \"Joe\", response: \"CU\"} .","title":"Procedure"},{"location":"serving/troubleshooting/debugging-application-issues/","text":"Debugging application issues \u00b6 If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application. Check terminal output \u00b6 Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1 Check Route status \u00b6 Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting . Check Ingress/Istio routing \u00b6 To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familiar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide . Check Ingress status \u00b6 Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue. Check Revision status \u00b6 If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status . Check Pod status \u00b6 To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"\u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898"},{"location":"serving/troubleshooting/debugging-application-issues/#debugging-application-issues","text":"If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application.","title":"Debugging application issues"},{"location":"serving/troubleshooting/debugging-application-issues/#check-terminal-output","text":"Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1","title":"Check terminal output"},{"location":"serving/troubleshooting/debugging-application-issues/#check-route-status","text":"Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting .","title":"Check Route status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingressistio-routing","text":"To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familiar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide .","title":"Check Ingress/Istio routing"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingress-status","text":"Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.","title":"Check Ingress status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-revision-status","text":"If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status .","title":"Check Revision status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-pod-status","text":"To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Check Pod status"}]}