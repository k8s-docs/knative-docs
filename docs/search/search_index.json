{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\/\\s\\-\\.]+"},"docs":[{"location":"","text":"","title":"\u4e3b\u9875"},{"location":"about-analytics-cookies/","text":"By using this website, you are confirming that you accept our use of cookies for the purposes of collecting anonymous usage data to improve the user experience and content on the website. Continue to sections below for details about knative.dev, or use the following resources to learn about cookies in general: Learn about basic site analytics usuage at: https://www.cookiechoices.org/ You can also watch a video about how Google uses cookies . What are cookies? \u00b6 Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior. What cookies are used on knative.dev \u00b6 We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement . Options for opting out \u00b6 Use the following options to prevent your data from being shared with websites. Opt-out Browser Add-on \u00b6 You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on . Disabling cookies \u00b6 You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Learn about Google Analytics cookies"},{"location":"about-analytics-cookies/#what-are-cookies","text":"Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior.","title":"What are cookies?"},{"location":"about-analytics-cookies/#what-cookies-are-used-on-knativedev","text":"We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement .","title":"What cookies are used on knative.dev"},{"location":"about-analytics-cookies/#options-for-opting-out","text":"Use the following options to prevent your data from being shared with websites.","title":"Options for opting out"},{"location":"about-analytics-cookies/#opt-out-browser-add-on","text":"You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on .","title":"Opt-out Browser Add-on"},{"location":"about-analytics-cookies/#disabling-cookies","text":"You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Disabling cookies"},{"location":"about/testimonials/","text":"Enterprise-grade Serverless on your own terms. Understanding Knative \"If Kubernetes is an electrical grid, then Knative is its light switch.\" \u2014Kelsey Hightower, Google Cloud Platform Knative is an automated system that helps development teams manage and maintain processes in Kubernetes. Its purpose is to simplify, automate, and monitor deployments of Kubernetes so teams spend less time on maintenance and more time on app development and projects. Knative takes over repetitive and time-intensive tasks while removing obstacles and delays. Knative does this through two features. The first is Knative Eventing. Eventing allows developers to set up detailed actions triggered by specific events within a broader environment. The second is Knative Serving, which automatically manages the creation and scaling of services through Kubernetes, including scaling down to zero. Each of these features aims to free up resources that teams would otherwise spend managing systems. They also save businesses money by reacting to conditions in real time. Meaning, companies only pay for the resources they are using, not the ones they might use. Scale to Zero is a feature of Knative Serving that automatically turns off services running in containers when there is no demand for them. Instead of running programs on standby, they can be turned off and turned back on when needed again. Scale to zero reduces costs over time and helps manage technical resources. The core idea behind Knative is to allow teams to harness the power of serverless application deployment. Serverless refers to managing cloud-based servers and virtual machines, often hosted on platforms like AWS, Google Cloud, Microsoft Azure, and more. Serverless is a great option for companies looking to move away from the costly endeavor of managing their own servers and infrastructure. \"I often think of Knative as part of 'Serverless 2.0.' It combines the good things about serverless with a loosening of constraints around execution time and availability of resources.\" -Michael Behrendt, Distinguished Engineer and Chief Architect of Serverless and Cloud Functions for IBM. IBM is a committed sponsor of Knative Knative in the broader ecosystem To understand Knative more fully, it is important to know that it exists in a larger ecosystem of services that work together. For example, Knative acts as a framework on top of Kubernetes that helps build a serverless platform. Kubernetes itself is a system that orchestrates the creation and running of containers used in app deployment, scaling, and more. Those containers can run anything, from simple tools written in python to complex Al systems. Containers were developed to help tackle the problem of complexity. As development teams build software products, they create massive codebases. Left unorganized, those codebases can become gigantic and confusing-even for those who make them. Containers solve this problem by breaking codebases into small, self-contained processes that can interact to do work. They also help developers manage complex webs of dependencies like APIs and databases. These containers are easier to maintain for teams looking to work fast while maintaining best practices. Knative's value in DevOps DevOps promises effective application development processes with faster deployments and fewer bugs. While Kubernetes helps facilitate this, it can produce significant complexity. Achieving value at scale with Kubernetes traditionally involves teams developing specialized knowledge. Knative cuts down on that by providing a serverless experience that removes the need for all development team members to know or understand the ins and outs of Kubernetes. \"What we are doing with Knative is to provide a developer experience that makes it easier to focus on code. Cloud developers focus on the business problems they are solving without having to coordinate or wait for approvals from platform teams to scale their apps. Knative is a framework that helps automate platform capabilities so your apps can scale as if they were running on Serverless compute.\" -Aparna Sinha, Director of Product Management, Google Tangible benefits of Knative for teams It has always been true that organizations need to develop and innovate faster than their competition while deploying products with fewer flaws. However, being bogged down by configuring networks and operating systems harms developer productivity and morale. Developers want to create things, and Knative helps them do that. \"The amount of internal work needed to use Knative is minuscule.\" -Tilen Kav\u010di\u010d, Backend Developer for Outfit7, which uses Knative for key backend system The advantage of Open Source Open source has been a powerful resource for creating business solutions for decades. Kubernetes and Knative are now paving the way for that relationship to become stronger. Each project has significant support from some of the biggest names in tech including IBM, Google, Red Hat, and VMware. The Kubernetes and Knative ecosystem consists of widely adopted projects that are proven across many installations for a multitude of uses. The open-source foundation of Knative means that anyone using the platform can participate in the community to get help, solve problems, and influence the direction of deployment for future versions. Find out more > Case Studies: Read about organizations using Knative, from platform developers to proven companies to innovative startups > Check the getting started guide to get up and running with Knative in an afternoon > Join the Knative Slack to talk to the community","title":"\u5956\u72b6"},{"location":"about/case-studies/deepc/","text":"deepc Case Study \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d -- Andrew Webber, Senior Software Engineer for deepc AI Startup deepc Connects Researchers to Radiologists with Knative Eventing deepc is a startup at the cutting edge, bringing advanced data techniques and artificial intelligence to healthcare. The German company helps radiologists access better diagnostics and resources through cloud-based technology. Their goal is to elevate treatment quality across the board while creating more efficiency and improvement in medical settings. This revolution in technology comes with hefty challenges. Care systems are highly regulated, making patient privacy and safety a top priority for deepc. Doctors and medical staff also demand that new technologies be reliable and stable when lives are on the line. Rising to the challenge deepc has risen to meet these challenges through carefully architected solutions, using tools like Knative Eventing to their full potential. Their product helps radiologists access a wide selection of artificial intelligence (AI) programs that analyze imaging, like x-rays and MRIs. The data generated from these AI programs help radiologists make more accurate diagnoses. The deepc workflow The radiologist uploads the image into deepcOS, initially to a virtual machine within the hospital IT infrastructure containing the deepcOS client application. After making a series of selections, deepcOS identifies the proper AI to use. It then removes the patient information from the scans before encrypting the data. deepcOS sends that data to the cloud-based deepc AI platform. This platform does the heavy lifting in providing the computing power the AI algorithms need to do their work. After the program finishes, the results are sent back. Finally, the data is reassociated with the patient, and the radiologist can take action based on the results. Critically, patient information always remains on-premises in the hospital and is not transmitted to deepc servers. A Knative-powered process The deepcOS workflow builds on a sophisticated implementation of Knative Eventing. Knative Eventing allows teams to deploy event-driven architecture with serverless applications quickly. In conjunction with Knative Serving, deepc resources and programs scale up and down automatically based on specific event triggers laid out by the developers. Knative takes care of the management, so the process does not need to wait for a person to take action. When data is sent to deepc's cloud-based platform, Knative emits an event that triggers a specific AI. After one is selected, Knative starts a container environment for the program to run. Some AI programs may only need one container. Others may require multiple, running parallel or in sequence. In the case of multiple containers, the deepc team created workflows using Knative Eventing to coordinate the complex processes. After the process finishes and provides the output for the radiologist, Knative triggers stop the active containers. \"Knative gives us a foundation of consistency,\" said Andrew Webber, Senior Software Engineer. Bridging between legacy and advanced systems The platform makes available AIs developed by leading global companies and researchers. Knative has also allowed integration with the work of independent researchers through an SDK implementation for radiologists. They don\u2019t need to be Kubernetes experts or take days to bring their work to patients through deepc\u2019s platform. \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d said Webber. Some implementations are more complex. They use legacy technology that does not fit into a standard container or they have unique architectures that require OS-level configuration. deepc has built out APIs and virtual machines that connect those technologies to their own cloud-based platform and still integrate with the Knative Eventing workflow. This approach ensures those programs work flawlessly within the system. The case for business The choice to develop their platform around Knative has had several business benefits for the startup. One of the most complicated aspects of growing a company is scaling. Many technology companies find their developers start scrambling when more customers are onboarded, uncovering new bugs and other issues. However, because of the nature of Knative, this is less of a problem for deepc. Knative's combination of automation and serverless methods means as more customers are onboarded, deepc will not need to build out more resources - it will all happen automatically. Knative has also allowed the startup to add real value to customers using their technology. For example, because many applications used by radiologists are built by different companies, medical professionals have had to interact with disparate systems and procedures. deepc provides access to the work of many researchers on one platform, ending complicated processes for professionals on the ground. Healthcare systems get simple, unified billing. Knative has helped deepc create a robust case for customers to use their platform. Looking forward deepc has already done amazing things as a company, with many more features planned. The company is a model for how Knative can help any organization build an impressive technical architecture capable of addressing some of today's most complex problems. Using features provided by Knative has enabled them to pioneer what is possible. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"deepc"},{"location":"about/case-studies/outfit7/","text":"Outfit7 Case Study \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Game maker Outfit7 automates high performance ad bidding with Knative Serving Since its founding in 2009, the mobile gaming company Outfit7 has seen astronomical growth\u2014garnering over 17 billion downloads and over 85 billion video views last year. Outfit7 was in the Top 5 game publishers list on iOS and Google Play worldwide by the number of game downloads for 6 consecutive years (2015 - 2020). With their latest launch, My Talking Angela 2, they were number 1 by global games downloads in July, August, and September (more than 120 million downloads). The success of the well-known game developer behind massive hits like the Talking Tom & Friends franchise and stand-alone titles like Mythic Legends created large-scale challenges. With up to 470 million monthly active users, 20 thousand server requests per second, and terabytes of data generated daily, they needed a stable, high performance solution. They turned to a Knative and Kubernetes solution to optimize real-time bidding ad sales in a way that can automatically scale up and down as needed. They were able to develop a system so easy to maintain that it freed up two software engineers who are now able to work on more important tasks, like optimizing backend costs and adding new game features. High performance in-app bidding Ad sales are an important revenue stream of Outfit7. The team needed to walk a careful balance: sell the space for the highest bid, use technical resources efficiently, and make sure ads are served to players quickly. To achieve this they decided to adopt an in-app bidding approach. The Outfit7 user base generates around 8,000 ad-related requests per second. With so many users spread worldwide, the amount of these requests can drop or surge depending on all sorts of factors. Not just predictable things like the time of day, but current events can suddenly create traffic. The pandemic saw their usage soar, for instance. To manage the process in-house, the team needed to be able to test and deploy very efficiently. \"There were two specific use cases we wanted to cover,\" explained Luka Draksler, backend engineer in the ad technology department at Outfit7. \"One was to have the ability to do zero downtime canary deployments using an automatic gradual rollout process. This works in a way that the new version of the software is deployed using a continuous deployment pipeline with a small amount of traffic first. If everything checks out, all production traffic is migrated to the new version. In the worst-case scenario (if requests start failing) traffic can be quickly migrated to the old version. The second use-case was the ability to have developers deploy versions to specific groups of users for instances of A/B testing and other use cases.\" The team decided to adopt Knative Serving as the backbone of their solution. Knative allowed Outfit7 to streamline deployments and cut down on development time. After being surprised at how easily they generated an internal proof of concept, the team saw that it could craft custom solutions tuned for their internal workflows\u2014without consuming valuable developer time. In addition, they could quickly configure A/B testing and deploy multiple versions of code simultaneously. Serverless solution Knative Serving gave Outfit7 access to a robust set of tools and features that allows their team to automate and monitor the deployment of applications to handle ad requests. When more requests are coming in, their system automatically spins up more containers that house the workers and tools. When these requests drop, unneeded containers shut down. Outfit7 only pays for the resources they require for the current load. Knative works as a layer installed on top of Kubernetes. It brings the power of serverless workloads to the scalable capabilities of Kubernetes. Teams quickly spin up container-based applications without needing to consider the details of Kubernetes. Knative also simplifies project deployments to Kubernetes. Mitja Bezen\u0161ek, the Lead Developer on Outfit7's backend team, estimated that the traditional development that Knative replaced would have required three full-time engineers to maintain. Their new platform operates with minimal work and allows the developers to deploy updates at will. The open source community Outfit7's team was blown away by the supportive and helpful community around Knative. After discovering a problem with network scaling, the team was surprised by how easy it was to find answers and solutions. \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Sharing their story The great experience with Knative encouraged their team to share their experience with fellow companies and engineers at a local meetup. The presentation which included several live demos was a success, helping spawn another meet-up focused on the technology. \"Tilen showed them the demo and what it's all about,\" said Bezen\u0161ek. \"I hope we got them engaged going forward.\" Looking forward Outfit7 shows no signs of slowing down. \u201cAs we want to support our vision in expanding our games portfolio, we are always looking for new strategic partners who can accompany us on this path,\u201d added Helder Lopes, Head of R&D in Cyprus headquarters. The company plans to incorporate and adopt Knative into other back-end systems \u2013 taking advantage of the easier workflows that Knative offers. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"Outfit7"},{"location":"about/case-studies/puppet/","text":"Puppet Case Study \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional\" -- Noah Fontes, Senior Principal Software Engineer for Puppet Relay by Puppet Brings Workflows to Everything using Knative Puppet is a software company specializing in infrastructure automation. The company was founded in 2009 to solve some of the most complex problems for those who work in operations. Around 2019, the team noticed that cloud operations teams weren\u2019t able to effectively manage modern cloud-native applications because they were relying on manual workflow processes. The group saw an opportunity here to build out a platform to connect events triggered by modern architectures to ensure cloud environments remained secure, compliant, and cost-effective. This is the story of how Relay , a cloud-native workflow automation platform, was created, and how Knative and Kubernetes modernize and super-charge business process automation. The glue for DevOps When Puppet first began exploring the power and flexibility of Knative to trigger their Tekton-based workflow execution engine, they weren't quite sure where their journey was going to take them. Knative offered an attractive feature set, so they began building and experimenting. They wanted to build an event-driven DevOps tool; they weren't interested in building just another continuous integration product. In addition, as they continued to explore, they realized that they wanted something flexible and not tied to just one vertical. Whatever they were building, it was not going to focus on just one market. As their target came into focus, they realized that the serverless applications and functions enabled by Knative Serving would be perfect for a cloud-based business process automation service. Out of this realization, they built Relay , a cloud workflow automation product that helps Cloud Ops teams solve the integration and eventing issues that arise as organizations adopt multiple clouds and SaaS products alongside legacy solutions. Containers and webhooks Containers and webhooks are key elements in the Relay architecture. Containers allow Puppet to offer a cloud-based solution where businesses can configure and deploy workflows as discrete business units. Since the containers provide self-contained environments, even legacy services and packages can be included. This proved to be an essential feature for business customers. Anything that can be contained in a Docker image, for example, can be part of a Relay workflow. \"We focused on containers because they provide isolation,\" explains Noah Fontes, Senior Principal Software Engineer for Puppet, \"Containers provide discrete units of execution, where users can decrease the maintenance burden of complex systems.\" Allowing fully-configurable webhooks gives users the flexibility needed to incorporate business processes of all kinds. With webhooks, Relay can interact with nearly any web-based API to trigger rich, fully featured workflows across third party SaaS products, cloud services, web applications, and even system utilities. Knative Serving provides important infrastructure for Relay. It allows webhooks and services to scale automatically, even down to zero . This allows Relay to support pretty much any integration, including those used by only a small number of users. With autoscaling, those services don't consume resources while they are not being used. What is Knative Serving? Modern cloud-based applications deal with massive scaling challenges through several approaches. At the core of most of these is the use of containers: discrete computing units that run single applications, single services, or even just single functions. This approach is incredibly powerful, allowing services to scale the number of resources they consume as demand dictates. However, while all of this sounds amazing, it can be difficult to manage and configure. One of the most successful solutions for delivering this advanced architecture is Knative Serving. This framework builds on top of Kubernetes to support the deployment and management of serverless applications, services, and functions. In particular, Knative Services focuses on being easy to configure, deploy, and manage. Workflow integrations The open architecture allows Relay to integrate dozens of different services and platforms into workflows. A look at the Relay integrations GitHub page provides a list of these integrations and demonstrates their commitment to the open source community. \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional,\" says Fontes. Results: automated infrastructure management While Relay's infrastructure runs on the Google Cloud Platform, it is a library of workflows, integrations, steps, and triggers that includes services across all major cloud service providers. Relay customers can integrate across Microsoft Azure, AWS, and Oracle Cloud Infrastructure among others. By combining these integrations with SaaS offerings, it truly is becoming the Zapier of infrastructure management. \u201cOur customers have diverse needs for managing their workloads that are often best implemented as web APIs. Our product provides a serverless microservice environment powered by Knative that allows them to build this complex tooling without the management and maintenance overhead of traditional deployment architectures. We pass the cost savings on to them, and everyone is happier,\" said Fontes. Building and deploying Relay would not have been possible without the existing infrastructure offered by systems such as Knative and Tekton . Remarkably, Fontes' team never grew above eight engineers. Once they solidified their plan for Relay, they were able to bring it to production in just three months, says Fontes. \"Thanks to Knative, getting Relay out the door was easier than we thought it would be.\" said Noah Fontes, Senior Principal Software Engineer. Knative aims to make scalable, secure, stateless architectures available quickly by abstracting away the complex details of a Kubernetes installation and enabling developers to focus on what matters. Find out more Getting started with Knative Knative Serving Knative Eventing A Basic Introduction to Webhooks","title":"Puppet"},{"location":"client/","text":"CLI \u5de5\u5177 \u00b6 kubectl \u00b6 \u53ef\u4ee5\u4f7f\u7528 kubectl \u5e94\u7528\u5b89\u88c5Knative\u7ec4\u4ef6\u6240\u9700\u7684YAML\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528YAML\u521b\u5efaKnative\u8d44\u6e90\uff0c\u4f8b\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\u3002 \u53c2\u89c1 \u5b89\u88c5\u548c\u8bbe\u7f6e kubectl . kn \u00b6 kn \u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u7b80\u5355\u7684\u63a5\u53e3\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn \u8fd8\u7b80\u5316\u4e86\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u7a0b\u5e8f\u7684\u5b8c\u6210\u3002 Note kn \u4e0d\u80fd\u7528\u4e8e\u5b89\u88c5Knative\u7ec4\u4ef6\uff0c\u5982\u670d\u52a1\u6216\u4e8b\u4ef6\u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u67e5\u770b \u5b89\u88c5 kn . \u53c2\u89c1Github\u4e2d\u7684 kn \u6587\u6863 \u3002 func \u00b6 func CLI\u4f7f\u60a8\u80fd\u591f\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72Knative\u51fd\u6570\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u53c2\u89c1 \u5b89\u88c5Knative\u51fd\u6570 . \u53c2\u89c1Github\u4e2d\u7684 func \u6587\u6863 \u3002 \u5c06CLI\u5de5\u5177\u8fde\u63a5\u5230\u96c6\u7fa4 \u00b6 \u5b89\u88c5\u4e86 kubectl \u6216 kn \u540e\uff0c\u8fd9\u4e9b\u5de5\u5177\u5c06\u5728\u9ed8\u8ba4\u4f4d\u7f6e $HOME/.kube/config \u4e2d\u641c\u7d22\u96c6\u7fa4\u7684 kubeconfig \u6587\u4ef6\uff0c\u5e76\u4f7f\u7528\u8be5\u6587\u4ef6\u8fde\u63a5\u5230\u96c6\u7fa4\u3002 \u5728\u521b\u5efaKubernetes\u96c6\u7fa4\u65f6\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u521b\u5efa\u4e00\u4e2a kubeconfig \u6587\u4ef6\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 --kubeconfig : \u4f7f\u7528\u6b64\u9009\u9879\u6307\u5411 kubeconfig \u6587\u4ef6\u3002\u8fd9\u76f8\u5f53\u4e8e\u8bbe\u7f6e $KUBECONFIG \u73af\u5883\u53d8\u91cf\u3002 --context : \u4f7f\u7528\u6b64\u9009\u9879\u53ef\u4ece\u73b0\u6709\u7684 kubeconfig \u6587\u4ef6\u4e2d\u6307\u5b9a\u4e0a\u4e0b\u6587\u7684\u540d\u79f0\u3002\u4f7f\u7528 kubectl \u8f93\u51fa\u4e2d\u7684\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6: Setting the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI --config option, for example, kn service list --config path/to/config.yaml . The default config is at ~/.config/kn/config.yaml . For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files . \u5728\u5e73\u53f0\u4e0a\u4f7f\u7528kubeconfig\u6587\u4ef6 \u00b6 \u4f7f\u7528 kubeconfig \u6587\u4ef6\u7684\u8bf4\u660e\u53ef\u7528\u4e8e\u4ee5\u4e0b\u5e73\u53f0: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform \u542f\u52a8 minikube \u4f1a\u81ea\u52a8\u5199\u5165\u8be5\u6587\u4ef6\uff0c\u6216\u8005\u5728\u73b0\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u63d0\u4f9b\u9002\u5f53\u7684\u4e0a\u4e0b\u6587\u3002","title":"kn \u6982\u8ff0"},{"location":"client/#cli","text":"","title":"CLI \u5de5\u5177"},{"location":"client/#kubectl","text":"\u53ef\u4ee5\u4f7f\u7528 kubectl \u5e94\u7528\u5b89\u88c5Knative\u7ec4\u4ef6\u6240\u9700\u7684YAML\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528YAML\u521b\u5efaKnative\u8d44\u6e90\uff0c\u4f8b\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\u3002 \u53c2\u89c1 \u5b89\u88c5\u548c\u8bbe\u7f6e kubectl .","title":"kubectl"},{"location":"client/#kn","text":"kn \u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u7b80\u5355\u7684\u63a5\u53e3\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn \u8fd8\u7b80\u5316\u4e86\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u7a0b\u5e8f\u7684\u5b8c\u6210\u3002 Note kn \u4e0d\u80fd\u7528\u4e8e\u5b89\u88c5Knative\u7ec4\u4ef6\uff0c\u5982\u670d\u52a1\u6216\u4e8b\u4ef6\u3002","title":"kn"},{"location":"client/#_1","text":"\u67e5\u770b \u5b89\u88c5 kn . \u53c2\u89c1Github\u4e2d\u7684 kn \u6587\u6863 \u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"client/#func","text":"func CLI\u4f7f\u60a8\u80fd\u591f\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72Knative\u51fd\u6570\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002","title":"func"},{"location":"client/#_2","text":"\u53c2\u89c1 \u5b89\u88c5Knative\u51fd\u6570 . \u53c2\u89c1Github\u4e2d\u7684 func \u6587\u6863 \u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"client/#cli_1","text":"\u5b89\u88c5\u4e86 kubectl \u6216 kn \u540e\uff0c\u8fd9\u4e9b\u5de5\u5177\u5c06\u5728\u9ed8\u8ba4\u4f4d\u7f6e $HOME/.kube/config \u4e2d\u641c\u7d22\u96c6\u7fa4\u7684 kubeconfig \u6587\u4ef6\uff0c\u5e76\u4f7f\u7528\u8be5\u6587\u4ef6\u8fde\u63a5\u5230\u96c6\u7fa4\u3002 \u5728\u521b\u5efaKubernetes\u96c6\u7fa4\u65f6\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u521b\u5efa\u4e00\u4e2a kubeconfig \u6587\u4ef6\u3002 \u60a8\u8fd8\u53ef\u4ee5\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf $KUBECONFIG \uff0c\u5e76\u5c06\u5176\u6307\u5411KUBECONFIG\u6587\u4ef6\u3002 --kubeconfig : \u4f7f\u7528\u6b64\u9009\u9879\u6307\u5411 kubeconfig \u6587\u4ef6\u3002\u8fd9\u76f8\u5f53\u4e8e\u8bbe\u7f6e $KUBECONFIG \u73af\u5883\u53d8\u91cf\u3002 --context : \u4f7f\u7528\u6b64\u9009\u9879\u53ef\u4ece\u73b0\u6709\u7684 kubeconfig \u6587\u4ef6\u4e2d\u6307\u5b9a\u4e0a\u4e0b\u6587\u7684\u540d\u79f0\u3002\u4f7f\u7528 kubectl \u8f93\u51fa\u4e2d\u7684\u4e00\u4e2a\u4e0a\u4e0b\u6587\u3002 \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6: Setting the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI --config option, for example, kn service list --config path/to/config.yaml . The default config is at ~/.config/kn/config.yaml . For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files .","title":"\u5c06CLI\u5de5\u5177\u8fde\u63a5\u5230\u96c6\u7fa4"},{"location":"client/#kubeconfig","text":"\u4f7f\u7528 kubeconfig \u6587\u4ef6\u7684\u8bf4\u660e\u53ef\u7528\u4e8e\u4ee5\u4e0b\u5e73\u53f0: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform \u542f\u52a8 minikube \u4f1a\u81ea\u52a8\u5199\u5165\u8be5\u6587\u4ef6\uff0c\u6216\u8005\u5728\u73b0\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u63d0\u4f9b\u9002\u5f53\u7684\u4e0a\u4e0b\u6587\u3002","title":"\u5728\u5e73\u53f0\u4e0a\u4f7f\u7528kubeconfig\u6587\u4ef6"},{"location":"client/configure-kn/","text":"\u5b9a\u5236 kn \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a config.yaml \u914d\u7f6e\u6587\u4ef6\u6765\u5b9a\u5236\u60a8\u7684 kn \u547d\u4ee4\u884c\u8bbe\u7f6e\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u2014\u2014config \u6807\u5fd7\u6765\u63d0\u4f9b\u6b64\u914d\u7f6e\uff0c\u5426\u5219\u914d\u7f6e\u5c06\u4ece\u9ed8\u8ba4\u4f4d\u7f6e\u83b7\u53d6\u3002 \u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u7b26\u5408 XDG\u57fa\u672c\u76ee\u5f55\u89c4\u8303 \uff0c\u5bf9\u4e8eUnix\u7cfb\u7edf\u548cWindows\u7cfb\u7edf\u662f\u4e0d\u540c\u7684\u3002 If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn . \u793a\u4f8b\u914d\u7f6e\u6587\u4ef6 \u00b6 plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services \u54ea\u91cc path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option (default: true ). Note: the path-lookup option has been deprecated and will be removed in a future version where path lookup will be enabled unconditionally. directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described earlier. This can be any directory that is visible to the user (default: $base_dir/plugins , where $base_dir is the directory where this configuration file is stored). sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The lowercased, plural name of the Kubernetes resource type. For example, services or brokers .","title":"kn \u5b9a\u5236"},{"location":"client/configure-kn/#kn","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a config.yaml \u914d\u7f6e\u6587\u4ef6\u6765\u5b9a\u5236\u60a8\u7684 kn \u547d\u4ee4\u884c\u8bbe\u7f6e\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u2014\u2014config \u6807\u5fd7\u6765\u63d0\u4f9b\u6b64\u914d\u7f6e\uff0c\u5426\u5219\u914d\u7f6e\u5c06\u4ece\u9ed8\u8ba4\u4f4d\u7f6e\u83b7\u53d6\u3002 \u9ed8\u8ba4\u914d\u7f6e\u4f4d\u7f6e\u7b26\u5408 XDG\u57fa\u672c\u76ee\u5f55\u89c4\u8303 \uff0c\u5bf9\u4e8eUnix\u7cfb\u7edf\u548cWindows\u7cfb\u7edf\u662f\u4e0d\u540c\u7684\u3002 If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn .","title":"\u5b9a\u5236 kn"},{"location":"client/configure-kn/#_1","text":"plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services \u54ea\u91cc path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option (default: true ). Note: the path-lookup option has been deprecated and will be removed in a future version where path lookup will be enabled unconditionally. directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described earlier. This can be any directory that is visible to the user (default: $base_dir/plugins , where $base_dir is the directory where this configuration file is stored). sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The lowercased, plural name of the Kubernetes resource type. For example, services or brokers .","title":"\u793a\u4f8b\u914d\u7f6e\u6587\u4ef6"},{"location":"client/install-kn/","text":"\u5b89\u88c5 Knative CLI \u00b6 \u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5Knative kn \u547d\u4ee4\u884c\u3002 \u5b89\u88c5Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . \u4f7f\u7528\u591c\u95f4\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5b89\u88c5kn \u00b6 Warning \u6bcf\u665a\u5bb9\u5668\u6620\u50cf\u5305\u62ec\u53ef\u80fd\u4e0d\u5305\u542b\u5728\u6700\u65b0Knative\u7248\u672c\u4e2d\u7684\u7279\u6027\uff0c\u5e76\u4e14\u88ab\u8ba4\u4e3a\u662f\u4e0d\u7a33\u5b9a\u7684\u3002 \u60f3\u8981\u5b89\u88c5 kn \u6700\u65b0\u9884\u53d1\u5e03\u7248\u672c\u7684\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u5230\u6700\u65b0\u7684\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u94fe\u63a5\u5728\u8fd9\u91cc: macOS Linux Windows Tekton\u4f7f\u7528 kn \u00b6 \u53c2\u89c1 Tekton\u6587\u6863 .","title":"kn \u5b89\u88c5"},{"location":"client/install-kn/#knative-cli","text":"\u672c\u6307\u5357\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5Knative kn \u547d\u4ee4\u884c\u3002","title":"\u5b89\u88c5 Knative CLI"},{"location":"client/install-kn/#knative-cli_1","text":"Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn .","title":"\u5b89\u88c5Knative CLI"},{"location":"client/install-kn/#kn","text":"Warning \u6bcf\u665a\u5bb9\u5668\u6620\u50cf\u5305\u62ec\u53ef\u80fd\u4e0d\u5305\u542b\u5728\u6700\u65b0Knative\u7248\u672c\u4e2d\u7684\u7279\u6027\uff0c\u5e76\u4e14\u88ab\u8ba4\u4e3a\u662f\u4e0d\u7a33\u5b9a\u7684\u3002 \u60f3\u8981\u5b89\u88c5 kn \u6700\u65b0\u9884\u53d1\u5e03\u7248\u672c\u7684\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u5230\u6700\u65b0\u7684\u591c\u95f4\u6784\u5efa\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u94fe\u63a5\u5728\u8fd9\u91cc: macOS Linux Windows","title":"\u4f7f\u7528\u591c\u95f4\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5b89\u88c5kn"},{"location":"client/install-kn/#tekton-kn","text":"\u53c2\u89c1 Tekton\u6587\u6863 .","title":"Tekton\u4f7f\u7528 kn"},{"location":"client/kn-plugins/","text":"kn \u63d2\u4ef6 \u00b6 kn \u547d\u4ee4\u884c\u652f\u6301\u4f7f\u7528\u63d2\u4ef6\u3002 \u63d2\u4ef6\u5141\u8bb8\u60a8\u901a\u8fc7\u6dfb\u52a0\u81ea\u5b9a\u4e49\u547d\u4ee4\u548c\u5176\u4ed6\u4e0d\u5c5e\u4e8e kn \u6838\u5fc3\u53d1\u884c\u7248\u7684\u5171\u4eab\u547d\u4ee4\u6765\u6269\u5c55 kn \u5b89\u88c5\u7684\u529f\u80fd\u3002 Warning \u63d2\u4ef6\u5fc5\u987b\u4ee5\u524d\u7f00 kn- \u547d\u540d\uff0c\u4ee5\u4fbf\u7531 kn \u68c0\u6d4b\u3002 \u4f8b\u5982\uff0c kn-func \u4f1a\u88ab\u68c0\u6d4b\u5230\uff0c\u4f46 func \u4e0d\u4f1a\u88ab\u68c0\u6d4b\u5230\u3002 kn \u6e90\u7684\u63d2\u4ef6 \u00b6 \u4e8b\u4ef6\u6e90\u63d2\u4ef6\u5177\u6709\u4ee5\u4e0b\u7279\u5f81: \u5b83\u7684\u540d\u79f0\u5c5e\u4e8e kn source \u7ec4\u7684\u4e00\u90e8\u5206\u3002 \u5b83\u63d0\u4f9bCRUD\u5b50\u547d\u4ee4; create , update , delete , describe , \u6709\u65f6 apply . \u5f53\u4f7f\u7528 create \u547d\u4ee4\u65f6\uff0c\u5b83\u8981\u6c42\u4f20\u9012\u4e00\u4e2a\u5f3a\u5236\u7684 --sink \u6807\u5fd7\u3002 Knative\u63d2\u4ef6\u5217\u8868 \u00b6 \u60a8\u53ef\u4ee5\u5728 Knative Sandbox\u5e93 \u4e2d\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684 kn \u63d2\u4ef6. \u63d2\u4ef6 \u63cf\u8ff0 \u53ef\u4ee5\u901a\u8fc7Homebrew? kn-plugin-admin kn plugin \u7528\u4e8e\u7ba1\u7406\u57fa\u4e8eKubernetes\u7684Knative\u5b89\u88c5 Y kn-plugin-diag kn plugin \u7528\u4e8e\u901a\u8fc7\u516c\u5f00Knative\u5bf9\u8c61\u7684\u4e0d\u540c\u5c42\u7684\u8be6\u7ec6\u4fe1\u606f\u6765\u8bca\u65ad\u95ee\u9898 N kn-plugin-event kn plugin \u7528\u4e8e\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230Knative\u63a5\u6536\u5668 Y kn-plugin-func kn plugin \u7528\u6237\u51fd\u6570 Y kn-plugin-migration kn plugin \u7528\u4e8e\u5c06Knative\u670d\u52a1\u4ece\u4e00\u4e2a\u96c6\u7fa4\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u96c6\u7fa4 N kn-plugin-operator kn plugin \u4f7f\u7528Knative Operator\u7ba1\u7406Knative N kn-plugin-quickstart kn plugin \u4e3a\u5f00\u53d1\u4eba\u5458\u5b89\u88c5\u4e00\u4e2a\u5feb\u901f\u542f\u52a8\u7684Knative\u96c6\u7fa4\uff0c\u4ee5\u8fdb\u884c\u5b9e\u9a8c Y kn-plugin-service-log kn plugin \u7528\u4e8e\u663e\u793aKnative\u670d\u52a1\u7684\u6807\u51c6\u8f93\u51fa N kn-plugin-source-kafka kn plugin \u7528\u4e8e\u7ba1\u7406Kafka\u4e8b\u4ef6\u6e90 Y kn-plugin-source-kamelet kn plugin \u7528\u4e8e\u7ba1\u7406Kamelets\u548cKameletBindings Y \u624b\u52a8\u5b89\u88c5\u63d2\u4ef6 \u00b6 \u60a8\u53ef\u4ee5\u624b\u52a8\u5b89\u88c5\u6240\u6709\u63d2\u4ef6\u3002\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6: \u4eceGitHub\u4e0b\u8f7d\u63d2\u4ef6\u7684\u5f53\u524d\u7248\u672c\u3002\u4f60\u53ef\u4ee5\u4e0b\u8f7d Knative\u63d2\u4ef6\u5217\u8868 \u3002 \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-admin-darwin-amd64 \u91cd\u547d\u540d\u4e3a kn-admin \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-admin \u3002 \u5c06\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\u4e2d\u3002\u4f8b\u5982, /usr/local/bin \u3002 \u4f7f\u7528Homebrew\u5b89\u88c5\u63d2\u4ef6 \u00b6 \u53ef\u4ee5\u4f7f\u7528 Knative plugins Homebrew Tap \u5b89\u88c5\u4e00\u4e9b\u63d2\u4ef6\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c brew install knative-sandbox/kn-plugins/admin \u6765\u5b89\u88c5 kn-admin \u63d2\u4ef6\u3002 \u53ef\u7528\u63d2\u4ef6\u5217\u8868 \u00b6 \u4f60\u53ef\u4ee5\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u6240\u6709\u53ef\u7528\u7684(\u5df2\u5b89\u88c5\u7684)\u63d2\u4ef6: kn plugin list","title":"kn \u63d2\u4ef6"},{"location":"client/kn-plugins/#kn","text":"kn \u547d\u4ee4\u884c\u652f\u6301\u4f7f\u7528\u63d2\u4ef6\u3002 \u63d2\u4ef6\u5141\u8bb8\u60a8\u901a\u8fc7\u6dfb\u52a0\u81ea\u5b9a\u4e49\u547d\u4ee4\u548c\u5176\u4ed6\u4e0d\u5c5e\u4e8e kn \u6838\u5fc3\u53d1\u884c\u7248\u7684\u5171\u4eab\u547d\u4ee4\u6765\u6269\u5c55 kn \u5b89\u88c5\u7684\u529f\u80fd\u3002 Warning \u63d2\u4ef6\u5fc5\u987b\u4ee5\u524d\u7f00 kn- \u547d\u540d\uff0c\u4ee5\u4fbf\u7531 kn \u68c0\u6d4b\u3002 \u4f8b\u5982\uff0c kn-func \u4f1a\u88ab\u68c0\u6d4b\u5230\uff0c\u4f46 func \u4e0d\u4f1a\u88ab\u68c0\u6d4b\u5230\u3002","title":"kn \u63d2\u4ef6"},{"location":"client/kn-plugins/#kn_1","text":"\u4e8b\u4ef6\u6e90\u63d2\u4ef6\u5177\u6709\u4ee5\u4e0b\u7279\u5f81: \u5b83\u7684\u540d\u79f0\u5c5e\u4e8e kn source \u7ec4\u7684\u4e00\u90e8\u5206\u3002 \u5b83\u63d0\u4f9bCRUD\u5b50\u547d\u4ee4; create , update , delete , describe , \u6709\u65f6 apply . \u5f53\u4f7f\u7528 create \u547d\u4ee4\u65f6\uff0c\u5b83\u8981\u6c42\u4f20\u9012\u4e00\u4e2a\u5f3a\u5236\u7684 --sink \u6807\u5fd7\u3002","title":"kn \u6e90\u7684\u63d2\u4ef6"},{"location":"client/kn-plugins/#knative","text":"\u60a8\u53ef\u4ee5\u5728 Knative Sandbox\u5e93 \u4e2d\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684 kn \u63d2\u4ef6. \u63d2\u4ef6 \u63cf\u8ff0 \u53ef\u4ee5\u901a\u8fc7Homebrew? kn-plugin-admin kn plugin \u7528\u4e8e\u7ba1\u7406\u57fa\u4e8eKubernetes\u7684Knative\u5b89\u88c5 Y kn-plugin-diag kn plugin \u7528\u4e8e\u901a\u8fc7\u516c\u5f00Knative\u5bf9\u8c61\u7684\u4e0d\u540c\u5c42\u7684\u8be6\u7ec6\u4fe1\u606f\u6765\u8bca\u65ad\u95ee\u9898 N kn-plugin-event kn plugin \u7528\u4e8e\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230Knative\u63a5\u6536\u5668 Y kn-plugin-func kn plugin \u7528\u6237\u51fd\u6570 Y kn-plugin-migration kn plugin \u7528\u4e8e\u5c06Knative\u670d\u52a1\u4ece\u4e00\u4e2a\u96c6\u7fa4\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u96c6\u7fa4 N kn-plugin-operator kn plugin \u4f7f\u7528Knative Operator\u7ba1\u7406Knative N kn-plugin-quickstart kn plugin \u4e3a\u5f00\u53d1\u4eba\u5458\u5b89\u88c5\u4e00\u4e2a\u5feb\u901f\u542f\u52a8\u7684Knative\u96c6\u7fa4\uff0c\u4ee5\u8fdb\u884c\u5b9e\u9a8c Y kn-plugin-service-log kn plugin \u7528\u4e8e\u663e\u793aKnative\u670d\u52a1\u7684\u6807\u51c6\u8f93\u51fa N kn-plugin-source-kafka kn plugin \u7528\u4e8e\u7ba1\u7406Kafka\u4e8b\u4ef6\u6e90 Y kn-plugin-source-kamelet kn plugin \u7528\u4e8e\u7ba1\u7406Kamelets\u548cKameletBindings Y","title":"Knative\u63d2\u4ef6\u5217\u8868"},{"location":"client/kn-plugins/#_1","text":"\u60a8\u53ef\u4ee5\u624b\u52a8\u5b89\u88c5\u6240\u6709\u63d2\u4ef6\u3002\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6: \u4eceGitHub\u4e0b\u8f7d\u63d2\u4ef6\u7684\u5f53\u524d\u7248\u672c\u3002\u4f60\u53ef\u4ee5\u4e0b\u8f7d Knative\u63d2\u4ef6\u5217\u8868 \u3002 \u91cd\u547d\u540d\u6587\u4ef6\u4ee5\u5220\u9664\u64cd\u4f5c\u7cfb\u7edf\u548c\u4f53\u7cfb\u7ed3\u6784\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5c06 kn-admin-darwin-amd64 \u91cd\u547d\u540d\u4e3a kn-admin \u3002 \u4f7f\u63d2\u4ef6\u53ef\u6267\u884c\u3002\u4f8b\u5982\uff0c chmod +x kn-admin \u3002 \u5c06\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\u4e2d\u3002\u4f8b\u5982, /usr/local/bin \u3002","title":"\u624b\u52a8\u5b89\u88c5\u63d2\u4ef6"},{"location":"client/kn-plugins/#homebrew","text":"\u53ef\u4ee5\u4f7f\u7528 Knative plugins Homebrew Tap \u5b89\u88c5\u4e00\u4e9b\u63d2\u4ef6\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c brew install knative-sandbox/kn-plugins/admin \u6765\u5b89\u88c5 kn-admin \u63d2\u4ef6\u3002","title":"\u4f7f\u7528Homebrew\u5b89\u88c5\u63d2\u4ef6"},{"location":"client/kn-plugins/#_2","text":"\u4f60\u53ef\u4ee5\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5217\u51fa\u6240\u6709\u53ef\u7528\u7684(\u5df2\u5b89\u88c5\u7684)\u63d2\u4ef6: kn plugin list","title":"\u53ef\u7528\u63d2\u4ef6\u5217\u8868"},{"location":"concepts/","text":"\u6982\u5ff5 \u00b6 \u672c\u8282\u4e2d\u7684\u6587\u6863\u89e3\u91ca\u4e86\u5e38\u7528\u7684Knative\u6982\u5ff5\u548c\u62bd\u8c61\uff0c\u5e76\u5e2e\u52a9\u60a8\u66f4\u597d\u5730\u7406\u89e3Knative\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 \u4ec0\u4e48\u662f Knative? \u00b6 Knative\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u8fd0\u884c \u65e0\u670d\u52a1\u5668 \u90e8\u7f72\u3002 Knative \u670d\u52a1 \u00b6 Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(crd)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002 Knative \u4e8b\u4ef6 \u00b6 Knative\u4e8b\u4ef6\u662f\u4e00\u4e2aapi\u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9bapi\u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94HTTP\u8bf7\u6c42\u3002 Knative\u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684HTTP POST\u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents\u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative\u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002","title":"\u6982\u8ff0"},{"location":"concepts/#_1","text":"\u672c\u8282\u4e2d\u7684\u6587\u6863\u89e3\u91ca\u4e86\u5e38\u7528\u7684Knative\u6982\u5ff5\u548c\u62bd\u8c61\uff0c\u5e76\u5e2e\u52a9\u60a8\u66f4\u597d\u5730\u7406\u89e3Knative\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002","title":"\u6982\u5ff5"},{"location":"concepts/#knative","text":"Knative\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u8fd0\u884c \u65e0\u670d\u52a1\u5668 \u90e8\u7f72\u3002","title":"\u4ec0\u4e48\u662f Knative?"},{"location":"concepts/#knative_1","text":"Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(crd)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002","title":"Knative \u670d\u52a1"},{"location":"concepts/#knative_2","text":"Knative\u4e8b\u4ef6\u662f\u4e00\u4e2aapi\u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9bapi\u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94HTTP\u8bf7\u6c42\u3002 Knative\u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684HTTP POST\u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents\u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative\u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002","title":"Knative \u4e8b\u4ef6"},{"location":"concepts/duck-typing/","text":"Duck typing \u00b6 Knative\u901a\u8fc7\u4f7f\u7528 duck\u7c7b\u578b \u652f\u6301\u5176\u7ec4\u4ef6\u7684 \u677e\u6563\u8026\u5408 \u3002 Duck\u7c7b\u578b\u610f\u5473\u7740\u5728Knative\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u8d44\u6e90\u7684\u517c\u5bb9\u6027\u7531\u7528\u4e8e\u8bc6\u522b\u8d44\u6e90\u63a7\u5236\u5e73\u9762\u5f62\u72b6\u548c\u884c\u4e3a\u7684\u67d0\u4e9b\u5c5e\u6027\u51b3\u5b9a\u3002 \u8fd9\u4e9b\u5c5e\u6027\u57fa\u4e8e\u4e00\u7ec4\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u8d44\u6e90\u7684\u516c\u5171\u5b9a\u4e49\uff0c\u79f0\u4e3aduck\u7c7b\u578b\u3002 Knative\u53ef\u4ee5\u50cf\u4f7f\u7528\u6cdb\u578b\u9e2d\u7c7b\u578b\u4e00\u6837\u4f7f\u7528\u8d44\u6e90\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u8d44\u6e90\u7c7b\u578b\u6709\u5177\u4f53\u7684\u4e86\u89e3\uff0c\u5982\u679c: \u8d44\u6e90\u5728\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u6a21\u5f0f\u4f4d\u7f6e\u4e2d\u5177\u6709\u76f8\u540c\u7684\u5b57\u6bb5 \u4e0e\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u7684\u63a7\u4ef6\u6216\u6570\u636e\u5e73\u9762\u884c\u4e3a \u6709\u4e9b\u8d44\u6e90\u53ef\u4ee5\u9009\u62e9\u52a0\u5165\u591a\u79cd\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u4e2dduck\u7c7b\u578b\u7684\u4e00\u4e2a\u57fa\u672c\u7528\u9014\u662f\u5728\u8d44\u6e90 \u89c4\u8303 \u4e2d\u4f7f\u7528\u5bf9\u8c61\u5f15\u7528\u6765\u6307\u5411\u5176\u4ed6\u8d44\u6e90\u3002 \u5305\u542b\u5f15\u7528\u7684\u5bf9\u8c61\u7684\u5b9a\u4e49\u89c4\u5b9a\u4e86\u88ab\u5f15\u7528\u8d44\u6e90\u7684\u9884\u671fduck\u7c7b\u578b\u3002 \u4f8b\u5b50 \u00b6 \u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e00\u4e2a\u540d\u4e3a pointer \u7684Knative \u793a\u4f8b \u8d44\u6e90\u5728\u5176\u89c4\u8303\u4e2d\u5f15\u7528\u4e86\u4e00\u4e2a\u540d\u4e3a pointee \u7684 Dog \u8d44\u6e90: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee \u5982\u679c\u4e00\u4e2a\u53ef\u89c2\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u671f\u671b\u5f62\u72b6\u662f\uff0c\u5728 status \u4e2d\uff0c\u6a21\u5f0f\u5f62\u72b6\u5982\u4e0b: status : height : <in centimetres> weight : <in kilograms> \u73b0\u5728 pointee \u7684\u5b9e\u4f8b\u770b\u8d77\u6765\u50cf\u8fd9\u6837: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 \u5f53 \u793a\u4f8b \u8d44\u6e90\u8d77\u4f5c\u7528\u65f6\uff0c\u5b83\u53ea\u4f5c\u7528\u4e8e\u5927\u5c3a\u5bf8\u9e2d\u5b50\u7c7b\u578b\u5f62\u72b6\u7684\u4fe1\u606f\uff0c\u800c Dog \u5b9e\u73b0\u53ef\u4ee5\u81ea\u7531\u5730\u62e5\u6709\u5bf9\u8be5\u8d44\u6e90\u6700\u6709\u610f\u4e49\u7684\u4fe1\u606f\u3002 \u5f53\u6211\u4eec\u7528\u4e00\u79cd\u65b0\u7c7b\u578b\u6269\u5c55\u7cfb\u7edf\u65f6\uff0cduck\u7c7b\u578b\u7684\u5a01\u529b\u662f\u663e\u800c\u6613\u89c1\u7684\uff0c\u4f8b\u5982\uff0c Human , \u5982\u679c\u65b0\u8d44\u6e90\u7b26\u5408\u5927\u516c\u53f8\u8bbe\u5b9a\u7684\u5408\u540c\u3002 apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 \u793a\u4f8b \u8d44\u6e90\u80fd\u591f\u5e94\u7528\u4e3a\u5176\u914d\u7f6e\u7684\u903b\u8f91\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u5730\u4e86\u89e3 Human \u6216 Dog \u3002 Knative Duck Types \u00b6 Knative\u5b9a\u4e49\u4e86\u51e0\u4e2a\u5728\u6574\u4e2a\u9879\u76ee\u4e2d\u4f7f\u7528\u7684duck\u7c7b\u578b\u7684\u5951\u7ea6: Duck typing \u4f8b\u5b50 Knative Duck Types Addressable Binding Source Addressable \u00b6 Addressable is expected to be the following shape: apiVersion : group/version kind : Kind status : address : url : http://host/path?query Binding \u00b6 With a direct subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name With an indirect subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value Source \u00b6 With a ref Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host With a uri Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query With ref and uri Sinks, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"Duck types"},{"location":"concepts/duck-typing/#duck-typing","text":"Knative\u901a\u8fc7\u4f7f\u7528 duck\u7c7b\u578b \u652f\u6301\u5176\u7ec4\u4ef6\u7684 \u677e\u6563\u8026\u5408 \u3002 Duck\u7c7b\u578b\u610f\u5473\u7740\u5728Knative\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u8d44\u6e90\u7684\u517c\u5bb9\u6027\u7531\u7528\u4e8e\u8bc6\u522b\u8d44\u6e90\u63a7\u5236\u5e73\u9762\u5f62\u72b6\u548c\u884c\u4e3a\u7684\u67d0\u4e9b\u5c5e\u6027\u51b3\u5b9a\u3002 \u8fd9\u4e9b\u5c5e\u6027\u57fa\u4e8e\u4e00\u7ec4\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u8d44\u6e90\u7684\u516c\u5171\u5b9a\u4e49\uff0c\u79f0\u4e3aduck\u7c7b\u578b\u3002 Knative\u53ef\u4ee5\u50cf\u4f7f\u7528\u6cdb\u578b\u9e2d\u7c7b\u578b\u4e00\u6837\u4f7f\u7528\u8d44\u6e90\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u8d44\u6e90\u7c7b\u578b\u6709\u5177\u4f53\u7684\u4e86\u89e3\uff0c\u5982\u679c: \u8d44\u6e90\u5728\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u6a21\u5f0f\u4f4d\u7f6e\u4e2d\u5177\u6709\u76f8\u540c\u7684\u5b57\u6bb5 \u4e0e\u516c\u5171\u5b9a\u4e49\u6307\u5b9a\u7684\u76f8\u540c\u7684\u63a7\u4ef6\u6216\u6570\u636e\u5e73\u9762\u884c\u4e3a \u6709\u4e9b\u8d44\u6e90\u53ef\u4ee5\u9009\u62e9\u52a0\u5165\u591a\u79cd\u9e2d\u5b50\u7c7b\u578b\u3002 Knative\u4e2dduck\u7c7b\u578b\u7684\u4e00\u4e2a\u57fa\u672c\u7528\u9014\u662f\u5728\u8d44\u6e90 \u89c4\u8303 \u4e2d\u4f7f\u7528\u5bf9\u8c61\u5f15\u7528\u6765\u6307\u5411\u5176\u4ed6\u8d44\u6e90\u3002 \u5305\u542b\u5f15\u7528\u7684\u5bf9\u8c61\u7684\u5b9a\u4e49\u89c4\u5b9a\u4e86\u88ab\u5f15\u7528\u8d44\u6e90\u7684\u9884\u671fduck\u7c7b\u578b\u3002","title":"Duck typing"},{"location":"concepts/duck-typing/#_1","text":"\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e00\u4e2a\u540d\u4e3a pointer \u7684Knative \u793a\u4f8b \u8d44\u6e90\u5728\u5176\u89c4\u8303\u4e2d\u5f15\u7528\u4e86\u4e00\u4e2a\u540d\u4e3a pointee \u7684 Dog \u8d44\u6e90: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee \u5982\u679c\u4e00\u4e2a\u53ef\u89c2\u7684\u9e2d\u5b50\u7c7b\u578b\u7684\u671f\u671b\u5f62\u72b6\u662f\uff0c\u5728 status \u4e2d\uff0c\u6a21\u5f0f\u5f62\u72b6\u5982\u4e0b: status : height : <in centimetres> weight : <in kilograms> \u73b0\u5728 pointee \u7684\u5b9e\u4f8b\u770b\u8d77\u6765\u50cf\u8fd9\u6837: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 \u5f53 \u793a\u4f8b \u8d44\u6e90\u8d77\u4f5c\u7528\u65f6\uff0c\u5b83\u53ea\u4f5c\u7528\u4e8e\u5927\u5c3a\u5bf8\u9e2d\u5b50\u7c7b\u578b\u5f62\u72b6\u7684\u4fe1\u606f\uff0c\u800c Dog \u5b9e\u73b0\u53ef\u4ee5\u81ea\u7531\u5730\u62e5\u6709\u5bf9\u8be5\u8d44\u6e90\u6700\u6709\u610f\u4e49\u7684\u4fe1\u606f\u3002 \u5f53\u6211\u4eec\u7528\u4e00\u79cd\u65b0\u7c7b\u578b\u6269\u5c55\u7cfb\u7edf\u65f6\uff0cduck\u7c7b\u578b\u7684\u5a01\u529b\u662f\u663e\u800c\u6613\u89c1\u7684\uff0c\u4f8b\u5982\uff0c Human , \u5982\u679c\u65b0\u8d44\u6e90\u7b26\u5408\u5927\u516c\u53f8\u8bbe\u5b9a\u7684\u5408\u540c\u3002 apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 \u793a\u4f8b \u8d44\u6e90\u80fd\u591f\u5e94\u7528\u4e3a\u5176\u914d\u7f6e\u7684\u903b\u8f91\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u5730\u4e86\u89e3 Human \u6216 Dog \u3002","title":"\u4f8b\u5b50"},{"location":"concepts/duck-typing/#knative-duck-types","text":"Knative\u5b9a\u4e49\u4e86\u51e0\u4e2a\u5728\u6574\u4e2a\u9879\u76ee\u4e2d\u4f7f\u7528\u7684duck\u7c7b\u578b\u7684\u5951\u7ea6: Duck typing \u4f8b\u5b50 Knative Duck Types Addressable Binding Source","title":"Knative Duck Types"},{"location":"concepts/duck-typing/#addressable","text":"Addressable is expected to be the following shape: apiVersion : group/version kind : Kind status : address : url : http://host/path?query","title":"Addressable"},{"location":"concepts/duck-typing/#binding","text":"With a direct subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name With an indirect subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value","title":"Binding"},{"location":"concepts/duck-typing/#source","text":"With a ref Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host With a uri Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query With ref and uri Sinks, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"Source"},{"location":"concepts/eventing-resources/brokers/","text":"\u4ee3\u7406 \u00b6 \u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002 \u76f8\u5173\u7684\u6982\u5ff5 \u00b6 \u89e6\u53d1\u5668 \u00b6 \u4e8b\u4ef6\u8fdb\u5165\u4ee3\u7406\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u5c06\u5176\u8f6c\u53d1\u7ed9\u8ba2\u9605\u8005\u3002 \u89e6\u53d1\u5668\u5141\u8bb8\u901a\u8fc7\u5c5e\u6027\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u8fd9\u6837\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u4e8b\u4ef6\u5c31\u53ef\u4ee5\u53d1\u9001\u5230\u6ce8\u518c\u4e86\u5bf9\u5177\u6709\u8fd9\u4e9b\u5c5e\u6027\u7684\u4e8b\u4ef6\u611f\u5174\u8da3\u7684\u8ba2\u9605\u670d\u52a1\u5668\u3002 \u8ba2\u9605\u8005 \u00b6 \u8ba2\u9605\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u4efb\u4f55URL\u6216\u53ef\u5bfb\u5740\u8d44\u6e90\u3002 \u8ba2\u9605\u8005\u8fd8\u53ef\u4ee5\u54cd\u5e94\u6765\u81ea\u4ee3\u7406\u7684\u6d3b\u52a8\u8bf7\u6c42\uff0c\u5e76\u4f7f\u7528\u53d1\u9001\u56de\u4ee3\u7406\u7684\u65b0\u4e8b\u4ef6\u8fdb\u884c\u54cd\u5e94\u3002","title":"\u4ee3\u7406"},{"location":"concepts/eventing-resources/brokers/#_1","text":"\u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002","title":"\u4ee3\u7406"},{"location":"concepts/eventing-resources/brokers/#_2","text":"","title":"\u76f8\u5173\u7684\u6982\u5ff5"},{"location":"concepts/eventing-resources/brokers/#_3","text":"\u4e8b\u4ef6\u8fdb\u5165\u4ee3\u7406\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u5c06\u5176\u8f6c\u53d1\u7ed9\u8ba2\u9605\u8005\u3002 \u89e6\u53d1\u5668\u5141\u8bb8\u901a\u8fc7\u5c5e\u6027\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u8fc7\u6ee4\uff0c\u8fd9\u6837\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u7684\u4e8b\u4ef6\u5c31\u53ef\u4ee5\u53d1\u9001\u5230\u6ce8\u518c\u4e86\u5bf9\u5177\u6709\u8fd9\u4e9b\u5c5e\u6027\u7684\u4e8b\u4ef6\u611f\u5174\u8da3\u7684\u8ba2\u9605\u670d\u52a1\u5668\u3002","title":"\u89e6\u53d1\u5668"},{"location":"concepts/eventing-resources/brokers/#_4","text":"\u8ba2\u9605\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u4efb\u4f55URL\u6216\u53ef\u5bfb\u5740\u8d44\u6e90\u3002 \u8ba2\u9605\u8005\u8fd8\u53ef\u4ee5\u54cd\u5e94\u6765\u81ea\u4ee3\u7406\u7684\u6d3b\u52a8\u8bf7\u6c42\uff0c\u5e76\u4f7f\u7528\u53d1\u9001\u56de\u4ee3\u7406\u7684\u65b0\u4e8b\u4ef6\u8fdb\u884c\u54cd\u5e94\u3002","title":"\u8ba2\u9605\u8005"},{"location":"concepts/serving-resources/revisions/","text":"\u4fee\u8ba2 \u00b6 \u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002 \u76f8\u5173\u7684\u6982\u5ff5 \u00b6 \u81ea\u52a8\u7f29\u653e \u00b6 \u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u653e\u5927\u6216\u7f29\u5c0f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u81ea\u52a8\u7f29\u653e \u3002 \u9010\u6b65\u63a8\u51fa\u4fee\u8ba2\u6d41\u91cf \u00b6 \u4fee\u8ba2\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u66f4\u6539\u7684\u9010\u6b65\u8f6c\u51fa\u548c\u56de\u6eda\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7248\u672c\u7684\u914d\u7f6e\u9009\u9879 . \u4fee\u8ba2\u7684\u914d\u7f6e\u9009\u9879 \u00b6 \u96c6\u7fa4\u7ba1\u7406\u5458(\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684)\u914d\u7f6e\u9009\u9879 \u5f00\u53d1\u4eba\u5458(\u6bcf\u4fee\u8ba2)\u914d\u7f6e\u9009\u9879 \u989d\u5916\u8d44\u6e90 \u00b6 \u4fee\u8ba2API\u89c4\u8303 \u4e0b\u4e00\u6b65 \u00b6 \u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u5220\u9664\u3001\u63cf\u8ff0\u548c\u5217\u51fa\u4fee\u8ba2 \u68c0\u67e5\u4fee\u8ba2\u7684\u72b6\u6001 \u5728\u670d\u52a1\u7684\u4fee\u8ba2\u4e4b\u95f4\u8def\u7531\u901a\u4fe1","title":"\u4fee\u6b63"},{"location":"concepts/serving-resources/revisions/#_1","text":"\u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002","title":"\u4fee\u8ba2"},{"location":"concepts/serving-resources/revisions/#_2","text":"","title":"\u76f8\u5173\u7684\u6982\u5ff5"},{"location":"concepts/serving-resources/revisions/#_3","text":"\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u653e\u5927\u6216\u7f29\u5c0f\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u81ea\u52a8\u7f29\u653e \u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"concepts/serving-resources/revisions/#_4","text":"\u4fee\u8ba2\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u66f4\u6539\u7684\u9010\u6b65\u8f6c\u51fa\u548c\u56de\u6eda\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u3002","title":"\u9010\u6b65\u63a8\u51fa\u4fee\u8ba2\u6d41\u91cf"},{"location":"concepts/serving-resources/revisions/#_5","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7248\u672c\u7684\u914d\u7f6e\u9009\u9879 .","title":"\u5783\u573e\u6536\u96c6"},{"location":"concepts/serving-resources/revisions/#_6","text":"\u96c6\u7fa4\u7ba1\u7406\u5458(\u5168\u5c40\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684)\u914d\u7f6e\u9009\u9879 \u5f00\u53d1\u4eba\u5458(\u6bcf\u4fee\u8ba2)\u914d\u7f6e\u9009\u9879","title":"\u4fee\u8ba2\u7684\u914d\u7f6e\u9009\u9879"},{"location":"concepts/serving-resources/revisions/#_7","text":"\u4fee\u8ba2API\u89c4\u8303","title":"\u989d\u5916\u8d44\u6e90"},{"location":"concepts/serving-resources/revisions/#_8","text":"\u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u5220\u9664\u3001\u63cf\u8ff0\u548c\u5217\u51fa\u4fee\u8ba2 \u68c0\u67e5\u4fee\u8ba2\u7684\u72b6\u6001 \u5728\u670d\u52a1\u7684\u4fee\u8ba2\u4e4b\u95f4\u8def\u7531\u901a\u4fe1","title":"\u4e0b\u4e00\u6b65"},{"location":"contributing/","text":"Welcome to the Knative community \u00b6 Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"\u6b22\u8fce\u6765\u5230Knative\u793e\u533a"},{"location":"contributing/#welcome-to-the-knative-community","text":"Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"Welcome to the Knative community"},{"location":"contributing/about/","text":"About the community \u00b6 This page provides links to documents for common Knative community practices and a description of Knative's audience. Community values \u00b6 This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team. Governance \u00b6 This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports. Processes \u00b6 This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain. Community calendar \u00b6 The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace. Knative's audience \u00b6 Knative is designed for different personas: Developers \u00b6 Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group. Operators \u00b6 Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers. Contributors \u00b6 With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components: Knative authors \u00b6 Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"\u5173\u4e8e\u793e\u533a"},{"location":"contributing/about/#about-the-community","text":"This page provides links to documents for common Knative community practices and a description of Knative's audience.","title":"About the community"},{"location":"contributing/about/#community-values","text":"This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team.","title":"Community values"},{"location":"contributing/about/#governance","text":"This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports.","title":"Governance"},{"location":"contributing/about/#processes","text":"This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain.","title":"Processes"},{"location":"contributing/about/#community-calendar","text":"The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace.","title":"Community calendar"},{"location":"contributing/about/#knatives-audience","text":"Knative is designed for different personas:","title":"Knative's audience"},{"location":"contributing/about/#developers","text":"Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group.","title":"Developers"},{"location":"contributing/about/#operators","text":"Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers.","title":"Operators"},{"location":"contributing/about/#contributors","text":"With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components:","title":"Contributors"},{"location":"contributing/about/#knative-authors","text":"Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"Knative authors"},{"location":"contributing/contributing/","text":"Contribute to Knative \u00b6 This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved. Prerequisites \u00b6 If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must sign the CNCF EasyCLA using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community . Contribute to the code \u00b6 Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: To get started with contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: To get started with contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): To get started with contributing, see the Client development workflow . For good starter issues, see Client issues . Functions: To get started with contributing, see the Functions development workflow . For good starter issues, see Functions issues . Documentation: To get started with contributing, see the Docs contributor guide . For good starter issues, see Documentation issues . Contribute code samples to the community \u00b6 Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site. Learn and connect \u00b6 Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers. Community Meetup \u00b6 This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter . More ways to get involved \u00b6 Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"\u8d21\u732e Knative"},{"location":"contributing/contributing/#contribute-to-knative","text":"This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved.","title":"Contribute to Knative"},{"location":"contributing/contributing/#prerequisites","text":"If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must sign the CNCF EasyCLA using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community .","title":"Prerequisites"},{"location":"contributing/contributing/#contribute-to-the-code","text":"Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: To get started with contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: To get started with contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): To get started with contributing, see the Client development workflow . For good starter issues, see Client issues . Functions: To get started with contributing, see the Functions development workflow . For good starter issues, see Functions issues . Documentation: To get started with contributing, see the Docs contributor guide . For good starter issues, see Documentation issues .","title":"Contribute to the code"},{"location":"contributing/contributing/#contribute-code-samples-to-the-community","text":"Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site.","title":"Contribute code samples to the community"},{"location":"contributing/contributing/#learn-and-connect","text":"Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers.","title":"Learn and connect"},{"location":"contributing/contributing/#community-meetup","text":"This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter .","title":"Community Meetup"},{"location":"contributing/contributing/#more-ways-to-get-involved","text":"Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"More ways to get involved"},{"location":"eventing/","text":"Knative \u4e8b\u4ef6 \u00b6 Knative\u4e8b\u4ef6\u662f\u4e00\u4e2aapi\u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9bapi\u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94HTTP\u8bf7\u6c42\u3002 Knative\u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684HTTP POST\u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents\u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative\u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002 \u652f\u6301\u7684Knative\u4e8b\u4ef6\u7528\u4f8b\u793a\u4f8b: \u5728\u4e0d\u521b\u5efa\u6d88\u8d39\u8005\u7684\u60c5\u51b5\u4e0b\u53d1\u5e03\u4e8b\u4ef6\u3002\u60a8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u4f5c\u4e3aHTTP POST\u53d1\u9001\u5230\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u7ed1\u5b9a\u5c06\u76ee\u6807\u914d\u7f6e\u4e0e\u751f\u6210\u4e8b\u4ef6\u7684\u5e94\u7528\u7a0b\u5e8f\u5206\u79bb\u5f00\u6765\u3002 \u5728\u4e0d\u521b\u5efa\u53d1\u5e03\u8005\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u4e8b\u4ef6\u3002\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u6839\u636e\u4e8b\u4ef6\u5c5e\u6027\u4f7f\u7528\u6765\u81ea\u4ee3\u7406\u7684\u4e8b\u4ef6\u3002\u5e94\u7528\u7a0b\u5e8f\u4ee5HTTP POST\u7684\u5f62\u5f0f\u63a5\u6536\u4e8b\u4ef6\u3002 Tip \u591a\u4e2a\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u53ef\u4ee5\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u521b\u5efa\u66f4\u9ad8\u7ea7\u7684 Knative\u4e8b\u4ef6 \u6d41\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7528\u4f8b\u3002 \u4e8b\u4ef6\u793a\u4f8b \u00b6 \u521b\u5efa\u5e76\u54cd\u5e94Kubernetes API\u4e8b\u4ef6 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u521b\u5efa\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u7ba1\u9053 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u5728\u5927\u89c4\u6a21\u3001\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u519c\u4e1a\u9879\u76ee\u4e2d\u4fc3\u8fdbAI\u5de5\u4f5c \u4e0b\u4e00\u6b65 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u4e8b\u4ef6\u5904\u7406.","title":"\u4e8b\u4ef6\u6982\u8ff0"},{"location":"eventing/#knative","text":"Knative\u4e8b\u4ef6\u662f\u4e00\u4e2aapi\u96c6\u5408\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u9a71\u52a8\u7684\u4f53\u7cfb\u7ed3\u6784 \u3002 \u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9bapi\u521b\u5efa\u5c06\u4e8b\u4ef6\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u8def\u7531\u5230\u4e8b\u4ef6\u6d88\u8d39\u8005(\u79f0\u4e3a\u63a5\u6536\u4e8b\u4ef6\u7684\u63a5\u6536\u5668)\u7684\u7ec4\u4ef6\u3002 \u8fd8\u53ef\u4ee5\u5c06\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u901a\u8fc7\u53d1\u9001\u54cd\u5e94\u4e8b\u4ef6\u6765\u54cd\u5e94HTTP\u8bf7\u6c42\u3002 Knative\u4e8b\u4ef6\u4f7f\u7528\u6807\u51c6\u7684HTTP POST\u8bf7\u6c42\u5728\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u8fd9\u4e9b\u4e8b\u4ef6\u7b26\u5408 CloudEvents\u89c4\u8303 \uff0c\u8be5\u89c4\u8303\u652f\u6301\u5728\u4efb\u4f55\u7f16\u7a0b\u8bed\u8a00\u4e2d\u521b\u5efa\u3001\u89e3\u6790\u3001\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 Knative\u4e8b\u4ef6\u7ec4\u4ef6\u662f\u677e\u6563\u8026\u5408\u7684\uff0c\u53ef\u4ee5\u5f7c\u6b64\u72ec\u7acb\u5730\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u4efb\u4f55\u751f\u4ea7\u8005\u90fd\u53ef\u4ee5\u5728\u6709\u6d3b\u52a8\u4e8b\u4ef6\u6d88\u8d39\u8005\u76d1\u542c\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\u751f\u6210\u4e8b\u4ef6\u3002 \u5728\u751f\u4ea7\u8005\u521b\u5efa\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u524d\uff0c\u4efb\u4f55\u4e8b\u4ef6\u6d88\u8d39\u8005\u90fd\u53ef\u4ee5\u8868\u8fbe\u5bf9\u4e00\u7c7b\u4e8b\u4ef6\u7684\u5174\u8da3\u3002 \u652f\u6301\u7684Knative\u4e8b\u4ef6\u7528\u4f8b\u793a\u4f8b: \u5728\u4e0d\u521b\u5efa\u6d88\u8d39\u8005\u7684\u60c5\u51b5\u4e0b\u53d1\u5e03\u4e8b\u4ef6\u3002\u60a8\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u4f5c\u4e3aHTTP POST\u53d1\u9001\u5230\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u7ed1\u5b9a\u5c06\u76ee\u6807\u914d\u7f6e\u4e0e\u751f\u6210\u4e8b\u4ef6\u7684\u5e94\u7528\u7a0b\u5e8f\u5206\u79bb\u5f00\u6765\u3002 \u5728\u4e0d\u521b\u5efa\u53d1\u5e03\u8005\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u4e8b\u4ef6\u3002\u53ef\u4ee5\u4f7f\u7528\u89e6\u53d1\u5668\u6839\u636e\u4e8b\u4ef6\u5c5e\u6027\u4f7f\u7528\u6765\u81ea\u4ee3\u7406\u7684\u4e8b\u4ef6\u3002\u5e94\u7528\u7a0b\u5e8f\u4ee5HTTP POST\u7684\u5f62\u5f0f\u63a5\u6536\u4e8b\u4ef6\u3002 Tip \u591a\u4e2a\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u63a5\u6536\u5668\u53ef\u4ee5\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u521b\u5efa\u66f4\u9ad8\u7ea7\u7684 Knative\u4e8b\u4ef6 \u6d41\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7528\u4f8b\u3002","title":"Knative \u4e8b\u4ef6"},{"location":"eventing/#_1","text":"\u521b\u5efa\u5e76\u54cd\u5e94Kubernetes API\u4e8b\u4ef6 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u521b\u5efa\u4e00\u4e2a\u56fe\u50cf\u5904\u7406\u7ba1\u9053 image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} \u5728\u5927\u89c4\u6a21\u3001\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u519c\u4e1a\u9879\u76ee\u4e2d\u4fc3\u8fdbAI\u5de5\u4f5c","title":"\u4e8b\u4ef6\u793a\u4f8b"},{"location":"eventing/#_2","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u4e8b\u4ef6\u5904\u7406.","title":"\u4e0b\u4e00\u6b65"},{"location":"eventing/accessing-traces/","text":"Accessing CloudEvent traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests. Before you begin \u00b6 You must have a Knative cluster running with the Eventing component installed. Learn more . Configuring tracing \u00b6 With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\" Configuration options \u00b6 You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server. Viewing your config-tracing ConfigMap \u00b6 To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml Editing and deploying your config-tracing ConfigMap \u00b6 To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing Accessing traces in Eventing \u00b6 To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger Example \u00b6 The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"\u8bbf\u95eeCloudEvent\u75d5\u8ff9"},{"location":"eventing/accessing-traces/#accessing-cloudevent-traces","text":"Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#before-you-begin","text":"You must have a Knative cluster running with the Eventing component installed. Learn more .","title":"Before you begin"},{"location":"eventing/accessing-traces/#configuring-tracing","text":"With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\"","title":"Configuring tracing"},{"location":"eventing/accessing-traces/#configuration-options","text":"You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server.","title":"Configuration options"},{"location":"eventing/accessing-traces/#viewing-your-config-tracing-configmap","text":"To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml","title":"Viewing your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#editing-and-deploying-your-config-tracing-configmap","text":"To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing","title":"Editing and deploying your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#accessing-traces-in-eventing","text":"To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger","title":"Accessing traces in Eventing"},{"location":"eventing/accessing-traces/#example","text":"The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Example"},{"location":"eventing/event-delivery/","text":"Handling Delivery Failure \u00b6 You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered Configuring Subscription event delivery \u00b6 You can configure how events are delivered for each Subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative Service or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. Configuring Broker event delivery \u00b6 You can configure how events are delivered for each Broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1. Broker support \u00b6 The following table summarizes which delivery parameters are supported for each Broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying Channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink must be a GCP Pub/Sub topic URI. googlecloud Broker only supports the exponential back off policy. Configuring Channel event delivery \u00b6 Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrordest Type: String Description: The original destination URL to which the failed event was sent. This could be either a delivery or reply URL based on which operation encountered the failed event. Constraints: Always present because every HTTP Request has a destination URL. Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Always present because every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Empty if the HTTP Response Body is empty, and may be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body... Channel support \u00b6 The following table summarizes which delivery parameters are supported for each Channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"\u5904\u7406\u4ea4\u8d27\u5931\u8d25"},{"location":"eventing/event-delivery/#handling-delivery-failure","text":"You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered","title":"Handling Delivery Failure"},{"location":"eventing/event-delivery/#configuring-subscription-event-delivery","text":"You can configure how events are delivered for each Subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative Service or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink.","title":"Configuring Subscription event delivery"},{"location":"eventing/event-delivery/#configuring-broker-event-delivery","text":"You can configure how events are delivered for each Broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1.","title":"Configuring Broker event delivery"},{"location":"eventing/event-delivery/#broker-support","text":"The following table summarizes which delivery parameters are supported for each Broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying Channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink must be a GCP Pub/Sub topic URI. googlecloud Broker only supports the exponential back off policy.","title":"Broker support"},{"location":"eventing/event-delivery/#configuring-channel-event-delivery","text":"Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrordest Type: String Description: The original destination URL to which the failed event was sent. This could be either a delivery or reply URL based on which operation encountered the failed event. Constraints: Always present because every HTTP Request has a destination URL. Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Always present because every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Empty if the HTTP Response Body is empty, and may be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body...","title":"Configuring Channel event delivery"},{"location":"eventing/event-delivery/#channel-support","text":"The following table summarizes which delivery parameters are supported for each Channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Channel support"},{"location":"eventing/event-registry/","text":"Event registry \u00b6 Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The event registry maintains a catalog of event types that each Broker can consume. The event types stored in the registry contain all required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. This topic provides information about how you can populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest. Note Before using the event registry, it is recommended that you have a basic understanding of Brokers, Triggers, Event Sources, and the CloudEvents spec (particularly the Context Attributes section). About EventType objects \u00b6 EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional Populate the registry with events \u00b6 You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources. Manual registration \u00b6 For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml> Automatic registration \u00b6 Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated. Support for automatic registration \u00b6 Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink. Procedure for automatic registration \u00b6 To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry. Example: Automatic registration using KafkaSource \u00b6 Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics. Discover events using the registry \u00b6 Using the registry, you can discover the different types of events that Broker event meshes can consume. View all event types you can subscribe to \u00b6 To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink. View the YAML for an EventType object \u00b6 To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page. About subscribing to events \u00b6 After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready. Next steps \u00b6 Knative code samples is a useful resource to better understand some of the event sources. Remember, you must point the sources to a Broker if you want automatic registration of EventTypes in the registry.","title":"\u4e8b\u4ef6\u6ce8\u518c\u8868"},{"location":"eventing/event-registry/#event-registry","text":"Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The event registry maintains a catalog of event types that each Broker can consume. The event types stored in the registry contain all required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. This topic provides information about how you can populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest. Note Before using the event registry, it is recommended that you have a basic understanding of Brokers, Triggers, Event Sources, and the CloudEvents spec (particularly the Context Attributes section).","title":"Event registry"},{"location":"eventing/event-registry/#about-eventtype-objects","text":"EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional","title":"About EventType objects"},{"location":"eventing/event-registry/#populate-the-registry-with-events","text":"You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources.","title":"Populate the registry with events"},{"location":"eventing/event-registry/#manual-registration","text":"For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml>","title":"Manual registration"},{"location":"eventing/event-registry/#automatic-registration","text":"Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated.","title":"Automatic registration"},{"location":"eventing/event-registry/#support-for-automatic-registration","text":"Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink.","title":"Support for automatic registration"},{"location":"eventing/event-registry/#procedure-for-automatic-registration","text":"To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry.","title":"Procedure for automatic registration"},{"location":"eventing/event-registry/#example-automatic-registration-using-kafkasource","text":"Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics.","title":"Example: Automatic registration using KafkaSource"},{"location":"eventing/event-registry/#discover-events-using-the-registry","text":"Using the registry, you can discover the different types of events that Broker event meshes can consume.","title":"Discover events using the registry"},{"location":"eventing/event-registry/#view-all-event-types-you-can-subscribe-to","text":"To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink.","title":"View all event types you can subscribe to"},{"location":"eventing/event-registry/#view-the-yaml-for-an-eventtype-object","text":"To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page.","title":"View the YAML for an EventType object"},{"location":"eventing/event-registry/#about-subscribing-to-events","text":"After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready.","title":"About subscribing to events"},{"location":"eventing/event-registry/#next-steps","text":"Knative code samples is a useful resource to better understand some of the event sources. Remember, you must point the sources to a Broker if you want automatic registration of EventTypes in the registry.","title":"Next steps"},{"location":"eventing/brokers/","text":"\u5173\u4e8e\u4ee3\u7406\u5668 \u00b6 \u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002 \u4e8b\u4ef6\u4ea4\u4ed8 \u00b6 \u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u662f\u4f9d\u8d56\u4e8e\u914d\u7f6e\u7684\u4ee3\u7406\u7c7b\u7684\u5b9e\u73b0\u7ec6\u8282\u3002 \u4f7f\u7528\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u53ef\u4ee5\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u4e8b\u4ef6\u4f7f\u7528\u8005\u62bd\u8c61\u4e8b\u4ef6\u8def\u7531\u7684\u7ec6\u8282\u3002 \u9ad8\u7ea7\u7528\u4f8b \u00b6 \u5bf9\u4e8e\u5927\u591a\u6570\u7528\u4f8b\u6765\u8bf4\uff0c\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u4e00\u4e2a\u4ee3\u7406\u5c31\u8db3\u591f\u4e86\uff0c\u4f46\u662f\u5728\u4e00\u4e9b\u7528\u4f8b\u4e2d\uff0c\u591a\u4e2a\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u4f53\u7cfb\u7ed3\u6784\u3002 \u4f8b\u5982\uff0c\u9488\u5bf9\u5305\u542b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u548c\u975ePII\u4e8b\u4ef6\u7684\u72ec\u7acb\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u5ba1\u8ba1\u548c\u8bbf\u95ee\u63a7\u5236\u89c4\u5219\u3002 \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u521b\u5efa\u4e00\u4e2a \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406 . \u914d\u7f6e \u9ed8\u8ba4\u4ee3\u7406ConfigMap\u8bbe\u7f6e . \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u4ee3\u7406\u6982\u5ff5\u6587\u6863 \u4ee3\u7406\u7684\u89c4\u8303","title":"\u5173\u4e8e\u4ee3\u7406\u5668"},{"location":"eventing/brokers/#_1","text":"\u4ee3\u7406\u662fKubernetes\u7684\u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u5b9a\u4e49\u4e86\u7528\u4e8e\u6536\u96c6\u4e8b\u4ef6\u6c60\u7684\u4e8b\u4ef6\u7f51\u683c\u3002 \u4ee3\u7406\u4e3a\u4e8b\u4ef6\u8f93\u5165\u63d0\u4f9b\u53ef\u53d1\u73b0\u7684\u7aef\u70b9\uff0c\u5e76\u4f7f\u7528\u89e6\u53d1\u5668\u8fdb\u884c\u4e8b\u4ef6\u4f20\u9012\u3002 \u4e8b\u4ef6\u751f\u4ea7\u8005\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u4e8b\u4ef6\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4ee3\u7406\u3002","title":"\u5173\u4e8e\u4ee3\u7406\u5668"},{"location":"eventing/brokers/#_2","text":"\u4e8b\u4ef6\u4f20\u9012\u673a\u5236\u662f\u4f9d\u8d56\u4e8e\u914d\u7f6e\u7684\u4ee3\u7406\u7c7b\u7684\u5b9e\u73b0\u7ec6\u8282\u3002 \u4f7f\u7528\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u53ef\u4ee5\u4ece\u4e8b\u4ef6\u751f\u4ea7\u8005\u548c\u4e8b\u4ef6\u4f7f\u7528\u8005\u62bd\u8c61\u4e8b\u4ef6\u8def\u7531\u7684\u7ec6\u8282\u3002","title":"\u4e8b\u4ef6\u4ea4\u4ed8"},{"location":"eventing/brokers/#_3","text":"\u5bf9\u4e8e\u5927\u591a\u6570\u7528\u4f8b\u6765\u8bf4\uff0c\u6bcf\u4e2a\u540d\u79f0\u7a7a\u95f4\u4e00\u4e2a\u4ee3\u7406\u5c31\u8db3\u591f\u4e86\uff0c\u4f46\u662f\u5728\u4e00\u4e9b\u7528\u4f8b\u4e2d\uff0c\u591a\u4e2a\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u4f53\u7cfb\u7ed3\u6784\u3002 \u4f8b\u5982\uff0c\u9488\u5bf9\u5305\u542b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u548c\u975ePII\u4e8b\u4ef6\u7684\u72ec\u7acb\u4ee3\u7406\u53ef\u4ee5\u7b80\u5316\u5ba1\u8ba1\u548c\u8bbf\u95ee\u63a7\u5236\u89c4\u5219\u3002","title":"\u9ad8\u7ea7\u7528\u4f8b"},{"location":"eventing/brokers/#_4","text":"\u521b\u5efa\u4e00\u4e2a \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406 . \u914d\u7f6e \u9ed8\u8ba4\u4ee3\u7406ConfigMap\u8bbe\u7f6e .","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"eventing/brokers/#_5","text":"\u4ee3\u7406\u6982\u5ff5\u6587\u6863 \u4ee3\u7406\u7684\u89c4\u8303","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"eventing/brokers/broker-admin-config-options/","text":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879 \u00b6 If you have cluster administrator permissions for your Knative installation, you can modify ConfigMaps to change the global default configuration options for Brokers on the cluster. Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default Broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing \u901a\u9053\u5b9e\u73b0\u9009\u9879 \u00b6 The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation. \u66f4\u6539\u540d\u79f0\u7a7a\u95f4\u7684\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0 \u00b6 You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing \u914d\u7f6e\u4ea4\u4ed8\u89c4\u8303\u9ed8\u8ba4\u503c \u00b6 You can configure default event delivery parameters for Brokers that are applied in cases where an event fails to be delivered: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing delivery: retry: 10 backoffDelay: PT0.2S backoffPolicy: exponential namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing delivery: deadLetterSink: ref: kind: Service namespace: example-namespace name: example-service apiVersion: v1 uri: example-uri retry: 10 backoffPolicy: exponential backoffDelay: \"PT0.2S\" \u6b7b\u4fe1\u69fd \u00b6 You can configure the deadLetterSink delivery parameter so that if an event fails to be delivered it is sent to the specified event sink. \u91cd\u8bd5 \u00b6 You can set a minimum number of times that the delivery must be retried before the event is sent to the dead letter sink, by configuring the retry delivery parameter with an integer value. \u540e\u9000\u5ef6\u8fdf \u00b6 You can set the backoffDelay delivery parameter to specify the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. \u9000\u51fa\u653f\u7b56 \u00b6 The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential. When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential backoff policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . \u4ee3\u7406\u7c7b\u9009\u9879 \u00b6 When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u914d\u7f6e\u4ee3\u7406\u7c7b \u00b6 To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing \u4e3a\u96c6\u7fa4\u914d\u7f6e\u9ed8\u8ba4\u7684BrokerClass \u00b6 You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class. Example \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker \u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u9ed8\u8ba4\u7684BrokerClass \u00b6 You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker \u5c06Istio\u4e0eKnative\u4ee3\u7406\u96c6\u6210 \u00b6 \u901a\u8fc7\u4f7f\u7528JSON Web Token (JWT)\u548cIstio\u6765\u4fdd\u62a4Knative\u4ee3\u7406 \u00b6 \u5148\u51b3\u6761\u4ef6 \u00b6 You have installed Knative Eventing. You have installed Istio. \u8fc7\u7a0b \u00b6 Label the knative-eventing namespace, so that Istio can handle JWT-based user authentication, by running the command: kubectl label namespace knative-eventing istio-injection = enabled Restart the broker ingress pod, so that the istio-proxy container can be injected as a sidecar, by running the command: kubectl delete pod <broker-ingress-pod-name> -n knative-eventing Where <broker-ingress-pod-name> is the name of your broker ingress pod. The pod now has two containers: knative-eventing <broker-ingress-pod-name> 2 /2 Running 1 175m Create a broker, then use get the URL of your broker by running the command: kubectl get broker <broker-name> Example output: NAMESPACE NAME URL AGE READY REASON default my-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker 6s True Start a curl pod: kubectl -n default run curl --image = radial/busyboxplus:curl -i --tty Send a CloudEvent with an HTTP POST against the broker URL: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> Where <broker-URL> is the URL of your broker. For example: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker You will receive a 202 HTTP response code, that the broker did accept the request: ... * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 13 :37:57 GMT < content-length: 0 < x-envoy-upstream-service-time: 79 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* Apply a AuthorizationPolicy object in the knative-eventing namespace to describe that the path to the Broker is restricted to a given user: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : require-jwt namespace : knative-eventing spec : action : ALLOW rules : - from : - source : requestPrincipals : [ \"testing@secure.istio.io/testing@secure.istio.io\" ] to : - operation : methods : [ \"POST\" ] paths : [ \"/default/my-broker\" ] Create a RequestAuthentication object for the user requestPrincipal in the istio-system namespace: apiVersion : security.istio.io/v1beta1 kind : RequestAuthentication metadata : name : \"jwt-example\" namespace : istio-system spec : jwtRules : - issuer : \"testing@secure.istio.io\" jwksUri : \"https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/jwks.json\" Now retrying the curl command results in a 403 - Forbidden response code from the server: ... * Mark bundle as not supporting multiuse < HTTP/1.1 403 Forbidden < content-length: 19 < content-type: text/plain < date: Tue, 15 Mar 2022 13 :47:53 GMT < server: istio-envoy < connection: close < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* To access the Broker, add the Bearer JSON Web Token as part of the request: TOKEN = $( curl https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/demo.jwt -s ) curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> The server now responds with a 202 response code, indicating that it has accepted the HTTP request: * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 14 :05:09 GMT < content-length: 0 < x-envoy-upstream-service-time: 40 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_1","text":"If you have cluster administrator permissions for your Knative installation, you can modify ConfigMaps to change the global default configuration options for Brokers on the cluster. Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default Broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_2","text":"The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation.","title":"\u901a\u9053\u5b9e\u73b0\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_3","text":"You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"\u66f4\u6539\u540d\u79f0\u7a7a\u95f4\u7684\u9ed8\u8ba4\u901a\u9053\u5b9e\u73b0"},{"location":"eventing/brokers/broker-admin-config-options/#_4","text":"You can configure default event delivery parameters for Brokers that are applied in cases where an event fails to be delivered: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing delivery: retry: 10 backoffDelay: PT0.2S backoffPolicy: exponential namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing delivery: deadLetterSink: ref: kind: Service namespace: example-namespace name: example-service apiVersion: v1 uri: example-uri retry: 10 backoffPolicy: exponential backoffDelay: \"PT0.2S\"","title":"\u914d\u7f6e\u4ea4\u4ed8\u89c4\u8303\u9ed8\u8ba4\u503c"},{"location":"eventing/brokers/broker-admin-config-options/#_5","text":"You can configure the deadLetterSink delivery parameter so that if an event fails to be delivered it is sent to the specified event sink.","title":"\u6b7b\u4fe1\u69fd"},{"location":"eventing/brokers/broker-admin-config-options/#_6","text":"You can set a minimum number of times that the delivery must be retried before the event is sent to the dead letter sink, by configuring the retry delivery parameter with an integer value.","title":"\u91cd\u8bd5"},{"location":"eventing/brokers/broker-admin-config-options/#_7","text":"You can set the backoffDelay delivery parameter to specify the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format.","title":"\u540e\u9000\u5ef6\u8fdf"},{"location":"eventing/brokers/broker-admin-config-options/#_8","text":"The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential. When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential backoff policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> .","title":"\u9000\u51fa\u653f\u7b56"},{"location":"eventing/brokers/broker-admin-config-options/#_9","text":"When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u4ee3\u7406\u7c7b\u9009\u9879"},{"location":"eventing/brokers/broker-admin-config-options/#_10","text":"To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing","title":"\u914d\u7f6e\u4ee3\u7406\u7c7b"},{"location":"eventing/brokers/broker-admin-config-options/#brokerclass","text":"You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class.","title":"\u4e3a\u96c6\u7fa4\u914d\u7f6e\u9ed8\u8ba4\u7684BrokerClass"},{"location":"eventing/brokers/broker-admin-config-options/#example","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker","title":"Example"},{"location":"eventing/brokers/broker-admin-config-options/#brokerclass_1","text":"You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"\u4e3a\u540d\u79f0\u7a7a\u95f4\u914d\u7f6e\u9ed8\u8ba4\u7684BrokerClass"},{"location":"eventing/brokers/broker-admin-config-options/#istioknative","text":"","title":"\u5c06Istio\u4e0eKnative\u4ee3\u7406\u96c6\u6210"},{"location":"eventing/brokers/broker-admin-config-options/#json-web-token-jwtistioknative","text":"","title":"\u901a\u8fc7\u4f7f\u7528JSON Web Token (JWT)\u548cIstio\u6765\u4fdd\u62a4Knative\u4ee3\u7406"},{"location":"eventing/brokers/broker-admin-config-options/#_11","text":"You have installed Knative Eventing. You have installed Istio.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/brokers/broker-admin-config-options/#_12","text":"Label the knative-eventing namespace, so that Istio can handle JWT-based user authentication, by running the command: kubectl label namespace knative-eventing istio-injection = enabled Restart the broker ingress pod, so that the istio-proxy container can be injected as a sidecar, by running the command: kubectl delete pod <broker-ingress-pod-name> -n knative-eventing Where <broker-ingress-pod-name> is the name of your broker ingress pod. The pod now has two containers: knative-eventing <broker-ingress-pod-name> 2 /2 Running 1 175m Create a broker, then use get the URL of your broker by running the command: kubectl get broker <broker-name> Example output: NAMESPACE NAME URL AGE READY REASON default my-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker 6s True Start a curl pod: kubectl -n default run curl --image = radial/busyboxplus:curl -i --tty Send a CloudEvent with an HTTP POST against the broker URL: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> Where <broker-URL> is the URL of your broker. For example: curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker You will receive a 202 HTTP response code, that the broker did accept the request: ... * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 13 :37:57 GMT < content-length: 0 < x-envoy-upstream-service-time: 79 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* Apply a AuthorizationPolicy object in the knative-eventing namespace to describe that the path to the Broker is restricted to a given user: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : require-jwt namespace : knative-eventing spec : action : ALLOW rules : - from : - source : requestPrincipals : [ \"testing@secure.istio.io/testing@secure.istio.io\" ] to : - operation : methods : [ \"POST\" ] paths : [ \"/default/my-broker\" ] Create a RequestAuthentication object for the user requestPrincipal in the istio-system namespace: apiVersion : security.istio.io/v1beta1 kind : RequestAuthentication metadata : name : \"jwt-example\" namespace : istio-system spec : jwtRules : - issuer : \"testing@secure.istio.io\" jwksUri : \"https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/jwks.json\" Now retrying the curl command results in a 403 - Forbidden response code from the server: ... * Mark bundle as not supporting multiuse < HTTP/1.1 403 Forbidden < content-length: 19 < content-type: text/plain < date: Tue, 15 Mar 2022 13 :47:53 GMT < server: istio-envoy < connection: close < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/* To access the Broker, add the Bearer JSON Web Token as part of the request: TOKEN = $( curl https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/demo.jwt -s ) curl -X POST -v \\ -H \"content-type: application/json\" \\ -H \"Authorization: Bearer ${ TOKEN } \" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: my/curl/command\" \\ -H \"ce-type: my.demo.event\" \\ -H \"ce-id: 0815\" \\ -d '{\"value\":\"Hello Knative\"}' \\ <broker-URL> The server now responds with a 202 response code, indicating that it has accepted the HTTP request: * Mark bundle as not supporting multiuse < HTTP/1.1 202 Accepted < allow: POST, OPTIONS < date: Tue, 15 Mar 2022 14 :05:09 GMT < content-length: 0 < x-envoy-upstream-service-time: 40 < server: istio-envoy < x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*","title":"\u8fc7\u7a0b"},{"location":"eventing/brokers/broker-developer-config-options/","text":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u4ee3\u7406\u914d\u7f6e\u793a\u4f8b \u00b6 The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" You can specify any valid name for your broker. Using default will create a broker named default . The namespace must be an existing namespace in your cluster. Using default will create the broker in the default namespace. You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the Kafka and RabbitMQBroker broker class. For more information see the Apache Kafka Broker or RabbitMQ Broker documentation. spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on Configure Broker defaults . spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on Event delivery .","title":"\u5f00\u53d1\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-developer-config-options/#_1","text":"","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"eventing/brokers/broker-developer-config-options/#_2","text":"The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" You can specify any valid name for your broker. Using default will create a broker named default . The namespace must be an existing namespace in your cluster. Using default will create the broker in the default namespace. You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the Kafka and RabbitMQBroker broker class. For more information see the Apache Kafka Broker or RabbitMQ Broker documentation. spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on Configure Broker defaults . spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on Event delivery .","title":"\u4ee3\u7406\u914d\u7f6e\u793a\u4f8b"},{"location":"eventing/brokers/create-mtbroker/","text":"\u521b\u5efa\u4ee3\u7406 \u00b6 \u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u521b\u5efa\u9ed8\u8ba4\u63d0\u4f9b\u7684\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u4f8b\u3002 \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406\u7684\u9ed8\u8ba4\u540e\u5907\u901a\u9053\u7c7b\u578b\u662fInMemoryChannel\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 kn CLI\u6216\u4f7f\u7528 kubectl \u5e94\u7528YAML\u6587\u4ef6\u6765\u521b\u5efa\u4ee3\u7406\u3002 kn kubectl \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: kn broker create <broker-name> -n <namespace> Note \u5982\u679c\u9009\u62e9\u4e0d\u6307\u5b9a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u4ee3\u7406\u5c06\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u53ef\u9009:\u9a8c\u8bc1\u662f\u5426\u901a\u8fc7\u5217\u51fa\u73b0\u6709\u7684\u4ee3\u7406\u521b\u5efa\u4e86\u4ee3\u7406\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker list \u53ef\u9009:\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u60a8\u521b\u5efa\u7684\u4ee3\u7406\u6765\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5b58\u5728\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker describe <broker-name> \u4e0b\u9762\u793a\u4f8b\u4e2d\u7684YAML\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efaYAML\u6587\u4ef6\uff0c\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u53ef\u9009:\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u6b63\u5e38\u5de5\u4f5c: kubectl -n <namespace> get broker <broker-name> \u8fd9\u5c06\u663e\u793a\u6709\u5173\u60a8\u7684\u4ee3\u7406\u7684\u4fe1\u606f\u3002\u5982\u679c\u4ee3\u7406\u6b63\u5e38\u5de5\u4f5c\uff0c\u5b83\u4f1a\u663e\u793a True \u7684 READY \u72b6\u6001: NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m \u5982\u679c READY \u72b6\u6001\u4e3a False \uff0c\u7b49\u5f85\u51e0\u5206\u949f\uff0c\u7136\u540e\u518d\u6b21\u8fd0\u884c\u547d\u4ee4\u3002","title":"\u521b\u5efa\u4ee3\u7406\u5668"},{"location":"eventing/brokers/create-mtbroker/#_1","text":"\u5b89\u88c5\u4e86Knative\u4e8b\u4ef6\u5904\u7406\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u521b\u5efa\u9ed8\u8ba4\u63d0\u4f9b\u7684\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u4f8b\u3002 \u57fa\u4e8eMT\u901a\u9053\u7684\u4ee3\u7406\u7684\u9ed8\u8ba4\u540e\u5907\u901a\u9053\u7c7b\u578b\u662fInMemoryChannel\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 kn CLI\u6216\u4f7f\u7528 kubectl \u5e94\u7528YAML\u6587\u4ef6\u6765\u521b\u5efa\u4ee3\u7406\u3002 kn kubectl \u60a8\u53ef\u4ee5\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: kn broker create <broker-name> -n <namespace> Note \u5982\u679c\u9009\u62e9\u4e0d\u6307\u5b9a\u540d\u79f0\u7a7a\u95f4\uff0c\u5219\u4ee3\u7406\u5c06\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u53ef\u9009:\u9a8c\u8bc1\u662f\u5426\u901a\u8fc7\u5217\u51fa\u73b0\u6709\u7684\u4ee3\u7406\u521b\u5efa\u4e86\u4ee3\u7406\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker list \u53ef\u9009:\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u60a8\u521b\u5efa\u7684\u4ee3\u7406\u6765\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5b58\u5728\u3002\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4: kn broker describe <broker-name> \u4e0b\u9762\u793a\u4f8b\u4e2d\u7684YAML\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u901a\u8fc7\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efaYAML\u6587\u4ef6\uff0c\u5728\u5f53\u524d\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4ee3\u7406: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u53ef\u9009:\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\uff0c\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u6b63\u5e38\u5de5\u4f5c: kubectl -n <namespace> get broker <broker-name> \u8fd9\u5c06\u663e\u793a\u6709\u5173\u60a8\u7684\u4ee3\u7406\u7684\u4fe1\u606f\u3002\u5982\u679c\u4ee3\u7406\u6b63\u5e38\u5de5\u4f5c\uff0c\u5b83\u4f1a\u663e\u793a True \u7684 READY \u72b6\u6001: NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m \u5982\u679c READY \u72b6\u6001\u4e3a False \uff0c\u7b49\u5f85\u51e0\u5206\u949f\uff0c\u7136\u540e\u518d\u6b21\u8fd0\u884c\u547d\u4ee4\u3002","title":"\u521b\u5efa\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/","text":"\u53ef\u7528\u7684\u4ee3\u7406\u7c7b\u578b \u00b6 \u4ee5\u4e0b\u4ee3\u7406\u7c7b\u578b\u53ef\u4e0eKnative\u4e8b\u4ef6\u4e00\u8d77\u4f7f\u7528\u3002 \u57fa\u4e8e\u591a\u79df\u6237\u901a\u9053\u7684\u4ee3\u7406 \u00b6 Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u4f7f\u7528\u901a\u9053\u8fdb\u884c\u4e8b\u4ef6\u8def\u7531\u3002 \u5728\u4f7f\u7528MT\u57fa\u4e8e\u901a\u9053\u7684\u4ee3\u7406\u4e4b\u524d\uff0c\u5fc5\u987b\u5b89\u88c5\u901a\u9053\u5b9e\u73b0\u3002 \u53ef\u9009\u7684\u4ee3\u7406\u5b9e\u73b0 \u00b6 \u5728Knative\u4e8b\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u53ea\u8981\u9075\u5b88 \u4ee3\u7406\u89c4\u8303 \uff0c\u5176\u4ed6\u4ee3\u7406\u5b9e\u73b0\u90fd\u662f\u53d7\u6b22\u8fce\u7684\u3002 \u4ee5\u4e0b\u662f\u793e\u533a\u6216\u4f9b\u5e94\u5546\u63d0\u4f9b\u7684\u4ee3\u7406\u5217\u8868: Apache Kafka\u4ee3\u7406 \u00b6 \u66f4\u591a\u4fe1\u606f\uff0c\u53c2\u89c1 Apache Kafka Broker . RabbitMQ \u4ee3\u7406 \u00b6 RabbitMQ\u4ee3\u7406\u4f7f\u7528 RabbitMQ \u4f5c\u4e3a\u5e95\u5c42\u5b9e\u73b0\u3002 \u8981\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u4ee3\u7406 \u6216 GitHub\u4e0a\u53ef\u7528\u7684\u6587\u6863 .","title":"\u4ee3\u7406\u7c7b\u578b"},{"location":"eventing/brokers/broker-types/#_1","text":"\u4ee5\u4e0b\u4ee3\u7406\u7c7b\u578b\u53ef\u4e0eKnative\u4e8b\u4ef6\u4e00\u8d77\u4f7f\u7528\u3002","title":"\u53ef\u7528\u7684\u4ee3\u7406\u7c7b\u578b"},{"location":"eventing/brokers/broker-types/#_2","text":"Knative\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u79df\u6237(MT)\u901a\u9053\u7684\u4ee3\u7406\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u4f7f\u7528\u901a\u9053\u8fdb\u884c\u4e8b\u4ef6\u8def\u7531\u3002 \u5728\u4f7f\u7528MT\u57fa\u4e8e\u901a\u9053\u7684\u4ee3\u7406\u4e4b\u524d\uff0c\u5fc5\u987b\u5b89\u88c5\u901a\u9053\u5b9e\u73b0\u3002","title":"\u57fa\u4e8e\u591a\u79df\u6237\u901a\u9053\u7684\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/#_3","text":"\u5728Knative\u4e8b\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u53ea\u8981\u9075\u5b88 \u4ee3\u7406\u89c4\u8303 \uff0c\u5176\u4ed6\u4ee3\u7406\u5b9e\u73b0\u90fd\u662f\u53d7\u6b22\u8fce\u7684\u3002 \u4ee5\u4e0b\u662f\u793e\u533a\u6216\u4f9b\u5e94\u5546\u63d0\u4f9b\u7684\u4ee3\u7406\u5217\u8868:","title":"\u53ef\u9009\u7684\u4ee3\u7406\u5b9e\u73b0"},{"location":"eventing/brokers/broker-types/#apache-kafka","text":"\u66f4\u591a\u4fe1\u606f\uff0c\u53c2\u89c1 Apache Kafka Broker .","title":"Apache Kafka\u4ee3\u7406"},{"location":"eventing/brokers/broker-types/#rabbitmq","text":"RabbitMQ\u4ee3\u7406\u4f7f\u7528 RabbitMQ \u4f5c\u4e3a\u5e95\u5c42\u5b9e\u73b0\u3002 \u8981\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RabbitMQ\u4ee3\u7406 \u6216 GitHub\u4e0a\u53ef\u7528\u7684\u6587\u6863 .","title":"RabbitMQ \u4ee3\u7406"},{"location":"eventing/brokers/broker-types/kafka-broker/","text":"Knative Kafka Broker \u00b6 The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix The Knative Kafka Broker stores incoming CloudEvents as Kafka records, using the binary content mode . This means all CloudEvent attributes and extensions are mapped as headers on the Kafka record , while the data of the CloudEvent corresponds to the value of the Kafka record. Prerequisites \u00b6 You have installed Knative Eventing. You have access to an Apache Kafka cluster. Tip If you need to set up a Kafka cluster, you can do this by following the instructions on the Strimzi Quickstart page . Installation \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s Create a Kafka Broker \u00b6 A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka # Optional annotation to point to an externally managed kafka topic: # kafka.eventing.knative.dev/external.topic: <topic-name> name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service Configure a Kafka Broker \u00b6 The spec.config should reference any ConfigMap in any namespace that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"3\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the Knative Eventing SYSTEM_NAMESPACE in the cluster. You can edit the global configuration depending on your needs. You can also override these settings on a per broker base, by referencing a different ConfigMap on a different namespace or with a different name on your Kafka Broker's spec.config field. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 . Set as default broker implementation \u00b6 To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set. Bring your own topic \u00b6 By default the Knative Kafka Broker creates its own internal topic, however it is possible to point to an externally managed topic, using the kafka.eventing.knative.dev/external.topic annotation: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka kafka.eventing.knative.dev/external.topic : <my-topic-name> name : default namespace : default spec : # other spec fields ... Note When using an external topic, the Knative Kafka Broker does not own the topic and is not responsible for managing the topic. This includes the topic lifecycle or its general validity. Other restrictions for general access to the topic may apply. See the documentation about using Access Control Lists (ACLs) . Consumer Offsets Commit Interval \u00b6 Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset. Kafka Producer and Consumer configurations \u00b6 Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations . Enable debug logging for data plane components \u00b6 The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher Configuring the order of delivered events \u00b6 When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee. Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Kafka\u4ee3\u7406\u5668"},{"location":"eventing/brokers/broker-types/kafka-broker/#knative-kafka-broker","text":"The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix The Knative Kafka Broker stores incoming CloudEvents as Kafka records, using the binary content mode . This means all CloudEvent attributes and extensions are mapped as headers on the Kafka record , while the data of the CloudEvent corresponds to the value of the Kafka record.","title":"Knative Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#prerequisites","text":"You have installed Knative Eventing. You have access to an Apache Kafka cluster. Tip If you need to set up a Kafka cluster, you can do this by following the instructions on the Strimzi Quickstart page .","title":"Prerequisites"},{"location":"eventing/brokers/broker-types/kafka-broker/#installation","text":"Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/brokers/broker-types/kafka-broker/#create-a-kafka-broker","text":"A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka # Optional annotation to point to an externally managed kafka topic: # kafka.eventing.knative.dev/external.topic: <topic-name> name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service","title":"Create a Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#configure-a-kafka-broker","text":"The spec.config should reference any ConfigMap in any namespace that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"3\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the Knative Eventing SYSTEM_NAMESPACE in the cluster. You can edit the global configuration depending on your needs. You can also override these settings on a per broker base, by referencing a different ConfigMap on a different namespace or with a different name on your Kafka Broker's spec.config field. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 .","title":"Configure a Kafka Broker"},{"location":"eventing/brokers/broker-types/kafka-broker/#set-as-default-broker-implementation","text":"To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing","title":"Set as default broker implementation"},{"location":"eventing/brokers/broker-types/kafka-broker/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/brokers/broker-types/kafka-broker/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/brokers/broker-types/kafka-broker/#bring-your-own-topic","text":"By default the Knative Kafka Broker creates its own internal topic, however it is possible to point to an externally managed topic, using the kafka.eventing.knative.dev/external.topic annotation: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka kafka.eventing.knative.dev/external.topic : <my-topic-name> name : default namespace : default spec : # other spec fields ... Note When using an external topic, the Knative Kafka Broker does not own the topic and is not responsible for managing the topic. This includes the topic lifecycle or its general validity. Other restrictions for general access to the topic may apply. See the documentation about using Access Control Lists (ACLs) .","title":"Bring your own topic"},{"location":"eventing/brokers/broker-types/kafka-broker/#consumer-offsets-commit-interval","text":"Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset.","title":"Consumer Offsets Commit Interval"},{"location":"eventing/brokers/broker-types/kafka-broker/#kafka-producer-and-consumer-configurations","text":"Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations .","title":"Kafka Producer and Consumer configurations"},{"location":"eventing/brokers/broker-types/kafka-broker/#enable-debug-logging-for-data-plane-components","text":"The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher","title":"Enable debug logging for data plane components"},{"location":"eventing/brokers/broker-types/kafka-broker/#configuring-the-order-of-delivered-events","text":"When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee.","title":"Configuring the order of delivered events"},{"location":"eventing/brokers/broker-types/kafka-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/","text":"\u521b\u5efa RabbitMQ Broker \u00b6 \u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa RabbitMQ Broker\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 knative \u4e8b\u4ef6 \u5df2\u5b89\u88c5 CertManager v1.5.4 - \u4e0e RabbitMQ \u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u7b26\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5 RabbitMQ \u6d88\u606f\u4f20\u9012\u62d3\u6251\u64cd\u4f5c\u7b26 - \u6211\u4eec\u7684\u5efa\u8bae\u662f\u4f7f\u7528 CertManager \u7684 \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u8bbf\u95ee\u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684 RabbitMQ \u5b9e\u4f8b\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 RabbitMQ \u96c6\u7fa4 Kubernetes \u64cd\u4f5c\u7b26 \u6765\u521b\u5efa\u4e00\u4e2a RabbitMQ \u5b9e\u4f8b\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 RabbitMQ \u7f51\u7ad9 . \u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668 \u00b6 \u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml \u9a8c\u8bc1 rabbitmq-broker-controller \u548c rabbitmq-broker-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s \u521b\u5efa\u4e00\u4e2a RabbitMQBrokerConfig \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1alpha1 kind : RabbitmqBrokerConfig metadata : name : <rabbitmq-broker-config-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials queueType : quorum \u5728\u54ea\u91cc: <rabbitmq-broker-config-name> \u662f\u4f60\u60f3\u8981\u7684 RabbitMQBrokerConfig \u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4e4b\u524d\u521b\u5efa\u7684 RabbitMQ \u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u4f60\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c \u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eBroker\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c \u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl create -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u4e00\u4e2a RabbitMQBroker \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <broker-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqBrokerConfig name : <rabbitmq-broker-config-name> \u5176\u4e2d <rabbitmq-broker-config-name> \u662f\u4f60\u5728\u4e0a\u9762\u6b65\u9aa4\u4e2d\u7ed9\u4f60\u7684 RabbitMQBrokerConfig \u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u914d\u7f6e\u6d88\u606f\u6392\u5e8f \u00b6 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u89e6\u53d1\u5668\u6bcf\u6b21\u4f7f\u7528\u4e00\u6761\u6d88\u606f\u4ee5\u4fdd\u6301\u987a\u5e8f\u3002 \u5982\u679c\u4e8b\u4ef6\u7684\u987a\u5e8f\u5e76\u4e0d\u91cd\u8981\uff0c\u5e76\u4e14\u5e0c\u671b\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 parallelism \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5c06 parallelism \u8bbe\u7f6e\u4e3a n \u4e3a\u89e6\u53d1\u5668\u521b\u5efa n \u4e2a worker\uff0c\u8fd9\u4e9b worker \u90fd\u5c06\u5e76\u884c\u4f7f\u7528\u6d88\u606f\u3002 \u4e0b\u9762\u7684 YAML \u663e\u793a\u4e86\u4e00\u4e2a\u5e76\u884c\u5ea6\u8bbe\u7f6e\u4e3a 10 \u7684\u89e6\u53d1\u5668\u793a\u4f8b: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : high-throughput-trigger annotations : rabbitmq.eventing.knative.dev/parallelism : \"10\" \u989d\u5916\u7684\u4fe1\u606f \u00b6 \u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github \u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2a bug \u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github \u5b58\u50a8\u5e93 \u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898.","title":"RabbitMQ\u4ee3\u7406\u5668"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmq-broker","text":"\u672c\u8282\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa RabbitMQ Broker\u3002","title":"\u521b\u5efa RabbitMQ Broker"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_1","text":"\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86 knative \u4e8b\u4ef6 \u5df2\u5b89\u88c5 CertManager v1.5.4 - \u4e0e RabbitMQ \u6d88\u606f\u62d3\u6251\u64cd\u4f5c\u7b26\u6700\u7b80\u5355\u7684\u96c6\u6210 \u60a8\u5df2\u7ecf\u5b89\u88c5 RabbitMQ \u6d88\u606f\u4f20\u9012\u62d3\u6251\u64cd\u4f5c\u7b26 - \u6211\u4eec\u7684\u5efa\u8bae\u662f\u4f7f\u7528 CertManager \u7684 \u6700\u65b0\u7248\u672c \u4f60\u53ef\u4ee5\u8bbf\u95ee\u4e00\u4e2a\u6b63\u5728\u5de5\u4f5c\u7684 RabbitMQ \u5b9e\u4f8b\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 RabbitMQ \u96c6\u7fa4 Kubernetes \u64cd\u4f5c\u7b26 \u6765\u521b\u5efa\u4e00\u4e2a RabbitMQ \u5b9e\u4f8b\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 RabbitMQ \u7f51\u7ad9 .","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmq","text":"\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml \u9a8c\u8bc1 rabbitmq-broker-controller \u548c rabbitmq-broker-webhook \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-eventing \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s","title":"\u5b89\u88c5 RabbitMQ \u63a7\u5236\u5668"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmqbrokerconfig","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1alpha1 kind : RabbitmqBrokerConfig metadata : name : <rabbitmq-broker-config-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials queueType : quorum \u5728\u54ea\u91cc: <rabbitmq-broker-config-name> \u662f\u4f60\u60f3\u8981\u7684 RabbitMQBrokerConfig \u5bf9\u8c61\u7684\u540d\u79f0\u3002 <cluster-name> \u662f\u4e4b\u524d\u521b\u5efa\u7684 RabbitMQ \u96c6\u7fa4\u7684\u540d\u79f0\u3002 Note \u4f60\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e name \u548c connectionSecret \uff0c \u56e0\u4e3a name \u662f\u9488\u5bf9\u4e0eBroker\u8fd0\u884c\u5728\u540c\u4e00\u96c6\u7fa4\u4e2d\u7684RabbitMQ\u96c6\u7fa4\u64cd\u4f5c\u5b9e\u4f8b\uff0c \u800c connectionSecret \u662f\u9488\u5bf9\u5916\u90e8RabbitMQ\u670d\u52a1\u5668\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl create -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a RabbitMQBrokerConfig \u5bf9\u8c61"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#rabbitmqbroker","text":"\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <broker-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqBrokerConfig name : <rabbitmq-broker-config-name> \u5176\u4e2d <rabbitmq-broker-config-name> \u662f\u4f60\u5728\u4e0a\u9762\u6b65\u9aa4\u4e2d\u7ed9\u4f60\u7684 RabbitMQBrokerConfig \u7684\u540d\u79f0\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa\u4e00\u4e2a RabbitMQBroker \u5bf9\u8c61"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_2","text":"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u89e6\u53d1\u5668\u6bcf\u6b21\u4f7f\u7528\u4e00\u6761\u6d88\u606f\u4ee5\u4fdd\u6301\u987a\u5e8f\u3002 \u5982\u679c\u4e8b\u4ef6\u7684\u987a\u5e8f\u5e76\u4e0d\u91cd\u8981\uff0c\u5e76\u4e14\u5e0c\u671b\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 parallelism \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5c06 parallelism \u8bbe\u7f6e\u4e3a n \u4e3a\u89e6\u53d1\u5668\u521b\u5efa n \u4e2a worker\uff0c\u8fd9\u4e9b worker \u90fd\u5c06\u5e76\u884c\u4f7f\u7528\u6d88\u606f\u3002 \u4e0b\u9762\u7684 YAML \u663e\u793a\u4e86\u4e00\u4e2a\u5e76\u884c\u5ea6\u8bbe\u7f6e\u4e3a 10 \u7684\u89e6\u53d1\u5668\u793a\u4f8b: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : high-throughput-trigger annotations : rabbitmq.eventing.knative.dev/parallelism : \"10\"","title":"\u914d\u7f6e\u6d88\u606f\u6392\u5e8f"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#_3","text":"\u66f4\u591a\u793a\u4f8b\u8bf7\u8bbf\u95ee eventing-rabbitmq Github \u5e93\u793a\u4f8b\u76ee\u5f55 \u8981\u62a5\u544a\u4e00\u4e2a bug \u6216\u8bf7\u6c42\u4e00\u4e2a\u7279\u6027\uff0c\u5728 eventing-rabbitmq Github \u5b58\u50a8\u5e93 \u4e2d\u6253\u5f00\u4e00\u4e2a\u95ee\u9898.","title":"\u989d\u5916\u7684\u4fe1\u606f"},{"location":"eventing/channels/","text":"Channels \u00b6 Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services. Next steps \u00b6 Learn about default available channel types Create a channel Create a subscription","title":"\u5173\u4e8e\u6e20\u9053"},{"location":"eventing/channels/#channels","text":"Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services.","title":"Channels"},{"location":"eventing/channels/#next-steps","text":"Learn about default available channel types Create a channel Create a subscription","title":"Next steps"},{"location":"eventing/channels/channel-types-defaults/","text":"Channel types and defaults \u00b6 Knative uses two types of Channels: A generic Channel object. Channel implementations that each have their own custom resource definitions (CRDs), such as InMemoryChannel and KafkaChannel. The KafkaChannel supports an ordered consumer delivery guarantee, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Custom Channel implementations each have their own event delivery mechanisms, such as in-memory or Broker-based. Examples of Brokers include KafkaBroker and the GCP Pub/Sub Broker. Knative provides the InMemoryChannel Channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS Channels. You can use the generic Channel object if you want to create a Channel without specifying which Channel implementation CRD is used. This is useful if you do not care about the properties a particular Channel implementation provides, such as ordering and persistence, and you want to use the implementation selected by the cluster administrator. Cluster administrators can modify the default Channel implementation settings by editing the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . Default Channels can be configured for the cluster, a namespace on the cluster, or both. Note If a default Channel implementation is configured for a namespace, this will overwrite the configuration for the cluster. In the following example, the cluster default Channel implementation is InMemoryChannel, while the namespace default Channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel Channels must not be used in production environments. Next steps \u00b6 Create an InMemoryChannel","title":"\u901a\u9053\u7c7b\u578b\u548c\u9ed8\u8ba4\u503c"},{"location":"eventing/channels/channel-types-defaults/#channel-types-and-defaults","text":"Knative uses two types of Channels: A generic Channel object. Channel implementations that each have their own custom resource definitions (CRDs), such as InMemoryChannel and KafkaChannel. The KafkaChannel supports an ordered consumer delivery guarantee, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Custom Channel implementations each have their own event delivery mechanisms, such as in-memory or Broker-based. Examples of Brokers include KafkaBroker and the GCP Pub/Sub Broker. Knative provides the InMemoryChannel Channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS Channels. You can use the generic Channel object if you want to create a Channel without specifying which Channel implementation CRD is used. This is useful if you do not care about the properties a particular Channel implementation provides, such as ordering and persistence, and you want to use the implementation selected by the cluster administrator. Cluster administrators can modify the default Channel implementation settings by editing the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . Default Channels can be configured for the cluster, a namespace on the cluster, or both. Note If a default Channel implementation is configured for a namespace, this will overwrite the configuration for the cluster. In the following example, the cluster default Channel implementation is InMemoryChannel, while the namespace default Channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel Channels must not be used in production environments.","title":"Channel types and defaults"},{"location":"eventing/channels/channel-types-defaults/#next-steps","text":"Create an InMemoryChannel","title":"Next steps"},{"location":"eventing/channels/channels-crds/","text":"This is a non-exhaustive list of the available Channels for Knative Eventing. Note Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Maintainer Description InMemoryChannel Stable Knative In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel Beta Knative Channels are backed by Apache Kafka topics. NatssChannel Alpha Knative Channels are backed by NATS Streaming .","title":"\u53ef\u7528\u7684\u9891\u9053"},{"location":"eventing/channels/create-default-channel/","text":"Creating a Channel using cluster or namespace defaults \u00b6 Developers can create Channels of any supported implementation type by creating an instance of a Channel object. To create a Channel: Create a YAML file for the Channel object using the following template: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. If you create this object in the default namespace, according to the default ConfigMap example in Channel types and defaults , it is an InMemoryChannel Channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default Channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. <channel-template-kind> is the kind of Channel, such as InMemoryChannel or KafkaChannel, based on the default ConfigMap. See an example in Channel types and defaults . Note The spec.channelTemplate property cannot be changed after creation, because it is set by the default Channel mechanism, not the user. The Channel controller creates a backing Channel instance based on the spec.channelTemplate . When this mechanism is used two objects are created; a generic Channel object and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object by copying its Subscriptions to, and setting its status to, that of the InMemoryChannel object. Note Defaults are only applied by the webhook when a Channel or Sequence is initially created. If the default settings are changed, the new defaults will only be applied to newly created Channels, Brokers, or Sequences. Existing resources are not updated automatically to use the new configuration.","title":"\u4f7f\u7528\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u9ed8\u8ba4\u503c\u521b\u5efa\u901a\u9053"},{"location":"eventing/channels/create-default-channel/#creating-a-channel-using-cluster-or-namespace-defaults","text":"Developers can create Channels of any supported implementation type by creating an instance of a Channel object. To create a Channel: Create a YAML file for the Channel object using the following template: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. If you create this object in the default namespace, according to the default ConfigMap example in Channel types and defaults , it is an InMemoryChannel Channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default Channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. <channel-template-kind> is the kind of Channel, such as InMemoryChannel or KafkaChannel, based on the default ConfigMap. See an example in Channel types and defaults . Note The spec.channelTemplate property cannot be changed after creation, because it is set by the default Channel mechanism, not the user. The Channel controller creates a backing Channel instance based on the spec.channelTemplate . When this mechanism is used two objects are created; a generic Channel object and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object by copying its Subscriptions to, and setting its status to, that of the InMemoryChannel object. Note Defaults are only applied by the webhook when a Channel or Sequence is initially created. If the default settings are changed, the new defaults will only be applied to newly created Channels, Brokers, or Sequences. Existing resources are not updated automatically to use the new configuration.","title":"Creating a Channel using cluster or namespace defaults"},{"location":"eventing/channels/subscriptions/","text":"Subscriptions \u00b6 After you have created a Channel and a Sink, you can create a Subscription to enable event delivery. The Subscription consists of a Subscription object, which specifies the Channel and the Sink (also known as the Subscriber) to deliver events to. You can also specify some Sink-specific options, such as how to handle failures. For more information about Subscription objects, see Subscription . Creating a Subscription \u00b6 kn YAML Create a Subscription between a Channel and a Sink by running: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel specifies the source for cloud events that should be processed. You must provide the Channel name. If you are not using the default Channel that is backed by the Channel resource, you must prefix the Channel name with the <Group:Version:Kind> for the specified Channel type. For example, this is messaging.knative.dev:v1beta1:KafkaChannel for a Kafka-backed Channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink-name> is interpreted as a Knative service of this name, in the same namespace as the Subscription. You can specify the type of the Sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as the destination. You can only reference default Channel types here. broker : An Eventing Broker. --sink-reply is an optional argument you can use to specify where the Sink reply is sent. It uses the same naming conventions for specifying the Sink as the --sink flag. --sink-dead-letter is an optional argument you can use to specify where to send the CloudEvent in case of a failure. It uses the same naming conventions for specifying the Sink as the --sink flag. ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as destination. Only default Channel types can be referenced here. broker : An Eventing Broker. --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the Sink reply is sent, and where to send the CloudEvent in case of a failure, respectively. Both use the same naming conventions for specifying the Sink as the --sink flag. This example command creates a Subscription named mysubscription that routes events from a Channel named mychannel to a Knative service named myservice . Note The Sink prefix is optional. You can also specify the service for --sink as just --sink <service-name> and omit the ksvc prefix. Create a YAML file for the Subscription object using the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Name of the Channel that the Subscription connects to. delivery : # Optional delivery configuration settings for events. deadLetterSink : # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> reply : # Optional configuration settings for the reply event. # This is the event Sink that events replied from the subscriber are delivered to. ref : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : <service-name> subscriber : # Required configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Listing Subscriptions \u00b6 You can list all existing Subscriptions by using the kn CLI tool. List all Subscriptions: kn subscription list List Subscriptions in YAML format: kn subscription list -o yaml Describing a Subscription \u00b6 You can print details about a Subscription by using the kn CLI tool: kn subscription describe <subscription-name> Deleting Subscriptions \u00b6 You can delete a Subscription by using the kn or kubectl CLI tools. kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name> Next steps \u00b6 Creating a Channel using cluster or namespace defaults","title":"\u8ba2\u9605"},{"location":"eventing/channels/subscriptions/#subscriptions","text":"After you have created a Channel and a Sink, you can create a Subscription to enable event delivery. The Subscription consists of a Subscription object, which specifies the Channel and the Sink (also known as the Subscriber) to deliver events to. You can also specify some Sink-specific options, such as how to handle failures. For more information about Subscription objects, see Subscription .","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#creating-a-subscription","text":"kn YAML Create a Subscription between a Channel and a Sink by running: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel specifies the source for cloud events that should be processed. You must provide the Channel name. If you are not using the default Channel that is backed by the Channel resource, you must prefix the Channel name with the <Group:Version:Kind> for the specified Channel type. For example, this is messaging.knative.dev:v1beta1:KafkaChannel for a Kafka-backed Channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink-name> is interpreted as a Knative service of this name, in the same namespace as the Subscription. You can specify the type of the Sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as the destination. You can only reference default Channel types here. broker : An Eventing Broker. --sink-reply is an optional argument you can use to specify where the Sink reply is sent. It uses the same naming conventions for specifying the Sink as the --sink flag. --sink-dead-letter is an optional argument you can use to specify where to send the CloudEvent in case of a failure. It uses the same naming conventions for specifying the Sink as the --sink flag. ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as destination. Only default Channel types can be referenced here. broker : An Eventing Broker. --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the Sink reply is sent, and where to send the CloudEvent in case of a failure, respectively. Both use the same naming conventions for specifying the Sink as the --sink flag. This example command creates a Subscription named mysubscription that routes events from a Channel named mychannel to a Knative service named myservice . Note The Sink prefix is optional. You can also specify the service for --sink as just --sink <service-name> and omit the ksvc prefix. Create a YAML file for the Subscription object using the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Name of the Channel that the Subscription connects to. delivery : # Optional delivery configuration settings for events. deadLetterSink : # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> reply : # Optional configuration settings for the reply event. # This is the event Sink that events replied from the subscriber are delivered to. ref : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : <service-name> subscriber : # Required configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Creating a Subscription"},{"location":"eventing/channels/subscriptions/#listing-subscriptions","text":"You can list all existing Subscriptions by using the kn CLI tool. List all Subscriptions: kn subscription list List Subscriptions in YAML format: kn subscription list -o yaml","title":"Listing Subscriptions"},{"location":"eventing/channels/subscriptions/#describing-a-subscription","text":"You can print details about a Subscription by using the kn CLI tool: kn subscription describe <subscription-name>","title":"Describing a Subscription"},{"location":"eventing/channels/subscriptions/#deleting-subscriptions","text":"You can delete a Subscription by using the kn or kubectl CLI tools. kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name>","title":"Deleting Subscriptions"},{"location":"eventing/channels/subscriptions/#next-steps","text":"Creating a Channel using cluster or namespace defaults","title":"Next steps"},{"location":"eventing/configuration/channel-configuration/","text":"Configure Channel defaults \u00b6 Knative Eventing provides a default-ch-webhook ConfigMap that contains the configuration settings that govern default Channel creation. The default default-ch-webhook ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel By changing the data.default-ch-config property we can define the clusterDefaults and per Namespace defaults. This configuration is used by the Channel custom resource definition (CRD) to create platform specific implementations. Note The clusterDefault setting determines the global, cluster-wide default Channel type. You can configure Channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"\u914d\u7f6e\u901a\u9053\u8fdd\u7ea6"},{"location":"eventing/configuration/channel-configuration/#configure-channel-defaults","text":"Knative Eventing provides a default-ch-webhook ConfigMap that contains the configuration settings that govern default Channel creation. The default default-ch-webhook ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel By changing the data.default-ch-config property we can define the clusterDefaults and per Namespace defaults. This configuration is used by the Channel custom resource definition (CRD) to create platform specific implementations. Note The clusterDefault setting determines the global, cluster-wide default Channel type. You can configure Channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Configure Channel defaults"},{"location":"eventing/configuration/kafka-channel-configuration/","text":"Configure Kafka Channels \u00b6 Note This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka Channels, you must: Install the KafkaChannel custom resource definition (CRD). Create a ConfigMap that specifies default configurations for how KafkaChannel instances are created. Create a kafka-channel ConfigMap \u00b6 Create a YAML file for the kafka-channel ConfigMap using the following template: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional. To create a Broker that uses Kafka Channels, specify the kafka-channel ConfigMap in the Broker spec. You can do this by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u914d\u7f6eKafka\u901a\u9053\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/kafka-channel-configuration/#configure-kafka-channels","text":"Note This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka Channels, you must: Install the KafkaChannel custom resource definition (CRD). Create a ConfigMap that specifies default configurations for how KafkaChannel instances are created.","title":"Configure Kafka Channels"},{"location":"eventing/configuration/kafka-channel-configuration/#create-a-kafka-channel-configmap","text":"Create a YAML file for the kafka-channel ConfigMap using the following template: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional. To create a Broker that uses Kafka Channels, specify the kafka-channel ConfigMap in the Broker spec. You can do this by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a kafka-channel ConfigMap"},{"location":"eventing/configuration/sources-configuration/","text":"Configure event source defaults \u00b6 This topic describes how to configure defaults for Knative event sources. You can configure event sources depending on how they generate events. Configure defaults for PingSource \u00b6 PingSource is an event source that produces events with a fixed payload on a specified cron schedule. For how to create a new PingSource, see Creating a PingSource object . For the available parameters, see PingSource reference . In addition to the parameters that you can configure in the PingSource resource, there is a global ConfigMap called config-ping-defaults . This ConfigMap allows you to change the maximum amount of data that the PingSource adds to the CloudEvents it produces. The data-max-size parameter allows you to set the maximum number of bytes allowed to be sent for a message excluding any base64 decoding. The default value, -1 , sets no limit for data. apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 You can edit this ConfigMap by running the command: kubectl edit cm config-ping-defaults -n knative-eventing","title":"\u914d\u7f6e\u4e8b\u4ef6\u6e90\u9ed8\u8ba4\u503c"},{"location":"eventing/configuration/sources-configuration/#configure-event-source-defaults","text":"This topic describes how to configure defaults for Knative event sources. You can configure event sources depending on how they generate events.","title":"Configure event source defaults"},{"location":"eventing/configuration/sources-configuration/#configure-defaults-for-pingsource","text":"PingSource is an event source that produces events with a fixed payload on a specified cron schedule. For how to create a new PingSource, see Creating a PingSource object . For the available parameters, see PingSource reference . In addition to the parameters that you can configure in the PingSource resource, there is a global ConfigMap called config-ping-defaults . This ConfigMap allows you to change the maximum amount of data that the PingSource adds to the CloudEvents it produces. The data-max-size parameter allows you to set the maximum number of bytes allowed to be sent for a message excluding any base64 decoding. The default value, -1 , sets no limit for data. apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 You can edit this ConfigMap by running the command: kubectl edit cm config-ping-defaults -n knative-eventing","title":"Configure defaults for PingSource"},{"location":"eventing/configuration/sugar-configuration/","text":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668 \u00b6 \u4ecb\u7ecd\u7cd6\u63a7\u5236\u5668\u7684\u914d\u7f6e\u65b9\u6cd5\u3002 \u60a8\u53ef\u4ee5\u914d\u7f6e\u7cd6\u63a7\u5236\u5668\uff0c\u4ee5\u4fbf\u5728\u4f7f\u7528\u914d\u7f6e\u7684\u6807\u7b7e\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u65f6\u521b\u5efa\u4ee3\u7406\u3002 \u53c2\u89c1 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5236\u5668 \u7684\u793a\u4f8b\u3002 \u9ed8\u8ba4\u7684 config-sugar ConfigMap \u901a\u8fc7\u5c06 namespace-selector \u548c trigger-selector \u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\u6765\u7981\u7528\u7cd6\u63a7\u5236\u5668\u3002 \u542f\u7528\u7cd6\u63a7\u5236\u5668 \u5bf9\u4e8e\u547d\u540d\u7a7a\u95f4\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector namespace-selector \u3002 \u5bf9\u4e8e\u89e6\u53d1\u5668\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector trigger-selector \u3002 \u5728\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u89e6\u53d1\u5668\u4e0a\u542f\u7528\u7cd6\u63a7\u5236\u5668\u7684\u793a\u4f8b\u914d\u7f6e apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : namespace-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] trigger-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] \u7cd6\u63a7\u5236\u5668\u53ea\u4f1a\u5728\u5e26\u6709 eventing.knative.dev/injection: enabled \u6807\u7b7e\u7684\u547d\u540d\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u4e0a\u64cd\u4f5c\u3002 \u8fd9\u4e5f\u6a21\u62df\u4e86\u547d\u540d\u7a7a\u95f4\u7684\u9057\u7559\u7cd6\u63a7\u5236\u5668\u884c\u4e3a\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u7f16\u8f91\u8fd9\u4e2a ConfigMap: kubectl edit cm config-sugar -n knative-eventing","title":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/configuration/sugar-configuration/#_1","text":"\u4ecb\u7ecd\u7cd6\u63a7\u5236\u5668\u7684\u914d\u7f6e\u65b9\u6cd5\u3002 \u60a8\u53ef\u4ee5\u914d\u7f6e\u7cd6\u63a7\u5236\u5668\uff0c\u4ee5\u4fbf\u5728\u4f7f\u7528\u914d\u7f6e\u7684\u6807\u7b7e\u521b\u5efa\u540d\u79f0\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u65f6\u521b\u5efa\u4ee3\u7406\u3002 \u53c2\u89c1 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5236\u5668 \u7684\u793a\u4f8b\u3002 \u9ed8\u8ba4\u7684 config-sugar ConfigMap \u901a\u8fc7\u5c06 namespace-selector \u548c trigger-selector \u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32\u6765\u7981\u7528\u7cd6\u63a7\u5236\u5668\u3002 \u542f\u7528\u7cd6\u63a7\u5236\u5668 \u5bf9\u4e8e\u547d\u540d\u7a7a\u95f4\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector namespace-selector \u3002 \u5bf9\u4e8e\u89e6\u53d1\u5668\uff0c\u53ef\u4ee5\u914d\u7f6e LabelSelector trigger-selector \u3002 \u5728\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u89e6\u53d1\u5668\u4e0a\u542f\u7528\u7cd6\u63a7\u5236\u5668\u7684\u793a\u4f8b\u914d\u7f6e apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : namespace-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] trigger-selector : | matchExpressions: - key: \"eventing.knative.dev/injection\" operator: \"In\" values: [\"enabled\"] \u7cd6\u63a7\u5236\u5668\u53ea\u4f1a\u5728\u5e26\u6709 eventing.knative.dev/injection: enabled \u6807\u7b7e\u7684\u547d\u540d\u7a7a\u95f4\u6216\u89e6\u53d1\u5668\u4e0a\u64cd\u4f5c\u3002 \u8fd9\u4e5f\u6a21\u62df\u4e86\u547d\u540d\u7a7a\u95f4\u7684\u9057\u7559\u7cd6\u63a7\u5236\u5668\u884c\u4e3a\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u7f16\u8f91\u8fd9\u4e2a ConfigMap: kubectl edit cm config-sugar -n knative-eventing","title":"\u914d\u7f6e\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/custom-event-source/","text":"Custom event sources \u00b6 If you need to ingress events from an event producer that is not included in Knative, or from a producer that emits events which are not in the CloudEvent format that is used by Knative, you can do this by using one of the following methods: Create a custom Knative event source . Use a PodSpecable object as an event source, by creating a SinkBinding . Use a container as an event source, by creating a ContainerSource .","title":"\u81ea\u5b9a\u4e49\u4e8b\u4ef6\u6e90\u6982\u8ff0"},{"location":"eventing/custom-event-source/#custom-event-sources","text":"If you need to ingress events from an event producer that is not included in Knative, or from a producer that emits events which are not in the CloudEvent format that is used by Knative, you can do this by using one of the following methods: Create a custom Knative event source . Use a PodSpecable object as an event source, by creating a SinkBinding . Use a container as an event source, by creating a ContainerSource .","title":"Custom event sources"},{"location":"eventing/custom-event-source/containersource/","text":"Create a ContainerSource \u00b6 The ContainerSource object starts a container image that generates events and sends messages to a sink URI. You can also use ContainerSource to support your own event sources in Knative. To create a custom event source using ContainerSource, you must create a container image, and a ContainerSource that uses your image URI. Before you begin \u00b6 Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster. Develop, build and publish a container image \u00b6 You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format . Create a ContainerSource object \u00b6 Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container. Verify the ContainerSource object \u00b6 View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Delete the ContainerSource object \u00b6 To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object. Reference Documentation \u00b6 See the ContainerSource reference .","title":"\u521b\u5efa ContainerSource"},{"location":"eventing/custom-event-source/containersource/#create-a-containersource","text":"The ContainerSource object starts a container image that generates events and sends messages to a sink URI. You can also use ContainerSource to support your own event sources in Knative. To create a custom event source using ContainerSource, you must create a container image, and a ContainerSource that uses your image URI.","title":"Create a ContainerSource"},{"location":"eventing/custom-event-source/containersource/#before-you-begin","text":"Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster.","title":"Before you begin"},{"location":"eventing/custom-event-source/containersource/#develop-build-and-publish-a-container-image","text":"You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format .","title":"Develop, build and publish a container image"},{"location":"eventing/custom-event-source/containersource/#create-a-containersource-object","text":"Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container.","title":"Create a ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#verify-the-containersource-object","text":"View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify the ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#delete-the-containersource-object","text":"To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object.","title":"Delete the ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#reference-documentation","text":"See the ContainerSource reference .","title":"Reference Documentation"},{"location":"eventing/custom-event-source/containersource/reference/","text":"ContainerSource reference \u00b6 This topic provides reference information about the configurable fields for the ContainerSource object. ContainerSource \u00b6 A ContainerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a ContainerSource object. Required metadata Specifies metadata that uniquely identifies the ContainerSource object. For example, a name . Required spec Specifies the configuration information for this ContainerSource object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.template A template in the shape of Deployment.spec.template to be used for this ContainerSource. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional Template parameter \u00b6 This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation . Example: template parameter \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ... CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. Example: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"ContainerSource \u53c2\u8003"},{"location":"eventing/custom-event-source/containersource/reference/#containersource-reference","text":"This topic provides reference information about the configurable fields for the ContainerSource object.","title":"ContainerSource reference"},{"location":"eventing/custom-event-source/containersource/reference/#containersource","text":"A ContainerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a ContainerSource object. Required metadata Specifies metadata that uniquely identifies the ContainerSource object. For example, a name . Required spec Specifies the configuration information for this ContainerSource object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.template A template in the shape of Deployment.spec.template to be used for this ContainerSource. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"ContainerSource"},{"location":"eventing/custom-event-source/containersource/reference/#template-parameter","text":"This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation .","title":"Template parameter"},{"location":"eventing/custom-event-source/containersource/reference/#example-template-parameter","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ...","title":"Example: template parameter"},{"location":"eventing/custom-event-source/containersource/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/custom-event-source/containersource/reference/#example-cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"Example: CloudEvent Overrides"},{"location":"eventing/custom-event-source/custom-event-source/","text":"Create a custom event source \u00b6 If you want to create a custom event source for a specific event producer type, you must create the components that enable forwarding events from that producer type to a Knative sink. This type of integration requires more effort than using some simpler integration types, such as SinkBinding or ContainerSource ; however, this provides the most polished result and is the easiest integration type for users to consume. By providing a custom resource definition (CRD) for your source rather than a general container definition, it is easier to expose meaningful configuration options and documentation to users and hide implementation details. Note If you have created a new event source type that is not a part of the core Knative project, you can open a pull request to add it to the list of Third-Party Sources , and announce the new source in one of the Knative Slack channels. You can also add your event source to the knative-sandbox organization, by following the instructions to create a sandbox repository . Required components \u00b6 To create a custom event source, you must create the following components: Component Description Receive adapter Contains logic that specifies how to get events from a producer, what the sink URI is, and how to translate events into the CloudEvent format. Kubernetes controller Manages the event source and reconciles underlying receive adapter deployments. Custom resource definition (CRD) Provides the configuration that the controller uses to manage the receive adapter. Using the sample source \u00b6 The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. For more information on using the sample source, see the documentation . Additional resources \u00b6 Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD.","title":"\u521b\u5efa\u4e8b\u4ef6\u6e90\u6982\u8ff0"},{"location":"eventing/custom-event-source/custom-event-source/#create-a-custom-event-source","text":"If you want to create a custom event source for a specific event producer type, you must create the components that enable forwarding events from that producer type to a Knative sink. This type of integration requires more effort than using some simpler integration types, such as SinkBinding or ContainerSource ; however, this provides the most polished result and is the easiest integration type for users to consume. By providing a custom resource definition (CRD) for your source rather than a general container definition, it is easier to expose meaningful configuration options and documentation to users and hide implementation details. Note If you have created a new event source type that is not a part of the core Knative project, you can open a pull request to add it to the list of Third-Party Sources , and announce the new source in one of the Knative Slack channels. You can also add your event source to the knative-sandbox organization, by following the instructions to create a sandbox repository .","title":"Create a custom event source"},{"location":"eventing/custom-event-source/custom-event-source/#required-components","text":"To create a custom event source, you must create the following components: Component Description Receive adapter Contains logic that specifies how to get events from a producer, what the sink URI is, and how to translate events into the CloudEvent format. Kubernetes controller Manages the event source and reconciles underlying receive adapter deployments. Custom resource definition (CRD) Provides the configuration that the controller uses to manage the receive adapter.","title":"Required components"},{"location":"eventing/custom-event-source/custom-event-source/#using-the-sample-source","text":"The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. For more information on using the sample source, see the documentation .","title":"Using the sample source"},{"location":"eventing/custom-event-source/custom-event-source/#additional-resources","text":"Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD.","title":"Additional resources"},{"location":"eventing/custom-event-source/custom-event-source/controller/","text":"Create a controller \u00b6 You can use the sample repository update-codegen.sh script to generate and inject the required components (the clientset , cache , informers , and listers ) into your custom controller. Example controller: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... } Procedure \u00b6 Generate the components by running the command: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Inject the components by running the command: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Pass the new controller implementation to the sharedmain method: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } A configmap.Watcher and a context, which the injected listers use for the reconciler struct arguments, are passed to this implementation. Import the base reconciler from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure that the event handlers are being filtered to the correct informers: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Ensure that informers are configured correctly for the secondary resources used by the sample source to deploy and bind the event source and the receive adapter: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"\u521b\u5efa\u4e00\u4e2a\u63a7\u5236\u5668"},{"location":"eventing/custom-event-source/custom-event-source/controller/#create-a-controller","text":"You can use the sample repository update-codegen.sh script to generate and inject the required components (the clientset , cache , informers , and listers ) into your custom controller. Example controller: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... }","title":"Create a controller"},{"location":"eventing/custom-event-source/custom-event-source/controller/#procedure","text":"Generate the components by running the command: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Inject the components by running the command: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Pass the new controller implementation to the sharedmain method: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } A configmap.Watcher and a context, which the injected listers use for the reconciler struct arguments, are passed to this implementation. Import the base reconciler from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure that the event handlers are being filtered to the correct informers: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Ensure that informers are configured correctly for the secondary resources used by the sample source to deploy and bind the event source and the receive adapter: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"Procedure"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/","text":"Publish an event source to your cluster \u00b6 Start a minikube cluster: minikube start Setup ko to use the minikube docker instance and local registry: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration YAML: ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"\u5c06\u4e8b\u4ef6\u6e90\u53d1\u5e03\u5230\u96c6\u7fa4"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/#publish-an-event-source-to-your-cluster","text":"Start a minikube cluster: minikube start Setup ko to use the minikube docker instance and local registry: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration YAML: ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Publish an event source to your cluster"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/","text":"Create a receive adapter \u00b6 As part of the source reconciliation process, you must create and deploy the underlying receive adapter. The receive adapter requires an injection-based main method that is located in cmd/receiver_adapter/main.go : // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } The receive adapter's pkg implementation consists of two main functions: A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter is passed the CloudEvents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In the case of the sample source: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } A Start function, implemented as an interface to the adapter struct : func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the adapter. Otherwise, the role of the function is to process the next event. In the case of the sample-source , this function creates a CloudEvent to forward to the specified sink every X interval, as specified by the EnvConfigAccessor parameter, which is loaded by the resource YAML: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } } Managing the Receive Adapter in the Controller \u00b6 Update the ObservedGeneration and initialize the Status conditions, as defined in the samplesource_lifecycle.go and samplesource_types.go files: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation Create a receive adapter. Verify that the specified Kubernetes resources are valid, and update the Status accordingly. Assemble the ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note The exact arguments may change based on functional requirements. Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment. Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing receive adapter deployment, create one: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected spec is different from the existing spec, and update the deployment if required: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) If successful, update the Status and MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) Create a SinkBinding to bind the receive adapter with the sink. Create a Reference for the receive adapter deployment. This deployment is the SinkBinding's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing SinkBinding, create one: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected spec is different to the existing spec, and update the SinkBinding if required: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink with the result: src . Status . MarkSink ( sb . Status . SinkURI ) Return a new reconciler event stating that the process is done: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"\u521b\u5efa\u63a5\u6536\u9002\u914d\u5668"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#create-a-receive-adapter","text":"As part of the source reconciliation process, you must create and deploy the underlying receive adapter. The receive adapter requires an injection-based main method that is located in cmd/receiver_adapter/main.go : // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } The receive adapter's pkg implementation consists of two main functions: A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter is passed the CloudEvents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In the case of the sample source: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } A Start function, implemented as an interface to the adapter struct : func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the adapter. Otherwise, the role of the function is to process the next event. In the case of the sample-source , this function creates a CloudEvent to forward to the specified sink every X interval, as specified by the EnvConfigAccessor parameter, which is loaded by the resource YAML: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Create a receive adapter"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#managing-the-receive-adapter-in-the-controller","text":"Update the ObservedGeneration and initialize the Status conditions, as defined in the samplesource_lifecycle.go and samplesource_types.go files: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation Create a receive adapter. Verify that the specified Kubernetes resources are valid, and update the Status accordingly. Assemble the ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note The exact arguments may change based on functional requirements. Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment. Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing receive adapter deployment, create one: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected spec is different from the existing spec, and update the deployment if required: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) If successful, update the Status and MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) Create a SinkBinding to bind the receive adapter with the sink. Create a Reference for the receive adapter deployment. This deployment is the SinkBinding's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing SinkBinding, create one: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected spec is different to the existing spec, and update the SinkBinding if required: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink with the result: src . Status . MarkSink ( sb . Status . SinkURI ) Return a new reconciler event stating that the process is done: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"Managing the Receive Adapter in the Controller"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/","text":"Using the Knative sample repository \u00b6 The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. Prerequisites \u00b6 You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. You have cloned the sample-source repository . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube or kind . Sample files overview \u00b6 Receiver adapter files \u00b6 cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents. Controller files \u00b6 cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method. CRD files \u00b6 pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file. Reconciler files \u00b6 pkg/reconciler/sample/samplesource.go - The reconciliation functions for the receive adapter. pkg/apis/samples/VERSION/samplesource_lifecycle.go - Contains status information for the event source\u2019s reconciliation details: Source ready Sink provided Deployed EventType provided Kubernetes resources correct Procedure \u00b6 Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go . This includes the fields that are required in the resource YAML, as well as those referenced in the controller using the source\u2019s clientset and API: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle to be reflected in the status and SinkURI fields: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Set the lifecycle conditions by defining the functions to be called from the reconciler functions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"\u914d\u7f6e\u793a\u4f8b"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#using-the-knative-sample-repository","text":"The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter.","title":"Using the Knative sample repository"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#prerequisites","text":"You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. You have cloned the sample-source repository . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube or kind .","title":"Prerequisites"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#sample-files-overview","text":"","title":"Sample files overview"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#receiver-adapter-files","text":"cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents.","title":"Receiver adapter files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#controller-files","text":"cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method.","title":"Controller files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#crd-files","text":"pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file.","title":"CRD files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#reconciler-files","text":"pkg/reconciler/sample/samplesource.go - The reconciliation functions for the receive adapter. pkg/apis/samples/VERSION/samplesource_lifecycle.go - Contains status information for the event source\u2019s reconciliation details: Source ready Sink provided Deployed EventType provided Kubernetes resources correct","title":"Reconciler files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#procedure","text":"Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go . This includes the fields that are required in the resource YAML, as well as those referenced in the controller using the source\u2019s clientset and API: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle to be reflected in the status and SinkURI fields: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Set the lifecycle conditions by defining the functions to be called from the reconciler functions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"Procedure"},{"location":"eventing/custom-event-source/sinkbinding/","text":"SinkBinding \u00b6 The SinkBinding object supports decoupling event production from delivery addressing. You can use sink binding to direct a subject to a sink. A subject is a Kubernetes resource that embeds a PodSpec template and produces events. A sink is an addressable Kubernetes object that can receive events. The SinkBinding object injects environment variables into the PodTemplateSpec of the sink. Because of this, the application code does not need to interact directly with the Kubernetes API to locate the event destination. These environment variables are as follows: K_SINK - The URL of the resolved sink. K_CE_OVERRIDES - A JSON object that specifies overrides to the outbound event.","title":"\u5173\u4e8e SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/#sinkbinding","text":"The SinkBinding object supports decoupling event production from delivery addressing. You can use sink binding to direct a subject to a sink. A subject is a Kubernetes resource that embeds a PodSpec template and produces events. A sink is an addressable Kubernetes object that can receive events. The SinkBinding object injects environment variables into the PodTemplateSpec of the sink. Because of this, the application code does not need to interact directly with the Kubernetes API to locate the event destination. These environment variables are as follows: K_SINK - The URL of the resolved sink. K_CE_OVERRIDES - A JSON object that specifies overrides to the outbound event.","title":"SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/","text":"Create a SinkBinding \u00b6 This topic describes how to create a SinkBinding object. SinkBinding resolves a sink as a URI, sets the URI in the environment variable K_SINK , and adds the URI to a subject using K_SINK . If the URI changes, SinkBinding updates the value of K_SINK . In the following examples, the sink is a Knative Service and the subject is a CronJob. If you have an existing subject and sink, you can replace the examples with your own values. Before you begin \u00b6 Before you can create a SinkBinding object: You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with SinkBinding, install the kn CLI. Optional: Choose SinkBinding namespace selection behavior \u00b6 The SinkBinding object operates in one of two modes: exclusion or inclusion . The default mode is exclusion . In exclusion mode, SinkBinding behavior is enabled for the namespace by default. To disallow a namespace from being evaluated for mutation you must exclude it using the label bindings.knative.dev/exclude: true . In inclusion mode, SinkBinding behavior is not enabled for the namespace. Before a namespace can be evaluated for mutation, you must explicitly include it using the label bindings.knative.dev/include: true . To set the SinkBinding object to inclusion mode: Change the value of SINK_BINDING_SELECTION_MODE from exclusion to inclusion by running: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion To verify that SINK_BINDING_SELECTION_MODE is set as desired, run: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING Create a namespace \u00b6 If you do not have an existing namespace, create a namespace for the SinkBinding object: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your SinkBinding to use. For example, sinkbinding-example . Note If you have selected inclusion mode, you must add the bindings.knative.dev/include: true label to the namespace to enable SinkBinding behavior. Create a sink \u00b6 The sink can be any addressable Kubernetes object that can receive events. If you do not have an existing sink that you want to connect to the SinkBinding object, create a Knative service. Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML Create a Knative service by running: kn service create <app-name> --image <image-url> Where: <app-name> is the name of the application. <image-url> is the URL of the image container. For example: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file for the Knative service using the following template: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> is the name of the application. For example, event-display . <image-url> is the URL of the image container. For example, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a subject \u00b6 The subject must be a PodSpecable resource. You can use any PodSpecable resource in your cluster, for example: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev If you do not have an existing PodSpecable subject that you want to use, you can use the following sample to create a CronJob object as the subject. The following CronJob makes a single cloud event that targets K_SINK and adds any extra overrides given by CE_OVERRIDES . Create a YAML file for the CronJob using the following example: apiVersion : batch/v1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a SinkBinding object \u00b6 Create a SinkBinding object that directs events from your subject to the sink. kn YAML Create a SinkBinding object by running: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> is the name of the SinkBinding object you want to create. <namespace> is the namespace you created for your SinkBinding to use. <subject> is the subject to connect. Examples: Job:batch/v1:app=heartbeat-cron matches all jobs in namespace with label app=heartbeat-cron . Deployment:apps/v1:myapp matches a deployment called myapp in the namespace. Service:serving.knative.dev/v1:hello matches the service called hello . <sink> is the sink to connect. For example http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> in the form key=value . Cloud Event overrides control the output format and modifications of the event sent to the sink and are applied before sending the event. You can provide this flag multiple times. For a list of available options, see the Knative client documentation . For example: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" Create a YAML file for the SinkBinding object using the following template: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> is the name of the SinkBinding object you want to create. For example, bind-heartbeat . <api-version> is the API version of the subject. For example batch/v1 . <kind> is the Kind of your subject. For example Job . <label-key>: <label-value> is a map of key-value pairs to select subjects that have a matching label. For example, app: heartbeat-cron selects any subject with the label app=heartbeat-cron . <sink> is the sink to connect. For example event-display . For more information about the fields you can configure for the SinkBinding object, see Sink Binding Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the SinkBinding object \u00b6 Verify that a message was sent to the Knative eventing system by looking at the service logs for your sink: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m From the output, observe the lines showing the request headers and body of the event message, sent by the source to the display function. For example: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" } Delete a SinkBinding \u00b6 To delete the SinkBinding object and all of the related resources in the namespace, delete the namespace by running: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that contains the SinkBinding object.","title":"\u521b\u5efa SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sinkbinding","text":"This topic describes how to create a SinkBinding object. SinkBinding resolves a sink as a URI, sets the URI in the environment variable K_SINK , and adds the URI to a subject using K_SINK . If the URI changes, SinkBinding updates the value of K_SINK . In the following examples, the sink is a Knative Service and the subject is a CronJob. If you have an existing subject and sink, you can replace the examples with your own values.","title":"Create a SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#before-you-begin","text":"Before you can create a SinkBinding object: You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with SinkBinding, install the kn CLI.","title":"Before you begin"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#optional-choose-sinkbinding-namespace-selection-behavior","text":"The SinkBinding object operates in one of two modes: exclusion or inclusion . The default mode is exclusion . In exclusion mode, SinkBinding behavior is enabled for the namespace by default. To disallow a namespace from being evaluated for mutation you must exclude it using the label bindings.knative.dev/exclude: true . In inclusion mode, SinkBinding behavior is not enabled for the namespace. Before a namespace can be evaluated for mutation, you must explicitly include it using the label bindings.knative.dev/include: true . To set the SinkBinding object to inclusion mode: Change the value of SINK_BINDING_SELECTION_MODE from exclusion to inclusion by running: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion To verify that SINK_BINDING_SELECTION_MODE is set as desired, run: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING","title":"Optional: Choose SinkBinding namespace selection behavior"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-namespace","text":"If you do not have an existing namespace, create a namespace for the SinkBinding object: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your SinkBinding to use. For example, sinkbinding-example . Note If you have selected inclusion mode, you must add the bindings.knative.dev/include: true label to the namespace to enable SinkBinding behavior.","title":"Create a namespace"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sink","text":"The sink can be any addressable Kubernetes object that can receive events. If you do not have an existing sink that you want to connect to the SinkBinding object, create a Knative service. Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML Create a Knative service by running: kn service create <app-name> --image <image-url> Where: <app-name> is the name of the application. <image-url> is the URL of the image container. For example: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file for the Knative service using the following template: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> is the name of the application. For example, event-display . <image-url> is the URL of the image container. For example, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a sink"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-subject","text":"The subject must be a PodSpecable resource. You can use any PodSpecable resource in your cluster, for example: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev If you do not have an existing PodSpecable subject that you want to use, you can use the following sample to create a CronJob object as the subject. The following CronJob makes a single cloud event that targets K_SINK and adds any extra overrides given by CE_OVERRIDES . Create a YAML file for the CronJob using the following example: apiVersion : batch/v1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a subject"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sinkbinding-object","text":"Create a SinkBinding object that directs events from your subject to the sink. kn YAML Create a SinkBinding object by running: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> is the name of the SinkBinding object you want to create. <namespace> is the namespace you created for your SinkBinding to use. <subject> is the subject to connect. Examples: Job:batch/v1:app=heartbeat-cron matches all jobs in namespace with label app=heartbeat-cron . Deployment:apps/v1:myapp matches a deployment called myapp in the namespace. Service:serving.knative.dev/v1:hello matches the service called hello . <sink> is the sink to connect. For example http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> in the form key=value . Cloud Event overrides control the output format and modifications of the event sent to the sink and are applied before sending the event. You can provide this flag multiple times. For a list of available options, see the Knative client documentation . For example: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" Create a YAML file for the SinkBinding object using the following template: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> is the name of the SinkBinding object you want to create. For example, bind-heartbeat . <api-version> is the API version of the subject. For example batch/v1 . <kind> is the Kind of your subject. For example Job . <label-key>: <label-value> is a map of key-value pairs to select subjects that have a matching label. For example, app: heartbeat-cron selects any subject with the label app=heartbeat-cron . <sink> is the sink to connect. For example event-display . For more information about the fields you can configure for the SinkBinding object, see Sink Binding Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a SinkBinding object"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#verify-the-sinkbinding-object","text":"Verify that a message was sent to the Knative eventing system by looking at the service logs for your sink: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m From the output, observe the lines showing the request headers and body of the event message, sent by the source to the display function. For example: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" }","title":"Verify the SinkBinding object"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#delete-a-sinkbinding","text":"To delete the SinkBinding object and all of the related resources in the namespace, delete the namespace by running: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that contains the SinkBinding object.","title":"Delete a SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/reference/","text":"SinkBinding reference \u00b6 This topic provides reference information about the configurable parameters for SinkBinding objects. Supported parameters \u00b6 A SinkBinding resource supports the following parameters: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a SinkBinding object. Required metadata Specifies metadata that uniquely identifies the SinkBinding object. For example, a name . Required spec Specifies the configuration information for this SinkBinding object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.subject A reference to the resources for which the \"runtime contract\" is augmented by Binding implementations. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional Subject parameter \u00b6 The Subject parameter references the resources for which the \"runtime contract\" is augmented by Binding implementations. A subject definition supports the following fields: Field Description Required or optional apiVersion API version of the referent. Required kind Kind of the referent. Required namespace Namespace of the referent. If omitted, this defaults to the object holding it. Optional name Name of the referent. Do not use if you configure selector . selector Selector of the referents. Do not use if you configure name . selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels Subject parameter examples \u00b6 Given the following YAML, the Deployment named mysubject in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... Given the following YAML, any Job with the label working=example in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job namespace : default selector : matchLabels : working : example ... Given the following YAML, any Pod with the label working=example OR working=sample in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ... CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. CloudEvent Overrides example \u00b6 apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"SinkBinding \u53c2\u8003"},{"location":"eventing/custom-event-source/sinkbinding/reference/#sinkbinding-reference","text":"This topic provides reference information about the configurable parameters for SinkBinding objects.","title":"SinkBinding reference"},{"location":"eventing/custom-event-source/sinkbinding/reference/#supported-parameters","text":"A SinkBinding resource supports the following parameters: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a SinkBinding object. Required metadata Specifies metadata that uniquely identifies the SinkBinding object. For example, a name . Required spec Specifies the configuration information for this SinkBinding object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.subject A reference to the resources for which the \"runtime contract\" is augmented by Binding implementations. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"Supported parameters"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter","text":"The Subject parameter references the resources for which the \"runtime contract\" is augmented by Binding implementations. A subject definition supports the following fields: Field Description Required or optional apiVersion API version of the referent. Required kind Kind of the referent. Required namespace Namespace of the referent. If omitted, this defaults to the object holding it. Optional name Name of the referent. Do not use if you configure selector . selector Selector of the referents. Do not use if you configure name . selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels","title":"Subject parameter"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter-examples","text":"Given the following YAML, the Deployment named mysubject in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... Given the following YAML, any Job with the label working=example in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job namespace : default selector : matchLabels : working : example ... Given the following YAML, any Pod with the label working=example OR working=sample in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ...","title":"Subject parameter examples"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides-example","text":"apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"CloudEvent Overrides example"},{"location":"eventing/experimental-features/","text":"Eventing experimental features \u00b6 To keep Knative innovative, the maintainers of this project have developed an experimental features process that allows new, experimental features to be delivered and tested by users without affecting the stability of the core project. Warning Experimental features are unstable and may cause issues in your Knative setup or even your cluster setup. These features should be used with caution, and should never be tested on a production environment. For more information about quality guarantees for features at different stages of development, see the Feature stage definition documentation. This document explains how to enable experimental features and which ones are available today. Before you begin \u00b6 You must have a Knative cluster running with Knative Eventing installed. Experimental features configuration \u00b6 When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled Available experimental features \u00b6 The following table gives an overview of the available experimental features in Knative Eventing: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter Specify a maximum retry duration that overrides HTTP Retry-After headers when calculating backoff times for retrying 429 and 503 responses. Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout When using the delivery spec to configure event delivery parameters, you can use the timeout field to specify the timeout for each sent HTTP request. Alpha, disabled by default KReference.Group field kreference-group Specify the API group of KReference resources without the API version. Alpha, disabled by default Knative reference mapping kreference-mapping Provide mappings from a Knative reference to a templated URI. Alpha, disabled by default New trigger filters new-trigger-filters Enables a new Trigger filters field that supports a set of powerful filter expressions. Alpha, disabled by default Strict Subscriber strict-subscriber Invalidates Subscriptions if the field spec.subscriber is not defined. Alpha, disabled by default","title":"\u5173\u4e8eevent\u5b9e\u9a8c\u529f\u80fd"},{"location":"eventing/experimental-features/#eventing-experimental-features","text":"To keep Knative innovative, the maintainers of this project have developed an experimental features process that allows new, experimental features to be delivered and tested by users without affecting the stability of the core project. Warning Experimental features are unstable and may cause issues in your Knative setup or even your cluster setup. These features should be used with caution, and should never be tested on a production environment. For more information about quality guarantees for features at different stages of development, see the Feature stage definition documentation. This document explains how to enable experimental features and which ones are available today.","title":"Eventing experimental features"},{"location":"eventing/experimental-features/#before-you-begin","text":"You must have a Knative cluster running with Knative Eventing installed.","title":"Before you begin"},{"location":"eventing/experimental-features/#experimental-features-configuration","text":"When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled","title":"Experimental features configuration"},{"location":"eventing/experimental-features/#available-experimental-features","text":"The following table gives an overview of the available experimental features in Knative Eventing: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter Specify a maximum retry duration that overrides HTTP Retry-After headers when calculating backoff times for retrying 429 and 503 responses. Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout When using the delivery spec to configure event delivery parameters, you can use the timeout field to specify the timeout for each sent HTTP request. Alpha, disabled by default KReference.Group field kreference-group Specify the API group of KReference resources without the API version. Alpha, disabled by default Knative reference mapping kreference-mapping Provide mappings from a Knative reference to a templated URI. Alpha, disabled by default New trigger filters new-trigger-filters Enables a new Trigger filters field that supports a set of powerful filter expressions. Alpha, disabled by default Strict Subscriber strict-subscriber Invalidates Subscriptions if the field spec.subscriber is not defined. Alpha, disabled by default","title":"Available experimental features"},{"location":"eventing/experimental-features/delivery-retryafter/","text":"DeliverySpec.RetryAfterMax field \u00b6 Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-retryafter/#deliveryspecretryaftermax-field","text":"Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-timeout/","text":"DeliverySpec.Timeout field \u00b6 Flag name : delivery-timeout Stage : Beta, enabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/delivery-timeout/#deliveryspectimeout-field","text":"Flag name : delivery-timeout Stage : Beta, enabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/kreference-group/","text":"KReference.Group field \u00b6 Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-group/#kreferencegroup-field","text":"Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-mapping/","text":"Knative reference mapping \u00b6 Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative\u5f15\u7528\u6620\u5c04"},{"location":"eventing/experimental-features/kreference-mapping/#knative-reference-mapping","text":"Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative reference mapping"},{"location":"eventing/experimental-features/new-trigger-filters/","text":"New trigger filters \u00b6 Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204 Overview \u00b6 This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service About the filters field \u00b6 An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression. Supported filter dialects \u00b6 The filters field supports the following dialects: exact \u00b6 CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push prefix \u00b6 CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github. suffix \u00b6 CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created all \u00b6 All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec any \u00b6 At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec not \u00b6 The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push cesql \u00b6 The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" Conflict with the current filter field \u00b6 The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service FAQ \u00b6 Why add yet another field? Why not make the current filter field more robust? \u00b6 The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field. Why filters and not another name that wouldn't conflict with the filter field? \u00b6 We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"\u65b0\u89e6\u53d1\u8fc7\u6ee4\u5668"},{"location":"eventing/experimental-features/new-trigger-filters/#new-trigger-filters","text":"Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204","title":"New trigger filters"},{"location":"eventing/experimental-features/new-trigger-filters/#overview","text":"This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Overview"},{"location":"eventing/experimental-features/new-trigger-filters/#about-the-filters-field","text":"An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression.","title":"About the filters field"},{"location":"eventing/experimental-features/new-trigger-filters/#supported-filter-dialects","text":"The filters field supports the following dialects:","title":"Supported filter dialects"},{"location":"eventing/experimental-features/new-trigger-filters/#exact","text":"CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push","title":"exact"},{"location":"eventing/experimental-features/new-trigger-filters/#prefix","text":"CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github.","title":"prefix"},{"location":"eventing/experimental-features/new-trigger-filters/#suffix","text":"CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created","title":"suffix"},{"location":"eventing/experimental-features/new-trigger-filters/#all","text":"All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"all"},{"location":"eventing/experimental-features/new-trigger-filters/#any","text":"At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"any"},{"location":"eventing/experimental-features/new-trigger-filters/#not","text":"The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push","title":"not"},{"location":"eventing/experimental-features/new-trigger-filters/#cesql","text":"The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"","title":"cesql"},{"location":"eventing/experimental-features/new-trigger-filters/#conflict-with-the-current-filter-field","text":"The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Conflict with the current filter field"},{"location":"eventing/experimental-features/new-trigger-filters/#faq","text":"","title":"FAQ"},{"location":"eventing/experimental-features/new-trigger-filters/#why-add-yet-another-field-why-not-make-the-current-filter-field-more-robust","text":"The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field.","title":"Why add yet another field? Why not make the current filter field more robust?"},{"location":"eventing/experimental-features/new-trigger-filters/#why-filters-and-not-another-name-that-wouldnt-conflict-with-the-filter-field","text":"We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"Why filters and not another name that wouldn't conflict with the filter field?"},{"location":"eventing/experimental-features/strict-subscriber/","text":"Strict Subscriber \u00b6 Flag name : strict-subscriber Stage : Beta, enabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"\u4e25\u683c\u7684\u7528\u6237"},{"location":"eventing/experimental-features/strict-subscriber/#strict-subscriber","text":"Flag name : strict-subscriber Stage : Beta, enabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"Strict Subscriber"},{"location":"eventing/flows/","text":"Eventing Flows \u00b6 Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"\u5173\u4e8e\u6d41"},{"location":"eventing/flows/#eventing-flows","text":"Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"Eventing Flows"},{"location":"eventing/flows/parallel/","text":"Parallel \u00b6 Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood. Usage \u00b6 Parallel Spec \u00b6 Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object. (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object. Parallel Status \u00b6 Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ). Examples \u00b6 Learn how to use Parallel by following the code samples .","title":"\u5e73\u884c"},{"location":"eventing/flows/parallel/#parallel","text":"Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood.","title":"Parallel"},{"location":"eventing/flows/parallel/#usage","text":"","title":"Usage"},{"location":"eventing/flows/parallel/#parallel-spec","text":"Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object. (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object.","title":"Parallel Spec"},{"location":"eventing/flows/parallel/#parallel-status","text":"Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ).","title":"Parallel Status"},{"location":"eventing/flows/parallel/#examples","text":"Learn how to use Parallel by following the code samples .","title":"Examples"},{"location":"eventing/flows/sequence/","text":"Sequence \u00b6 Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Info Sequence needs \"hairpin\" traffic. Please verify that your pod can reach itself via the service IP. If the \"hairpin\" traffic is not available, you can reach out to your cluster administrator since its a cluster level (typically CNI) setting. Usage \u00b6 Sequence Spec \u00b6 Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to. Sequence Status \u00b6 Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence. Examples \u00b6 For each of the following examples, you use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage. Sequence with no reply \u00b6 For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. See Sequence with no reply (terminal last Step) . Sequence with reply \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. See Sequence with reply (last Step produces output) . Chaining Sequences together \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). See Chaining Sequences together . Using Sequence with Broker/Trigger model \u00b6 You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Triggers. See Using Sequence with Broker/Trigger model .","title":"\u5173\u4e8e\u5e8f\u5217"},{"location":"eventing/flows/sequence/#sequence","text":"Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Info Sequence needs \"hairpin\" traffic. Please verify that your pod can reach itself via the service IP. If the \"hairpin\" traffic is not available, you can reach out to your cluster administrator since its a cluster level (typically CNI) setting.","title":"Sequence"},{"location":"eventing/flows/sequence/#usage","text":"","title":"Usage"},{"location":"eventing/flows/sequence/#sequence-spec","text":"Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to.","title":"Sequence Spec"},{"location":"eventing/flows/sequence/#sequence-status","text":"Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence.","title":"Sequence Status"},{"location":"eventing/flows/sequence/#examples","text":"For each of the following examples, you use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage.","title":"Examples"},{"location":"eventing/flows/sequence/#sequence-with-no-reply","text":"For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. See Sequence with no reply (terminal last Step) .","title":"Sequence with no reply"},{"location":"eventing/flows/sequence/#sequence-with-reply","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. See Sequence with reply (last Step produces output) .","title":"Sequence with reply"},{"location":"eventing/flows/sequence/#chaining-sequences-together","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). See Chaining Sequences together .","title":"Chaining Sequences together"},{"location":"eventing/flows/sequence/#using-sequence-with-brokertrigger-model","text":"You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Triggers. See Using Sequence with Broker/Trigger model .","title":"Using Sequence with Broker/Trigger model"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/","text":"Sequence wired to event-display \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources to be created: kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"\u663e\u793a\u987a\u5e8f\u8f93\u51fa"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#sequence-wired-to-event-display","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to event-display"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-knative-services","text":"Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources to be created: kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/","text":"Sequence wired to another Sequence \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml Create the first Sequence \u00b6 The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./sequence1.yaml Create the second Sequence \u00b6 The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the first Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"\u4f7f\u7528\u5e8f\u5217"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#sequence-wired-to-another-sequence","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to another Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-knative-services","text":"Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-first-sequence","text":"The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./sequence1.yaml","title":"Create the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-second-sequence","text":"The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml","title":"Create the second Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-pingsource-targeting-the-first-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-terminal/","text":"Sequence terminal \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"\u521b\u5efa\u989d\u5916\u7684\u4e8b\u4ef6"},{"location":"eventing/flows/sequence/sequence-terminal/#sequence-terminal","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence terminal"},{"location":"eventing/flows/sequence/sequence-terminal/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-terminal/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-knative-services","text":"First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/","text":"Using Sequence with Broker and Trigger \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events. Prerequisites \u00b6 Knative Serving InMemoryChannel Note The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Setup \u00b6 Creating the Broker \u00b6 To create the cluster default Broker type, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the Knative Services \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change default in the following command to create the services in the namespace where you have configured your broker: kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change default in the following command to create the sequence in the namespace where you have configured your broker: kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Broker \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change default in the following command to create the PingSource in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./ping-source.yaml Create the Trigger targeting the Sequence \u00b6 apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change default in the following command to create the trigger in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./trigger.yaml Create the Service and Trigger displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default in the following command to create the service and trigger in the namespace where you have configured your broker: kubectl -n default create -f ./display-trigger.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"\u4e0e\u4ee3\u7406\u548c\u89e6\u53d1\u5668\u4e00\u8d77\u4f7f\u7528"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#using-sequence-with-broker-and-trigger","text":"We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events.","title":"Using Sequence with Broker and Trigger"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#prerequisites","text":"Knative Serving InMemoryChannel Note The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#creating-the-broker","text":"To create the cluster default Broker type, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Creating the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-knative-services","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change default in the following command to create the services in the namespace where you have configured your broker: kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change default in the following command to create the sequence in the namespace where you have configured your broker: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-pingsource-targeting-the-broker","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change default in the following command to create the PingSource in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-trigger-targeting-the-sequence","text":"apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change default in the following command to create the trigger in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./trigger.yaml","title":"Create the Trigger targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-service-and-trigger-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default in the following command to create the service and trigger in the namespace where you have configured your broker: kubectl -n default create -f ./display-trigger.yaml","title":"Create the Service and Trigger displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/observability/logging/collecting-logs/","text":"\u65e5\u5fd7 \u00b6 You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders. \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6 \u00b6 Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath. \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. \u8fc7\u7a0b \u00b6 Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 \u5efa\u7acb\u8d27\u4ee3 \u00b6 See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True \u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668 \u00b6 Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in. Docker \u684c\u9762 \u00b6 Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"\u6536\u96c6\u65e5\u5fd7"},{"location":"eventing/observability/logging/collecting-logs/#_1","text":"You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.","title":"\u65e5\u5fd7"},{"location":"eventing/observability/logging/collecting-logs/#_2","text":"Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.","title":"\u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6"},{"location":"eventing/observability/logging/collecting-logs/#_3","text":"The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"eventing/observability/logging/collecting-logs/#_4","text":"Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"\u8fc7\u7a0b"},{"location":"eventing/observability/logging/collecting-logs/#_5","text":"See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"\u5efa\u7acb\u8d27\u4ee3"},{"location":"eventing/observability/logging/collecting-logs/#_6","text":"Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"\u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668"},{"location":"eventing/observability/logging/collecting-logs/#kind","text":"When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in.","title":"Kind"},{"location":"eventing/observability/logging/collecting-logs/#docker","text":"Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker \u684c\u9762"},{"location":"eventing/observability/logging/collecting-logs/#minikube","text":"Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Minikube"},{"location":"eventing/observability/logging/config-logging/","text":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e \u00b6 Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bb0\u5f55"},{"location":"eventing/observability/logging/config-logging/#_1","text":"Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e"},{"location":"eventing/observability/metrics/collecting-metrics/","text":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf \u00b6 Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics. \u5173\u4e8e Prometheus \u00b6 Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used. \u914d\u7f6e Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml \u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . \u75ca\u6108 OpenTelemetry \u00b6 OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries. \u7406\u89e3\u6536\u96c6\u5668 \u00b6 The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector. \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' \u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u6536\u96c6\u5ea6\u91cf\u6807\u51c6"},{"location":"eventing/observability/metrics/collecting-metrics/#knative","text":"Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics.","title":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus","text":"Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.","title":"\u5173\u4e8e Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus_1","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"\u914d\u7f6e Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#prometheus_2","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"\u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b"},{"location":"eventing/observability/metrics/collecting-metrics/#opentelemetry","text":"OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.","title":"\u75ca\u6108 OpenTelemetry"},{"location":"eventing/observability/metrics/collecting-metrics/#_1","text":"The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.","title":"\u7406\u89e3\u6536\u96c6\u5668"},{"location":"eventing/observability/metrics/collecting-metrics/#_2","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"eventing/observability/metrics/collecting-metrics/#_3","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e"},{"location":"eventing/observability/metrics/eventing-metrics/","text":"Knative Eventing metrics \u00b6 Administrators can view metrics for Knative Eventing components. Broker - Ingress \u00b6 Use the following metrics to debug how broker ingress performs and what events are dispatched via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable Broker - Filter \u00b6 Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable In-memory Dispatcher \u00b6 In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section . Eventing sources \u00b6 Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Knative\u4e8b\u4ef6\u6307\u6807"},{"location":"eventing/observability/metrics/eventing-metrics/#knative-eventing-metrics","text":"Administrators can view metrics for Knative Eventing components.","title":"Knative Eventing metrics"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-ingress","text":"Use the following metrics to debug how broker ingress performs and what events are dispatched via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable","title":"Broker - Ingress"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-filter","text":"Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable","title":"Broker - Filter"},{"location":"eventing/observability/metrics/eventing-metrics/#in-memory-dispatcher","text":"In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section .","title":"In-memory Dispatcher"},{"location":"eventing/observability/metrics/eventing-metrics/#eventing-sources","text":"Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Eventing sources"},{"location":"eventing/reference/eventing-api/","text":"This file is updated to the correct version from the eventing repo (docs/eventing-api.md) during the build.","title":"Eventing API"},{"location":"eventing/sinks/","text":"About sinks \u00b6 When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources. Knative Services, Channels, and Brokers are all examples of sinks. Addressable objects receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed. Sink as a parameter \u00b6 Sink is used as a reference to an object that resolves to a URI to use as the sink. A sink definition supports the following fields: Field Description Required or optional ref This points to an Addressable. Required if not using uri ref.apiVersion API version of the referent. Required if using ref ref.kind Kind of the referent. Required if using ref ref.namespace Namespace of the referent. If omitted this defaults to the object holding it. Optional ref.name Name of the referent. Required if using ref uri This can be an absolute URL with a non-empty scheme and non-empty host that points to the target or a relative URI. Relative URIs are resolved using the base URI retrieved from Ref. Required if not using ref Note At least one of ref or uri is required. If both are specified, uri is resolved into the URL from the Addressable ref result. Sink parameter example \u00b6 Given the following YAML, if ref resolves into \"http://mysink.default.svc.cluster.local\" , then uri is added to this resulting in \"http://mysink.default.svc.cluster.local/extra/path\" . apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract This results in the K_SINK environment variable being set on the subject as \"http://mysink.default.svc.cluster.local/extra/path\" . Using custom resources as sinks \u00b6 To use a Kubernetes custom resource (CR) as a sink for events, you must: Make the CR Addressable. You must ensure that the CR contains a status.address.url . For more information, see the spec for Addressable resources . Create an Addressable-resolver ClusterRole to obtain the necessary RBAC rules for the sink to receive events. For example, you can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to KafkaSink objects and statuses: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch Filtering events sent to sinks by using Triggers \u00b6 You can connect a Trigger to a sink, so that events are filtered before they are sent to the sink. A sink that is connected to a Trigger is configured as a subscriber in the Trigger resource spec. For example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> is the name of the Trigger being connected to the sink. <kafka-sink-name> is the name of a KafkaSink object. Specifying sinks using the kn CLI --sink flag \u00b6 When you create an event-producing CR by using the Knative ( kn ) CLI, you can specify a sink where events are sent to from that resource, by using the --sink flag. The following example creates a SinkBinding that uses a Service, http://event-display.svc.cluster.local , as the sink: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" The svc in http://event-display.svc.cluster.local determines that the sink is a Knative Service. Other default sink prefixes include Channel and Broker. Tip You can configure which resources can be used with the --sink flag for kn CLI commands by customizing kn . Supported third-party sink types \u00b6 Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"\u5173\u4e8e sinks"},{"location":"eventing/sinks/#about-sinks","text":"When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources. Knative Services, Channels, and Brokers are all examples of sinks. Addressable objects receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed.","title":"About sinks"},{"location":"eventing/sinks/#sink-as-a-parameter","text":"Sink is used as a reference to an object that resolves to a URI to use as the sink. A sink definition supports the following fields: Field Description Required or optional ref This points to an Addressable. Required if not using uri ref.apiVersion API version of the referent. Required if using ref ref.kind Kind of the referent. Required if using ref ref.namespace Namespace of the referent. If omitted this defaults to the object holding it. Optional ref.name Name of the referent. Required if using ref uri This can be an absolute URL with a non-empty scheme and non-empty host that points to the target or a relative URI. Relative URIs are resolved using the base URI retrieved from Ref. Required if not using ref Note At least one of ref or uri is required. If both are specified, uri is resolved into the URL from the Addressable ref result.","title":"Sink as a parameter"},{"location":"eventing/sinks/#sink-parameter-example","text":"Given the following YAML, if ref resolves into \"http://mysink.default.svc.cluster.local\" , then uri is added to this resulting in \"http://mysink.default.svc.cluster.local/extra/path\" . apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract This results in the K_SINK environment variable being set on the subject as \"http://mysink.default.svc.cluster.local/extra/path\" .","title":"Sink parameter example"},{"location":"eventing/sinks/#using-custom-resources-as-sinks","text":"To use a Kubernetes custom resource (CR) as a sink for events, you must: Make the CR Addressable. You must ensure that the CR contains a status.address.url . For more information, see the spec for Addressable resources . Create an Addressable-resolver ClusterRole to obtain the necessary RBAC rules for the sink to receive events. For example, you can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to KafkaSink objects and statuses: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch","title":"Using custom resources as sinks"},{"location":"eventing/sinks/#filtering-events-sent-to-sinks-by-using-triggers","text":"You can connect a Trigger to a sink, so that events are filtered before they are sent to the sink. A sink that is connected to a Trigger is configured as a subscriber in the Trigger resource spec. For example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> is the name of the Trigger being connected to the sink. <kafka-sink-name> is the name of a KafkaSink object.","title":"Filtering events sent to sinks by using Triggers"},{"location":"eventing/sinks/#specifying-sinks-using-the-kn-cli-sink-flag","text":"When you create an event-producing CR by using the Knative ( kn ) CLI, you can specify a sink where events are sent to from that resource, by using the --sink flag. The following example creates a SinkBinding that uses a Service, http://event-display.svc.cluster.local , as the sink: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" The svc in http://event-display.svc.cluster.local determines that the sink is a Knative Service. Other default sink prefixes include Channel and Broker. Tip You can configure which resources can be used with the --sink flag for kn CLI commands by customizing kn .","title":"Specifying sinks using the kn CLI --sink flag"},{"location":"eventing/sinks/#supported-third-party-sink-types","text":"Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Supported third-party sink types"},{"location":"eventing/sinks/kafka-sink/","text":"Apache Kafka Sink \u00b6 This page shows how to install and configure an Apache KafkaSink. Prerequisites \u00b6 You must have access to a Kubernetes cluster with Knative Eventing installed . Installation \u00b6 Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaSink data plane: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver Deployments are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s KafkaSink example \u00b6 A KafkaSink object looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 Output Topic Content Mode \u00b6 The CloudEvent specification defines 2 modes to transport a CloudEvent: structured and binary. A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: binary. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured Security \u00b6 Knative supports the following Apache Kafka security features: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication Enabling security features \u00b6 To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._ Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set. Kafka Producer configurations \u00b6 A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations . Enable debug logging for data plane components \u00b6 To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"KafkaSink"},{"location":"eventing/sinks/kafka-sink/#apache-kafka-sink","text":"This page shows how to install and configure an Apache KafkaSink.","title":"Apache Kafka Sink"},{"location":"eventing/sinks/kafka-sink/#prerequisites","text":"You must have access to a Kubernetes cluster with Knative Eventing installed .","title":"Prerequisites"},{"location":"eventing/sinks/kafka-sink/#installation","text":"Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaSink data plane: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver Deployments are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/sinks/kafka-sink/#kafkasink-example","text":"A KafkaSink object looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092","title":"KafkaSink example"},{"location":"eventing/sinks/kafka-sink/#output-topic-content-mode","text":"The CloudEvent specification defines 2 modes to transport a CloudEvent: structured and binary. A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: binary. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured","title":"Output Topic Content Mode"},{"location":"eventing/sinks/kafka-sink/#security","text":"Knative supports the following Apache Kafka security features: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication","title":"Security"},{"location":"eventing/sinks/kafka-sink/#enabling-security-features","text":"To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._","title":"Enabling security features"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/sinks/kafka-sink/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#kafka-producer-configurations","text":"A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations .","title":"Kafka Producer configurations"},{"location":"eventing/sinks/kafka-sink/#enable-debug-logging-for-data-plane-components","text":"To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"Enable debug logging for data plane components"},{"location":"eventing/sources/","text":"Event sources \u00b6 An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kn kubectl You can list existing event sources on your cluster by entering the kn command: kn source list You can list existing event sources on your cluster by entering the command: kubectl get sources Note Event Sources that import events from other messaging technologies such as Kafka or RabbitMQ are not responsible for setting Optional Attributes such as the datacontenttype . This is a responsibility of the original event producer; the Source just appends attributes if they exist. Knative Sources \u00b6 Name Status Maintainer Description APIServerSource Stable Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. Apache CouchDB Alpha Knative Brings Apache CouchDB messages into Knative. Apache Kafka Stable Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. ContainerSource Stable Knative The ContainerSource instantiates container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, the ContainerSource keeps a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub Beta Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab Beta Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. KogitoSource Alpha Knative An implementation of the Kogito Runtime custom resource managed by the Kogito Operator . PingSource Stable Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Stable Knative Brings RabbitMQ messages into Knative. RedisSource Alpha Knative Brings Redis Stream into Knative. SinkBinding Stable Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. Third-Party Sources \u00b6 Name Status Maintainer Description Amazon CloudWatch Stable TriggerMesh Collects metrics from Amazon CloudWatch . ( installation ) ( example ) Amazon CloudWatch Logs Stable TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. ( installation ) ( example ) AWS CodeCommit Stable TriggerMesh Registers for events emitted by an AWS CodeCommit source code repository. ( installation ) ( example ) Amazon Cognito Identity Stable TriggerMesh Registers for events from Amazon Cognito identity pools. ( installation ) ( example ) Amazon Cognito User Stable TriggerMesh Registers for events from Amazon Cognito user pools. ( installation ) ( example ) Amazon DynamoDB Stable TriggerMesh Reads records from an Amazon DynamoDB stream. ( installation ) ( example ) Amazon Kinesis Stable TriggerMesh Reads records from an Amazon Kinesis stream. ( installation ) ( example ) Amazon RDS Performance Insights Stable TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( installation ) ( example ) Amazon S3 Stable TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( installation ) ( example ) Amazon SNS Stable TriggerMesh Subscribes to messages from an Amazon SNS topic. ( installation ) ( example ) Amazon SQS Stable TriggerMesh Consumes messages from an Amazon SQS queue. ( installation ) ( example ) Apache Camel Stable Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Azure Activity Logs Stable TriggerMesh Capture activity logs from Azure Activity Logs . ( installation ) ( example ) Azure Blob Storage Stable TriggerMesh Subscribes to events from an Azure Blob Storage account. ( installation ) ( example ) Azure Event Grid Stable TriggerMesh Retrieves events from Azure Event Grid . ( installation ) ( example ) Azure Event Hubs Stable TriggerMesh Consumes events from Azure Event Hubs . ( installation ) ( example ) Azure IoT Hub Stable TriggerMesh Consumes event from Azure IoT Hub . ( installation ) ( example ) Azure Queue Storage Stable TriggerMesh Retrieves messages from Azure Queue Storage . ( installation ) ( example ) Azure Service Bus Queues Stable TriggerMesh Consumes messages from an Azure Service Bus queue. ( installation ) ( example ) Azure Service Bus Topics Stable TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( installation ) ( example ) Direktiv Alpha Direktiv Receive events from Direktiv . DockerHubSource Alpha None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. Google Cloud Audit Logs Stable TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( installation ) ( example ) Google Cloud Billing Stable TriggerMesh Captures budget notifications from Google Cloud Billing . ( installation ) ( example ) Google Cloud IoT Stable TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( installation ) ( example ) Google Cloud Pub/Sub Stable TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( installation ) ( example ) Google Cloud Source Repositories Stable TriggerMesh Consumes events from Google Cloud Source Repositories . ( installation ) ( example ) Google Cloud Storage Stable TriggerMesh Captures change notifications from a Google Cloud Storage bucket. ( installation ) ( example ) HTTP Poller Stable TriggerMesh Periodically pulls events from an HTTP/S URL. ( installation ) ( example ) Oracle Cloud Infrastructure Stable TriggerMesh Retrieves metrics from Oracle Cloud Infrastructure . ( installation ) ( example ) Salesforce Stable TriggerMesh Consumes events from a Salesforce channel. ( installation ) ( example ) Slack Stable TriggerMesh Subscribes to events from Slack . ( installation ) ( example ) Twilio Supported TriggerMesh Receive events from Twilio . ( installation ) ( example ) VMware Alpha VMware Brings vSphere events into Knative. Webhook Stable TriggerMesh Ingest events from a webhook using HTTP. ( installation ) ( example ) Zendesk Stable TriggerMesh Subscribes to events from Zendesk. ( installation ) ( example ) Additional resources \u00b6 If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"\u5173\u4e8e\u4e8b\u4ef6\u6e90"},{"location":"eventing/sources/#event-sources","text":"An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kn kubectl You can list existing event sources on your cluster by entering the kn command: kn source list You can list existing event sources on your cluster by entering the command: kubectl get sources Note Event Sources that import events from other messaging technologies such as Kafka or RabbitMQ are not responsible for setting Optional Attributes such as the datacontenttype . This is a responsibility of the original event producer; the Source just appends attributes if they exist.","title":"Event sources"},{"location":"eventing/sources/#knative-sources","text":"Name Status Maintainer Description APIServerSource Stable Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. Apache CouchDB Alpha Knative Brings Apache CouchDB messages into Knative. Apache Kafka Stable Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. ContainerSource Stable Knative The ContainerSource instantiates container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, the ContainerSource keeps a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub Beta Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab Beta Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. KogitoSource Alpha Knative An implementation of the Kogito Runtime custom resource managed by the Kogito Operator . PingSource Stable Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Stable Knative Brings RabbitMQ messages into Knative. RedisSource Alpha Knative Brings Redis Stream into Knative. SinkBinding Stable Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details.","title":"Knative Sources"},{"location":"eventing/sources/#third-party-sources","text":"Name Status Maintainer Description Amazon CloudWatch Stable TriggerMesh Collects metrics from Amazon CloudWatch . ( installation ) ( example ) Amazon CloudWatch Logs Stable TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. ( installation ) ( example ) AWS CodeCommit Stable TriggerMesh Registers for events emitted by an AWS CodeCommit source code repository. ( installation ) ( example ) Amazon Cognito Identity Stable TriggerMesh Registers for events from Amazon Cognito identity pools. ( installation ) ( example ) Amazon Cognito User Stable TriggerMesh Registers for events from Amazon Cognito user pools. ( installation ) ( example ) Amazon DynamoDB Stable TriggerMesh Reads records from an Amazon DynamoDB stream. ( installation ) ( example ) Amazon Kinesis Stable TriggerMesh Reads records from an Amazon Kinesis stream. ( installation ) ( example ) Amazon RDS Performance Insights Stable TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( installation ) ( example ) Amazon S3 Stable TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( installation ) ( example ) Amazon SNS Stable TriggerMesh Subscribes to messages from an Amazon SNS topic. ( installation ) ( example ) Amazon SQS Stable TriggerMesh Consumes messages from an Amazon SQS queue. ( installation ) ( example ) Apache Camel Stable Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Azure Activity Logs Stable TriggerMesh Capture activity logs from Azure Activity Logs . ( installation ) ( example ) Azure Blob Storage Stable TriggerMesh Subscribes to events from an Azure Blob Storage account. ( installation ) ( example ) Azure Event Grid Stable TriggerMesh Retrieves events from Azure Event Grid . ( installation ) ( example ) Azure Event Hubs Stable TriggerMesh Consumes events from Azure Event Hubs . ( installation ) ( example ) Azure IoT Hub Stable TriggerMesh Consumes event from Azure IoT Hub . ( installation ) ( example ) Azure Queue Storage Stable TriggerMesh Retrieves messages from Azure Queue Storage . ( installation ) ( example ) Azure Service Bus Queues Stable TriggerMesh Consumes messages from an Azure Service Bus queue. ( installation ) ( example ) Azure Service Bus Topics Stable TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( installation ) ( example ) Direktiv Alpha Direktiv Receive events from Direktiv . DockerHubSource Alpha None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. Google Cloud Audit Logs Stable TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( installation ) ( example ) Google Cloud Billing Stable TriggerMesh Captures budget notifications from Google Cloud Billing . ( installation ) ( example ) Google Cloud IoT Stable TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( installation ) ( example ) Google Cloud Pub/Sub Stable TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( installation ) ( example ) Google Cloud Source Repositories Stable TriggerMesh Consumes events from Google Cloud Source Repositories . ( installation ) ( example ) Google Cloud Storage Stable TriggerMesh Captures change notifications from a Google Cloud Storage bucket. ( installation ) ( example ) HTTP Poller Stable TriggerMesh Periodically pulls events from an HTTP/S URL. ( installation ) ( example ) Oracle Cloud Infrastructure Stable TriggerMesh Retrieves metrics from Oracle Cloud Infrastructure . ( installation ) ( example ) Salesforce Stable TriggerMesh Consumes events from a Salesforce channel. ( installation ) ( example ) Slack Stable TriggerMesh Subscribes to events from Slack . ( installation ) ( example ) Twilio Supported TriggerMesh Receive events from Twilio . ( installation ) ( example ) VMware Alpha VMware Brings vSphere events into Knative. Webhook Stable TriggerMesh Ingest events from a webhook using HTTP. ( installation ) ( example ) Zendesk Stable TriggerMesh Subscribes to events from Zendesk. ( installation ) ( example )","title":"Third-Party Sources"},{"location":"eventing/sources/#additional-resources","text":"If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Additional resources"},{"location":"eventing/sources/apiserversource/","text":"\u5173\u4e8e ApiServerSource \u00b6 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6 Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u76d1\u542c Kubernetes API \u670d\u52a1\u5668\u53d1\u51fa\u7684\u4e8b\u4ef6(\u5982 Pod \u521b\u5efa\u3001\u90e8\u7f72\u66f4\u65b0\u7b49)\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u8f6c\u53d1\u5230\u4e00\u4e2a\u63a5\u6536\u5668\u3002 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6\u5904\u7406\u6838\u5fc3\u7ec4\u4ef6\u7684\u4e00\u90e8\u5206\uff0c\u5728\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406\u65f6\u9ed8\u8ba4\u63d0\u4f9b\u3002 \u7528\u6237\u53ef\u4ee5\u521b\u5efa ApiServerSource \u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6e90"},{"location":"eventing/sources/apiserversource/#apiserversource","text":"API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6 Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90\uff0c\u5b83\u76d1\u542c Kubernetes API \u670d\u52a1\u5668\u53d1\u51fa\u7684\u4e8b\u4ef6(\u5982 Pod \u521b\u5efa\u3001\u90e8\u7f72\u66f4\u65b0\u7b49)\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u8f6c\u53d1\u5230\u4e00\u4e2a\u63a5\u6536\u5668\u3002 API \u670d\u52a1\u5668\u6e90\u662f Knative \u4e8b\u4ef6\u5904\u7406\u6838\u5fc3\u7ec4\u4ef6\u7684\u4e00\u90e8\u5206\uff0c\u5728\u5b89\u88c5 Knative \u4e8b\u4ef6\u5904\u7406\u65f6\u9ed8\u8ba4\u63d0\u4f9b\u3002 \u7528\u6237\u53ef\u4ee5\u521b\u5efa ApiServerSource \u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e ApiServerSource"},{"location":"eventing/sources/apiserversource/getting-started/","text":"\u521b\u5efa ApiServerSource \u5bf9\u8c61 \u00b6 \u4ecb\u7ecd\u5982\u4f55\u521b\u5efa ApiServerSource \u5bf9\u8c61\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u521b\u5efa ApiServerSource \u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5 Knative \u4e8b\u4ef6 \u3002 \u5fc5\u987b\u5b89\u88c5 kubectl CLI \u5de5\u5177\u3002 \u53ef\u9009:\u5982\u679c\u8981\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u5de5\u5177\u3002 \u521b\u5efa ApiServerSource \u5bf9\u8c61 \u00b6 \u53ef\u9009:\u4e3a API \u670d\u52a1\u5668\u6e90\u5b9e\u4f8b\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 Note \u4e3a\u60a8\u7684ApiServerSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c \u5141\u8bb8\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c \u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u53ef\u80fd\u5b58\u5728\u4e8e default \u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa ServiceAccount: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> \u662f\u8981\u521b\u5efa\u7684 ServiceAccount \u7684\u540d\u79f0\u3002 <namespace> \u662f\u60a8\u5728\u524d\u9762\u7684\u6b65\u9aa4 1 \u4e2d\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u89d2\u8272: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch !!! note The only required verbs are `get`, `list` and `watch`. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: === \"kn\" - To create the ApiServerSource, run the command: ```bash kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> ``` Where: - `<apiserversource>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<sink-name>` is the name of your sink, for example, `http://event-display.pingsource-example.svc.cluster.local`. For a list of available options, see the [Knative client documentation](https://github.com/knative/client/blob/main/docs/cmd/kn_source_apiserver_create.md#kn-source-apiserver-create). === \"YAML\" 1. Create a YAML file using the following template: ```yaml apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: <apiserversource-name> namespace: <namespace> spec: serviceAccountName: <service-account> mode: <event-mode> resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: <sink-kind> name: <sink-name> ``` Where: - `<apiserversource-name>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<event-mode>` is either `Resource` or `Reference`. If set to `Resource`, the event payload contains the entire resource that the event is for. If set to `Reference`, the event payload only contains a reference to the resource that the event is for. The default is `Reference`. - `<sink-kind>` is any supported Addressable object that you want to use as a sink, for example, a `Service` or `Deployment`. - `<sink-name>` is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see [ApiServerSource reference](reference.md). 1. Apply the YAML file by running the command: ```bash kubectl apply -f <filename>.yaml ``` Where `<filename>` is the name of the file you created in the previous step. \u9a8c\u8bc1 ApiServerSource \u5bf9\u8c61 \u00b6 Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" } \u5220\u9664 ApiServerSource \u5bf9\u8c61 \u00b6 To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource","text":"\u4ecb\u7ecd\u5982\u4f55\u521b\u5efa ApiServerSource \u5bf9\u8c61\u3002","title":"\u521b\u5efa ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#_1","text":"\u5728\u521b\u5efa ApiServerSource \u5bf9\u8c61\u4e4b\u524d: \u60a8\u5fc5\u987b\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5 Knative \u4e8b\u4ef6 \u3002 \u5fc5\u987b\u5b89\u88c5 kubectl CLI \u5de5\u5177\u3002 \u53ef\u9009:\u5982\u679c\u8981\u4f7f\u7528 kn \u547d\u4ee4\uff0c\u8bf7\u5b89\u88c5 kn \u5de5\u5177\u3002","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_1","text":"\u53ef\u9009:\u4e3a API \u670d\u52a1\u5668\u6e90\u5b9e\u4f8b\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> \u5176\u4e2d <namespace> \u662f\u60a8\u60f3\u8981\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u7684\u540d\u79f0\u3002 Note \u4e3a\u60a8\u7684ApiServerSource\u548c\u76f8\u5173\u7ec4\u4ef6\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c \u5141\u8bb8\u60a8\u66f4\u5bb9\u6613\u5730\u67e5\u770b\u6b64\u5de5\u4f5c\u6d41\u7684\u66f4\u6539\u548c\u4e8b\u4ef6\uff0c \u56e0\u4e3a\u8fd9\u4e9b\u4e0e\u53ef\u80fd\u5b58\u5728\u4e8e default \u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u9694\u79bb\u5f00\u6765\u3002 \u5b83\u8fd8\u4f7f\u5220\u9664\u6e90\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5220\u9664\u540d\u79f0\u7a7a\u95f4\u6765\u5220\u9664\u6240\u6709\u8d44\u6e90\u3002 \u521b\u5efa ServiceAccount: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> \u662f\u8981\u521b\u5efa\u7684 ServiceAccount \u7684\u540d\u79f0\u3002 <namespace> \u662f\u60a8\u5728\u524d\u9762\u7684\u6b65\u9aa4 1 \u4e2d\u521b\u5efa\u7684\u540d\u79f0\u7a7a\u95f4\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u521b\u5efa\u89d2\u8272: \u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch !!! note The only required verbs are `get`, `list` and `watch`. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: === \"kn\" - To create the ApiServerSource, run the command: ```bash kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> ``` Where: - `<apiserversource>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<sink-name>` is the name of your sink, for example, `http://event-display.pingsource-example.svc.cluster.local`. For a list of available options, see the [Knative client documentation](https://github.com/knative/client/blob/main/docs/cmd/kn_source_apiserver_create.md#kn-source-apiserver-create). === \"YAML\" 1. Create a YAML file using the following template: ```yaml apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: <apiserversource-name> namespace: <namespace> spec: serviceAccountName: <service-account> mode: <event-mode> resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: <sink-kind> name: <sink-name> ``` Where: - `<apiserversource-name>` is the name of the source that you want to create. - `<namespace>` is the name of the namespace that you created in step 1 earlier. - `<service-account>` is the name of the ServiceAccount that you created in step 2 earlier. - `<event-mode>` is either `Resource` or `Reference`. If set to `Resource`, the event payload contains the entire resource that the event is for. If set to `Reference`, the event payload only contains a reference to the resource that the event is for. The default is `Reference`. - `<sink-kind>` is any supported Addressable object that you want to use as a sink, for example, a `Service` or `Deployment`. - `<sink-name>` is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see [ApiServerSource reference](reference.md). 1. Apply the YAML file by running the command: ```bash kubectl apply -f <filename>.yaml ``` Where `<filename>` is the name of the file you created in the previous step.","title":"\u521b\u5efa ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_2","text":"Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" }","title":"\u9a8c\u8bc1 ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/getting-started/#apiserversource_3","text":"To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"\u5220\u9664 ApiServerSource \u5bf9\u8c61"},{"location":"eventing/sources/apiserversource/reference/","text":"ApiServerSource \u53c2\u8003 \u00b6 This topic provides reference information about the configurable fields for the ApiServerSource object. ApiServerSource \u00b6 An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional \u8d44\u6e90\u53c2\u6570 \u00b6 The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels Example: Resources parameter \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment \u793a\u4f8b:\u4f7f\u7528matchExpressions\u7684\u8d44\u6e90\u53c2\u6570 \u00b6 Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp \u793a\u4f8b:\u4f7f\u7528matchLabels\u7684\u8d44\u6e90\u53c2\u6570 \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp ServiceAccountName parameter \u00b6 ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object. \u4f8b\u5b50:\u8ffd\u8e2aPod \u00b6 The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ... \u4e3b\u53c2\u6570 \u00b6 ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required \u793a\u4f8b:\u4e3b\u53c2\u6570 \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ... CloudEvent \u8986\u76d6 \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. \u4f8b\u5982:CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u6e90\u53c2\u8003"},{"location":"eventing/sources/apiserversource/reference/#apiserversource","text":"This topic provides reference information about the configurable fields for the ApiServerSource object.","title":"ApiServerSource \u53c2\u8003"},{"location":"eventing/sources/apiserversource/reference/#apiserversource_1","text":"An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"ApiServerSource"},{"location":"eventing/sources/apiserversource/reference/#_1","text":"The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels","title":"\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter","text":"Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment","title":"Example: Resources parameter"},{"location":"eventing/sources/apiserversource/reference/#matchexpressions","text":"Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp","title":"\u793a\u4f8b:\u4f7f\u7528matchExpressions\u7684\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#matchlabels","text":"Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp","title":"\u793a\u4f8b:\u4f7f\u7528matchLabels\u7684\u8d44\u6e90\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#serviceaccountname-parameter","text":"ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object.","title":"ServiceAccountName parameter"},{"location":"eventing/sources/apiserversource/reference/#pod","text":"The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ...","title":"\u4f8b\u5b50:\u8ffd\u8e2aPod"},{"location":"eventing/sources/apiserversource/reference/#_2","text":"ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required","title":"\u4e3b\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#_3","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ...","title":"\u793a\u4f8b:\u4e3b\u53c2\u6570"},{"location":"eventing/sources/apiserversource/reference/#cloudevent","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent \u8986\u76d6"},{"location":"eventing/sources/apiserversource/reference/#cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u4f8b\u5982:CloudEvent Overrides"},{"location":"eventing/sources/kafka-source/","text":"Apache Kafka Source \u00b6 The KafkaSource reads messages stored in existing Apache Kafka topics, and sends those messages as CloudEvents through HTTP to its configured sink . The KafkaSource preserves the order of the messages stored in the topic partitions. It does this by waiting for a successful response from the sink before it delivers the next message in the same partition. Install the KafkaSource controller \u00b6 Install the KafkaSource controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s Optional: Create a Kafka topic \u00b6 Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME CLUSTER PARTITIONS REPLICATION FACTOR knative-demo-topic my-cluster 3 1 Create a Service \u00b6 Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s Kafka event source \u00b6 Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h Verify \u00b6 Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" } Optional: Specify the key deserializer \u00b6 When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Optional: Specify the initial offset \u00b6 By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group. Connecting to a TLS-enabled Kafka Broker \u00b6 The KafkaSource supports TLS and SASL authentication methods. To enable TLS authentication, you must have the following files: CA Certificate Client Certificate and Key KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Enabling SASL for KafkaSources \u00b6 Simple Authentication and Security Layer (SASL) is used by Apache Kafka for authentication. If you use SASL authentication on your cluster, users must provide credentials to Knative for communicating with the Kafka cluster, otherwise events cannot be produced or consumed. Prerequisites \u00b6 You have access to a Kafka cluster that has Simple Authentication and Security Layer (SASL). Procedure \u00b6 Create a secret that uses the Kafka cluster's SASL information, by running the following commands: STRIMZI_CRT = $( kubectl -n kafka get secret example-cluster-cluster-ca-cert --template = '{{index.data \"ca.crt\"}}' | base64 --decode ) SASL_PASSWD = $( kubectl -n kafka get secret example-user --template = '{{index.data \"password\"}}' | base64 --decode ) kubectl create secret -n default generic <secret_name> \\ --from-literal = ca.crt = \" $STRIMZI_CRT \" \\ --from-literal = password = \" $SASL_PASSWD \" \\ --from-literal = saslType = \"SCRAM-SHA-512\" \\ --from-literal = user = \"example-user\" Create or modify a KafkaSource so that it contains the following spec options: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : example-source spec : ... net : sasl : enable : true user : secretKeyRef : name : <secret_name> key : user password : secretKeyRef : name : <secret_name> key : password saslType : secretKeyRef : name : <secret_name> key : saslType tls : enable : true caCert : secretKeyRef : name : <secret_name> key : ca.crt ... Where <secret_name> is the name of the secret generated in the previous step. Clean up steps \u00b6 Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"KafkaSource"},{"location":"eventing/sources/kafka-source/#apache-kafka-source","text":"The KafkaSource reads messages stored in existing Apache Kafka topics, and sends those messages as CloudEvents through HTTP to its configured sink . The KafkaSource preserves the order of the messages stored in the topic partitions. It does this by waiting for a successful response from the sink before it delivers the next message in the same partition.","title":"Apache Kafka Source"},{"location":"eventing/sources/kafka-source/#install-the-kafkasource-controller","text":"Install the KafkaSource controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s","title":"Install the KafkaSource controller"},{"location":"eventing/sources/kafka-source/#optional-create-a-kafka-topic","text":"Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME CLUSTER PARTITIONS REPLICATION FACTOR knative-demo-topic my-cluster 3 1","title":"Optional: Create a Kafka topic"},{"location":"eventing/sources/kafka-source/#create-a-service","text":"Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"Create a Service"},{"location":"eventing/sources/kafka-source/#kafka-event-source","text":"Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h","title":"Kafka event source"},{"location":"eventing/sources/kafka-source/#verify","text":"Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" }","title":"Verify"},{"location":"eventing/sources/kafka-source/#optional-specify-the-key-deserializer","text":"When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"Optional: Specify the key deserializer"},{"location":"eventing/sources/kafka-source/#optional-specify-the-initial-offset","text":"By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group.","title":"Optional: Specify the initial offset"},{"location":"eventing/sources/kafka-source/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaSource supports TLS and SASL authentication methods. To enable TLS authentication, you must have the following files: CA Certificate Client Certificate and Key KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"Connecting to a TLS-enabled Kafka Broker"},{"location":"eventing/sources/kafka-source/#enabling-sasl-for-kafkasources","text":"Simple Authentication and Security Layer (SASL) is used by Apache Kafka for authentication. If you use SASL authentication on your cluster, users must provide credentials to Knative for communicating with the Kafka cluster, otherwise events cannot be produced or consumed.","title":"Enabling SASL for KafkaSources"},{"location":"eventing/sources/kafka-source/#prerequisites","text":"You have access to a Kafka cluster that has Simple Authentication and Security Layer (SASL).","title":"Prerequisites"},{"location":"eventing/sources/kafka-source/#procedure","text":"Create a secret that uses the Kafka cluster's SASL information, by running the following commands: STRIMZI_CRT = $( kubectl -n kafka get secret example-cluster-cluster-ca-cert --template = '{{index.data \"ca.crt\"}}' | base64 --decode ) SASL_PASSWD = $( kubectl -n kafka get secret example-user --template = '{{index.data \"password\"}}' | base64 --decode ) kubectl create secret -n default generic <secret_name> \\ --from-literal = ca.crt = \" $STRIMZI_CRT \" \\ --from-literal = password = \" $SASL_PASSWD \" \\ --from-literal = saslType = \"SCRAM-SHA-512\" \\ --from-literal = user = \"example-user\" Create or modify a KafkaSource so that it contains the following spec options: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : example-source spec : ... net : sasl : enable : true user : secretKeyRef : name : <secret_name> key : user password : secretKeyRef : name : <secret_name> key : password saslType : secretKeyRef : name : <secret_name> key : saslType tls : enable : true caCert : secretKeyRef : name : <secret_name> key : ca.crt ... Where <secret_name> is the name of the secret generated in the previous step.","title":"Procedure"},{"location":"eventing/sources/kafka-source/#clean-up-steps","text":"Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"Clean up steps"},{"location":"eventing/sources/ping-source/","text":"\u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61 \u00b6 \u4ecb\u7ecd\u5982\u4f55\u521b\u5efaPingSource\u5bf9\u8c61\u3002 PingSource\u662f\u4e00\u79cd\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684 cron \u8c03\u5ea6\u4e0a\u4f7f\u7528\u56fa\u5b9a\u7684\u6709\u6548\u8d1f\u8f7d\u751f\u6210\u4e8b\u4ef6\u3002 \u4e0b\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5c06PingSource\u914d\u7f6e\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u8be5\u4e8b\u4ef6\u6e90\u6bcf\u5206\u949f\u5411\u540d\u4e3a event-display \u7684Knative\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\uff0c\u8be5\u670d\u52a1\u7528\u4f5c\u63a5\u6536\u5668\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002 \u5728\u5f00\u59cb\u4e4b\u524d \u00b6 \u521b\u5efaPingSource: You must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. You can use either kubectl or kn commands to create components such as a sink and PingSource. You can use either kubectl or kail for logging during the verification step in this procedure. \u521b\u5efaPingSource\u5bf9\u8c61 \u00b6 \u53ef\u9009:\u4e3a\u4f60\u7684PingSource\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> Where <namespace> is the namespace that you want your PingSource to use. For example, pingsource-example . Note Creating a namespace for your PingSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. \u521b\u5efa\u4e00\u4e2a\u6c34\u69fd\u3002\u5982\u679c\u60a8\u6ca1\u6709\u81ea\u5df1\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u670d\u52a1\u5c06\u4f20\u5165\u7684\u6d88\u606f\u8f6c\u50a8\u5230\u65e5\u5fd7\u4e2d: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u521b\u5efaPingSource\u5bf9\u8c61\u3002 Note The data you want to send must be represented as text in the PingSource YAML file. Events that send binary data cannot be directly serialized in YAML. However, you can send binary data that is base64 encoded by using dataBase64 in place of data in the PingSource spec. Use one of the following options: kn kn: binary data YAML YAML: binary data To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends binary data, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<base64-data>' \\ --encoding 'base64' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, a Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create a PingSource that sends binary data: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source-binary . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, a Kubernetes Service. <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u9a8c\u8bc1PingSource\u5bf9\u8c61 \u00b6 \u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m \u9a8c\u8bc1\u8f93\u51fa\u662f\u5426\u8fd4\u56dePingSource\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u5c5e\u6027\u3002 In the example below, the command has returned the Attributes and Data properties of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } \u5220\u9664PingSource\u5bf9\u8c61 \u00b6 \u60a8\u53ef\u4ee5\u5220\u9664PingSource\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u5220\u9664\u8d44\u6e90: To remove the PingSource object and all of the related resources, delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the PingSource object. To delete the PingSource instance only, run the command: kn kubectl kn source ping delete <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . kubectl delete pingsources.sources.knative.dev <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . To delete the sink only, run the command: kn kubectl kn service delete <sink-name> Where <sink-name> is the name of your sink, for example, event-display . kubectl delete service.serving.knative.dev <sink-name> Where <sink-name> is the name of your sink, for example, event-display .","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource","text":"\u4ecb\u7ecd\u5982\u4f55\u521b\u5efaPingSource\u5bf9\u8c61\u3002 PingSource\u662f\u4e00\u79cd\u4e8b\u4ef6\u6e90\uff0c\u5b83\u5728\u6307\u5b9a\u7684 cron \u8c03\u5ea6\u4e0a\u4f7f\u7528\u56fa\u5b9a\u7684\u6709\u6548\u8d1f\u8f7d\u751f\u6210\u4e8b\u4ef6\u3002 \u4e0b\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5c06PingSource\u914d\u7f6e\u4e3a\u4e8b\u4ef6\u6e90\uff0c\u8be5\u4e8b\u4ef6\u6e90\u6bcf\u5206\u949f\u5411\u540d\u4e3a event-display \u7684Knative\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\uff0c\u8be5\u670d\u52a1\u7528\u4f5c\u63a5\u6536\u5668\u3002 \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u7528\u60a8\u81ea\u5df1\u7684\u503c\u66ff\u6362\u793a\u4f8b\u3002","title":"\u521b\u5efa\u4e00\u4e2aPingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#_1","text":"\u521b\u5efaPingSource: You must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. You can use either kubectl or kn commands to create components such as a sink and PingSource. You can use either kubectl or kail for logging during the verification step in this procedure.","title":"\u5728\u5f00\u59cb\u4e4b\u524d"},{"location":"eventing/sources/ping-source/#pingsource_1","text":"\u53ef\u9009:\u4e3a\u4f60\u7684PingSource\u521b\u5efa\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u3002 kubectl create namespace <namespace> Where <namespace> is the namespace that you want your PingSource to use. For example, pingsource-example . Note Creating a namespace for your PingSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. \u521b\u5efa\u4e00\u4e2a\u6c34\u69fd\u3002\u5982\u679c\u60a8\u6ca1\u6709\u81ea\u5df1\u7684\u63a5\u6536\u5668\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u670d\u52a1\u5c06\u4f20\u5165\u7684\u6d88\u606f\u8f6c\u50a8\u5230\u65e5\u5fd7\u4e2d: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. \u521b\u5efaPingSource\u5bf9\u8c61\u3002 Note The data you want to send must be represented as text in the PingSource YAML file. Events that send binary data cannot be directly serialized in YAML. However, you can send binary data that is base64 encoded by using dataBase64 in place of data in the PingSource spec. Use one of the following options: kn kn: binary data YAML YAML: binary data To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends binary data, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<base64-data>' \\ --encoding 'base64' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, a Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create a PingSource that sends binary data: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source-binary . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, a Kubernetes Service. <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u521b\u5efaPingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource_2","text":"\u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m \u9a8c\u8bc1\u8f93\u51fa\u662f\u5426\u8fd4\u56dePingSource\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u7684\u5c5e\u6027\u3002 In the example below, the command has returned the Attributes and Data properties of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" }","title":"\u9a8c\u8bc1PingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/#pingsource_3","text":"\u60a8\u53ef\u4ee5\u5220\u9664PingSource\u548c\u6240\u6709\u76f8\u5173\u8d44\u6e90\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u5220\u9664\u8d44\u6e90: To remove the PingSource object and all of the related resources, delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the PingSource object. To delete the PingSource instance only, run the command: kn kubectl kn source ping delete <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . kubectl delete pingsources.sources.knative.dev <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . To delete the sink only, run the command: kn kubectl kn service delete <sink-name> Where <sink-name> is the name of your sink, for example, event-display . kubectl delete service.serving.knative.dev <sink-name> Where <sink-name> is the name of your sink, for example, event-display .","title":"\u5220\u9664PingSource\u5bf9\u8c61"},{"location":"eventing/sources/ping-source/reference/","text":"PingSource reference \u00b6 This topic provides reference information about the configurable fields for the PingSource object. PingSource \u00b6 A PingSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a PingSource object. Required metadata Specifies metadata that uniquely identifies the PingSource object. For example, a name . Required spec Specifies the configuration information for this PingSource object. Required spec.contentType The media type of data or dataBase64 . Default is empty. Optional spec.data The data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with dataBase64 . Required if not sending base64 encoded data spec.dataBase64 A base64-encoded string of the actual event's body posted to the sink. Default is empty. Mutually exclusive with data . Required if sending base64 encoded data spec.schedule Specifies the cron schedule. Defaults to * * * * * . Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.timezone Modifies the actual time relative to the specified timezone. Defaults to the system time zone. See the list of valid tz database time zones on Wikipedia. For general information about time zones, see the IANA website. Optional spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional status Defines the observed state of PingSource. Optional status.observedGeneration The 'Generation' of the Service that was last processed by the controller. Optional status.conditions The latest available observations of a resource's current state. Optional status.sinkUri The current active sink URI that has been configured for the Source. Optional CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. Example: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"\u6e90\u53c2\u8003"},{"location":"eventing/sources/ping-source/reference/#pingsource-reference","text":"This topic provides reference information about the configurable fields for the PingSource object.","title":"PingSource reference"},{"location":"eventing/sources/ping-source/reference/#pingsource","text":"A PingSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a PingSource object. Required metadata Specifies metadata that uniquely identifies the PingSource object. For example, a name . Required spec Specifies the configuration information for this PingSource object. Required spec.contentType The media type of data or dataBase64 . Default is empty. Optional spec.data The data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with dataBase64 . Required if not sending base64 encoded data spec.dataBase64 A base64-encoded string of the actual event's body posted to the sink. Default is empty. Mutually exclusive with data . Required if sending base64 encoded data spec.schedule Specifies the cron schedule. Defaults to * * * * * . Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.timezone Modifies the actual time relative to the specified timezone. Defaults to the system time zone. See the list of valid tz database time zones on Wikipedia. For general information about time zones, see the IANA website. Optional spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional status Defines the observed state of PingSource. Optional status.observedGeneration The 'Generation' of the Service that was last processed by the controller. Optional status.conditions The latest available observations of a resource's current state. Optional status.sinkUri The current active sink URI that has been configured for the Source. Optional","title":"PingSource"},{"location":"eventing/sources/ping-source/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/sources/ping-source/reference/#example-cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"Example: CloudEvent Overrides"},{"location":"eventing/sources/rabbitmq-source/","text":"Creating a RabbitMQSource \u00b6 This topic describes how to create a RabbitMQSource. Prerequisites \u00b6 You have installed Knative Eventing You have installed CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator You have installed RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager A working RabbitMQ Instance, we recommend to create one Using the RabbitMQ Cluster Operator . For more information about configuring the RabbitmqCluster CRD, see the RabbitMQ website Install the RabbitMQ controller \u00b6 Install the RabbitMQSource controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-source.yaml Verify that rabbitmq-controller-manager and rabbitmq-webhook are running: kubectl get deployments.apps -n knative-sources Example output: NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-controller-manager 1 /1 1 1 3s rabbitmq-webhook 1 /1 1 1 4s \u521b\u5efa\u670d\u52a1 \u00b6 \u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s Create a RabbitMQSource object \u00b6 Create a YAML file using the following template: apiVersion : sources.knative.dev/v1alpha1 kind : RabbitmqSource metadata : name : <source-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials rabbitmqResourcesConfig : parallelism : 10 exchangeName : \"eventing-rabbitmq-source\" queueName : \"eventing-rabbitmq-source\" delivery : retry : 5 backoffPolicy : \"linear\" backoffDelay : \"PT1S\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Where: <source-name> is the name you want for your RabbitMQSource object. <cluster-name> is the name of the RabbitMQ cluster you created earlier. Note You cannot set name and connectionSecret at the same time, since name is for a RabbitMQ Cluster Operator instance running in the same cluster as the Source, and connectionSecret is for an external RabbitMQ server. Apply the YAML file by running the command: kubectl apply -f <filename> Where <filename> is the name of the file you created in the previous step. Verify \u00b6 Check the event-display Service to see if it is receiving events. It might take a while for the Source to start sending events to the Sink. kubectl -l = 'serving.knative.dev/service=event-display' logs -c user-container \u2601\ufe0f cloudevents.Event Context Attributes, specversion: 1 .0 type: dev.knative.rabbitmq.event source: /apis/v1/namespaces/default/rabbitmqsources/<source-name> subject: f147099d-c64d-41f7-b8eb-a2e53b228349 id: f147099d-c64d-41f7-b8eb-a2e53b228349 time: 2021 -12-16T20:11:39.052276498Z datacontenttype: application/json Data, { ... Random Data ... } Cleanup \u00b6 Delete the RabbitMQSource: kubectl delete -f <source-yaml-filename> Delete the RabbitMQ credentials secret: kubectl delete -f <secret-yaml-filename> Delete the event display Service: kubectl delete -f <service-yaml-filename> Additional information \u00b6 For more samples visit the eventing-rabbitmq Github repository samples directory To report a bug or request a feature, open an issue in the eventing-rabbitmq Github repository .","title":"RabbitMQSource"},{"location":"eventing/sources/rabbitmq-source/#creating-a-rabbitmqsource","text":"This topic describes how to create a RabbitMQSource.","title":"Creating a RabbitMQSource"},{"location":"eventing/sources/rabbitmq-source/#prerequisites","text":"You have installed Knative Eventing You have installed CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator You have installed RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager A working RabbitMQ Instance, we recommend to create one Using the RabbitMQ Cluster Operator . For more information about configuring the RabbitmqCluster CRD, see the RabbitMQ website","title":"Prerequisites"},{"location":"eventing/sources/rabbitmq-source/#install-the-rabbitmq-controller","text":"Install the RabbitMQSource controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-source.yaml Verify that rabbitmq-controller-manager and rabbitmq-webhook are running: kubectl get deployments.apps -n knative-sources Example output: NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-controller-manager 1 /1 1 1 3s rabbitmq-webhook 1 /1 1 1 4s","title":"Install the RabbitMQ controller"},{"location":"eventing/sources/rabbitmq-source/#_1","text":"\u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/sources/rabbitmq-source/#create-a-rabbitmqsource-object","text":"Create a YAML file using the following template: apiVersion : sources.knative.dev/v1alpha1 kind : RabbitmqSource metadata : name : <source-name> spec : rabbitmqClusterReference : # Configure name if a RabbitMQ Cluster Operator is being used. name : <cluster-name> # Configure connectionSecret if an external RabbitMQ cluster is being used. connectionSecret : name : rabbitmq-secret-credentials rabbitmqResourcesConfig : parallelism : 10 exchangeName : \"eventing-rabbitmq-source\" queueName : \"eventing-rabbitmq-source\" delivery : retry : 5 backoffPolicy : \"linear\" backoffDelay : \"PT1S\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Where: <source-name> is the name you want for your RabbitMQSource object. <cluster-name> is the name of the RabbitMQ cluster you created earlier. Note You cannot set name and connectionSecret at the same time, since name is for a RabbitMQ Cluster Operator instance running in the same cluster as the Source, and connectionSecret is for an external RabbitMQ server. Apply the YAML file by running the command: kubectl apply -f <filename> Where <filename> is the name of the file you created in the previous step.","title":"Create a RabbitMQSource object"},{"location":"eventing/sources/rabbitmq-source/#verify","text":"Check the event-display Service to see if it is receiving events. It might take a while for the Source to start sending events to the Sink. kubectl -l = 'serving.knative.dev/service=event-display' logs -c user-container \u2601\ufe0f cloudevents.Event Context Attributes, specversion: 1 .0 type: dev.knative.rabbitmq.event source: /apis/v1/namespaces/default/rabbitmqsources/<source-name> subject: f147099d-c64d-41f7-b8eb-a2e53b228349 id: f147099d-c64d-41f7-b8eb-a2e53b228349 time: 2021 -12-16T20:11:39.052276498Z datacontenttype: application/json Data, { ... Random Data ... }","title":"Verify"},{"location":"eventing/sources/rabbitmq-source/#cleanup","text":"Delete the RabbitMQSource: kubectl delete -f <source-yaml-filename> Delete the RabbitMQ credentials secret: kubectl delete -f <secret-yaml-filename> Delete the event display Service: kubectl delete -f <service-yaml-filename>","title":"Cleanup"},{"location":"eventing/sources/rabbitmq-source/#additional-information","text":"For more samples visit the eventing-rabbitmq Github repository samples directory To report a bug or request a feature, open an issue in the eventing-rabbitmq Github repository .","title":"Additional information"},{"location":"eventing/sources/redis/","text":"\u5173\u4e8e RedisStreamSource \u00b6 RedisStreamSource \u4ece Redis \u6d41 \u8bfb\u53d6\u6d88\u606f\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u53d1\u9001\u5230\u5f15\u7528\u7684\u63a5\u6536\u5668\u3002","title":"\u5173\u4e8e\u6e90"},{"location":"eventing/sources/redis/#redisstreamsource","text":"RedisStreamSource \u4ece Redis \u6d41 \u8bfb\u53d6\u6d88\u606f\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a CloudEvents \u53d1\u9001\u5230\u5f15\u7528\u7684\u63a5\u6536\u5668\u3002","title":"\u5173\u4e8e RedisStreamSource"},{"location":"eventing/sources/redis/getting-started/","text":"\u521b\u5efa RedisStreamSource \u00b6 \u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efa\u4e00\u4e2a RedisStreamSource \u5bf9\u8c61\u3002 \u5b89\u88c5 RedisStreamSource \u63d2\u4ef6 \u00b6 RedisStreamSource \u662f\u4e00\u4e2a Knative \u4e8b\u4ef6\u9644\u52a0\u7ec4\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RedisStreamSource: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-redis/latest/redis-source.yaml \u9a8c\u8bc1 redis-controller-manager \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE redis-controller-manager 1 /1 1 1 3s \u521b\u5efa\u670d\u52a1 \u00b6 \u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s \u521b\u5efa RedisStreamSource \u5bf9\u8c61 \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684 YAML \u6a21\u677f\u521b\u5efa RedisStreamSource \u5bf9\u8c61: apiVersion : sources.knative.dev/v1alpha1 kind : RedisStreamSource metadata : name : <redis-stream-source> spec : address : <redis-uri> stream : <redis-stream-name> group : <consumer-group-name> sink : <sink> Where: <redis-stream-source> \u662f\u4f60\u7684\u6e90\u540d\u5b57\u3002(\u5fc5\u9700) <redis-uri> \u662f Redis URI. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Redis \u6587\u6863 \u3002(\u5fc5\u9700) <redis-stream-name> \u662f Redis \u6d41\u7684\u540d\u79f0. (\u5fc5\u9700) <consumer-group-name> \u662f Redis \u6d88\u8d39\u7fa4\u4f53\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002(\u53ef\u9009) <sink> \u5b83\u662f\u53d1\u9001\u4e8b\u4ef6\u3002(\u5fc5\u9700) \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u9a8c\u8bc1 RedisStreamSource \u5bf9\u8c61 \u00b6 \u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl logs -l app = event-display --tail = 100 \u6837\u4f8b\u8f93\u51fa: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ] \u5220\u9664 RedisStreamSource \u5bf9\u8c61 \u00b6 \u5220\u9664 RedisStreamSource \u5bf9\u8c61: kubectl delete -f <filename> \u989d\u5916\u7684\u4fe1\u606f \u00b6 \u6709\u5173 Redis \u6d41\u6e90\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 eventing-redis Github \u5e93","title":"\u521b\u5efa\u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource","text":"\u672c\u4e3b\u9898\u63cf\u8ff0\u5982\u4f55\u521b\u5efa\u4e00\u4e2a RedisStreamSource \u5bf9\u8c61\u3002","title":"\u521b\u5efa RedisStreamSource"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_1","text":"RedisStreamSource \u662f\u4e00\u4e2a Knative \u4e8b\u4ef6\u9644\u52a0\u7ec4\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5b89\u88c5 RedisStreamSource: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-redis/latest/redis-source.yaml \u9a8c\u8bc1 redis-controller-manager \u6b63\u5728\u8fd0\u884c: kubectl get deployments.apps -n knative-sources \u793a\u4f8b\u8f93\u51fa: NAME READY UP-TO-DATE AVAILABLE AGE redis-controller-manager 1 /1 1 1 3s","title":"\u5b89\u88c5 RedisStreamSource \u63d2\u4ef6"},{"location":"eventing/sources/redis/getting-started/#_1","text":"\u521b\u5efa event-display \u670d\u52a1\u4f5c\u4e3a\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u793a\u4f8b\u8f93\u51fa: service.serving.knative.dev/event-display created \u786e\u4fdd\u670d\u52a1 Pod \u6b63\u5728\u8fd0\u884c\uff0c\u8fd0\u884c\u547d\u4ee4: kubectl get pods Pod \u540d\u79f0\u7684\u524d\u7f00\u662f event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"\u521b\u5efa\u670d\u52a1"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_2","text":"\u4f7f\u7528\u4e0b\u9762\u7684 YAML \u6a21\u677f\u521b\u5efa RedisStreamSource \u5bf9\u8c61: apiVersion : sources.knative.dev/v1alpha1 kind : RedisStreamSource metadata : name : <redis-stream-source> spec : address : <redis-uri> stream : <redis-stream-name> group : <consumer-group-name> sink : <sink> Where: <redis-stream-source> \u662f\u4f60\u7684\u6e90\u540d\u5b57\u3002(\u5fc5\u9700) <redis-uri> \u662f Redis URI. \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 Redis \u6587\u6863 \u3002(\u5fc5\u9700) <redis-stream-name> \u662f Redis \u6d41\u7684\u540d\u79f0. (\u5fc5\u9700) <consumer-group-name> \u662f Redis \u6d88\u8d39\u7fa4\u4f53\u7684\u540d\u79f0\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u5c06\u81ea\u52a8\u4e3a\u8be5\u6e90\u521b\u5efa\u4e00\u4e2a\u7ec4\uff0c\u5e76\u5728\u5220\u9664\u8be5\u6e90\u65f6\u5220\u9664\u8be5\u7ec4\u3002(\u53ef\u9009) <sink> \u5b83\u662f\u53d1\u9001\u4e8b\u4ef6\u3002(\u5fc5\u9700) \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename> \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u521b\u5efa RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_3","text":"\u67e5\u770b event-display \u4e8b\u4ef6\u6d88\u8d39\u8005\u7684\u65e5\u5fd7: kubectl logs -l app = event-display --tail = 100 \u6837\u4f8b\u8f93\u51fa: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.redisstream source: /mystream id: 1597775814718 -0 time: 2020 -08-18T18:36:54.719802342Z datacontenttype: application/json Data, [ \"fruit\" , \"banana\" \"color\" , \"yellow\" ]","title":"\u9a8c\u8bc1 RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#redisstreamsource_4","text":"\u5220\u9664 RedisStreamSource \u5bf9\u8c61: kubectl delete -f <filename>","title":"\u5220\u9664 RedisStreamSource \u5bf9\u8c61"},{"location":"eventing/sources/redis/getting-started/#_2","text":"\u6709\u5173 Redis \u6d41\u6e90\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 eventing-redis Github \u5e93","title":"\u989d\u5916\u7684\u4fe1\u606f"},{"location":"eventing/sugar/","text":"Knative \u4e8b\u4ef6\u7cd6\u63a7 \u00b6 Knative \u4e8b\u4ef6\u7cd6\u63a7\u5c06\u5bf9\u914d\u7f6e\u7684\u6807\u7b7e\u505a\u51fa\u53cd\u5e94\uff0c\u4ee5\u751f\u6210\u6216\u63a7\u5236\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u4e8b\u4ef6\u8d44\u6e90\u3002 \u8fd9\u5141\u8bb8\u96c6\u7fa4\u64cd\u4f5c\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e13\u6ce8\u4e8e\u521b\u5efa\u66f4\u5c11\u7684\u8d44\u6e90\uff0c\u5e76\u6309\u9700\u521b\u5efa\u5e95\u5c42\u4e8b\u4ef6\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u5728\u4e0d\u518d\u9700\u8981\u65f6\u8fdb\u884c\u6e05\u7406\u3002 \u5b89\u88c5 \u00b6 \u7cd6\u63a7\u9ed8\u8ba4\u662f disabled \u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e config-sugar ConfigMap \u542f\u7528\u3002 \u53c2\u89c1\u4e0b\u9762\u7684\u7b80\u5355\u793a\u4f8b\uff0c\u5e76\u914d\u7f6e\u7cd6\u63a7\u4ee5\u83b7\u5f97\u66f4\u591a\u7ec6\u8282\u3002 \u81ea\u52a8\u521b\u5efa\u4ee3\u7406 \u00b6 \u521b\u5efa\u4ee3\u7406\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u624b\u52a8\u5c06\u8d44\u6e90\u5e94\u7528\u5230\u96c6\u7fa4: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d` ``\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u4f8b\u5982\u5728\u540d\u79f0\u7a7a\u95f4\u521b\u5efa\u65f6\uff0c\u6216\u8005\u5728\u89e6\u53d1\u5668\u521b\u5efa\u65f6\u3002 \u7cd6\u63a7\u4f7f\u8fd9\u4e9b\u7528\u4f8b\u6210\u4e3a\u53ef\u80fd\u3002 \u4e0b\u9762\u662f sugar-config ConfigMap \u7684\u793a\u4f8b\u914d\u7f6e\uff0c\u4e3a\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u6240\u6709\u89e6\u53d1\u5668\u542f\u7528\u7cd6\u63a7\u3002 apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Specify a label selector to selectively apply sugaring to certain namespaces namespace-selector : | matchExpressions: - key: \"my.custom.injection.key\" operator: \"In\" values: [\"enabled\"] # Use an empty object to enable for all triggers trigger-selector : | {} \u5f53\u4f7f\u7528\u6807\u7b7e my.custom.injection.key: enabled \u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u4e00\u4e2a\u4ee3\u7406\u88ab\u5220\u9664\uff0c\u4f46\u5f15\u7528\u7684\u6807\u7b7e\u9009\u62e9\u5668\u6b63\u5728\u4f7f\u7528\u65f6\uff0c\u7cd6\u63a7\u5c06\u81ea\u52a8\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u9ed8\u8ba4\u4ee3\u7406\u3002 \u540d\u79f0\u7a7a\u95f4\u7684\u4f8b\u5b50 \u00b6 \u5728\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : v1 kind : Namespace metadata : name : example labels : my.custom.injection.key : enabled \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u8981\u5728\u547d\u540d\u7a7a\u95f4\u5b58\u5728\u540e\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u8bf7\u5c06\u547d\u540d\u7a7a\u95f4\u6807\u8bb0\u4e3a: kubectl label namespace default my.custom.injection.key = enabled \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\u3002 \u89e6\u53d1\u7684\u4f8b\u5b50 \u00b6 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u5728\u89e6\u53d1\u5668\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF \u8fd9\u5c06\u5728\u547d\u540d\u7a7a\u95f4\u201chello\u201d\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\uff0c\u5e76\u5c1d\u8bd5\u5411\u201cevent-display\u201d\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\u3002 \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\uff0c\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u62e5\u6709\u73b0\u6709\u7684\u4ee3\u7406\u3002","title":"\u7cd6\u63a7\u5236\u5668"},{"location":"eventing/sugar/#knative","text":"Knative \u4e8b\u4ef6\u7cd6\u63a7\u5c06\u5bf9\u914d\u7f6e\u7684\u6807\u7b7e\u505a\u51fa\u53cd\u5e94\uff0c\u4ee5\u751f\u6210\u6216\u63a7\u5236\u96c6\u7fa4\u6216\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u4e8b\u4ef6\u8d44\u6e90\u3002 \u8fd9\u5141\u8bb8\u96c6\u7fa4\u64cd\u4f5c\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e13\u6ce8\u4e8e\u521b\u5efa\u66f4\u5c11\u7684\u8d44\u6e90\uff0c\u5e76\u6309\u9700\u521b\u5efa\u5e95\u5c42\u4e8b\u4ef6\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u5728\u4e0d\u518d\u9700\u8981\u65f6\u8fdb\u884c\u6e05\u7406\u3002","title":"Knative \u4e8b\u4ef6\u7cd6\u63a7"},{"location":"eventing/sugar/#_1","text":"\u7cd6\u63a7\u9ed8\u8ba4\u662f disabled \u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e config-sugar ConfigMap \u542f\u7528\u3002 \u53c2\u89c1\u4e0b\u9762\u7684\u7b80\u5355\u793a\u4f8b\uff0c\u5e76\u914d\u7f6e\u7cd6\u63a7\u4ee5\u83b7\u5f97\u66f4\u591a\u7ec6\u8282\u3002","title":"\u5b89\u88c5"},{"location":"eventing/sugar/#_2","text":"\u521b\u5efa\u4ee3\u7406\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9ed8\u8ba4\u8bbe\u7f6e\u624b\u52a8\u5c06\u8d44\u6e90\u5e94\u7528\u5230\u96c6\u7fa4: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d` ``\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u4f8b\u5982\u5728\u540d\u79f0\u7a7a\u95f4\u521b\u5efa\u65f6\uff0c\u6216\u8005\u5728\u89e6\u53d1\u5668\u521b\u5efa\u65f6\u3002 \u7cd6\u63a7\u4f7f\u8fd9\u4e9b\u7528\u4f8b\u6210\u4e3a\u53ef\u80fd\u3002 \u4e0b\u9762\u662f sugar-config ConfigMap \u7684\u793a\u4f8b\u914d\u7f6e\uff0c\u4e3a\u9009\u5b9a\u7684\u540d\u79f0\u7a7a\u95f4\u548c\u6240\u6709\u89e6\u53d1\u5668\u542f\u7528\u7cd6\u63a7\u3002 apiVersion : v1 kind : ConfigMap metadata : name : config-sugar namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Specify a label selector to selectively apply sugaring to certain namespaces namespace-selector : | matchExpressions: - key: \"my.custom.injection.key\" operator: \"In\" values: [\"enabled\"] # Use an empty object to enable for all triggers trigger-selector : | {} \u5f53\u4f7f\u7528\u6807\u7b7e my.custom.injection.key: enabled \u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u7cd6\u63a7\u5c06\u5728\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a default \u7684\u4ee3\u7406\u3002 \u5f53\u4e00\u4e2a\u4ee3\u7406\u88ab\u5220\u9664\uff0c\u4f46\u5f15\u7528\u7684\u6807\u7b7e\u9009\u62e9\u5668\u6b63\u5728\u4f7f\u7528\u65f6\uff0c\u7cd6\u63a7\u5c06\u81ea\u52a8\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u9ed8\u8ba4\u4ee3\u7406\u3002","title":"\u81ea\u52a8\u521b\u5efa\u4ee3\u7406"},{"location":"eventing/sugar/#_3","text":"\u5728\u521b\u5efa\u547d\u540d\u7a7a\u95f4\u65f6\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: \u5c06\u4ee5\u4e0b YAML \u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u4e2d: apiVersion : v1 kind : Namespace metadata : name : example labels : my.custom.injection.key : enabled \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u8981\u5728\u547d\u540d\u7a7a\u95f4\u5b58\u5728\u540e\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\uff0c\u8bf7\u5c06\u547d\u540d\u7a7a\u95f4\u6807\u8bb0\u4e3a: kubectl label namespace default my.custom.injection.key = enabled \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\u3002","title":"\u540d\u79f0\u7a7a\u95f4\u7684\u4f8b\u5b50"},{"location":"eventing/sugar/#_4","text":"\u5f53\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u65f6\uff0c\u5728\u89e6\u53d1\u5668\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efa\u4e00\u4e2a\"default\"\u4ee3\u7406: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF \u8fd9\u5c06\u5728\u547d\u540d\u7a7a\u95f4\u201chello\u201d\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\uff0c\u5e76\u5c1d\u8bd5\u5411\u201cevent-display\u201d\u670d\u52a1\u53d1\u9001\u4e8b\u4ef6\u3002 \u5982\u679c\u547d\u540d\u4e3a\u201cdefault\u201d\u7684\u4ee3\u7406\u5df2\u7ecf\u5b58\u5728\u4e8e\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u7cd6\u63a7\u5c06\u4e0d\u505a\u4efb\u4f55\u4e8b\u60c5\uff0c\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u62e5\u6709\u73b0\u6709\u7684\u4ee3\u7406\u3002","title":"\u89e6\u53d1\u7684\u4f8b\u5b50"},{"location":"eventing/triggers/","text":"\u4f7f\u7528\u89e6\u53d1\u5668 \u00b6 \u89e6\u53d1\u5668\u8868\u793a\u4ece\u7279\u5b9a\u4ee3\u7406\u8ba2\u9605\u4e8b\u4ef6\u7684\u613f\u671b\u3002 subscriber \u503c\u5fc5\u987b\u662f type Destination . \u89e6\u53d1\u5668\u4e3e\u4f8b \u00b6 \u4e0b\u9762\u7684\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9 Knative \u670d\u52a1 my-service : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u4ee5\u4e0b\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4ea4\u4ed8\u5230 Kubernetes \u670d\u52a1 my-service \u7684\u81ea\u5b9a\u4e49\u8def\u5f84 /my-custom-path : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u89e6\u53d1\u8fc7\u6ee4 \u00b6 \u652f\u6301\u5bf9\u4efb\u610f\u6570\u91cf\u7684 CloudEvents \u5c5e\u6027\u548c\u6269\u5c55\u8fdb\u884c\u7cbe\u786e\u5339\u914d\u7b5b\u9009\u3002 \u5982\u679c\u8fc7\u6ee4\u5668\u8bbe\u7f6e\u4e86\u591a\u4e2a\u5c5e\u6027\uff0c\u90a3\u4e48\u4e00\u4e2a\u4e8b\u4ef6\u5fc5\u987b\u5177\u6709\u89e6\u53d1\u5668\u7b5b\u9009\u5b83\u6240\u9700\u7684\u6240\u6709\u5c5e\u6027\u3002 \u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u652f\u6301\u5b57\u7b26\u4e32\u503c\u7684\u7cbe\u786e\u5339\u914d\u3002 \u4e3e\u4f8b \u00b6 \u6b64\u793a\u4f8b\u8fc7\u6ee4\u6765\u81ea default \u4ee3\u7406\u7684\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u7c7b\u578b\u4e3a dev.knative.foo.bar \uff0c\u6269\u5c55\u540d\u4e3a myextension \uff0c\u503c\u4e3a my-extension-value \u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u89e6\u53d1\u5668\u6ce8\u91ca \u00b6 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u4ee5\u4e0b\u4e24\u4e2a\u6ce8\u91ca\u6765\u4fee\u6539\u89e6\u53d1\u5668\u7684\u884c\u4e3a: eventing.knative.dev/injection : \u5982\u679c\u8bbe\u7f6e\u4e3a enabled \uff0c\u4e8b\u4ef6\u5c06\u81ea\u52a8\u4e3a\u89e6\u53d1\u5668\u521b\u5efa\u4e00\u4e2a\u4e0d\u5b58\u5728\u7684\u4ee3\u7406\u3002 \u4ee3\u7406\u5728\u521b\u5efa\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u6b64\u6ce8\u91ca\u4ec5\u5728\u542f\u7528 \u7cd6\u63a7\u5236\u5668 \u65f6\u6709\u6548\uff0c\u8fd9\u662f\u53ef\u9009\u7684\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u542f\u7528\u3002 knative.dev/dependency : \u6b64\u6ce8\u91ca\u7528\u4e8e\u6807\u8bb0\u89e6\u53d1\u5668\u6240\u4f9d\u8d56\u7684\u6e90\u3002 \u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u4f9d\u8d56\u9879\u6ca1\u6709\u51c6\u5907\u597d\uff0c\u90a3\u4e48\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u51c6\u5907\u597d\u3002 \u4e0b\u9762\u7684 YAML \u662f\u4e00\u4e2a\u5e26\u6709\u4f9d\u8d56\u7684\u89e6\u53d1\u5668\u7684\u4f8b\u5b50: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"\u4f7f\u7528\u89e6\u53d1\u5668"},{"location":"eventing/triggers/#_1","text":"\u89e6\u53d1\u5668\u8868\u793a\u4ece\u7279\u5b9a\u4ee3\u7406\u8ba2\u9605\u4e8b\u4ef6\u7684\u613f\u671b\u3002 subscriber \u503c\u5fc5\u987b\u662f type Destination .","title":"\u4f7f\u7528\u89e6\u53d1\u5668"},{"location":"eventing/triggers/#_2","text":"\u4e0b\u9762\u7684\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9 Knative \u670d\u52a1 my-service : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002 \u4ee5\u4e0b\u89e6\u53d1\u5668\u4ece default \u4ee3\u7406\u63a5\u6536\u6240\u6709\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u4ea4\u4ed8\u5230 Kubernetes \u670d\u52a1 my-service \u7684\u81ea\u5b9a\u4e49\u8def\u5f84 /my-custom-path : \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d' '\u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u89e6\u53d1\u5668\u4e3e\u4f8b"},{"location":"eventing/triggers/#_3","text":"\u652f\u6301\u5bf9\u4efb\u610f\u6570\u91cf\u7684 CloudEvents \u5c5e\u6027\u548c\u6269\u5c55\u8fdb\u884c\u7cbe\u786e\u5339\u914d\u7b5b\u9009\u3002 \u5982\u679c\u8fc7\u6ee4\u5668\u8bbe\u7f6e\u4e86\u591a\u4e2a\u5c5e\u6027\uff0c\u90a3\u4e48\u4e00\u4e2a\u4e8b\u4ef6\u5fc5\u987b\u5177\u6709\u89e6\u53d1\u5668\u7b5b\u9009\u5b83\u6240\u9700\u7684\u6240\u6709\u5c5e\u6027\u3002 \u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u652f\u6301\u5b57\u7b26\u4e32\u503c\u7684\u7cbe\u786e\u5339\u914d\u3002","title":"\u89e6\u53d1\u8fc7\u6ee4"},{"location":"eventing/triggers/#_4","text":"\u6b64\u793a\u4f8b\u8fc7\u6ee4\u6765\u81ea default \u4ee3\u7406\u7684\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u7c7b\u578b\u4e3a dev.knative.foo.bar \uff0c\u6269\u5c55\u540d\u4e3a myextension \uff0c\u503c\u4e3a my-extension-value \u3002 \u4f7f\u7528\u4ee5\u4e0b\u793a\u4f8b\u521b\u5efa\u4e00\u4e2a YAML \u6587\u4ef6: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528 YAML \u6587\u4ef6: kubectl apply -f <filename>.yaml \u5176\u4e2d <filename> \u662f\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6587\u4ef6\u7684\u540d\u79f0\u3002","title":"\u4e3e\u4f8b"},{"location":"eventing/triggers/#_5","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u4ee5\u4e0b\u4e24\u4e2a\u6ce8\u91ca\u6765\u4fee\u6539\u89e6\u53d1\u5668\u7684\u884c\u4e3a: eventing.knative.dev/injection : \u5982\u679c\u8bbe\u7f6e\u4e3a enabled \uff0c\u4e8b\u4ef6\u5c06\u81ea\u52a8\u4e3a\u89e6\u53d1\u5668\u521b\u5efa\u4e00\u4e2a\u4e0d\u5b58\u5728\u7684\u4ee3\u7406\u3002 \u4ee3\u7406\u5728\u521b\u5efa\u89e6\u53d1\u5668\u7684\u540d\u79f0\u7a7a\u95f4\u4e2d\u521b\u5efa\u3002 \u6b64\u6ce8\u91ca\u4ec5\u5728\u542f\u7528 \u7cd6\u63a7\u5236\u5668 \u65f6\u6709\u6548\uff0c\u8fd9\u662f\u53ef\u9009\u7684\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u542f\u7528\u3002 knative.dev/dependency : \u6b64\u6ce8\u91ca\u7528\u4e8e\u6807\u8bb0\u89e6\u53d1\u5668\u6240\u4f9d\u8d56\u7684\u6e90\u3002 \u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u4f9d\u8d56\u9879\u6ca1\u6709\u51c6\u5907\u597d\uff0c\u90a3\u4e48\u89e6\u53d1\u5668\u4e5f\u4e0d\u4f1a\u51c6\u5907\u597d\u3002 \u4e0b\u9762\u7684 YAML \u662f\u4e00\u4e2a\u5e26\u6709\u4f9d\u8d56\u7684\u89e6\u53d1\u5668\u7684\u4f8b\u5b50: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"\u89e6\u53d1\u5668\u6ce8\u91ca"},{"location":"eventing/troubleshooting/","text":"Debugging Knative Eventing \u00b6 This is an evolving document on how to debug a non-working Knative Eventing setup. Audience \u00b6 This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together. Prerequisites \u00b6 Setup Knative Eventing and an Eventing-contrib resource . Example \u00b6 This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml Triggering Events \u00b6 Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2 Where are my events? \u00b6 You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem? Check created resources \u00b6 The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc . fn \u00b6 kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide. svc \u00b6 kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn . chan \u00b6 chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready . Service \u00b6 chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller . src \u00b6 src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' sub \u00b6 sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml Controllers \u00b6 Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document. Service Controller \u00b6 The Kubernetes Service Controller, controlling svc , is out of scope for this document. Channel Controller \u00b6 There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error . Source Controller \u00b6 Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller . ApiServerSource Controller \u00b6 The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error . Subscription Controller \u00b6 The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error . Data Plane \u00b6 The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel. Channel Dispatcher \u00b6 The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"\u8c03\u8bd5"},{"location":"eventing/troubleshooting/#debugging-knative-eventing","text":"This is an evolving document on how to debug a non-working Knative Eventing setup.","title":"Debugging Knative Eventing"},{"location":"eventing/troubleshooting/#audience","text":"This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together.","title":"Audience"},{"location":"eventing/troubleshooting/#prerequisites","text":"Setup Knative Eventing and an Eventing-contrib resource .","title":"Prerequisites"},{"location":"eventing/troubleshooting/#example","text":"This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml","title":"Example"},{"location":"eventing/troubleshooting/#triggering-events","text":"Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2","title":"Triggering Events"},{"location":"eventing/troubleshooting/#where-are-my-events","text":"You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem?","title":"Where are my events?"},{"location":"eventing/troubleshooting/#check-created-resources","text":"The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc .","title":"Check created resources"},{"location":"eventing/troubleshooting/#fn","text":"kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide.","title":"fn"},{"location":"eventing/troubleshooting/#svc","text":"kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn .","title":"svc"},{"location":"eventing/troubleshooting/#chan","text":"chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready .","title":"chan"},{"location":"eventing/troubleshooting/#service","text":"chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller .","title":"Service"},{"location":"eventing/troubleshooting/#src","text":"src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}'","title":"src"},{"location":"eventing/troubleshooting/#sub","text":"sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml","title":"sub"},{"location":"eventing/troubleshooting/#controllers","text":"Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document.","title":"Controllers"},{"location":"eventing/troubleshooting/#service-controller","text":"The Kubernetes Service Controller, controlling svc , is out of scope for this document.","title":"Service Controller"},{"location":"eventing/troubleshooting/#channel-controller","text":"There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Channel Controller"},{"location":"eventing/troubleshooting/#source-controller","text":"Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller .","title":"Source Controller"},{"location":"eventing/troubleshooting/#apiserversource-controller","text":"The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"ApiServerSource Controller"},{"location":"eventing/troubleshooting/#subscription-controller","text":"The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Subscription Controller"},{"location":"eventing/troubleshooting/#data-plane","text":"The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel.","title":"Data Plane"},{"location":"eventing/troubleshooting/#channel-dispatcher","text":"The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"Channel Dispatcher"},{"location":"functions/","text":"Knative \u51fd\u6570\u6982\u8ff0 \u00b6 Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002 \u51fd\u6570\u6a21\u677f \u00b6 Knative\u51fd\u6570\u901a\u8fc7\u5728\u8fd0\u884c create \u547d\u4ee4\u65f6\u542f\u52a8\u51fd\u6570\u9879\u76ee\u6837\u677f\uff0c\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u521b\u5efa\u57fa\u672c\u51fd\u6570\u7684\u6a21\u677f\u3002 \u6a21\u677f\u5141\u8bb8\u60a8\u4e3a\u51fd\u6570\u9009\u62e9\u8bed\u8a00\u548c\u8c03\u7528\u683c\u5f0f\u3002\u4ee5\u4e0b\u6a21\u677f\u9002\u7528\u4e8eCloudEvent\u548cHTTP\u8c03\u7528\u683c\u5f0f: Node.js Python Go Quarkus Rust TypeScript \u8bed\u8a00\u5305 \u00b6 \u51fd\u6570\u53ef\u4ee5\u7528\u53ef\u7528 \u8bed\u8a00\u5305 \u652f\u6301\u7684\u4efb\u4f55\u8bed\u8a00\u7f16\u5199. \u51fd\u6570\u5165\u95e8 \u00b6 \u5728\u4f7f\u7528Knative\u51fd\u6570\u4e4b\u524d\uff0c\u5fc5\u987b\u80fd\u591f\u8bbf\u95eeKnative\u5f00\u53d1\u73af\u5883\u3002 \u8981\u5efa\u7acb\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\uff0c\u60a8\u53ef\u4ee5\u8ddf\u968f Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b \u3002","title":"\u51fd\u6570\u6982\u8ff0"},{"location":"functions/#knative","text":"Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"Knative \u51fd\u6570\u6982\u8ff0"},{"location":"functions/#_1","text":"Knative\u51fd\u6570\u901a\u8fc7\u5728\u8fd0\u884c create \u547d\u4ee4\u65f6\u542f\u52a8\u51fd\u6570\u9879\u76ee\u6837\u677f\uff0c\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u521b\u5efa\u57fa\u672c\u51fd\u6570\u7684\u6a21\u677f\u3002 \u6a21\u677f\u5141\u8bb8\u60a8\u4e3a\u51fd\u6570\u9009\u62e9\u8bed\u8a00\u548c\u8c03\u7528\u683c\u5f0f\u3002\u4ee5\u4e0b\u6a21\u677f\u9002\u7528\u4e8eCloudEvent\u548cHTTP\u8c03\u7528\u683c\u5f0f: Node.js Python Go Quarkus Rust TypeScript","title":"\u51fd\u6570\u6a21\u677f"},{"location":"functions/#_2","text":"\u51fd\u6570\u53ef\u4ee5\u7528\u53ef\u7528 \u8bed\u8a00\u5305 \u652f\u6301\u7684\u4efb\u4f55\u8bed\u8a00\u7f16\u5199.","title":"\u8bed\u8a00\u5305"},{"location":"functions/#_3","text":"\u5728\u4f7f\u7528Knative\u51fd\u6570\u4e4b\u524d\uff0c\u5fc5\u987b\u80fd\u591f\u8bbf\u95eeKnative\u5f00\u53d1\u73af\u5883\u3002 \u8981\u5efa\u7acb\u4e00\u4e2a\u5f00\u53d1\u73af\u5883\uff0c\u60a8\u53ef\u4ee5\u8ddf\u968f Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b \u3002","title":"\u51fd\u6570\u5165\u95e8"},{"location":"functions/building-functions/","text":"\u6784\u5efa\u51fd\u6570 \u00b6 \u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u672c\u5730\u6784\u5efa \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 build \u547d\u4ee4\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u800c\u65e0\u9700\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build \u96c6\u7fa4\u6784\u5efa \u00b6 \u5982\u679c\u60a8\u6ca1\u6709\u8fd0\u884c\u672c\u5730Docker\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6216\u8005\u60a8\u6b63\u5728\u4f7f\u7528CI/CD\u7ba1\u9053\uff0c\u90a3\u4e48\u60a8\u53ef\u80fd\u5e0c\u671b\u5728\u96c6\u7fa4\u4e0a\u6784\u5efa\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u672c\u5730\u6784\u5efa\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func deploy --remote \u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u96c6\u7fa4\u4e0a\u6784\u5efa\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8be5\u51fd\u6570\u5fc5\u987b\u5b58\u5728\u4e8eGit\u5b58\u50a8\u5e93\u4e2d\u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684\u96c6\u7fa4\u4ee5\u4f7f\u7528Tekton pipeline\u3002\u8bf7\u53c2\u9605 \u96c6\u7fa4\u6784\u5efa \u6587\u6863\u3002 \u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\u65f6\uff0c\u5fc5\u987b\u6307\u5b9a\u8be5\u51fd\u6570\u7684Git URL: func kn func func deploy --remote --registry <registry> --git-url <git-url> -p hello kn func deploy --remote --registry <registry> --git-url <git-url> -p hello \u5728\u4e3a\u51fd\u6570\u6307\u5b9aGit URL\u4e00\u6b21\u4e4b\u540e\uff0c\u53ef\u4ee5\u5728\u540e\u7eed\u547d\u4ee4\u4e2d\u7701\u7565\u5b83\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"functions/building-functions/#_1","text":"\u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"functions/building-functions/#_2","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 build \u547d\u4ee4\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u800c\u65e0\u9700\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002","title":"\u672c\u5730\u6784\u5efa"},{"location":"functions/building-functions/#_3","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/building-functions/#_4","text":"build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u8fc7\u7a0b"},{"location":"functions/building-functions/#_5","text":"\u5982\u679c\u60a8\u6ca1\u6709\u8fd0\u884c\u672c\u5730Docker\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6216\u8005\u60a8\u6b63\u5728\u4f7f\u7528CI/CD\u7ba1\u9053\uff0c\u90a3\u4e48\u60a8\u53ef\u80fd\u5e0c\u671b\u5728\u96c6\u7fa4\u4e0a\u6784\u5efa\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u672c\u5730\u6784\u5efa\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func deploy --remote \u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u96c6\u7fa4\u4e0a\u6784\u5efa\u3002","title":"\u96c6\u7fa4\u6784\u5efa"},{"location":"functions/building-functions/#_6","text":"\u8be5\u51fd\u6570\u5fc5\u987b\u5b58\u5728\u4e8eGit\u5b58\u50a8\u5e93\u4e2d\u3002 \u60a8\u5fc5\u987b\u914d\u7f6e\u60a8\u7684\u96c6\u7fa4\u4ee5\u4f7f\u7528Tekton pipeline\u3002\u8bf7\u53c2\u9605 \u96c6\u7fa4\u6784\u5efa \u6587\u6863\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/building-functions/#_7","text":"\u7b2c\u4e00\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\u65f6\uff0c\u5fc5\u987b\u6307\u5b9a\u8be5\u51fd\u6570\u7684Git URL: func kn func func deploy --remote --registry <registry> --git-url <git-url> -p hello kn func deploy --remote --registry <registry> --git-url <git-url> -p hello \u5728\u4e3a\u51fd\u6570\u6307\u5b9aGit URL\u4e00\u6b21\u4e4b\u540e\uff0c\u53ef\u4ee5\u5728\u540e\u7eed\u547d\u4ee4\u4e2d\u7701\u7565\u5b83\u3002","title":"\u8fc7\u7a0b"},{"location":"functions/creating-functions/","text":"\u521b\u5efa\u51fd\u6570 \u00b6 \u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"functions/creating-functions/#_1","text":"\u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"functions/deploying-functions/","text":"\u90e8\u7f72\u51fd\u6570 \u00b6 \u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002 \u8fc7\u7a0b \u00b6 deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"functions/deploying-functions/#_1","text":"\u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"functions/deploying-functions/#_2","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/deploying-functions/#_3","text":"deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"functions/install-func/","text":"\u5b89\u88c5 Knative \u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002 \u5b89\u88c5 func \u547d\u4ee4\u884c \u00b6 Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func . \u5b89\u88c5 kn func \u547d\u4ee4\u884c\u63d2\u4ef6 \u00b6 kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5\u51fd\u6570"},{"location":"functions/install-func/#knative","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002","title":"\u5b89\u88c5 Knative \u51fd\u6570"},{"location":"functions/install-func/#func","text":"Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func .","title":"\u5b89\u88c5 func \u547d\u4ee4\u884c"},{"location":"functions/install-func/#kn-func","text":"kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5  kn func \u547d\u4ee4\u884c\u63d2\u4ef6"},{"location":"functions/invoking-functions/","text":"\u8c03\u7528\u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u6765\u8c03\u7528\u672c\u5730\u6216Knative\u96c6\u7fa4\u4e0a\u7684\u51fd\u6570\u3002 \u8fd9\u4e2a\u547d\u4ee4\u53ef\u4ee5\u7528\u6765\u6d4b\u8bd5\u4e00\u4e2a\u51fd\u6570\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff0c\u662f\u5426\u80fd\u591f\u6b63\u786e\u5730\u63a5\u6536HTTP\u8bf7\u6c42\u548cCloudEvents\u3002 \u5982\u679c\u4f60\u7684\u51fd\u6570\u5728\u672c\u5730\u8fd0\u884c\uff0c func invoke \u4f1a\u5411\u672c\u5730\u5b9e\u4f8b\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u9001\u5230\u5e26\u6709 --data \u6807\u5fd7\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5176\u4ed6\u9009\u9879\u6765\u6a21\u62df\u4e0d\u540c\u7c7b\u578b\u7684\u8bf7\u6c42\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 \u51fd\u6570\u8c03\u7528 \u6587\u6863\u3002","title":"\u8c03\u7528\u51fd\u6570"},{"location":"functions/invoking-functions/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u6765\u8c03\u7528\u672c\u5730\u6216Knative\u96c6\u7fa4\u4e0a\u7684\u51fd\u6570\u3002 \u8fd9\u4e2a\u547d\u4ee4\u53ef\u4ee5\u7528\u6765\u6d4b\u8bd5\u4e00\u4e2a\u51fd\u6570\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff0c\u662f\u5426\u80fd\u591f\u6b63\u786e\u5730\u63a5\u6536HTTP\u8bf7\u6c42\u548cCloudEvents\u3002 \u5982\u679c\u4f60\u7684\u51fd\u6570\u5728\u672c\u5730\u8fd0\u884c\uff0c func invoke \u4f1a\u5411\u672c\u5730\u5b9e\u4f8b\u53d1\u9001\u4e00\u4e2a\u6d4b\u8bd5\u8bf7\u6c42\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 func invoke \u547d\u4ee4\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u9001\u5230\u5e26\u6709 --data \u6807\u5fd7\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5176\u4ed6\u9009\u9879\u6765\u6a21\u62df\u4e0d\u540c\u7c7b\u578b\u7684\u8bf7\u6c42\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u89c1 \u51fd\u6570\u8c03\u7528 \u6587\u6863\u3002","title":"\u8c03\u7528\u51fd\u6570"},{"location":"functions/language-packs/","text":"\u8bed\u8a00\u5305 \u00b6 \u8bed\u8a00\u5305\u53ef\u7528\u4e8e\u6269\u5c55Knative\u51fd\u6570\uff0c\u4ee5\u652f\u6301\u989d\u5916\u7684\u8fd0\u884c\u65f6\u3001\u51fd\u6570\u7b7e\u540d\u3001\u64cd\u4f5c\u7cfb\u7edf\u548c\u5df2\u5b89\u88c5\u7684\u51fd\u6570\u5de5\u5177\u3002 \u8bed\u8a00\u5305\u901a\u8fc7Git\u5b58\u50a8\u5e93\u6216\u4f5c\u4e3a\u78c1\u76d8\u4e0a\u7684\u76ee\u5f55\u5206\u53d1\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u8bed\u8a00\u5305 \u6587\u6863\u3002 \u4f7f\u7528\u5916\u90e8Git\u5b58\u50a8\u5e93 \u00b6 \u5728\u521b\u5efa\u65b0\u51fd\u6570\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9aGit\u5b58\u50a8\u5e93\u4f5c\u4e3a\u6a21\u677f\u6587\u4ef6\u7684\u6e90\u3002 Knative\u6c99\u76d2\u7ef4\u62a4\u4e86\u4e00\u7ec4 \u793a\u4f8b\u6a21\u677f \uff0c\u53ef\u4ee5\u5728\u9879\u76ee\u521b\u5efa\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e3aNode.js\u4f7f\u7528 metacontroller \u6a21\u677f: func create myfunc -l nodejs -t metacontroller --repository https://github.com/knative-sandbox/func-tastic \u5728\u672c\u5730\u5b89\u88c5\u8bed\u8a00\u5305 \u00b6 \u8bed\u8a00\u5305\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func repository \u547d\u4ee4\u5728\u672c\u5730\u5b89\u88c5\u3002 \u4f8b\u5982\uff0c\u8981\u6dfb\u52a0Knative Sandbox\u793a\u4f8b\u6a21\u677f\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func repository add knative https://github.com/knative-sandbox/func-tastic \u5b89\u88c5Knative\u6c99\u76d2\u793a\u4f8b\u6a21\u677f\u540e\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 create \u547d\u4ee4\u4e2d\u6307\u5b9a Knative \u524d\u7f00\u6765\u4f7f\u7528 metacontroller \u6a21\u677f: func create -t knative/metacontroller -l nodejs my-controller-function","title":"\u8bed\u8a00\u5305"},{"location":"functions/language-packs/#_1","text":"\u8bed\u8a00\u5305\u53ef\u7528\u4e8e\u6269\u5c55Knative\u51fd\u6570\uff0c\u4ee5\u652f\u6301\u989d\u5916\u7684\u8fd0\u884c\u65f6\u3001\u51fd\u6570\u7b7e\u540d\u3001\u64cd\u4f5c\u7cfb\u7edf\u548c\u5df2\u5b89\u88c5\u7684\u51fd\u6570\u5de5\u5177\u3002 \u8bed\u8a00\u5305\u901a\u8fc7Git\u5b58\u50a8\u5e93\u6216\u4f5c\u4e3a\u78c1\u76d8\u4e0a\u7684\u76ee\u5f55\u5206\u53d1\u3002 \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u8bed\u8a00\u5305 \u6587\u6863\u3002","title":"\u8bed\u8a00\u5305"},{"location":"functions/language-packs/#git","text":"\u5728\u521b\u5efa\u65b0\u51fd\u6570\u65f6\uff0c\u53ef\u4ee5\u6307\u5b9aGit\u5b58\u50a8\u5e93\u4f5c\u4e3a\u6a21\u677f\u6587\u4ef6\u7684\u6e90\u3002 Knative\u6c99\u76d2\u7ef4\u62a4\u4e86\u4e00\u7ec4 \u793a\u4f8b\u6a21\u677f \uff0c\u53ef\u4ee5\u5728\u9879\u76ee\u521b\u5efa\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e3aNode.js\u4f7f\u7528 metacontroller \u6a21\u677f: func create myfunc -l nodejs -t metacontroller --repository https://github.com/knative-sandbox/func-tastic","title":"\u4f7f\u7528\u5916\u90e8Git\u5b58\u50a8\u5e93"},{"location":"functions/language-packs/#_2","text":"\u8bed\u8a00\u5305\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func repository \u547d\u4ee4\u5728\u672c\u5730\u5b89\u88c5\u3002 \u4f8b\u5982\uff0c\u8981\u6dfb\u52a0Knative Sandbox\u793a\u4f8b\u6a21\u677f\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func repository add knative https://github.com/knative-sandbox/func-tastic \u5b89\u88c5Knative\u6c99\u76d2\u793a\u4f8b\u6a21\u677f\u540e\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 create \u547d\u4ee4\u4e2d\u6307\u5b9a Knative \u524d\u7f00\u6765\u4f7f\u7528 metacontroller \u6a21\u677f: func create -t knative/metacontroller -l nodejs my-controller-function","title":"\u5728\u672c\u5730\u5b89\u88c5\u8bed\u8a00\u5305"},{"location":"functions/running-functions/","text":"\u8fd0\u884c\u51fd\u6570 \u00b6 \u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 \u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"functions/running-functions/#_1","text":"\u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"functions/running-functions/#_2","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"functions/running-functions/#_3","text":"\u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/","text":"\u6b22\u8fce\u6765\u5230Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b \u00b6 \u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528Knative Quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730Knative\u5b89\u88c5\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u7b80\u5355\u7684Knative\u90e8\u7f72\u6765\u5c1d\u8bd5 Knative\u670d\u52a1 \u548c Knative\u4e8b\u4ef6 \u7684\u5e38\u7528\u7279\u6027\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002 \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002 Tip \u5728\u952e\u76d8\u4e0a\u70b9\u51fb . (\u53e5\u53f7) \u53ef\u4ee5\u5728\u6559\u7a0b\u4e2d\u524d\u8fdb. \u7528 , (\u9017\u53f7) \u968f\u65f6\u53ef\u4ee5\u56de\u53bb.","title":"\u6559\u7a0b\u7b80\u4ecb"},{"location":"getting-started/#knative","text":"\u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528Knative Quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730Knative\u5b89\u88c5\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u7b80\u5355\u7684Knative\u90e8\u7f72\u6765\u5c1d\u8bd5 Knative\u670d\u52a1 \u548c Knative\u4e8b\u4ef6 \u7684\u5e38\u7528\u7279\u6027\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002","title":"\u6b22\u8fce\u6765\u5230Knative\u5feb\u901f\u5165\u95e8\u6559\u7a0b"},{"location":"getting-started/#_1","text":"Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002 Tip \u5728\u952e\u76d8\u4e0a\u70b9\u51fb . (\u53e5\u53f7) \u53ef\u4ee5\u5728\u6559\u7a0b\u4e2d\u524d\u8fdb. \u7528 , (\u9017\u53f7) \u968f\u65f6\u53ef\u4ee5\u56de\u53bb.","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"getting-started/about-knative-functions/","text":"\u5173\u4e8e Knative \u51fd\u6570 \u00b6 Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"\u5173\u4e8e\u51fd\u6570"},{"location":"getting-started/about-knative-functions/#knative","text":"Knative\u51fd\u6570\u4e3a\u5728Knative\u4e0a\u4f7f\u7528\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u6df1\u5165\u4e86\u89e3Knative\u3001Kubernetes\u3001\u5bb9\u5668\u6216dockerfile\u3002 Knative\u51fd\u6570\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 func CLI\u8f7b\u677e\u521b\u5efa\u3001\u6784\u5efa\u548c\u90e8\u7f72\u4f5c\u4e3aKnative\u670d\u52a1\u7684\u65e0\u72b6\u6001\u4e8b\u4ef6\u9a71\u52a8\u51fd\u6570\u3002 \u5f53\u60a8\u6784\u5efa\u6216\u8fd0\u884c\u4e00\u4e2a\u51fd\u6570\u65f6\uff0c\u4e00\u4e2a \u5f00\u653e\u5bb9\u5668\u521d\u59cb\u5316(OCI)\u683c\u5f0f \u5bb9\u5668\u6620\u50cf\u5c06\u81ea\u52a8\u4e3a\u60a8\u751f\u6210\uff0c\u5e76\u5b58\u50a8\u5728\u5bb9\u5668\u6ce8\u518c\u8868\u4e2d\u3002 \u6bcf\u6b21\u66f4\u65b0\u4ee3\u7801\u5e76\u8fd0\u884c\u6216\u90e8\u7f72\u5b83\u65f6\uff0c\u5bb9\u5668\u6620\u50cf\u4e5f\u4f1a\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 func CLI\u6216\u4f7f\u7528Knative CLI\u7684 kn func \u63d2\u4ef6\u6765\u521b\u5efa\u51fd\u6570\u548c\u7ba1\u7406\u51fd\u6570\u5de5\u4f5c\u6d41\u3002","title":"\u5173\u4e8e Knative \u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/","text":"\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570 \u00b6 \u5728\u521b\u5efa\u4e86\u51fd\u6570\u9879\u76ee\u4e4b\u540e\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u4f8b\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\u3002 \u8fd0\u884c\u51fd\u6570 \u00b6 \u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 \u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body: \u90e8\u7f72\u51fd\u6570 \u00b6 \u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002 \u8fc7\u7a0b \u00b6 deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body: \u6784\u5efa\u51fd\u6570 \u00b6 \u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u8fc7\u7a0b \u00b6 build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u6784\u5efa\u8fd0\u884c\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_1","text":"\u5728\u521b\u5efa\u4e86\u51fd\u6570\u9879\u76ee\u4e4b\u540e\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u4f8b\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\u3002","title":"\u6784\u5efa\u3001\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_2","text":"\u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u51fd\u6570\u4e4b\u524d\uff0c\u8fd0\u884c\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u955c\u50cf\uff0c\u4f46\u4e0d\u4f1a\u5c06\u51fd\u6570\u90e8\u7f72\u5230\u96c6\u7fa4\u3002 \u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u6d4b\u8bd5\u573a\u666f\u8fd0\u884c\u51fd\u6570\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u8fd0\u884c\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_3","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_4","text":"\u5982\u679c\u9700\u8981\uff0c run \u547d\u4ee4\u4e3a\u51fd\u6570\u6784\u5efa\u4e00\u4e2a\u6620\u50cf\uff0c\u5e76\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6620\u50cf\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e0a\u3002 func kn func \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u5f3a\u5236\u6620\u50cf\u7684\u91cd\u5efa: func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528\u6784\u5efa: func run --build = false \u5728\u672c\u5730\u8fd0\u884c\u51fd\u6570\uff0c\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4: kn func run \u5982\u679c\u9700\u8981\uff0c\u4f7f\u7528\u6b64\u547d\u4ee4\u8fd8\u4f1a\u6784\u5efa\u51fd\u6570\u3002 \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5f3a\u5236\u91cd\u5efa\u6620\u50cf: kn func run --build \u4e5f\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u7981\u7528\u6784\u5efa: kn func run --build = false \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u8fd0\u884c: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/build-run-deploy-func/#_5","text":"\u90e8\u7f72\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u5e76\u5c06\u8be5\u5bb9\u5668\u6620\u50cf\u63a8\u5230\u6620\u50cf\u6ce8\u518c\u8868\u4e2d\u3002 \u8be5\u529f\u80fd\u4f5c\u4e3aKnative\u670d\u52a1\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u91cd\u65b0\u90e8\u7f72\u51fd\u6570\u5c06\u66f4\u65b0\u5728\u96c6\u7fa4\u4e0a\u8fd0\u884c\u7684\u5bb9\u5668\u6620\u50cf\u548c\u751f\u6210\u7684\u670d\u52a1\u3002 \u5df2\u7ecf\u90e8\u7f72\u5230\u96c6\u7fa4\u7684\u51fd\u6570\u53ef\u4ee5\u5728\u96c6\u7fa4\u4e0a\u8bbf\u95ee\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6Knative\u670d\u52a1\u4e00\u6837\u3002","title":"\u90e8\u7f72\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_6","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee\u5bb9\u5668\u6ce8\u518c\u4e2d\u5fc3\uff0c\u5e76\u80fd\u591f\u5c06\u56fe\u50cf\u63a8\u9001\u5230\u8be5\u6ce8\u518c\u4e2d\u5fc3\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_7","text":"deploy \u547d\u4ee4\u4f7f\u7528\u51fd\u6570\u9879\u76ee\u540d\u79f0\u4f5c\u4e3aKnative\u670d\u52a1\u540d\u79f0\u3002 \u5728\u6784\u5efa\u51fd\u6570\u65f6\uff0c\u5c06\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u56fe\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u56fe\u50cf\u540d\u79f0\u3002 func kn func \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: func deploy --registry <registry> \u901a\u8fc7\u5728\u9879\u76ee\u76ee\u5f55\u4e2d\u8fd0\u884c\u547d\u4ee4\u6765\u90e8\u7f72\u51fd\u6570: kn func deploy --registry <registry> Expected output \ud83d\ude4c Function image built: <registry>/hello:latest \u2705 Function deployed in namespace \"default\" and exposed at URL: http://hello.default.127.0.0.1.sslip.io \u4f60\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 invoke \u547d\u4ee4\u5e76\u89c2\u5bdf\u8f93\u51fa\u6765\u9a8c\u8bc1\u4f60\u7684\u51fd\u6570\u5df2\u7ecf\u6210\u529f\u90e8\u7f72: func kn func func invoke kn func invoke Expected output Received response POST / HTTP/1.1 hello.default.127.0.0.1.sslip.io User-Agent: Go-http-client/1.1 Content-Length: 25 Accept-Encoding: gzip Content-Type: application/json K-Proxy-Request: activator X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c Forwarded: for = 10 .244.0.15 ; proto = http X-Forwarded-For: 10 .244.0.15, 10 .244.0.9 X-Forwarded-Proto: http Body:","title":"\u8fc7\u7a0b"},{"location":"getting-started/build-run-deploy-func/#_8","text":"\u6784\u5efa\u51fd\u6570\u4f1a\u4e3a\u51fd\u6570\u521b\u5efa\u4e00\u4e2aOCI\u5bb9\u5668\u6620\u50cf\uff0c\u53ef\u4ee5\u5c06\u5176\u63a8\u5165\u5bb9\u5668\u6ce8\u518c\u8868\u3002 \u5b83\u4e0d\u8fd0\u884c\u6216\u90e8\u7f72\u51fd\u6570\uff0c\u5982\u679c\u60a8\u60f3\u5728\u672c\u5730\u4e3a\u51fd\u6570\u6784\u5efa\u5bb9\u5668\u6620\u50cf\uff0c\u4f46\u4e0d\u60f3\u81ea\u52a8\u8fd0\u884c\u51fd\u6570\u6216\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4(\u4f8b\u5982\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d)\uff0c\u8fd9\u53ef\u80fd\u5f88\u6709\u7528\u3002","title":"\u6784\u5efa\u51fd\u6570"},{"location":"getting-started/build-run-deploy-func/#_9","text":"\u60a8\u7684\u672c\u5730\u673a\u5668\u4e0a\u6709\u4e00\u4e2aDocker\u5b88\u62a4\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u5df2\u7ecf\u4f7f\u7528\u4e86\u5feb\u901f\u5165\u95e8\u5b89\u88c5\uff0c\u5219\u5df2\u7ecf\u63d0\u4f9b\u4e86\u8be5\u529f\u80fd\u3002","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"getting-started/build-run-deploy-func/#_10","text":"build \u547d\u4ee4\u4f7f\u7528\u9879\u76ee\u540d\u79f0\u548c\u955c\u50cf\u6ce8\u518c\u8868\u540d\u79f0\u4e3a\u51fd\u6570\u6784\u9020\u4e00\u4e2a\u5b8c\u5168\u9650\u5b9a\u7684\u5bb9\u5668\u955c\u50cf\u540d\u79f0\u3002 \u5982\u679c\u4e4b\u524d\u6ca1\u6709\u6784\u5efa\u8be5\u51fd\u6570\u9879\u76ee\uff0c\u5219\u4f1a\u63d0\u793a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u955c\u50cf\u6ce8\u518c\u8868\u3002 func kn func \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: func build \u8981\u6784\u5efa\u8be5\u51fd\u6570\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn func build","title":"\u8fc7\u7a0b"},{"location":"getting-started/clean-up/","text":"\u6e05\u7406 \u00b6 \u6211\u4eec\u5efa\u8bae\u60a8\u5220\u9664\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u96c6\u7fa4\uff0c\u4ee5\u91ca\u653e\u672c\u5730\u673a\u5668\u4e0a\u7684\u8d44\u6e90\u3002 \u5982\u679c\u60a8\u60f3\u5728\u5220\u9664\u96c6\u7fa4\u540e\u7ee7\u7eed\u4f7f\u7528Knative\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 quickstart plugin \u5728\u65b0\u7684\u96c6\u7fa4\u4e0a\u91cd\u65b0\u5b89\u88c5Knative\u3002 \u5220\u9664\u96c6\u7fa4 \u00b6 kind minikube Delete your kind cluster by running the command: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] Delete your minikube cluster by running the command: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"\u6e05\u7406"},{"location":"getting-started/clean-up/#_1","text":"\u6211\u4eec\u5efa\u8bae\u60a8\u5220\u9664\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u96c6\u7fa4\uff0c\u4ee5\u91ca\u653e\u672c\u5730\u673a\u5668\u4e0a\u7684\u8d44\u6e90\u3002 \u5982\u679c\u60a8\u60f3\u5728\u5220\u9664\u96c6\u7fa4\u540e\u7ee7\u7eed\u4f7f\u7528Knative\uff0c \u4f60\u53ef\u4ee5\u4f7f\u7528 quickstart plugin \u5728\u65b0\u7684\u96c6\u7fa4\u4e0a\u91cd\u65b0\u5b89\u88c5Knative\u3002","title":"\u6e05\u7406"},{"location":"getting-started/clean-up/#_2","text":"kind minikube Delete your kind cluster by running the command: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] Delete your minikube cluster by running the command: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"\u5220\u9664\u96c6\u7fa4"},{"location":"getting-started/create-a-function/","text":"\u521b\u5efa\u51fd\u6570 \u00b6 \u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"getting-started/create-a-function/#_1","text":"\u5b89\u88c5Knative\u51fd\u6570\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 func \u547d\u4ee4\u884c\u6216 kn func \u63d2\u4ef6\u521b\u5efa\u51fd\u6570\u9879\u76ee: func CLI kn func \u63d2\u4ef6 func create -l <language> < function -name> \u4e3e\u4f8b: func create -l go hello kn func create -l <language> < function -name> \u4f8b\u5b50: kn func create -l go hello Expected output Created go function in hello \u6709\u5173\u51fd\u6570 create \u547d\u4ee4\u9009\u9879\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 func create \u6587\u6863\u3002","title":"\u521b\u5efa\u51fd\u6570"},{"location":"getting-started/first-autoscale/","text":"\u81ea\u52a8\u5b9a\u91cf \u00b6 Knative\u670d\u52a1\u63d0\u4f9b\u81ea\u52a8\u7f29\u653e\uff0c\u4e5f\u79f0\u4e3a \u81ea\u52a8\u7f29\u653e \u3002 \u8fd9\u610f\u5473\u7740\uff0cKnative\u670d\u52a1\u5728\u4e0d\u4f7f\u7528\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u51cf\u5c11\u5230\u96f6\u4e2a\u8fd0\u884c\u7684pod\u3002 \u5217\u51fa\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u67e5\u770bKnative\u670d\u52a1\u6240\u5728\u7684URL: kn kubectl \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kn service list Expected output NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kubectl get ksvc Expected output NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True \u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u524d\u9762\u7684URL\u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello World! \u4f60\u770b\u5230 curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io ? \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u60a8\u7684DNS\u670d\u52a1\u5668\u53ef\u80fd\u8bbe\u7f6e\u4e3a\u4e0d\u89e3\u6790 *.sslip.io \u5730\u5740\u3002 \u5982\u679c\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u79f0\u670d\u52a1\u5668\u6765\u89e3\u51b3\u8fd9\u4e9b\u5730\u5740\u3002 \u5177\u4f53\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u53d1\u884c\u7248\u6709\u6240\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu\u6d3e\u751f\u7cfb\u7edf\uff0c\u4f60\u53ef\u4ee5\u5728 /etc/systemd/resolved.conf \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u6761\u76ee: [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. \u7136\u540e\u7b80\u5355\u5730\u7528 sudo service systemd-resolved restart \u91cd\u65b0\u542f\u52a8\u670d\u52a1\u3002 \u5bf9\u4e8eMacOS\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528 \u8fd9\u91cc \u6240\u8ff0\u7684\u7f51\u7edc\u8bbe\u7f6e\u6dfb\u52a0DNS\u548c\u57df\u3002 \u89c2\u5bdf\u81ea\u52a8\u4f38\u7f29 \u00b6 \u89c2\u5bdf\u8fd9\u4e9bPod\uff0c\u770b\u770b\u5728\u6d41\u91cf\u505c\u6b62\u8bbf\u95eeURL\u540e\uff0c\u5b83\u4eec\u662f\u5982\u4f55\u7f29\u5c0f\u5230\u96f6\u7684: kubectl get pod -l serving.knative.dev/service = hello -w Note \u53ef\u80fd\u9700\u89812\u5206\u949f\u624d\u80fd\u8ba9\u4f60\u7684\u8231\u7f29\u5c0f\u3002\u518d\u6b21ping\u60a8\u7684\u670d\u52a1\u5c06\u91cd\u7f6e\u6b64\u8ba1\u65f6\u5668\u3002 Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating \u6269\u5927\u60a8\u7684Knative\u670d\u52a1 \u00b6 \u5728\u6d4f\u89c8\u5668\u4e2d\u91cd\u65b0\u8fd0\u884cKnative\u670d\u52a1\u3002\u4f60\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u65b0\u7684Pod\u518d\u6b21\u8fd0\u884c: Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running \u7528 Ctrl+c \u9000\u51fa kubectl watch \u547d\u4ee4\u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"getting-started/first-autoscale/#_1","text":"Knative\u670d\u52a1\u63d0\u4f9b\u81ea\u52a8\u7f29\u653e\uff0c\u4e5f\u79f0\u4e3a \u81ea\u52a8\u7f29\u653e \u3002 \u8fd9\u610f\u5473\u7740\uff0cKnative\u670d\u52a1\u5728\u4e0d\u4f7f\u7528\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u51cf\u5c11\u5230\u96f6\u4e2a\u8fd0\u884c\u7684pod\u3002","title":"\u81ea\u52a8\u5b9a\u91cf"},{"location":"getting-started/first-autoscale/#knative","text":"\u4f7f\u7528Knative ( kn )\u547d\u4ee4\u884c\u67e5\u770bKnative\u670d\u52a1\u6240\u5728\u7684URL: kn kubectl \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kn service list Expected output NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u8fd0\u884c\u547d\u4ee4\u67e5\u770bKnative\u670d\u52a1\u5217\u8868: kubectl get ksvc Expected output NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True","title":"\u5217\u51fa\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-autoscale/#knative_1","text":"\u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u524d\u9762\u7684URL\u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello World! \u4f60\u770b\u5230 curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io ? \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u60a8\u7684DNS\u670d\u52a1\u5668\u53ef\u80fd\u8bbe\u7f6e\u4e3a\u4e0d\u89e3\u6790 *.sslip.io \u5730\u5740\u3002 \u5982\u679c\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u79f0\u670d\u52a1\u5668\u6765\u89e3\u51b3\u8fd9\u4e9b\u5730\u5740\u3002 \u5177\u4f53\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u53d1\u884c\u7248\u6709\u6240\u4e0d\u540c\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu\u6d3e\u751f\u7cfb\u7edf\uff0c\u4f60\u53ef\u4ee5\u5728 /etc/systemd/resolved.conf \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u6761\u76ee: [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. \u7136\u540e\u7b80\u5355\u5730\u7528 sudo service systemd-resolved restart \u91cd\u65b0\u542f\u52a8\u670d\u52a1\u3002 \u5bf9\u4e8eMacOS\u7528\u6237\uff0c\u53ef\u4ee5\u4f7f\u7528 \u8fd9\u91cc \u6240\u8ff0\u7684\u7f51\u7edc\u8bbe\u7f6e\u6dfb\u52a0DNS\u548c\u57df\u3002","title":"\u8bbf\u95ee\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-autoscale/#_2","text":"\u89c2\u5bdf\u8fd9\u4e9bPod\uff0c\u770b\u770b\u5728\u6d41\u91cf\u505c\u6b62\u8bbf\u95eeURL\u540e\uff0c\u5b83\u4eec\u662f\u5982\u4f55\u7f29\u5c0f\u5230\u96f6\u7684: kubectl get pod -l serving.knative.dev/service = hello -w Note \u53ef\u80fd\u9700\u89812\u5206\u949f\u624d\u80fd\u8ba9\u4f60\u7684\u8231\u7f29\u5c0f\u3002\u518d\u6b21ping\u60a8\u7684\u670d\u52a1\u5c06\u91cd\u7f6e\u6b64\u8ba1\u65f6\u5668\u3002 Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating","title":"\u89c2\u5bdf\u81ea\u52a8\u4f38\u7f29"},{"location":"getting-started/first-autoscale/#knative_2","text":"\u5728\u6d4f\u89c8\u5668\u4e2d\u91cd\u65b0\u8fd0\u884cKnative\u670d\u52a1\u3002\u4f60\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u65b0\u7684Pod\u518d\u6b21\u8fd0\u884c: Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running \u7528 Ctrl+c \u9000\u51fa kubectl watch \u547d\u4ee4\u3002","title":"\u6269\u5927\u60a8\u7684Knative\u670d\u52a1"},{"location":"getting-started/first-broker/","text":"\u6765\u6e90\u3001\u4ee3\u7406\u548c\u89e6\u53d1\u5668 \u00b6 \u4f5c\u4e3a kn quickstart \u5b89\u88c5\u7684\u4e00\u90e8\u5206\uff0c\u4e00\u4e2a InMemoryChannel-backed \u4ee3\u7406\u88ab\u5b89\u88c5\u5728\u4f60\u7684\u96c6\u7fa4\u4e0a\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5df2\u5b89\u88c5: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning InMemoryChannel-backed\u4ee3\u7406\u4ec5\u7528\u4e8e\u5f00\u53d1\uff0c\u4e0d\u80fd\u7528\u4e8e\u751f\u4ea7\u90e8\u7f72\u3002 \u63a5\u4e0b\u6765\uff0c\u60a8\u5c06\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3a\u4e91\u4e8b\u4ef6\u64ad\u653e\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u4e86\u89e3\u6e90\u3001\u4ee3\u7406\u3001\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668\u7684 \u7b80\u5355\u5b9e\u73b0 \u3002","title":"\u6765\u6e90\u4ee3\u7406\u89e6\u53d1\u5668"},{"location":"getting-started/first-broker/#_1","text":"\u4f5c\u4e3a kn quickstart \u5b89\u88c5\u7684\u4e00\u90e8\u5206\uff0c\u4e00\u4e2a InMemoryChannel-backed \u4ee3\u7406\u88ab\u5b89\u88c5\u5728\u4f60\u7684\u96c6\u7fa4\u4e0a\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u4ee3\u7406\u662f\u5426\u5df2\u5b89\u88c5: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning InMemoryChannel-backed\u4ee3\u7406\u4ec5\u7528\u4e8e\u5f00\u53d1\uff0c\u4e0d\u80fd\u7528\u4e8e\u751f\u4ea7\u90e8\u7f72\u3002 \u63a5\u4e0b\u6765\uff0c\u60a8\u5c06\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3a\u4e91\u4e8b\u4ef6\u64ad\u653e\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u4e86\u89e3\u6e90\u3001\u4ee3\u7406\u3001\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668\u7684 \u7b80\u5355\u5b9e\u73b0 \u3002","title":"\u6765\u6e90\u3001\u4ee3\u7406\u548c\u89e6\u53d1\u5668"},{"location":"getting-started/first-service/","text":"\u90e8\u7f72 Knative \u670d\u52a1 \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a\"Hello world\"Knative\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u63a5\u53d7\u73af\u5883\u53d8\u91cf TARGET \u5e76\u6253\u5370 Hello ${TARGET}! kn YAML \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72\u670d\u52a1: kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World Expected output Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io The value of ${LOADBALANCER_IP} above depends on your type of cluster, for kind it will be 127.0.0.1 for minikube depends on the local tunnel. \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72Knative Service: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello created","title":"\u90e8\u7f72\u670d\u52a1"},{"location":"getting-started/first-service/#knative","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a\"Hello world\"Knative\u670d\u52a1\uff0c\u8be5\u670d\u52a1\u63a5\u53d7\u73af\u5883\u53d8\u91cf TARGET \u5e76\u6253\u5370 Hello ${TARGET}! kn YAML \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72\u670d\u52a1: kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World Expected output Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io The value of ${LOADBALANCER_IP} above depends on your type of cluster, for kind it will be 127.0.0.1 for minikube depends on the local tunnel. \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u8fd0\u884c\u547d\u4ee4\u90e8\u7f72Knative Service: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello created","title":"\u90e8\u7f72 Knative \u670d\u52a1"},{"location":"getting-started/first-source/","text":"\u4f7f\u7528Knative\u670d\u52a1\u4f5c\u4e3a\u6e90 \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4f7f\u7528 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u6765\u5c55\u793aKnative\u4e8b\u4ef6\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u5728\u672c\u6559\u7a0b\u7ed3\u675f\u65f6\uff0c\u60a8\u5e94\u8be5\u62e5\u6709\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u4f53\u7cfb\u7ed3\u6784: \u4e0a\u9762\u7684\u56fe\u50cf\u662f Knative in Action \u7684\u56fe6.6 \u521b\u5efa\u7b2c\u4e00\u4e2a\u6e90\u4ee3\u7801 \u00b6 \u901a\u8fc7\u5c06\u4ee3\u7406\u7684URL( BROKER_URL )\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0cCloudEvents Player\u5145\u5f53\u4e86CloudEvents\u7684\u6e90\u3002 \u60a8\u5c06\u901a\u8fc7CloudEvents Player\u5e94\u7528\u7a0b\u5e8f\u5411\u4ee3\u7406\u53d1\u9001CloudEvents\u3002 \u521b\u5efaCloudEvents\u64ad\u653e\u5668\u670d\u52a1: kn YAML \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u9884\u671f\u7684\u8f93\u51fa Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \u4e3a\u4ec0\u4e48\u6211\u7684\u4fee\u8ba2\u7248\u7684\u540d\u5b57\u4e0d\u4e00\u6837! \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5206\u914d \u7248\u672c\u540d \uff0c\u6240\u4ee5\u670d\u52a1\u4f1a\u81ea\u52a8\u4e3a\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u3002\u5982\u679c\u4f60\u7684\u4fee\u8ba2\u7248\u547d\u540d\u4e0d\u540c\u4e5f\u6ca1\u5173\u7cfb\u3002 \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a cloudevents-player.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created \u6d4b\u8bd5CloudEvents Player \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528CloudEvents Player\u6765\u53d1\u9001\u548c\u63a5\u6536CloudEvents\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , \u4f8b\u5982, http://cloudevents-player.default.127.0.0.1.sslip.io \u90a3\u79cd . \u8fd9\u4e9b\u5b57\u6bb5\u662f\u4ec0\u4e48\u610f\u601d? Field Description Event ID \u4e00\u4e2a\u60df\u4e00\u7684ID\u3002\u5355\u51fb\u5faa\u73af\u56fe\u6807\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u5faa\u73af\u3002 Event Type \u4e00\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u3002 Event Source \u4e00\u4e2a\u4e8b\u4ef6\u6e90\u3002 Specversion \u754c\u5b9a\u60a8\u6b63\u5728\u4f7f\u7528\u7684CloudEvents\u89c4\u8303(\u5e94\u8be5\u603b\u662f1.0)\u3002 Message CloudEvent\u7684 data \u90e8\u5206\uff0c\u4e00\u4e2a\u627f\u8f7d\u60a8\u60f3\u8981\u4f20\u9012\u7684\u6570\u636e\u7684\u6709\u6548\u8f7d\u8377\u3002 \u6709\u5173CloudEvents\u89c4\u8303\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CloudEvents\u89c4\u8303 . \u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6 \u00b6 \u5c1d\u8bd5\u4f7f\u7528CloudEvents Player\u754c\u9762\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: \u5728\u8868\u683c\u4e2d\u586b\u4e0a\u4f60\u60f3\u8981\u7684\u4efb\u4f55\u6570\u636e\u3002 \u786e\u4fdd\u4e8b\u4ef6\u6e90\u4e0d\u5305\u542b\u4efb\u4f55\u7a7a\u683c\u3002 \u70b9\u51fb SEND EVENT . \u5355\u51fb \u5411\u60a8\u5c55\u793a\u4ee3\u7406\u770b\u5230\u7684CloudEvent\u3002 \u60f3\u8981\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001\u4e8b\u4ef6\u5417? \u4f5c\u4e3aWeb\u8868\u5355\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001/\u67e5\u770b\u4e8b\u4ef6\u3002 \u53d1\u5e03\u4e8b\u4ef6: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' \u548c\u67e5\u770b\u4e8b\u4ef6: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages \"Status\"\u5217\u4e2d\u7684 \u56fe\u6807\u610f\u5473\u7740\u4e8b\u4ef6\u5df2\u7ecf\u53d1\u9001\u5230\u6211\u4eec\u7684\u4ee3\u7406\u2026 \u4f46\u8fd9\u4e00\u4e8b\u4ef6\u5230\u54ea\u91cc\u53bb\u4e86? \u73b0\u5728\uff0c \u54ea\u513f\u4e5f\u53bb\u4e0d\u4e86! \u4ee3\u7406\u53ea\u662f\u4e8b\u4ef6\u7684\u5bb9\u5668\u3002 \u4e3a\u4e86\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4efb\u4f55\u5730\u65b9\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4e8b\u4ef6\u5e76\u5c06\u5b83\u4eec\u653e\u7f6e\u5230\u67d0\u4e2a\u5730\u65b9\u3002 \u4f60\u5f88\u5e78\u8fd0;\u4f60\u5c06\u5728\u4e0b\u4e00\u9875\u521b\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668!","title":"\u4f7f\u7528\u670d\u52a1\u4f5c\u4e3a\u6e90"},{"location":"getting-started/first-source/#knative","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4f7f\u7528 CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u6765\u5c55\u793aKnative\u4e8b\u4ef6\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u5728\u672c\u6559\u7a0b\u7ed3\u675f\u65f6\uff0c\u60a8\u5e94\u8be5\u62e5\u6709\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u4f53\u7cfb\u7ed3\u6784: \u4e0a\u9762\u7684\u56fe\u50cf\u662f Knative in Action \u7684\u56fe6.6","title":"\u4f7f\u7528Knative\u670d\u52a1\u4f5c\u4e3a\u6e90"},{"location":"getting-started/first-source/#_1","text":"\u901a\u8fc7\u5c06\u4ee3\u7406\u7684URL( BROKER_URL )\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0cCloudEvents Player\u5145\u5f53\u4e86CloudEvents\u7684\u6e90\u3002 \u60a8\u5c06\u901a\u8fc7CloudEvents Player\u5e94\u7528\u7a0b\u5e8f\u5411\u4ee3\u7406\u53d1\u9001CloudEvents\u3002 \u521b\u5efaCloudEvents\u64ad\u653e\u5668\u670d\u52a1: kn YAML \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u9884\u671f\u7684\u8f93\u51fa Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \u4e3a\u4ec0\u4e48\u6211\u7684\u4fee\u8ba2\u7248\u7684\u540d\u5b57\u4e0d\u4e00\u6837! \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5206\u914d \u7248\u672c\u540d \uff0c\u6240\u4ee5\u670d\u52a1\u4f1a\u81ea\u52a8\u4e3a\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u3002\u5982\u679c\u4f60\u7684\u4fee\u8ba2\u7248\u547d\u540d\u4e0d\u540c\u4e5f\u6ca1\u5173\u7cfb\u3002 \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a cloudevents-player.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker \u901a\u8fc7\u8fd0\u884c\u8be5\u547d\u4ee4\u5e94\u7528YAML\u6587\u4ef6: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created","title":"\u521b\u5efa\u7b2c\u4e00\u4e2a\u6e90\u4ee3\u7801"},{"location":"getting-started/first-source/#cloudevents-player","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528CloudEvents Player\u6765\u53d1\u9001\u548c\u63a5\u6536CloudEvents\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 \u5982\u679c\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u670d\u52a1URL\uff0c\u5219\u4f1a\u51fa\u73b0 Create Event \u8868\u5355\u3002 http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , \u4f8b\u5982, http://cloudevents-player.default.127.0.0.1.sslip.io \u90a3\u79cd . \u8fd9\u4e9b\u5b57\u6bb5\u662f\u4ec0\u4e48\u610f\u601d? Field Description Event ID \u4e00\u4e2a\u60df\u4e00\u7684ID\u3002\u5355\u51fb\u5faa\u73af\u56fe\u6807\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u5faa\u73af\u3002 Event Type \u4e00\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u3002 Event Source \u4e00\u4e2a\u4e8b\u4ef6\u6e90\u3002 Specversion \u754c\u5b9a\u60a8\u6b63\u5728\u4f7f\u7528\u7684CloudEvents\u89c4\u8303(\u5e94\u8be5\u603b\u662f1.0)\u3002 Message CloudEvent\u7684 data \u90e8\u5206\uff0c\u4e00\u4e2a\u627f\u8f7d\u60a8\u60f3\u8981\u4f20\u9012\u7684\u6570\u636e\u7684\u6709\u6548\u8f7d\u8377\u3002 \u6709\u5173CloudEvents\u89c4\u8303\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b CloudEvents\u89c4\u8303 .","title":"\u6d4b\u8bd5CloudEvents Player"},{"location":"getting-started/first-source/#_2","text":"\u5c1d\u8bd5\u4f7f\u7528CloudEvents Player\u754c\u9762\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6: \u5728\u8868\u683c\u4e2d\u586b\u4e0a\u4f60\u60f3\u8981\u7684\u4efb\u4f55\u6570\u636e\u3002 \u786e\u4fdd\u4e8b\u4ef6\u6e90\u4e0d\u5305\u542b\u4efb\u4f55\u7a7a\u683c\u3002 \u70b9\u51fb SEND EVENT . \u5355\u51fb \u5411\u60a8\u5c55\u793a\u4ee3\u7406\u770b\u5230\u7684CloudEvent\u3002 \u60f3\u8981\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001\u4e8b\u4ef6\u5417? \u4f5c\u4e3aWeb\u8868\u5355\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u53d1\u9001/\u67e5\u770b\u4e8b\u4ef6\u3002 \u53d1\u5e03\u4e8b\u4ef6: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' \u548c\u67e5\u770b\u4e8b\u4ef6: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages \"Status\"\u5217\u4e2d\u7684 \u56fe\u6807\u610f\u5473\u7740\u4e8b\u4ef6\u5df2\u7ecf\u53d1\u9001\u5230\u6211\u4eec\u7684\u4ee3\u7406\u2026 \u4f46\u8fd9\u4e00\u4e8b\u4ef6\u5230\u54ea\u91cc\u53bb\u4e86? \u73b0\u5728\uff0c \u54ea\u513f\u4e5f\u53bb\u4e0d\u4e86! \u4ee3\u7406\u53ea\u662f\u4e8b\u4ef6\u7684\u5bb9\u5668\u3002 \u4e3a\u4e86\u5c06\u4e8b\u4ef6\u53d1\u9001\u5230\u4efb\u4f55\u5730\u65b9\uff0c\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\u6765\u76d1\u542c\u4e8b\u4ef6\u5e76\u5c06\u5b83\u4eec\u653e\u7f6e\u5230\u67d0\u4e2a\u5730\u65b9\u3002 \u4f60\u5f88\u5e78\u8fd0;\u4f60\u5c06\u5728\u4e0b\u4e00\u9875\u521b\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668!","title":"\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6"},{"location":"getting-started/first-traffic-split/","text":"\u6d41\u91cf\u5206\u53d1 \u00b6 \u6d41\u91cf\u5206\u53d1\u5bf9\u4e8e \u84dd/\u7eff\u90e8\u7f72 \u548c \u91d1\u4e1d\u96c0\u90e8\u7f72 \u5f88\u6709\u7528\u3002 \u4fee\u8ba2\u7248 \u662f\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u5b9e\u65f6\u5feb\u7167\u3002 \u6bcf\u6b21\u66f4\u6539Knative\u670d\u52a1\u7684\u914d\u7f6e\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u5f53\u5206\u6d41\u6d41\u91cf\u65f6\uff0cKnative\u4f1a\u5728Knative\u670d\u52a1\u7684\u4e0d\u540c\u4fee\u8ba2\u7248\u4e4b\u95f4\u5206\u6d41\u6d41\u91cf\u3002 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2 \u00b6 \u66ff\u6362 TARGET=World \uff0c\u66f4\u65b0Knative\u670d\u52a1\u4e0a\u7684\u73af\u5883\u53d8\u91cf TARGET \u503c\u4e3a\"Knative\"\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kn service update hello \\ --env TARGET = Knative \u548c\u524d\u9762\u4e00\u6837\uff0c kn \u5411CLI\u8f93\u51fa\u4e00\u4e9b\u6709\u7528\u7684\u4fe1\u606f\u3002 Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u7f16\u8f91\u4f60\u73b0\u6709\u7684 hello.yaml \u6587\u4ef6\u4ee5\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured \u56e0\u4e3a\u60a8\u6b63\u5728\u66f4\u65b0\u4e00\u4e2a\u73b0\u6709\u7684Knative\u670d\u52a1\uff0c\u6240\u4ee5URL\u4e0d\u4f1a\u6539\u53d8\uff0c\u4f46\u662f\u65b0\u7684\u4fee\u8ba2\u7248\u672c\u6709\u4e86\u65b0\u7684\u540d\u79f0 hello-00002 \u3002 \u8bbf\u95ee\u65b0\u7684\u4fee\u8ba2\u7248\u672c \u00b6 \u8981\u67e5\u770b\u66f4\u6539\uff0c\u8bf7\u5728\u6d4f\u89c8\u5668\u4e0a\u518d\u6b21\u8bbf\u95eeKnative\u670d\u52a1\u6216\u5728\u7ec8\u7aef\u4e0a\u4f7f\u7528 curl : echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! \u67e5\u770b\u73b0\u6709\u7684\u4fee\u6b63 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528Knative ( kn ) or kubectl \u547d\u4ee4\u884c\u67e5\u770b\u73b0\u6709\u4fee\u8ba2\u7684\u5217\u8868: kn kubectl \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 \u5f53\u8fd0\u884c kn \u547d\u4ee4\u65f6\uff0c\u76f8\u5173\u5217\u4e3a TRAFFIC \u3002 \u4f60\u53ef\u4ee5\u770b\u5230100%\u7684\u6d41\u91cf\u90fd\u6d41\u5411\u4e86\u6700\u65b0\u7684\u7248\u672c hello-00002 \uff0c\u5b83\u4f4d\u4e8e GENERATION \u6700\u9ad8\u7684\u90a3\u4e00\u884c\u3002 0%\u7684\u6d41\u91cf\u5c06\u6d41\u5411\u4fee\u8ba2\u7248 hello-00001 \u3002 \u5f53\u60a8\u521b\u5efaKnative\u670d\u52a1\u7684\u65b0\u7248\u672c\u65f6\uff0cKnative\u9ed8\u8ba4\u5c06100%\u7684\u6d41\u91cf\u5bfc\u5411\u8fd9\u4e2a\u6700\u65b0\u7248\u672c\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5e0c\u671b\u6bcf\u4e2a\u4fee\u8ba2\u63a5\u6536\u591a\u5c11\u6d41\u91cf\u6765\u66f4\u6539\u6b64\u9ed8\u8ba4\u884c\u4e3a\u3002 \u5728\u4fee\u8ba2\u7248\u4e4b\u95f4\u5212\u5206\u6d41\u91cf \u00b6 \u5728\u4e24\u4e2a\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u5212\u5206\u6d41\u91cf: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" traffic : - latestRevision : true percent : 50 - latestRevision : false percent : 50 revisionName : hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest always points to the \"latest\" Revision, which in this case is hello-00002 . \u9a8c\u8bc1\u6d41\u91cf\u5206\u6d41 \u00b6 \u82e5\u8981\u9a8c\u8bc1\u6d41\u91cf\u5206\u5272\u7684\u914d\u7f6e\u662f\u5426\u6b63\u786e\uff0c\u8bf7\u518d\u6b21\u901a\u8fc7\u6267\u884c\u8be5\u547d\u4ee4\u5217\u51fa\u4fee\u6539\u9879: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True \u5728\u6d4f\u89c8\u5668\u4e2d\u591a\u6b21\u8bbf\u95eeKnative Service\uff0c\u4ee5\u67e5\u770b\u6bcf\u4e2aRevision\u63d0\u4f9b\u7684\u4e0d\u540c\u8f93\u51fa\u3002 \u7c7b\u4f3c\u5730\uff0c\u60a8\u53ef\u4ee5\u4ece\u7ec8\u7aef\u591a\u6b21\u8bbf\u95eeService URL\uff0c\u4ee5\u67e5\u770b\u5728\u4fee\u8ba2\u4e4b\u95f4\u5212\u5206\u7684\u6d41\u91cf\u3002 echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"\u6d41\u91cf\u5206\u53d1"},{"location":"getting-started/first-traffic-split/#_1","text":"\u6d41\u91cf\u5206\u53d1\u5bf9\u4e8e \u84dd/\u7eff\u90e8\u7f72 \u548c \u91d1\u4e1d\u96c0\u90e8\u7f72 \u5f88\u6709\u7528\u3002 \u4fee\u8ba2\u7248 \u662f\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u5b9e\u65f6\u5feb\u7167\u3002 \u6bcf\u6b21\u66f4\u6539Knative\u670d\u52a1\u7684\u914d\u7f6e\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u5f53\u5206\u6d41\u6d41\u91cf\u65f6\uff0cKnative\u4f1a\u5728Knative\u670d\u52a1\u7684\u4e0d\u540c\u4fee\u8ba2\u7248\u4e4b\u95f4\u5206\u6d41\u6d41\u91cf\u3002","title":"\u6d41\u91cf\u5206\u53d1"},{"location":"getting-started/first-traffic-split/#_2","text":"\u66ff\u6362 TARGET=World \uff0c\u66f4\u65b0Knative\u670d\u52a1\u4e0a\u7684\u73af\u5883\u53d8\u91cf TARGET \u503c\u4e3a\"Knative\"\u3002 kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kn service update hello \\ --env TARGET = Knative \u548c\u524d\u9762\u4e00\u6837\uff0c kn \u5411CLI\u8f93\u51fa\u4e00\u4e9b\u6709\u7528\u7684\u4fe1\u606f\u3002 Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u7f16\u8f91\u4f60\u73b0\u6709\u7684 hello.yaml \u6587\u4ef6\u4ee5\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72Knative\u670d\u52a1\u7684\u66f4\u65b0\u7248\u672c: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured \u56e0\u4e3a\u60a8\u6b63\u5728\u66f4\u65b0\u4e00\u4e2a\u73b0\u6709\u7684Knative\u670d\u52a1\uff0c\u6240\u4ee5URL\u4e0d\u4f1a\u6539\u53d8\uff0c\u4f46\u662f\u65b0\u7684\u4fee\u8ba2\u7248\u672c\u6709\u4e86\u65b0\u7684\u540d\u79f0 hello-00002 \u3002","title":"\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2"},{"location":"getting-started/first-traffic-split/#_3","text":"\u8981\u67e5\u770b\u66f4\u6539\uff0c\u8bf7\u5728\u6d4f\u89c8\u5668\u4e0a\u518d\u6b21\u8bbf\u95eeKnative\u670d\u52a1\u6216\u5728\u7ec8\u7aef\u4e0a\u4f7f\u7528 curl : echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative!","title":"\u8bbf\u95ee\u65b0\u7684\u4fee\u8ba2\u7248\u672c"},{"location":"getting-started/first-traffic-split/#_4","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528Knative ( kn ) or kubectl \u547d\u4ee4\u884c\u67e5\u770b\u73b0\u6709\u4fee\u8ba2\u7684\u5217\u8868: kn kubectl \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True \u8fd0\u884c\u8be5\u547d\u4ee4\u67e5\u770b\u7248\u672c\u5217\u8868: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 \u5f53\u8fd0\u884c kn \u547d\u4ee4\u65f6\uff0c\u76f8\u5173\u5217\u4e3a TRAFFIC \u3002 \u4f60\u53ef\u4ee5\u770b\u5230100%\u7684\u6d41\u91cf\u90fd\u6d41\u5411\u4e86\u6700\u65b0\u7684\u7248\u672c hello-00002 \uff0c\u5b83\u4f4d\u4e8e GENERATION \u6700\u9ad8\u7684\u90a3\u4e00\u884c\u3002 0%\u7684\u6d41\u91cf\u5c06\u6d41\u5411\u4fee\u8ba2\u7248 hello-00001 \u3002 \u5f53\u60a8\u521b\u5efaKnative\u670d\u52a1\u7684\u65b0\u7248\u672c\u65f6\uff0cKnative\u9ed8\u8ba4\u5c06100%\u7684\u6d41\u91cf\u5bfc\u5411\u8fd9\u4e2a\u6700\u65b0\u7248\u672c\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5e0c\u671b\u6bcf\u4e2a\u4fee\u8ba2\u63a5\u6536\u591a\u5c11\u6d41\u91cf\u6765\u66f4\u6539\u6b64\u9ed8\u8ba4\u884c\u4e3a\u3002","title":"\u67e5\u770b\u73b0\u6709\u7684\u4fee\u6b63"},{"location":"getting-started/first-traffic-split/#_5","text":"\u5728\u4e24\u4e2a\u4fee\u8ba2\u7248\u672c\u4e4b\u95f4\u5212\u5206\u6d41\u91cf: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"Knative\" traffic : - latestRevision : true percent : 50 - latestRevision : false percent : 50 revisionName : hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest always points to the \"latest\" Revision, which in this case is hello-00002 .","title":"\u5728\u4fee\u8ba2\u7248\u4e4b\u95f4\u5212\u5206\u6d41\u91cf"},{"location":"getting-started/first-traffic-split/#_6","text":"\u82e5\u8981\u9a8c\u8bc1\u6d41\u91cf\u5206\u5272\u7684\u914d\u7f6e\u662f\u5426\u6b63\u786e\uff0c\u8bf7\u518d\u6b21\u901a\u8fc7\u6267\u884c\u8be5\u547d\u4ee4\u5217\u51fa\u4fee\u6539\u9879: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True \u5728\u6d4f\u89c8\u5668\u4e2d\u591a\u6b21\u8bbf\u95eeKnative Service\uff0c\u4ee5\u67e5\u770b\u6bcf\u4e2aRevision\u63d0\u4f9b\u7684\u4e0d\u540c\u8f93\u51fa\u3002 \u7c7b\u4f3c\u5730\uff0c\u60a8\u53ef\u4ee5\u4ece\u7ec8\u7aef\u591a\u6b21\u8bbf\u95eeService URL\uff0c\u4ee5\u67e5\u770b\u5728\u4fee\u8ba2\u4e4b\u95f4\u5212\u5206\u7684\u6d41\u91cf\u3002 echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"\u9a8c\u8bc1\u6d41\u91cf\u5206\u6d41"},{"location":"getting-started/first-trigger/","text":"\u4f7f\u7528\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668 \u00b6 \u5728\u4e0a\u4e00\u4e2a\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u5411\u4ee3\u7406\u53d1\u9001\u4e8b\u4ef6\u3002 \u73b0\u5728\u6211\u4eec\u5e0c\u671b\u4e8b\u4ef6\u4ece Broker \u8f6c\u79fb\u5230\u4e8b\u4ef6\u63a5\u6536\u5668\u3002 \u5728\u672c\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u63a5\u6536\u5668\u548c\u6e90\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u6211\u4eec\u5c06\u4f7f\u7528 Trigger \u6765\u76d1\u542c Broker \u4e2d\u8981\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u3002 \u521b\u5efa\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668 \u00b6 \u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u4ece\u4e8b\u4ef6\u6e90\u76d1\u542c CloudEvents\uff0c\u5e76\u5c06\u5b83\u4eec\u653e\u5165\u63a5\u6536\u5668\u4e2d\uff0c\u8fd9\u4e5f\u662f CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u3002 kn YAML \u8981\u521b\u5efa\u89e6\u53d1\u5668\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a ce-trigger.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u521b\u5efa\u89e6\u53d1\u5668: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created \u6211\u7684\u89e6\u53d1\u5668\u5728\u542c\u4ec0\u4e48CloudEvents ? \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728 kn \u547d\u4ee4\u4e2d\u6307\u5b9a --filter \uff0c\u6240\u4ee5\u89e6\u53d1\u5668\u6b63\u5728\u76d1\u542c\u8fdb\u5165\u4ee3\u7406\u7684\u4efb\u4f55CloudEvents\u3002 \u5c55\u5f00\u4e0b\u4e00\u4e2a\u6ce8\u91ca\uff0c\u67e5\u770b\u5982\u4f55\u4f7f\u7528\u8fc7\u6ee4\u5668\u3002 \u73b0\u5728\uff0c\u5f53\u6211\u4eec\u8fd4\u56de\u5230CloudEvents Player\u5e76\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u65f6\uff0c\u6211\u4eec\u770b\u5230CloudEvents\u65e2\u88abCloudEvents\u53d1\u9001\uff0c\u4e5f\u88abCloudEvents Player\u63a5\u6536: \u60a8\u53ef\u80fd\u9700\u8981\u5237\u65b0\u9875\u9762\u4ee5\u67e5\u770b\u66f4\u6539\u3002 \u5982\u679c\u6211\u60f3\u8fc7\u6ee4CloudEvent\u5c5e\u6027\u600e\u4e48\u529e? \u9996\u5148\uff0c\u5220\u9664\u73b0\u6709\u7684\u89e6\u53d1\u5668: bash kn trigger delete cloudevents-trigger \u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u6dfb\u52a0\u4e00\u4e2a\u76d1\u542c\u67d0\u4e2aCloudEvent\u7c7b\u578b\u7684\u89e6\u53d1\u5668: bash kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type=some-type \u5982\u679c\u4f60\u53d1\u9001\u4e00\u4e2a\u7c7b\u578b\u4e3a' some-type '\u7684CloudEvent\uff0c\u5b83\u4f1a\u53cd\u6620\u5728CloudEvents Player UI\u4e2d\u3002 \u89e6\u53d1\u5668\u5ffd\u7565\u4efb\u4f55\u5176\u4ed6\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u8fc7\u6ee4CloudEvent\u7684\u4efb\u4f55\u65b9\u9762\u3002 \u6709\u4e9b\u4eba\u79f0\u4e4b\u4e3a \u201c\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u201d \uff0c\u53ef\u4ee5\u7528\u6765\u5728Kubernetes\u4e0a\u521b\u5efa\u81ea\u5df1\u7684 \u201c\u529f\u80fd\u5373\u670d\u52a1\u201d \u3002","title":"\u4f7f\u7528\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668"},{"location":"getting-started/first-trigger/#_1","text":"\u5728\u4e0a\u4e00\u4e2a\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u4e8b\u4ef6\u6e90\u5411\u4ee3\u7406\u53d1\u9001\u4e8b\u4ef6\u3002 \u73b0\u5728\u6211\u4eec\u5e0c\u671b\u4e8b\u4ef6\u4ece Broker \u8f6c\u79fb\u5230\u4e8b\u4ef6\u63a5\u6536\u5668\u3002 \u5728\u672c\u4e3b\u9898\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u4f5c\u4e3a\u63a5\u6536\u5668\u548c\u6e90\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u4f7f\u7528 CloudEvents Player \u6765\u53d1\u9001\u548c\u63a5\u6536\u4e8b\u4ef6\u3002 \u6211\u4eec\u5c06\u4f7f\u7528 Trigger \u6765\u76d1\u542c Broker \u4e2d\u8981\u53d1\u9001\u5230\u63a5\u6536\u5668\u7684\u4e8b\u4ef6\u3002","title":"\u4f7f\u7528\u89e6\u53d1\u5668\u548c\u63a5\u6536\u5668"},{"location":"getting-started/first-trigger/#_2","text":"\u521b\u5efa\u4e00\u4e2a\u89e6\u53d1\u5668\uff0c\u4ece\u4e8b\u4ef6\u6e90\u76d1\u542c CloudEvents\uff0c\u5e76\u5c06\u5b83\u4eec\u653e\u5165\u63a5\u6536\u5668\u4e2d\uff0c\u8fd9\u4e5f\u662f CloudEvents Player \u5e94\u7528\u7a0b\u5e8f\u3002 kn YAML \u8981\u521b\u5efa\u89e6\u53d1\u5668\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . \u5c06\u4ee5\u4e0bYAML\u590d\u5236\u5230\u540d\u4e3a ce-trigger.yaml \u7684\u6587\u4ef6\u4e2d: apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player \u901a\u8fc7\u8fd0\u884c\u547d\u4ee4\u521b\u5efa\u89e6\u53d1\u5668: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created \u6211\u7684\u89e6\u53d1\u5668\u5728\u542c\u4ec0\u4e48CloudEvents ? \u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728 kn \u547d\u4ee4\u4e2d\u6307\u5b9a --filter \uff0c\u6240\u4ee5\u89e6\u53d1\u5668\u6b63\u5728\u76d1\u542c\u8fdb\u5165\u4ee3\u7406\u7684\u4efb\u4f55CloudEvents\u3002 \u5c55\u5f00\u4e0b\u4e00\u4e2a\u6ce8\u91ca\uff0c\u67e5\u770b\u5982\u4f55\u4f7f\u7528\u8fc7\u6ee4\u5668\u3002 \u73b0\u5728\uff0c\u5f53\u6211\u4eec\u8fd4\u56de\u5230CloudEvents Player\u5e76\u53d1\u9001\u4e00\u4e2a\u4e8b\u4ef6\u65f6\uff0c\u6211\u4eec\u770b\u5230CloudEvents\u65e2\u88abCloudEvents\u53d1\u9001\uff0c\u4e5f\u88abCloudEvents Player\u63a5\u6536: \u60a8\u53ef\u80fd\u9700\u8981\u5237\u65b0\u9875\u9762\u4ee5\u67e5\u770b\u66f4\u6539\u3002 \u5982\u679c\u6211\u60f3\u8fc7\u6ee4CloudEvent\u5c5e\u6027\u600e\u4e48\u529e? \u9996\u5148\uff0c\u5220\u9664\u73b0\u6709\u7684\u89e6\u53d1\u5668: bash kn trigger delete cloudevents-trigger \u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u6dfb\u52a0\u4e00\u4e2a\u76d1\u542c\u67d0\u4e2aCloudEvent\u7c7b\u578b\u7684\u89e6\u53d1\u5668: bash kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type=some-type \u5982\u679c\u4f60\u53d1\u9001\u4e00\u4e2a\u7c7b\u578b\u4e3a' some-type '\u7684CloudEvent\uff0c\u5b83\u4f1a\u53cd\u6620\u5728CloudEvents Player UI\u4e2d\u3002 \u89e6\u53d1\u5668\u5ffd\u7565\u4efb\u4f55\u5176\u4ed6\u7c7b\u578b\u3002 \u60a8\u53ef\u4ee5\u8fc7\u6ee4CloudEvent\u7684\u4efb\u4f55\u65b9\u9762\u3002 \u6709\u4e9b\u4eba\u79f0\u4e4b\u4e3a \u201c\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u201d \uff0c\u53ef\u4ee5\u7528\u6765\u5728Kubernetes\u4e0a\u521b\u5efa\u81ea\u5df1\u7684 \u201c\u529f\u80fd\u5373\u670d\u52a1\u201d \u3002","title":"\u521b\u5efa\u7b2c\u4e00\u4e2a\u89e6\u53d1\u5668"},{"location":"getting-started/getting-started-eventing/","text":"\u5173\u4e8e Knative \u4e8b\u4ef6 \u00b6 Knative\u4e8b\u4ef6\u901a\u8fc7\u8f7b\u677e\u5730\u5c06\u4e8b\u4ef6\u6e90\u3001\u89e6\u53d1\u5668\u548c\u5176\u4ed6\u9009\u9879\u9644\u52a0\u5230Knative\u670d\u52a1\uff0c\u4e3a\u60a8\u63d0\u4f9b\u4e86\u7528\u4e8e\u521b\u5efa\u4e8b\u4ef6\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u7528\u5de5\u5177\u3002 \u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u88ab\u8bbe\u8ba1\u4e3a\u5728\u4e8b\u4ef6\u53d1\u751f\u65f6\u68c0\u6d4b\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u4e8b\u4ef6\u5904\u7406\u8fc7\u7a0b\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u3002 Tip \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784\u548cKnative\u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u67e5\u770bCNCF\u5173\u4e8e \u5e26\u6709Knative\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784 \u7684\u4f1a\u8bae \u5b89\u88c5Knative event\u4e4b\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u3001\u53d1\u9001\u548c\u9a8c\u8bc1\u4e8b\u4ef6\u3002 \u672c\u6559\u7a0b\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u57fa\u672c\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u4f7f\u7528 \u4e8b\u4ef6\u6e90 \u3001 \u4ee3\u7406 \u3001 \u89e6\u53d1\u5668 \u548c \u6c34\u69fd \u7684\u4e8b\u4ef6\u3002","title":"\u5173\u4e8e\u4e8b\u4ef6"},{"location":"getting-started/getting-started-eventing/#knative","text":"Knative\u4e8b\u4ef6\u901a\u8fc7\u8f7b\u677e\u5730\u5c06\u4e8b\u4ef6\u6e90\u3001\u89e6\u53d1\u5668\u548c\u5176\u4ed6\u9009\u9879\u9644\u52a0\u5230Knative\u670d\u52a1\uff0c\u4e3a\u60a8\u63d0\u4f9b\u4e86\u7528\u4e8e\u521b\u5efa\u4e8b\u4ef6\u9a71\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u6709\u7528\u5de5\u5177\u3002 \u4e8b\u4ef6\u9a71\u52a8\u7684\u5e94\u7528\u7a0b\u5e8f\u88ab\u8bbe\u8ba1\u4e3a\u5728\u4e8b\u4ef6\u53d1\u751f\u65f6\u68c0\u6d4b\u4e8b\u4ef6\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u4e8b\u4ef6\u5904\u7406\u8fc7\u7a0b\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u3002 Tip \u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784\u548cKnative\u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u8bf7\u67e5\u770bCNCF\u5173\u4e8e \u5e26\u6709Knative\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u9a71\u52a8\u4f53\u7cfb\u7ed3\u6784 \u7684\u4f1a\u8bae \u5b89\u88c5Knative event\u4e4b\u540e\uff0c\u53ef\u4ee5\u521b\u5efa\u3001\u53d1\u9001\u548c\u9a8c\u8bc1\u4e8b\u4ef6\u3002 \u672c\u6559\u7a0b\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u57fa\u672c\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u4f7f\u7528 \u4e8b\u4ef6\u6e90 \u3001 \u4ee3\u7406 \u3001 \u89e6\u53d1\u5668 \u548c \u6c34\u69fd \u7684\u4e8b\u4ef6\u3002","title":"\u5173\u4e8e Knative \u4e8b\u4ef6"},{"location":"getting-started/install-func/","text":"\u5b89\u88c5 Knative \u51fd\u6570 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002 \u5b89\u88c5 func CLI \u00b6 Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func . \u5b89\u88c5 kn func CLI \u63d2\u4ef6 \u00b6 kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5\u51fd\u6570"},{"location":"getting-started/install-func/#knative","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u72ec\u7acb\u7684 func \u547d\u4ee4\u884c\u5b89\u88c5Knative\u51fd\u6570\uff0c\u6216\u8005\u901a\u8fc7\u5b89\u88c5\u53ef\u7528\u4e8eKnative kn \u547d\u4ee4\u884c\u7684 kn func \u63d2\u4ef6\u3002","title":"\u5b89\u88c5 Knative \u51fd\u6570"},{"location":"getting-started/install-func/#func-cli","text":"Homebrew \u53ef\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6 Go \u5bb9\u5668\u955c\u50cf \u8981\u4f7f\u7528Homebrew\u5b89\u88c5 func \uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4: brew tap knative-sandbox/kn-plugins brew install func If you have already installed the kn CLI by using Homebrew, the func CLI is automatically recognized as a plugin to kn , and can be referenced as kn func or func interchangeably. Note Use brew upgrade instead if you are upgrading from a previous version. You can install func by downloading the executable binary for your system and placing it in the system path. Download the binary for your system from the func release page . Rename the binary to func and make it executable by running the following commands: mv <path-to-binary-file> func chmod +x func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv func /usr/local/bin Verify that the CLI is working by running the command: func version Check out the func client repository and navigate to the func directory: git clone https://github.com/knative/func.git func cd func/ Build an executable binary: make Move func into your system path, and verify that func commands are working properly. For example: func version \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c func \u3002\u4f8b\u5982: docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc Links to images are available here: Latest release Note Running func from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use func .","title":"\u5b89\u88c5 func CLI"},{"location":"getting-started/install-func/#kn-func-cli","text":"kn plugin \u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06Knative\u51fd\u6570\u4f5c\u4e3a kn CLI\u63d2\u4ef6\u5b89\u88c5\u3002 Download the binary for your system from the func release page . Rename the binary to kn-func , and make it executable by running the following commands: mv <path-to-binary-file> kn-func chmod +x kn-func Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, func_darwin_amd64 or func_linux_amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn-func /usr/local/bin Verify that the CLI is working by running the command: kn func version","title":"\u5b89\u88c5 kn func CLI \u63d2\u4ef6"},{"location":"getting-started/next-steps/","text":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u672c\u4e3b\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u5217\u8868\uff0c\u4ee5\u5e2e\u52a9\u60a8\u7ee7\u7eed\u60a8\u7684 Knative \u4e4b\u65c5\u3002 Knative \u6837\u54c1 \u00b6 \u8bd5\u8bd5\u4e0b\u9762\u7684 Knative \u793a\u4f8b: \u5c06 Kubernetes \u670d\u52a1\u8f6c\u6362\u4e3a Knative \u670d\u52a1 Knative \u670d\u52a1\u4ee3\u7801\u793a\u4f8b Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u63a2\u7d22 Knative \u6587\u6863 \u00b6 \u8bf7\u53c2\u9605\u4ee5\u4e0b\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u7684\u6587\u6863\u6307\u5357: Serving Guide Eventing Guide Knative \u4e66\u7c4d \u00b6 \u5e2e\u52a9\u4f60\u7406\u89e3 Knative \u6982\u5ff5\u5e76\u83b7\u5f97\u989d\u5916\u793a\u4f8b\u7684\u4e66\u7c4d: Knative \u5b9e\u4f8b \u60f3\u8981Knative\u5b9e\u4f8b\u7684\u514d\u8d39\u6570\u5b57\u62f7\u8d1d? VMWare\u6177\u6168\u5730\u6350\u8d60\u4e86Knative\u5b9e\u4f8b\u7535\u5b50\u4e66\u7684\u514d\u8d39\u8bbf\u95ee\u6743\u9650\uff0c \u5728\u8fd9\u91cc\u83b7\u53d6\u60a8\u7684\u526f\u672c! Knative Cookbook \u5176\u5b83 Knative \u94fe\u63a5 \u00b6 \u5176\u4ed6\u94fe\u63a5\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5f00\u59cb\u4f7f\u7528 Knative: Knative YouTube \u9891\u9053 Knative.tips \u6211\u4eec\u9057\u6f0f\u4e86\u4ec0\u4e48\u5417? \u00b6 \u6211\u4eec\u5f88\u4e50\u610f\u5728\u60a8\u7684 Knative \u65c5\u7a0b\u7684\u4e0b\u4e00\u6b65\u5e2e\u52a9\u60a8\u3002 \u5982\u679c\u6211\u4eec\u5728\u672c\u9875\u4e0a\u9057\u6f0f\u4e86\u60a8\u8ba4\u4e3a\u5e94\u8be5\u5728\u8fd9\u91cc\u7684\u5185\u5bb9\uff0c\u8bf7 \u7ed9\u6211\u4eec\u53cd\u9988 !","title":"\u63a5\u4e0b\u6765\u662f\u4ec0\u4e48?"},{"location":"getting-started/next-steps/#_1","text":"\u672c\u4e3b\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u5217\u8868\uff0c\u4ee5\u5e2e\u52a9\u60a8\u7ee7\u7eed\u60a8\u7684 Knative \u4e4b\u65c5\u3002","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"getting-started/next-steps/#knative","text":"\u8bd5\u8bd5\u4e0b\u9762\u7684 Knative \u793a\u4f8b: \u5c06 Kubernetes \u670d\u52a1\u8f6c\u6362\u4e3a Knative \u670d\u52a1 Knative \u670d\u52a1\u4ee3\u7801\u793a\u4f8b Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b","title":"Knative \u6837\u54c1"},{"location":"getting-started/next-steps/#knative_1","text":"\u8bf7\u53c2\u9605\u4ee5\u4e0b\u9488\u5bf9\u60a8\u7684\u7528\u4f8b\u7684\u6587\u6863\u6307\u5357: Serving Guide Eventing Guide","title":"\u63a2\u7d22 Knative \u6587\u6863"},{"location":"getting-started/next-steps/#knative_2","text":"\u5e2e\u52a9\u4f60\u7406\u89e3 Knative \u6982\u5ff5\u5e76\u83b7\u5f97\u989d\u5916\u793a\u4f8b\u7684\u4e66\u7c4d: Knative \u5b9e\u4f8b \u60f3\u8981Knative\u5b9e\u4f8b\u7684\u514d\u8d39\u6570\u5b57\u62f7\u8d1d? VMWare\u6177\u6168\u5730\u6350\u8d60\u4e86Knative\u5b9e\u4f8b\u7535\u5b50\u4e66\u7684\u514d\u8d39\u8bbf\u95ee\u6743\u9650\uff0c \u5728\u8fd9\u91cc\u83b7\u53d6\u60a8\u7684\u526f\u672c! Knative Cookbook","title":"Knative \u4e66\u7c4d"},{"location":"getting-started/next-steps/#knative_3","text":"\u5176\u4ed6\u94fe\u63a5\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5f00\u59cb\u4f7f\u7528 Knative: Knative YouTube \u9891\u9053 Knative.tips","title":"\u5176\u5b83 Knative \u94fe\u63a5"},{"location":"getting-started/next-steps/#_2","text":"\u6211\u4eec\u5f88\u4e50\u610f\u5728\u60a8\u7684 Knative \u65c5\u7a0b\u7684\u4e0b\u4e00\u6b65\u5e2e\u52a9\u60a8\u3002 \u5982\u679c\u6211\u4eec\u5728\u672c\u9875\u4e0a\u9057\u6f0f\u4e86\u60a8\u8ba4\u4e3a\u5e94\u8be5\u5728\u8fd9\u91cc\u7684\u5185\u5bb9\uff0c\u8bf7 \u7ed9\u6211\u4eec\u53cd\u9988 !","title":"\u6211\u4eec\u9057\u6f0f\u4e86\u4ec0\u4e48\u5417?"},{"location":"getting-started/quickstart-install/","text":"\u4f7f\u7528\u5feb\u901f\u542f\u52a8\u5b89\u88c5Knative \u00b6 \u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730Knative\u5b89\u88c5\u3002 \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002 \u5b89\u88c5Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . \u5b89\u88c5Knative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6 \u00b6 \u8981\u5f00\u59cb\uff0c\u5b89\u88c5knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go Do one of the following: To install the quickstart plugin by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH by running the command: mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help \u8fd0\u884cKnative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6 \u00b6 quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684Kubernetes\u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5Knative\u670d\u52a1 \u5176\u4e2d\uff0cKourier\u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io\u4f5c\u4e3aDNS \u5b89\u88c5Knative\u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97Knative\u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube Install Knative and Kubernetes using kind by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u73b0\u5728\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86Knative\uff0c\u60a8\u53ef\u4ee5\u5728\u672c\u6559\u7a0b\u7684\u4e0b\u4e00\u4e2a\u4e3b\u9898\u4e2d\u5b66\u4e60\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2aKnative\u670d\u52a1\u3002","title":"\u4f7f\u7528quickstart\u5b89\u88c5"},{"location":"getting-started/quickstart-install/#knative","text":"\u672c\u5feb\u901f\u5165\u95e8\u6559\u7a0b\u901a\u8fc7\u4f7f\u7528Knative quickstart \u63d2\u4ef6\u4e3a\u60a8\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5316\u7684\u672c\u5730Knative\u5b89\u88c5\u3002","title":"\u4f7f\u7528\u5feb\u901f\u542f\u52a8\u5b89\u88c5Knative"},{"location":"getting-started/quickstart-install/#_1","text":"Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"getting-started/quickstart-install/#knative-cli","text":"Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn .","title":"\u5b89\u88c5Knative CLI"},{"location":"getting-started/quickstart-install/#knative_1","text":"\u8981\u5f00\u59cb\uff0c\u5b89\u88c5knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go Do one of the following: To install the quickstart plugin by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH by running the command: mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help","title":"\u5b89\u88c5Knative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6"},{"location":"getting-started/quickstart-install/#knative_2","text":"quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684Kubernetes\u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5Knative\u670d\u52a1 \u5176\u4e2d\uff0cKourier\u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io\u4f5c\u4e3aDNS \u5b89\u88c5Knative\u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97Knative\u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube Install Knative and Kubernetes using kind by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list","title":"\u8fd0\u884cKnative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6"},{"location":"getting-started/quickstart-install/#_2","text":"\u73b0\u5728\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86Knative\uff0c\u60a8\u53ef\u4ee5\u5728\u672c\u6559\u7a0b\u7684\u4e0b\u4e00\u4e2a\u4e3b\u9898\u4e2d\u5b66\u4e60\u5982\u4f55\u90e8\u7f72\u60a8\u7684\u7b2c\u4e00\u4e2aKnative\u670d\u52a1\u3002","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"install/","text":"\u5b89\u88c5 Knative \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u90e8\u7f72\u9009\u9879\u4e4b\u4e00\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5: \u4f7f\u7528 Knative Quickstart plugin \u5b89\u88c5\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u53d1\u884c\u7248\uff0c\u7528\u4e8e\u5f00\u53d1\u76ee\u7684\u3002 \u4f7f\u7528\u57fa\u4e8eyaml\u7684\u5b89\u88c5\u6765\u5b89\u88c5\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72: \u4f7f\u7528YAML\u5b89\u88c5Knative\u670d\u52a1 \u4f7f\u7528YAML\u5b89\u88c5Knative\u4e8b\u4ef6 \u4f7f\u7528 Knative Operator \u5b89\u88c5\u548c\u914d\u7f6e\u4e00\u4e2a\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72\u3002 \u9075\u5faa\u4f9b\u5e94\u5546\u7ba1\u7406 Knative\u4ea7\u54c1 \u7684\u6587\u6863. \u60a8\u8fd8\u53ef\u4ee5 \u5347\u7ea7\u73b0\u6709\u7684Knative\u5b89\u88c5 . Note Knative\u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u6b63\u5728\u8fd0\u884c\u5e26\u6709Bash shell\u7684Mac\u6216Linux\u3002","title":"\u5b89\u88c5Knative"},{"location":"install/#knative","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u90e8\u7f72\u9009\u9879\u4e4b\u4e00\u5728\u96c6\u7fa4\u4e0a\u5b89\u88c5\u670d\u52a1\u7ec4\u4ef6\u3001\u4e8b\u4ef6\u7ec4\u4ef6\u6216\u4e24\u8005\u90fd\u5b89\u88c5: \u4f7f\u7528 Knative Quickstart plugin \u5b89\u88c5\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u53d1\u884c\u7248\uff0c\u7528\u4e8e\u5f00\u53d1\u76ee\u7684\u3002 \u4f7f\u7528\u57fa\u4e8eyaml\u7684\u5b89\u88c5\u6765\u5b89\u88c5\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72: \u4f7f\u7528YAML\u5b89\u88c5Knative\u670d\u52a1 \u4f7f\u7528YAML\u5b89\u88c5Knative\u4e8b\u4ef6 \u4f7f\u7528 Knative Operator \u5b89\u88c5\u548c\u914d\u7f6e\u4e00\u4e2a\u751f\u4ea7\u5c31\u7eea\u90e8\u7f72\u3002 \u9075\u5faa\u4f9b\u5e94\u5546\u7ba1\u7406 Knative\u4ea7\u54c1 \u7684\u6587\u6863. \u60a8\u8fd8\u53ef\u4ee5 \u5347\u7ea7\u73b0\u6709\u7684Knative\u5b89\u88c5 . Note Knative\u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u6b63\u5728\u8fd0\u884c\u5e26\u6709Bash shell\u7684Mac\u6216Linux\u3002","title":"\u5b89\u88c5 Knative"},{"location":"install/installing-cert-manager/","text":"Installing cert-manager for TLS certificates \u00b6 Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager. Before you begin \u00b6 You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guide . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher. Downloading and installing cert-manager \u00b6 To download and install cert-manager, follow the Installation steps from the official cert-manager website. Completing the Knative configuration for TLS support \u00b6 Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"\u5b89\u88c5cert-manager"},{"location":"install/installing-cert-manager/#installing-cert-manager-for-tls-certificates","text":"Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager.","title":"Installing cert-manager for TLS certificates"},{"location":"install/installing-cert-manager/#before-you-begin","text":"You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guide . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher.","title":"Before you begin"},{"location":"install/installing-cert-manager/#downloading-and-installing-cert-manager","text":"To download and install cert-manager, follow the Installation steps from the official cert-manager website.","title":"Downloading and installing cert-manager"},{"location":"install/installing-cert-manager/#completing-the-knative-configuration-for-tls-support","text":"Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Completing the Knative configuration for TLS support"},{"location":"install/installing-istio/","text":"Installing Istio for Knative \u00b6 This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation. Before you begin \u00b6 You need: A Kubernetes cluster created. istioctl installed. Supported Istio versions \u00b6 You can view the latest tested Istio version on the Knative Net Istio releases page . Installing Istio \u00b6 When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits. Choosing an Istio installation \u00b6 You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars . Installing Istio without sidecar injection \u00b6 Enter the following command to install Istio: To install Istio without sidecar injection: istioctl install -y Installing Istio with sidecar injection \u00b6 If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled Using Istio mTLS feature \u00b6 Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway . Updating the config-istio configmap to use a non-default local gateway \u00b6 If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly. Verifying your Istio install \u00b6 View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode. Configuring DNS \u00b6 Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\" Istio resources \u00b6 For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference . Clean up Istio \u00b6 See the Uninstall Istio . What's next \u00b6 View the Knative Serving documentation . Try some Knative Serving code samples .","title":"\u5b89\u88c5Istio"},{"location":"install/installing-istio/#installing-istio-for-knative","text":"This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#before-you-begin","text":"You need: A Kubernetes cluster created. istioctl installed.","title":"Before you begin"},{"location":"install/installing-istio/#supported-istio-versions","text":"You can view the latest tested Istio version on the Knative Net Istio releases page .","title":"Supported Istio versions"},{"location":"install/installing-istio/#installing-istio","text":"When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits.","title":"Installing Istio"},{"location":"install/installing-istio/#choosing-an-istio-installation","text":"You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars .","title":"Choosing an Istio installation"},{"location":"install/installing-istio/#installing-istio-without-sidecar-injection","text":"Enter the following command to install Istio: To install Istio without sidecar injection: istioctl install -y","title":"Installing Istio without sidecar injection"},{"location":"install/installing-istio/#installing-istio-with-sidecar-injection","text":"If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled","title":"Installing Istio with sidecar injection"},{"location":"install/installing-istio/#using-istio-mtls-feature","text":"Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway .","title":"Using Istio mTLS feature"},{"location":"install/installing-istio/#updating-the-config-istio-configmap-to-use-a-non-default-local-gateway","text":"If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly.","title":"Updating the config-istio configmap to use a non-default local gateway"},{"location":"install/installing-istio/#verifying-your-istio-install","text":"View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode.","title":"Verifying your Istio install"},{"location":"install/installing-istio/#configuring-dns","text":"Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\"","title":"Configuring DNS"},{"location":"install/installing-istio/#istio-resources","text":"For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference .","title":"Istio resources"},{"location":"install/installing-istio/#clean-up-istio","text":"See the Uninstall Istio .","title":"Clean up Istio"},{"location":"install/installing-istio/#whats-next","text":"View the Knative Serving documentation . Try some Knative Serving code samples .","title":"What's next"},{"location":"install/knative-offerings/","text":"Knative Offerings \u00b6 Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Nutanix Karbon : Extend Nutanix Karbon with serverless capabilities by installing Knative on a Karbon managed Kubernetes cluster. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh : On-premises and fully-managed Knative and event-driven integration platform. With support for AWS, Azure, Google, and many more cloud and enterprise event sources and brokers. Commercial Knative support and professional services available.","title":"\u4f7f\u7528\u57fa\u4e8eKnative-based\u4ea7\u54c1"},{"location":"install/knative-offerings/#knative-offerings","text":"Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Nutanix Karbon : Extend Nutanix Karbon with serverless capabilities by installing Knative on a Karbon managed Kubernetes cluster. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh : On-premises and fully-managed Knative and event-driven integration platform. With support for AWS, Azure, Google, and many more cloud and enterprise event sources and brokers. Commercial Knative support and professional services available.","title":"Knative Offerings"},{"location":"install/quickstart-install/","text":"Install Knative using quickstart \u00b6 Following this quickstart tutorial provides you with a simplified, local Knative installation by using the Knative quickstart plugin. \u5728\u4f60\u5f00\u59cb\u4e4b\u524d \u00b6 Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002 \u5b89\u88c5Knative CLI \u00b6 Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . \u5b89\u88c5Knative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6 \u00b6 \u8981\u5f00\u59cb\uff0c\u5b89\u88c5knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go Do one of the following: To install the quickstart plugin by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH by running the command: mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help \u8fd0\u884cKnative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6 \u00b6 quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684Kubernetes\u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5Knative\u670d\u52a1 \u5176\u4e2d\uff0cKourier\u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io\u4f5c\u4e3aDNS \u5b89\u88c5Knative\u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97Knative\u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube Install Knative and Kubernetes using kind by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list Next steps \u00b6 Learn how to deploy your first Service in the Knative tutorial . Try out Knative code samples . See the Knative Serving and Knative Eventing guides.","title":"\u5feb\u901f\u542f\u52a8\u5b89\u88c5"},{"location":"install/quickstart-install/#install-knative-using-quickstart","text":"Following this quickstart tutorial provides you with a simplified, local Knative installation by using the Knative quickstart plugin.","title":"Install Knative using quickstart"},{"location":"install/quickstart-install/#_1","text":"Warning Knative \u5feb\u901f\u542f\u52a8 \u73af\u5883\u4ec5\u7528\u4e8e\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eyaml\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 . \u5728\u5f00\u59cb\u4f7f\u7528Knative \u5feb\u901f\u5165\u95e8 \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5Knative: kind (Kubernetes in Docker) \u6216 minikube \u4f7f\u60a8\u80fd\u591f\u4f7f\u7528Docker\u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730Kubernetes\u96c6\u7fa4\u3002 Kubernetes CLI ( kubectl ) \u5728Kubernetes\u96c6\u7fa4\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002 \u53ef\u4ee5\u4f7f\u7528 kubectl \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3001\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282\u3002 \u8981\u521b\u5efa\u7684\u96c6\u7fa4\u81f3\u5c11\u9700\u89813\u4e2acpu\u548c3\u4e2aGB\u7684RAM\u3002","title":"\u5728\u4f60\u5f00\u59cb\u4e4b\u524d"},{"location":"install/quickstart-install/#knative-cli","text":"Knative CLI ( kn )\u4e3a\u521b\u5efaKnative\u8d44\u6e90(\u5982Knative\u670d\u52a1\u548c\u4e8b\u4ef6\u6e90)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u800c\u7b80\u5355\u7684\u754c\u9762\uff0c\u800c\u4e0d\u9700\u8981\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539YAML\u6587\u4ef6\u3002 kn CLI\u8fd8\u7b80\u5316\u4e86\u8bf8\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u5206\u5272\u7b49\u590d\u6742\u8fc7\u7a0b\u7684\u5b8c\u6210\u3002 \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go \u4f7f\u7528\u5bb9\u5668\u6620\u50cf \u505a\u4ee5\u4e0b\u4efb\u4f55\u4e00\u4ef6\u4e8b: To install kn by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative/client/kn Having issues upgrading kn using Homebrew? If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the master branch was renamed to main . Resolve this issue by running the command: brew uninstall kn brew untap knative/client --force brew install knative/client/kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u7cfb\u7edf\u7684\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002 Download the binary for your system from the kn release page . Rename the binary to kn and make it executable by running the commands: mv <path-to-binary-file> kn chmod +x kn Where <path-to-binary-file> is the path to the binary file you downloaded in the previous step, for example, kn-darwin-amd64 or kn-linux-amd64 . Move the executable binary file to a directory on your PATH by running the command: mv kn /usr/local/bin Verify that the plugin is working by running the command: kn version Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Links to images are available here: Latest release You can run kn from a container image. For example: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn .","title":"\u5b89\u88c5Knative CLI"},{"location":"install/quickstart-install/#knative","text":"\u8981\u5f00\u59cb\uff0c\u5b89\u88c5knative quickstart \u63d2\u4ef6: \u4f7f\u7528 Homebrew \u4f7f\u7528\u4e8c\u8fdb\u5236 \u4f7f\u7528 Go Do one of the following: To install the quickstart plugin by using Homebrew , run the command (Use brew upgrade instead if you are upgrading from a previous version): brew install knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH by running the command: mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help","title":"\u5b89\u88c5Knative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6"},{"location":"install/quickstart-install/#knative_1","text":"quickstart \u63d2\u4ef6\u5b8c\u6210\u4ee5\u4e0b\u529f\u80fd: \u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86\u9009\u5b9a\u7684Kubernetes\u5b9e\u4f8b \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a knative \u7684\u96c6\u7fa4 \u5b89\u88c5Knative\u670d\u52a1 \u5176\u4e2d\uff0cKourier\u4f5c\u4e3a\u9ed8\u8ba4\u7684\u7f51\u7edc\u5c42\uff0csslip.io\u4f5c\u4e3aDNS \u5b89\u88c5Knative\u4e8b\u4ef6 \u5e76\u521b\u5efa\u5185\u5b58\u4e2d\u4ee3\u7406\u548c\u901a\u9053\u5b9e\u73b0 \u8981\u83b7\u5f97Knative\u7684\u672c\u5730\u90e8\u7f72\uff0c\u8fd0\u884c quickstart \u63d2\u4ef6: \u4f7f\u7528 kind \u4f7f\u7528 minikube Install Knative and Kubernetes using kind by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list","title":"\u8fd0\u884cKnative\u5feb\u901f\u542f\u52a8\u63d2\u4ef6"},{"location":"install/quickstart-install/#next-steps","text":"Learn how to deploy your first Service in the Knative tutorial . Try out Knative code samples . See the Knative Serving and Knative Eventing guides.","title":"Next steps"},{"location":"install/uninstall/","text":"Uninstalling Knative \u00b6 To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure. Uninstalling a YAML-based Knative installation \u00b6 To uninstall a YAML-based Knative installation: Uninstalling optional Serving extensions \u00b6 Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Uninstalling a networking layer \u00b6 Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Uninstalling the Serving component \u00b6 Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Uninstalling optional Eventing extensions \u00b6 Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml Uninstalling an optional Broker (Eventing) layer \u00b6 Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml Uninstalling optional channel (messaging) layers \u00b6 Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub. Uninstalling the Eventing component \u00b6 Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Uninstall an Operator-based Knative installation \u00b6 To uninstall an Operator-based Knative installation, follow these procedures: Removing the Knative Serving component \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Operator: \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"\u5378\u8f7dKnative"},{"location":"install/uninstall/#uninstalling-knative","text":"To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure.","title":"Uninstalling Knative"},{"location":"install/uninstall/#uninstalling-a-yaml-based-knative-installation","text":"To uninstall a YAML-based Knative installation:","title":"Uninstalling a YAML-based Knative installation"},{"location":"install/uninstall/#uninstalling-optional-serving-extensions","text":"Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml","title":"Uninstalling optional Serving extensions"},{"location":"install/uninstall/#uninstalling-a-networking-layer","text":"Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml","title":"Uninstalling a networking layer"},{"location":"install/uninstall/#uninstalling-the-serving-component","text":"Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml","title":"Uninstalling the Serving component"},{"location":"install/uninstall/#uninstalling-optional-eventing-extensions","text":"Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml","title":"Uninstalling optional Eventing extensions"},{"location":"install/uninstall/#uninstalling-an-optional-broker-eventing-layer","text":"Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml","title":"Uninstalling an optional Broker (Eventing) layer"},{"location":"install/uninstall/#uninstalling-optional-channel-messaging-layers","text":"Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub.","title":"Uninstalling optional channel (messaging) layers"},{"location":"install/uninstall/#uninstalling-the-eventing-component","text":"Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml","title":"Uninstalling the Eventing component"},{"location":"install/uninstall/#uninstall-an-operator-based-knative-installation","text":"To uninstall an Operator-based Knative installation, follow these procedures:","title":"Uninstall an Operator-based Knative installation"},{"location":"install/uninstall/#removing-the-knative-serving-component","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/uninstall/#removing-knative-eventing-component","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Removing Knative Eventing component"},{"location":"install/uninstall/#removing-the-knative-operator","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/operator/configuring-eventing-cr/","text":"Configuring the Eventing Operator custom resource \u00b6 You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). Setting a default channel \u00b6 If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting. Setting the default channel for the broker \u00b6 If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 Private repository and private secrets \u00b6 The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In this example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: ```bash kn operator configure images --component eventing --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/broker-eventing-controller:latest -n knative-eventing ``` - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key. Download images from different repositories without secrets \u00b6 If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, given the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook docker.io/knative-images-repo2/eventing-webhook:latest controller docker.io/knative-images-repo3/imc-controller:latest dispatcher docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller eventing-controller docker.io/knative-images-repo5/broker-eventing-controller:latest You must modify the KnativeEventing CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --imageKey DISPATCHER_IMAGE -controller --imageURL docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest -n knative-eventing Download images with secrets \u00b6 If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... Configuring the default broker class \u00b6 Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported. Override the resources \u00b6 The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100M limits : cpu : 1000m memory : 250M You can also run the following command to make the equivalent change: kn operator configure resources --component eventing --deployName eventing-controller --container eventing-controller --requestCPU 300m --requestMemory 100M --limitCPU 1000m --limitMemory 250M -n knative-eventing Override the nodeSelector \u00b6 The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelector --component eventing --deployName eventing-controller --key disktype --value hdd -n knative-eventing Override the tolerations \u00b6 The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component eventing --deployName eventing-controller --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-eventing Override the affinity \u00b6 The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd Override the environment variables \u00b6 The KnativeEventing resource is able to override or add the environment variables for the containers in the Knative Eventing deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container eventing-controller into \"knative.dev/my-repo\" for the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller env : - container : eventing-controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component eventing --deployName eventing-controller --container eventing-controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-eventing Override system services \u00b6 If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported. Override labels and annotations and selector \u00b6 The following KnativeEventing resource overrides the eventing-webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : services : - name : eventing-webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component eventing --serviceName eventing-webhook --key mylabel --value foo -n knative-eventing kn operator configure annotations --component eventing --serviceName eventing-webhook --key myannotations --value bar -n knative-eventing kn operator configure selectors --component eventing --serviceName eventing-webhook --key myselector --value bar -n knative-eventing Override system podDisruptionBudgets \u00b6 A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Eventing based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named eventing-webhook , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : podDisruptionBudgets : - name : eventing-webhook minAvailable : 70%","title":"\u914d\u7f6eKnative\u4e8b\u4ef6CRDs"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-eventing-operator-custom-resource","text":"You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR).","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","text":"If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Setting a default channel"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","text":"If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1","title":"Setting the default channel for the broker"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","text":"The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In this example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: ```bash kn operator configure images --component eventing --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/broker-eventing-controller:latest -n knative-eventing ``` - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key.","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","text":"If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, given the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook docker.io/knative-images-repo2/eventing-webhook:latest controller docker.io/knative-images-repo3/imc-controller:latest dispatcher docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller eventing-controller docker.io/knative-images-repo5/broker-eventing-controller:latest You must modify the KnativeEventing CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest You can also run the following commands to make the equivalent change: kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing kn operator configure images --component eventing --imageKey DISPATCHER_IMAGE -controller --imageURL docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest -n knative-eventing","title":"Download images from different repositories without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","text":"Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker","title":"Configuring the default broker class"},{"location":"install/operator/configuring-eventing-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-eventing-cr/#override-the-resources","text":"The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100M limits : cpu : 1000m memory : 250M You can also run the following command to make the equivalent change: kn operator configure resources --component eventing --deployName eventing-controller --container eventing-controller --requestCPU 300m --requestMemory 100M --limitCPU 1000m --limitMemory 250M -n knative-eventing","title":"Override the resources"},{"location":"install/operator/configuring-eventing-cr/#override-the-nodeselector","text":"The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelector --component eventing --deployName eventing-controller --key disktype --value hdd -n knative-eventing","title":"Override the nodeSelector"},{"location":"install/operator/configuring-eventing-cr/#override-the-tolerations","text":"The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component eventing --deployName eventing-controller --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-eventing","title":"Override the tolerations"},{"location":"install/operator/configuring-eventing-cr/#override-the-affinity","text":"The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-eventing-cr/#override-the-environment-variables","text":"The KnativeEventing resource is able to override or add the environment variables for the containers in the Knative Eventing deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container eventing-controller into \"knative.dev/my-repo\" for the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller env : - container : eventing-controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component eventing --deployName eventing-controller --container eventing-controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-eventing","title":"Override the environment variables"},{"location":"install/operator/configuring-eventing-cr/#override-system-services","text":"If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported.","title":"Override system services"},{"location":"install/operator/configuring-eventing-cr/#override-labels-and-annotations-and-selector","text":"The following KnativeEventing resource overrides the eventing-webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : services : - name : eventing-webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component eventing --serviceName eventing-webhook --key mylabel --value foo -n knative-eventing kn operator configure annotations --component eventing --serviceName eventing-webhook --key myannotations --value bar -n knative-eventing kn operator configure selectors --component eventing --serviceName eventing-webhook --key myselector --value bar -n knative-eventing","title":"Override labels and annotations and selector"},{"location":"install/operator/configuring-eventing-cr/#override-system-poddisruptionbudgets","text":"A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Eventing based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named eventing-webhook , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : podDisruptionBudgets : - name : eventing-webhook minAvailable : 70%","title":"Override system podDisruptionBudgets"},{"location":"install/operator/configuring-serving-cr/","text":"Configuring the Knative Serving Operator custom resource \u00b6 You can modify the KnativeServing CR to configure different options for Knative Serving. Configure the Knative Serving version \u00b6 Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v1.5, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"1.5\" You can also run the following command to make the equivalent change: kn operator install --component serving -v 1 .5 -n knative-serving If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator does nothing. The Knative Operator always includes the latest 3 release versions. For example, if the current version of the Knative Operator is v1.5, the earliest version of Knative Serving available through the Operator is v1.2. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v1.3, you must upgrade to v1.4 before upgrading to v1.5. Install customized Knative Serving \u00b6 There are two modes available that you can use to install customized Knative Serving manifests: overwrite mode and append mode . If you are using overwrite mode, under .spec.manifests , you must define all required manifests to install Knative Serving, because the Operator does not install any default manifests. If you are using append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode \u00b6 You can use overwrite mode when you want to customize all Knative Serving manifests. Important You must specify both the version and valid URLs for your custom Knative Serving manifests. For example, if you want to install customized versions of both Knative Serving and the Istio ingress, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. Important The ordering of manifest URLs is critical. Put the manifest you want to apply first at the top of the list. This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml . Append mode \u00b6 You can use append mode to add your customized Knative Serving manifests in addition to the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving manifests, and then installs the customized resources available at https://my-serving/serving-custom.yaml for the version $spec_version . Private repository and private secrets \u00b6 You can use the spec.registry section of the Operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Container Docker Image activator docker.io/knative-images/activator:latest autoscaler docker.io/knative-images/autoscaler:latest controller docker.io/knative-images/controller:latest webhook docker.io/knative-images/webhook:latest autoscaler-hpa docker.io/knative-images/autoscaler-hpa:latest net-istio-controller docker.io/knative-images/net-istio-controller:latest queue-proxy docker.io/knative-images/queue-proxy:latest Define your operator CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:latest You can also run the following command to make the equivalent change: ```bash kn operator configure images --component serving --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-serving ``` Download images individually without secrets \u00b6 If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:latest autoscaler docker.io/knative-images-repo2/autoscaler:latest controller docker.io/knative-images-repo3/controller:latest webhook docker.io/knative-images-repo4/webhook:latest autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:latest queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:latest You must modify the Operator CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:latest autoscaler : docker.io/knative-images-repo2/autoscaler:latest controller : docker.io/knative-images-repo3/controller:latest webhook : docker.io/knative-images-repo4/webhook:latest autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:latest queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:latest You can also run the following commands to make the equivalent change: kn operator configure images --component serving --imageKey activator --imageURL docker.io/knative-images-repo1/activator:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler --imageURL docker.io/knative-images-repo2/autoscaler:latest -n knative-serving kn operator configure images --component serving --imageKey controller --imageURL docker.io/knative-images-repo3/controller:latest -n knative-serving kn operator configure images --component serving --imageKey webhook --imageURL docker.io/knative-images-repo4/webhook:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler-hpa --imageURL docker.io/knative-images-repo5/autoscaler-hpa:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-controller --imageKey controller --imageURL docker.io/knative-images-repo6/prefix-net-istio-controller:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-webhook --imageKey webhook --imageURL docker.io/knative-images-repo6/net-istio-webhook:latest -n knative-serving kn operator configure images --component serving --imageKey queue-proxy --imageURL docker.io/knative-images-repo7/queue-proxy-suffix:latest -n knative-serving Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook . Download images with secrets \u00b6 If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... SSL certificate for controller \u00b6 To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap Replace the default Istio ingress gateway service \u00b6 Create a gateway Service and Deployment instance . Update the Knative gateway by updating the ingress.istio.knative-ingress-gateway spec to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Replace the ingress gateway \u00b6 Create a gateway . Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Configuration of cluster local gateway \u00b6 Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway: Default local gateway name \u00b6 Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway . Non-default local gateway name \u00b6 If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\" Servers configuration for Istio gateways: \u00b6 You can leverage the KnativeServing CR to configure the hosts and port of the servers stanzas for knative-local-gateway or knative-ingress-gateway gateways. For example, you would like to specify the host into <test-ip> and configure the port with number: 443 , name: https , protocol: HTTPS , and target_port: 8443 for knative-local-gateway , apply the following yaml content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : servers : - port : number : 443 name : https protocol : HTTPS target_port : 8443 hosts : - <test-ip> config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\" High availability \u00b6 By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 You can also run the following command to make the equivalent change: kn operator configure replicas --component serving --replicas 3 -n knative-serving The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 . Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by modifying the deployments spec in the KnativeServing CR. Currently resources , replicas , labels , annotations and nodeSelector are supported. Override the resources \u00b6 The KnativeServing CR is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within a deployment. For example, the following KnativeServing CR configures the container controller in the deployment controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller resources : - container : controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi You can also run the following command to make the equivalent change: kn operator configure resources --component serving --deployName controller --container controller --requestCPU 300m --requestMemory 100Mi --limitCPU 1000m --limitMemory 250Mi -n knative-serving Override replicas, labels and annotations \u00b6 The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotations: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotations : bar You can also run the following commands to make the equivalent change: kn operator configure replicas --component serving --replicas 2 -n knative-serving kn operator configure replicas --component serving --deployName webhook --replicas 3 -n knative-serving kn operator configure labels --component serving --deployName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --deployName webhook --key myannotations --value bar -n knative-serving Note The KnativeServing CR label and annotation settings override the webhook's labels and annotations for Deployments and Pods. Override the nodeSelector \u00b6 The following KnativeServing CR overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelectors --component serving --deployName webhook --key disktype --value hdd -n knative-serving Override the tolerations \u00b6 The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component serving --deployName activator --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-serving Override the affinity \u00b6 The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd Override the environment variables \u00b6 The KnativeServing resource is able to override or add the environment variables for the containers in the Knative Serving deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container controller into \"knative.dev/my-repo\" for the deployment controller , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller env : - container : controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component serving --deployName controller --container controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-serving Override system services \u00b6 If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported. Override labels and annotations and selector \u00b6 The following KnativeServing resource overrides the webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : services : - name : webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component serving --serviceName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --serviceName webhook --key myannotations --value bar -n knative-serving kn operator configure selectors --component serving --serviceName webhook --key myselector --value bar -n knative-serving Override system podDisruptionBudgets \u00b6 A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Serving based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named activator-pdb , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : podDisruptionBudgets : - name : activator-pdb minAvailable : 70%","title":"\u914d\u7f6eKnative\u670d\u52a1CRDs"},{"location":"install/operator/configuring-serving-cr/#configuring-the-knative-serving-operator-custom-resource","text":"You can modify the KnativeServing CR to configure different options for Knative Serving.","title":"Configuring the Knative Serving Operator custom resource"},{"location":"install/operator/configuring-serving-cr/#configure-the-knative-serving-version","text":"Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v1.5, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"1.5\" You can also run the following command to make the equivalent change: kn operator install --component serving -v 1 .5 -n knative-serving If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator does nothing. The Knative Operator always includes the latest 3 release versions. For example, if the current version of the Knative Operator is v1.5, the earliest version of Knative Serving available through the Operator is v1.2. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v1.3, you must upgrade to v1.4 before upgrading to v1.5.","title":"Configure the Knative Serving version"},{"location":"install/operator/configuring-serving-cr/#install-customized-knative-serving","text":"There are two modes available that you can use to install customized Knative Serving manifests: overwrite mode and append mode . If you are using overwrite mode, under .spec.manifests , you must define all required manifests to install Knative Serving, because the Operator does not install any default manifests. If you are using append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.","title":"Install customized Knative Serving"},{"location":"install/operator/configuring-serving-cr/#overwrite-mode","text":"You can use overwrite mode when you want to customize all Knative Serving manifests. Important You must specify both the version and valid URLs for your custom Knative Serving manifests. For example, if you want to install customized versions of both Knative Serving and the Istio ingress, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. Important The ordering of manifest URLs is critical. Put the manifest you want to apply first at the top of the list. This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml .","title":"Overwrite mode"},{"location":"install/operator/configuring-serving-cr/#append-mode","text":"You can use append mode to add your customized Knative Serving manifests in addition to the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create a KnativeServing CR similar to the following example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving manifests, and then installs the customized resources available at https://my-serving/serving-custom.yaml for the version $spec_version .","title":"Append mode"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","text":"You can use the spec.registry section of the Operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} . To define your image links: Push images to the following image tags: Container Docker Image activator docker.io/knative-images/activator:latest autoscaler docker.io/knative-images/autoscaler:latest controller docker.io/knative-images/controller:latest webhook docker.io/knative-images/webhook:latest autoscaler-hpa docker.io/knative-images/autoscaler-hpa:latest net-istio-controller docker.io/knative-images/net-istio-controller:latest queue-proxy docker.io/knative-images/queue-proxy:latest Define your operator CR with following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:latest You can also run the following command to make the equivalent change: ```bash kn operator configure images --component serving --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-serving ```","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","text":"If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:latest autoscaler docker.io/knative-images-repo2/autoscaler:latest controller docker.io/knative-images-repo3/controller:latest webhook docker.io/knative-images-repo4/webhook:latest autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:latest queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:latest You must modify the Operator CR to include the full list. For example: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:latest autoscaler : docker.io/knative-images-repo2/autoscaler:latest controller : docker.io/knative-images-repo3/controller:latest webhook : docker.io/knative-images-repo4/webhook:latest autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:latest net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:latest net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:latest queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:latest You can also run the following commands to make the equivalent change: kn operator configure images --component serving --imageKey activator --imageURL docker.io/knative-images-repo1/activator:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler --imageURL docker.io/knative-images-repo2/autoscaler:latest -n knative-serving kn operator configure images --component serving --imageKey controller --imageURL docker.io/knative-images-repo3/controller:latest -n knative-serving kn operator configure images --component serving --imageKey webhook --imageURL docker.io/knative-images-repo4/webhook:latest -n knative-serving kn operator configure images --component serving --imageKey autoscaler-hpa --imageURL docker.io/knative-images-repo5/autoscaler-hpa:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-controller --imageKey controller --imageURL docker.io/knative-images-repo6/prefix-net-istio-controller:latest -n knative-serving kn operator configure images --component serving --deployName net-istio-webhook --imageKey webhook --imageURL docker.io/knative-images-repo6/net-istio-webhook:latest -n knative-serving kn operator configure images --component serving --imageKey queue-proxy --imageURL docker.io/knative-images-repo7/queue-proxy-suffix:latest -n knative-serving Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook .","title":"Download images individually without secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","text":"To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap","title":"SSL certificate for controller"},{"location":"install/operator/configuring-serving-cr/#replace-the-default-istio-ingress-gateway-service","text":"Create a gateway Service and Deployment instance . Update the Knative gateway by updating the ingress.istio.knative-ingress-gateway spec to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Replace the default Istio ingress gateway service"},{"location":"install/operator/configuring-serving-cr/#replace-the-ingress-gateway","text":"Create a gateway . Update the Istio ingress gateway ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Replace the ingress gateway"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","text":"Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway:","title":"Configuration of cluster local gateway"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","text":"Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway .","title":"Default local gateway name"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","text":"If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Non-default local gateway name"},{"location":"install/operator/configuring-serving-cr/#servers-configuration-for-istio-gateways","text":"You can leverage the KnativeServing CR to configure the hosts and port of the servers stanzas for knative-local-gateway or knative-ingress-gateway gateways. For example, you would like to specify the host into <test-ip> and configure the port with number: 443 , name: https , protocol: HTTPS , and target_port: 8443 for knative-local-gateway , apply the following yaml content: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : servers : - port : number : 443 name : https protocol : HTTPS target_port : 8443 hosts : - <test-ip> config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Servers configuration for Istio gateways:"},{"location":"install/operator/configuring-serving-cr/#high-availability","text":"By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 You can also run the following command to make the equivalent change: kn operator configure replicas --component serving --replicas 3 -n knative-serving The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 .","title":"High availability"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by modifying the deployments spec in the KnativeServing CR. Currently resources , replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-serving-cr/#override-the-resources","text":"The KnativeServing CR is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within a deployment. For example, the following KnativeServing CR configures the container controller in the deployment controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller resources : - container : controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi You can also run the following command to make the equivalent change: kn operator configure resources --component serving --deployName controller --container controller --requestCPU 300m --requestMemory 100Mi --limitCPU 1000m --limitMemory 250Mi -n knative-serving","title":"Override the resources"},{"location":"install/operator/configuring-serving-cr/#override-replicas-labels-and-annotations","text":"The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotations: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotations : bar You can also run the following commands to make the equivalent change: kn operator configure replicas --component serving --replicas 2 -n knative-serving kn operator configure replicas --component serving --deployName webhook --replicas 3 -n knative-serving kn operator configure labels --component serving --deployName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --deployName webhook --key myannotations --value bar -n knative-serving Note The KnativeServing CR label and annotation settings override the webhook's labels and annotations for Deployments and Pods.","title":"Override replicas, labels and annotations"},{"location":"install/operator/configuring-serving-cr/#override-the-nodeselector","text":"The following KnativeServing CR overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd You can also run the following command to make the equivalent change: kn operator configure nodeSelectors --component serving --deployName webhook --key disktype --value hdd -n knative-serving","title":"Override the nodeSelector"},{"location":"install/operator/configuring-serving-cr/#override-the-tolerations","text":"The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" You can also run the following command to make the equivalent change: kn operator configure tolerations --component serving --deployName activator --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-serving","title":"Override the tolerations"},{"location":"install/operator/configuring-serving-cr/#override-the-affinity","text":"The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-serving-cr/#override-the-environment-variables","text":"The KnativeServing resource is able to override or add the environment variables for the containers in the Knative Serving deployment resources. For example, if you would like to change the value of environment variable METRICS_DOMAIN in the container controller into \"knative.dev/my-repo\" for the deployment controller , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : controller env : - container : controller envVars : - name : METRICS_DOMAIN value : \"knative.dev/my-repo\" You can also run the following command to make the equivalent change: kn operator configure envvars --component serving --deployName controller --container controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-serving","title":"Override the environment variables"},{"location":"install/operator/configuring-serving-cr/#override-system-services","text":"If you would like to override some configurations for a specific service, you can override the configuration by using spec.services in CR. Currently labels , annotations and selector are supported.","title":"Override system services"},{"location":"install/operator/configuring-serving-cr/#override-labels-and-annotations-and-selector","text":"The following KnativeServing resource overrides the webhook service to have the label mylabel: foo , the annotation myannotations: bar , the selector myselector: bar . apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : services : - name : webhook labels : mylabel : foo annotations : myannotations : bar selector : myselector : bar You can also run the following commands to make the equivalent change: kn operator configure labels --component serving --serviceName webhook --key mylabel --value foo -n knative-serving kn operator configure annotations --component serving --serviceName webhook --key myannotations --value bar -n knative-serving kn operator configure selectors --component serving --serviceName webhook --key myselector --value bar -n knative-serving","title":"Override labels and annotations and selector"},{"location":"install/operator/configuring-serving-cr/#override-system-poddisruptionbudgets","text":"A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the minAvailable for a specific podDisruptionBudget resource in Serving based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here . For example, if you would like to change minAvailable into 70% for the podDisruptionBudget named activator-pdb , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : podDisruptionBudgets : - name : activator-pdb minAvailable : 70%","title":"Override system podDisruptionBudgets"},{"location":"install/operator/configuring-with-operator/","text":"Configuring Knative by using the Operator \u00b6 The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data . Examples \u00b6 You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"\u4f7f\u7528Operator\u914d\u7f6eKnative"},{"location":"install/operator/configuring-with-operator/#configuring-knative-by-using-the-operator","text":"The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data .","title":"Configuring Knative by using the Operator"},{"location":"install/operator/configuring-with-operator/#examples","text":"You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"Examples"},{"location":"install/operator/knative-with-operator-cli/","text":"Install by using the Knative Operator CLI Plugin \u00b6 Knative provides a CLI Plugin to install, configure and manage Knative via the command lines. This CLI plugin facilitates you with a parameter-driven way to configure the Knative cluster, without interacting with the complexities of the custom resources. \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Operator CLI Plugin \u00b6 Before you install the Knative Operator CLI Plugin, first install the Knative CLI . MacOS Linux Download the binary kn-operator-darwin-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-darwin-amd64 kn-operator Download the binary kn-operator-linux-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-linux-amd64 kn-operator Make the plugin executable by running the command: chmod +x kn-operator Create the directory for the kn plugin: mkdir -p ~/.config/kn/plugins Move the file to a plugin directory for kn : cp kn-operator ~/.config/kn/plugins Verify the installation of the Knative Operator CLI Plugin \u00b6 You can run the following command to verify the installation: kn operator -h You should see more information about how to use this CLI plugin. Install the Knative Operator \u00b6 You can install Knative Operator of any specific version under any specific namespace. By default, the namespace is default , and the version is the latest. To install the latest version of Knative Operator, run: kn operator install To install Knative Operator under a certain namespace, e.g. knative-operator, run: kn operator install -n knative-operator To install Knative Operator of a specific version, e.g. 1.7.1, run: kn operator install -v 1 .7.1 Installing the Knative Serving component \u00b6 You can install Knative Serving of any specific version under any specific namespace. By default, the namespace is knative-serving , and the version is the latest. To install the latest version of Knative Serving, run: kn operator install --component serving To install Knative Serving under a certain namespace, e.g. knative-serving, run: kn operator install --component serving -n knative-serving To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component serving -n knative-serving -v \"1.7\" To install the ingress plugin, e.g Kourier, together with the install command, run: kn operator install --component serving -n knative-serving -v \"1.7\" --kourier If you do not specify the ingress plugin, istio is used as the default. However, you need to make sure you install Istio first. Install the networking layer \u00b6 You can configure the network layer option via the Operator CLI Plugin. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, run the command as follows: kn operator enable ingress --kourier -n knative-serving The following steps install Istio to enable its Knative integration: Install Istio . To configure Knative Serving to use Istio, run the command as follows: kn operator enable ingress --istio -n knative-serving The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, run the command as follows: kn operator enable ingress --contour -n knative-serving Installing the Knative Eventing component \u00b6 You can install Knative Eventing of any specific version under any specific namespace. By default, the namespace is knative-eventing , and the version is the latest. To install the latest version of Knative Eventing, run: kn operator install --component eventing To install Knative Eventing under a certain namespace, e.g. knative-eventing, run: kn operator install --component eventing -n knative-eventing To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component eventing -n knative-eventing -v \"1.7\" Installing Knative Eventing with event sources \u00b6 Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To install the eventing source Ceph, run the following command: kn operator enable eventing-source --ceph --namespace knative-eventing To install the eventing source Github, run the following command: kn operator enable eventing-source --github --namespace knative-eventing To install the eventing source Gitlab, run the following command: kn operator enable eventing-source --gitlab --namespace knative-eventing To install the eventing source Kafka, run the following command: kn operator enable eventing-source --kafka --namespace knative-eventing To install the eventing source RabbitMQ, run the following command: kn operator enable eventing-source --rabbitmq --namespace knative-eventing To install the eventing source Redis, run the following command: kn operator enable eventing-source --redis --namespace knative-eventing What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"\u4f7f\u7528Knative Operator CLI\u63d2\u4ef6\u8fdb\u884c\u5b89\u88c5"},{"location":"install/operator/knative-with-operator-cli/#install-by-using-the-knative-operator-cli-plugin","text":"Knative provides a CLI Plugin to install, configure and manage Knative via the command lines. This CLI plugin facilitates you with a parameter-driven way to configure the Knative cluster, without interacting with the complexities of the custom resources.","title":"Install by using the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator-cli-plugin","text":"Before you install the Knative Operator CLI Plugin, first install the Knative CLI . MacOS Linux Download the binary kn-operator-darwin-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-darwin-amd64 kn-operator Download the binary kn-operator-linux-amd64 for your system from the release page . Rename the binary to kn-operator : mv kn-operator-linux-amd64 kn-operator Make the plugin executable by running the command: chmod +x kn-operator Create the directory for the kn plugin: mkdir -p ~/.config/kn/plugins Move the file to a plugin directory for kn : cp kn-operator ~/.config/kn/plugins","title":"Install the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#verify-the-installation-of-the-knative-operator-cli-plugin","text":"You can run the following command to verify the installation: kn operator -h You should see more information about how to use this CLI plugin.","title":"Verify the installation of the Knative Operator CLI Plugin"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator","text":"You can install Knative Operator of any specific version under any specific namespace. By default, the namespace is default , and the version is the latest. To install the latest version of Knative Operator, run: kn operator install To install Knative Operator under a certain namespace, e.g. knative-operator, run: kn operator install -n knative-operator To install Knative Operator of a specific version, e.g. 1.7.1, run: kn operator install -v 1 .7.1","title":"Install the Knative Operator"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-serving-component","text":"You can install Knative Serving of any specific version under any specific namespace. By default, the namespace is knative-serving , and the version is the latest. To install the latest version of Knative Serving, run: kn operator install --component serving To install Knative Serving under a certain namespace, e.g. knative-serving, run: kn operator install --component serving -n knative-serving To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component serving -n knative-serving -v \"1.7\" To install the ingress plugin, e.g Kourier, together with the install command, run: kn operator install --component serving -n knative-serving -v \"1.7\" --kourier If you do not specify the ingress plugin, istio is used as the default. However, you need to make sure you install Istio first.","title":"Installing the Knative Serving component"},{"location":"install/operator/knative-with-operator-cli/#install-the-networking-layer","text":"You can configure the network layer option via the Operator CLI Plugin. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, run the command as follows: kn operator enable ingress --kourier -n knative-serving The following steps install Istio to enable its Knative integration: Install Istio . To configure Knative Serving to use Istio, run the command as follows: kn operator enable ingress --istio -n knative-serving The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, run the command as follows: kn operator enable ingress --contour -n knative-serving","title":"Install the networking layer"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-eventing-component","text":"You can install Knative Eventing of any specific version under any specific namespace. By default, the namespace is knative-eventing , and the version is the latest. To install the latest version of Knative Eventing, run: kn operator install --component eventing To install Knative Eventing under a certain namespace, e.g. knative-eventing, run: kn operator install --component eventing -n knative-eventing To install Knative Operator of a specific version, e.g. 1.7, run: kn operator install --component eventing -n knative-eventing -v \"1.7\"","title":"Installing the Knative Eventing component"},{"location":"install/operator/knative-with-operator-cli/#installing-knative-eventing-with-event-sources","text":"Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To install the eventing source Ceph, run the following command: kn operator enable eventing-source --ceph --namespace knative-eventing To install the eventing source Github, run the following command: kn operator enable eventing-source --github --namespace knative-eventing To install the eventing source Gitlab, run the following command: kn operator enable eventing-source --gitlab --namespace knative-eventing To install the eventing source Kafka, run the following command: kn operator enable eventing-source --kafka --namespace knative-eventing To install the eventing source RabbitMQ, run the following command: kn operator enable eventing-source --rabbitmq --namespace knative-eventing To install the eventing source Redis, run the following command: kn operator enable eventing-source --redis --namespace knative-eventing","title":"Installing Knative Eventing with event sources"},{"location":"install/operator/knative-with-operator-cli/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/operator/knative-with-operators/","text":"Install by using the Knative Operator \u00b6 Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. The following table describes the supported versions of Serving and Eventing for the Knative Operator: Operator Serving Eventing v1.8 v1.8.0 v1.7.0, v1.7.1 and v1.7.2 v1.6.0 and v1.6.1 v1.5.0 v1.8.0 v1.7.0, v1.7.1, v1.7.2 and v1.7.3 v1.6.0, v1.6.1 and v1.6.2 v1.5.0, v1.5.1, v1.5.2, v1.5.3, v1.5.4, v1.5.5 and v1.5.6 \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Operator \u00b6 Before you install the Knative Serving and Eventing components, first install the Knative Operator. Warning Knative Operator 1.5 is the last version that supports CRDs with both v1alpha1 and v1beta1 . If you are upgrading an existing Operator install from v1.2 or earlier to v1.3 or later, run the following command to upgrade the existing custom resources to v1beta1 before installing the current version: kubectl create -f https://github.com/knative/operator/releases/download/knative-v1.5.1/operator-post-install.yaml To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page . Verify your Knative Operator installation \u00b6 Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h Track the log \u00b6 To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator Install Knative Serving \u00b6 To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS. Create the Knative Serving custom resource \u00b6 To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Install the networking layer \u00b6 Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later. Verify the Knative Serving deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True \u914d\u7f6e DNS \u00b6 \u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) \u771f\u6b63\u7684DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Install Knative Eventing \u00b6 To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources. Create the Knative Eventing custom resource \u00b6 To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Installing a specific version of Eventing \u00b6 Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v1.7, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"1.7\" You can also run the following command to make the equivalent change: kn operator install --component eventing -v 1 .7 -n knative-eventing If spec.version is not specified, the Knative Operator installs the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 1.4, you must upgrade to 1.5 before upgrading to 1.6. Installing customized Knative Eventing \u00b6 The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, under .spec.manifests , you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode \u00b6 Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. Append mode \u00b6 You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them. Installing Knative Eventing with event sources \u00b6 Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the Knative Eventing deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True Uninstalling Knative \u00b6 Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Serving component \u00b6 To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing Removing the Knative Operator: \u00b6 If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/ What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"\u4f7f\u7528Operator\u5b89\u88c5"},{"location":"install/operator/knative-with-operators/#install-by-using-the-knative-operator","text":"Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. The following table describes the supported versions of Serving and Eventing for the Knative Operator: Operator Serving Eventing v1.8 v1.8.0 v1.7.0, v1.7.1 and v1.7.2 v1.6.0 and v1.6.1 v1.5.0 v1.8.0 v1.7.0, v1.7.1, v1.7.2 and v1.7.3 v1.6.0, v1.6.1 and v1.6.2 v1.5.0, v1.5.1, v1.5.2, v1.5.3, v1.5.4, v1.5.5 and v1.5.6","title":"Install by using the Knative Operator"},{"location":"install/operator/knative-with-operators/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/operator/knative-with-operators/#install-the-knative-operator","text":"Before you install the Knative Serving and Eventing components, first install the Knative Operator. Warning Knative Operator 1.5 is the last version that supports CRDs with both v1alpha1 and v1beta1 . If you are upgrading an existing Operator install from v1.2 or earlier to v1.3 or later, run the following command to upgrade the existing custom resources to v1beta1 before installing the current version: kubectl create -f https://github.com/knative/operator/releases/download/knative-v1.5.1/operator-post-install.yaml To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page .","title":"Install the Knative Operator"},{"location":"install/operator/knative-with-operators/#verify-your-knative-operator-installation","text":"Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h","title":"Verify your Knative Operator installation"},{"location":"install/operator/knative-with-operators/#track-the-log","text":"To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator","title":"Track the log"},{"location":"install/operator/knative-with-operators/#install-knative-serving","text":"To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS.","title":"Install Knative Serving"},{"location":"install/operator/knative-with-operators/#create-the-knative-serving-custom-resource","text":"To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create the Knative Serving custom resource"},{"location":"install/operator/knative-with-operators/#install-the-networking-layer","text":"Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later.","title":"Install the networking layer"},{"location":"install/operator/knative-with-operators/#verify-the-knative-serving-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True","title":"Verify the Knative Serving deployment"},{"location":"install/operator/knative-with-operators/#dns","text":"\u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) \u771f\u6b63\u7684DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"\u914d\u7f6e DNS"},{"location":"install/operator/knative-with-operators/#install-knative-eventing","text":"To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources.","title":"Install Knative Eventing"},{"location":"install/operator/knative-with-operators/#create-the-knative-eventing-custom-resource","text":"To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create the Knative Eventing custom resource"},{"location":"install/operator/knative-with-operators/#installing-a-specific-version-of-eventing","text":"Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v1.7, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"1.7\" You can also run the following command to make the equivalent change: kn operator install --component eventing -v 1 .7 -n knative-eventing If spec.version is not specified, the Knative Operator installs the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 1.4, you must upgrade to 1.5 before upgrading to 1.6.","title":"Installing a specific version of Eventing"},{"location":"install/operator/knative-with-operators/#installing-customized-knative-eventing","text":"The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, under .spec.manifests , you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, under .spec.additionalManifests , you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.","title":"Installing customized Knative Eventing"},{"location":"install/operator/knative-with-operators/#overwrite-mode","text":"Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field.","title":"Overwrite mode"},{"location":"install/operator/knative-with-operators/#append-mode","text":"You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them.","title":"Append mode"},{"location":"install/operator/knative-with-operators/#installing-knative-eventing-with-event-sources","text":"Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph GitHub GitLab Apache Kafka RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Installing Knative Eventing with event sources"},{"location":"install/operator/knative-with-operators/#verify-the-knative-eventing-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True","title":"Verify the Knative Eventing deployment"},{"location":"install/operator/knative-with-operators/#uninstalling-knative","text":"Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Uninstalling Knative"},{"location":"install/operator/knative-with-operators/#removing-the-knative-serving-component","text":"To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/operator/knative-with-operators/#removing-knative-eventing-component","text":"To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing","title":"Removing Knative Eventing component"},{"location":"install/operator/knative-with-operators/#removing-the-knative-operator","text":"If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/operator/knative-with-operators/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/upgrade/","text":"Upgrading Knative \u00b6 Knative supports upgrading by a single minor version number. For example, if you have v0.21.0 installed, you must upgrade to v0.22.0 before attempting to upgrade to v0.23.0. To verify your current version, see Checking your Knative version . To upgrade Knative: If you installed Knative using YAML, see Upgrading with kubectl . If you installed Knative using the Knative Operator, see Upgrading using the Knative Operator .","title":"\u5347\u7ea7Knative"},{"location":"install/upgrade/#upgrading-knative","text":"Knative supports upgrading by a single minor version number. For example, if you have v0.21.0 installed, you must upgrade to v0.22.0 before attempting to upgrade to v0.23.0. To verify your current version, see Checking your Knative version . To upgrade Knative: If you installed Knative using YAML, see Upgrading with kubectl . If you installed Knative using the Knative Operator, see Upgrading using the Knative Operator .","title":"Upgrading Knative"},{"location":"install/upgrade/check-install-version/","text":"Checking your Knative version \u00b6 To check the version of your Knative installation, use one of the following commands, depending on whether you installed Knative with YAML or with the Operator. If you installed with YAML \u00b6 To verify the version of the Knative component that you have running on your cluster, query for the <component>.knative.dev/release label. Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Example output: v0.23.0 Check the installed Knative Eventing version by running the command: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' Example output: v0.23.0 If you installed with the Operator \u00b6 To verify the version of your current Knative installation: Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get KnativeServing knative-serving --namespace knative-serving Example output: NAME VERSION READY REASON knative-serving 0 .23.0 True Check the installed Knative Eventing version by running the command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing Example output: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"\u68c0\u67e5Knative\u7248\u672c"},{"location":"install/upgrade/check-install-version/#checking-your-knative-version","text":"To check the version of your Knative installation, use one of the following commands, depending on whether you installed Knative with YAML or with the Operator.","title":"Checking your Knative version"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-yaml","text":"To verify the version of the Knative component that you have running on your cluster, query for the <component>.knative.dev/release label. Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Example output: v0.23.0 Check the installed Knative Eventing version by running the command: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' Example output: v0.23.0","title":"If you installed with YAML"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-the-operator","text":"To verify the version of your current Knative installation: Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get KnativeServing knative-serving --namespace knative-serving Example output: NAME VERSION READY REASON knative-serving 0 .23.0 True Check the installed Knative Eventing version by running the command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing Example output: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"If you installed with the Operator"},{"location":"install/upgrade/upgrade-installation-with-operator/","text":"Upgrading using the Knative Operator \u00b6 This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"1.1\" to upgrade to the 1.1 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 1.5, it bundles and supports the installation of Knative versions 1.5, 1.4, 1.3 and 1.2. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace. Performing the upgrade \u00b6 To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verifying an upgrade by viewing pods \u00b6 You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods -n knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods -n knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s Verifying an upgrade by viewing custom resources \u00b6 You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving -n knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 1 .1.0 True kubectl get KnativeEventing knative-eventing -n knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 1 .1.0 True Rollback to an earlier version \u00b6 If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 1.2, and your previous version is 1.1, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 1.1. Knative Serving Knative Eventing To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u4f7f\u7528Operator\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation-with-operator/#upgrading-using-the-knative-operator","text":"This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"1.1\" to upgrade to the 1.1 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 1.5, it bundles and supports the installation of Knative versions 1.5, 1.4, 1.3 and 1.2. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace.","title":"Upgrading using the Knative Operator"},{"location":"install/upgrade/upgrade-installation-with-operator/#performing-the-upgrade","text":"To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Performing the upgrade"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-pods","text":"You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods -n knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods -n knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s","title":"Verifying an upgrade by viewing pods"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-custom-resources","text":"You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving -n knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 1 .1.0 True kubectl get KnativeEventing knative-eventing -n knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 1 .1.0 True","title":"Verifying an upgrade by viewing custom resources"},{"location":"install/upgrade/upgrade-installation-with-operator/#rollback-to-an-earlier-version","text":"If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 1.2, and your previous version is 1.1, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 1.1. Knative Serving Knative Eventing To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Rollback to an earlier version"},{"location":"install/upgrade/upgrade-installation/","text":"Upgrading with kubectl \u00b6 If you installed Knative using YAML, you can use the kubectl apply command in this topic to upgrade your Knative components and plugins. If you installed using the Operator, see Upgrading using the Knative Operator . Before you begin \u00b6 Before upgrading, there are a few steps you must take to ensure a successful upgrade process. Identify breaking changes \u00b6 You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub. View current pod status \u00b6 Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing Upgrade plugins \u00b6 If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components. Run pre-install tools before upgrade \u00b6 For some upgrades, there are steps that must be completed before the actual upgrade. These steps, where applicable, are identified in the release notes. Upgrade existing resources to the latest stored version \u00b6 Knative custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required. Performing the upgrade \u00b6 To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. Before upgrading, check your Knative version . For a cluster running version 1.1 of the Knative Serving and Knative Eventing components, the following command upgrades the installation to version 1.2: kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/knative-v1.2.0/eventing.yaml \\ Run post-install tools after the upgrade \u00b6 In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes. Verifying the upgrade \u00b6 To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"\u4f7f\u7528kubectl\u5347\u7ea7"},{"location":"install/upgrade/upgrade-installation/#upgrading-with-kubectl","text":"If you installed Knative using YAML, you can use the kubectl apply command in this topic to upgrade your Knative components and plugins. If you installed using the Operator, see Upgrading using the Knative Operator .","title":"Upgrading with kubectl"},{"location":"install/upgrade/upgrade-installation/#before-you-begin","text":"Before upgrading, there are a few steps you must take to ensure a successful upgrade process.","title":"Before you begin"},{"location":"install/upgrade/upgrade-installation/#identify-breaking-changes","text":"You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub.","title":"Identify breaking changes"},{"location":"install/upgrade/upgrade-installation/#view-current-pod-status","text":"Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing","title":"View current pod status"},{"location":"install/upgrade/upgrade-installation/#upgrade-plugins","text":"If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components.","title":"Upgrade plugins"},{"location":"install/upgrade/upgrade-installation/#run-pre-install-tools-before-upgrade","text":"For some upgrades, there are steps that must be completed before the actual upgrade. These steps, where applicable, are identified in the release notes.","title":"Run pre-install tools before upgrade"},{"location":"install/upgrade/upgrade-installation/#upgrade-existing-resources-to-the-latest-stored-version","text":"Knative custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required.","title":"Upgrade existing resources to the latest stored version"},{"location":"install/upgrade/upgrade-installation/#performing-the-upgrade","text":"To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. Before upgrading, check your Knative version . For a cluster running version 1.1 of the Knative Serving and Knative Eventing components, the following command upgrades the installation to version 1.2: kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/knative-v1.2.0/eventing.yaml \\","title":"Performing the upgrade"},{"location":"install/upgrade/upgrade-installation/#run-post-install-tools-after-the-upgrade","text":"In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes.","title":"Run post-install tools after the upgrade"},{"location":"install/upgrade/upgrade-installation/#verifying-the-upgrade","text":"To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods -n knative-serving kubectl get pods -n knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Verifying the upgrade"},{"location":"install/yaml-install/","text":"About YAML-based installation \u00b6 You can install the Serving component, Eventing component, or both on your cluster by applying YAML files. Install Knative Serving with YAML Install Knative Eventing with YAML","title":"\u5173\u4e8eYAML-based\u5b89\u88c5"},{"location":"install/yaml-install/#about-yaml-based-installation","text":"You can install the Serving component, Eventing component, or both on your cluster by applying YAML files. Install Knative Serving with YAML Install Knative Eventing with YAML","title":"About YAML-based installation"},{"location":"install/yaml-install/eventing/eventing-installation-files/","text":"Knative Eventing installation files \u00b6 This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"\u4e8b\u4ef6\u5b89\u88c5\u6587\u4ef6"},{"location":"install/yaml-install/eventing/eventing-installation-files/#knative-eventing-installation-files","text":"This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing installation files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/","text":"Installing Knative Eventing using YAML files \u00b6 This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI. \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install Knative Eventing \u00b6 To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files . Verify the installation \u00b6 Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s Optional: Install a default Channel (messaging) layer \u00b6 The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel In-Memory (standalone) NATS Channel The following commands install the KafkaChannel and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaChannel data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section. Optional: Install a Broker layer \u00b6 The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub. Install optional Eventing extensions \u00b6 The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml To learn more, try the Apache Kafka source sample . Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"YAML\u5b89\u88c5\u4e8b\u4ef6\u5904\u7406"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#installing-knative-eventing-using-yaml-files","text":"This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI.","title":"Installing Knative Eventing using YAML files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-knative-eventing","text":"To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files .","title":"Install Knative Eventing"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#verify-the-installation","text":"Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s","title":"Verify the installation"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","text":"The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel In-Memory (standalone) NATS Channel The following commands install the KafkaChannel and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaChannel data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section.","title":"Optional: Install a default Channel (messaging) layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-broker-layer","text":"The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub.","title":"Optional: Install a Broker layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-optional-eventing-extensions","text":"The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml If you're upgrading from the previous version, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml To learn more, try the Apache Kafka source sample . Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"Install optional Eventing extensions"},{"location":"install/yaml-install/serving/install-serving-with-yaml/","text":"Installing Knative Serving using YAML files \u00b6 This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI. \u5148\u51b3\u6761\u4ef6 \u00b6 \u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Serving component \u00b6 To install the Knative Serving component: Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files . Install a networking layer \u00b6 The following tabs expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section. Verify the installation \u00b6 Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s \u914d\u7f6e DNS \u00b6 \u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) Real DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Install optional Serving extensions \u00b6 The following tabs expand to show instructions for installing each Serving extension. HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"YAML\u5b89\u88c5\u670d\u52a1"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#installing-knative-serving-using-yaml-files","text":"This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI.","title":"Installing Knative Serving using YAML files"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#_1","text":"\u5728\u5b89\u88c5Knative\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u6ee1\u8db3\u4ee5\u4e0b\u524d\u63d0\u6761\u4ef6: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.23 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-the-knative-serving-component","text":"To install the Knative Serving component: Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files .","title":"Install the Knative Serving component"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-a-networking-layer","text":"The following tabs expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section.","title":"Install a networking layer"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#verify-the-installation","text":"Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s","title":"Verify the installation"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#dns","text":"\u53ef\u4ee5\u914d\u7f6eDNS\u4ee5\u907f\u514d\u4f7f\u7528\u4e3b\u673a\u6807\u5934\u8fd0\u884ccurl\u547d\u4ee4\u3002 \u4e0b\u9762\u7684\u9009\u9879\u5361\u5c55\u5f00\u663e\u793a\u914d\u7f6eDNS\u7684\u8bf4\u660e\u3002 \u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u9009\u62e9DNS: Magic DNS (sslip.io) Real DNS \u4e34\u65f6DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. \u8981\u4e3aKnative\u914d\u7f6eDNS\uff0c\u8bf7\u4ece\u5efa\u7acb\u7f51\u7edc\u4e2d\u83b7\u53d6External IP\u6216CNAME\uff0c\u5e76\u6309\u7167\u4ee5\u4e0b\u65b9\u6cd5\u4e0e\u60a8\u7684DNS\u63d0\u4f9b\u5546\u8fdb\u884c\u914d\u7f6e: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 curl \u8bbf\u95ee\u6837\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u6216\u8005\u60a8\u81ea\u5df1\u7684Knative\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4e14\u4e0d\u80fd\u4f7f\u7528\"Magic DNS (sslip.io)\"\u6216\u201cReal DNS\u201d\u65b9\u6cd5\uff0c\u90a3\u4e48\u6709\u4e00\u79cd\u4e34\u65f6\u7684\u65b9\u6cd5\u3002 \u8fd9\u5bf9\u4e8e\u90a3\u4e9b\u5e0c\u671b\u5728\u4e0d\u66f4\u6539DNS\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30Knative\u7684\u4eba\u662f\u5f88\u6709\u7528\u7684\uff0c\u5c31\u50cf\"Real DNS\"\u65b9\u6cd5\u4e00\u6837\uff0c\u6216\u8005\u56e0\u4e3a\u4f7f\u7528\u672c\u5730minikube\u6216IPv6\u96c6\u7fa4\u800c\u4e0d\u80fd\u4f7f\u7528 \"Magic DNS\"\u65b9\u6cd5\u3002 \u8981\u4f7f\u7528curl\u8bbf\u95ee\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5: \u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u540e\uff0c\u83b7\u53d6\u5e94\u7528\u7a0b\u5e8f\u7684URL: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"\u914d\u7f6e DNS"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-optional-serving-extensions","text":"The following tabs expand to show instructions for installing each Serving extension. HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"Install optional Serving extensions"},{"location":"install/yaml-install/serving/serving-installation-files/","text":"Knative Serving installation files \u00b6 This guide provides reference information about the core Knative Serving YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Serving. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Serving using YAML files . The following table describes the installation files included in Knative Serving: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://sslip.io as the default DNS suffix. serving-core.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"\u670d\u52a1\u5b89\u88c5\u6587\u4ef6"},{"location":"install/yaml-install/serving/serving-installation-files/#knative-serving-installation-files","text":"This guide provides reference information about the core Knative Serving YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Serving. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Serving using YAML files . The following table describes the installation files included in Knative Serving: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://sslip.io as the default DNS suffix. serving-core.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"Knative Serving installation files"},{"location":"reference/relnotes/","text":"Knative \u53d1\u5e03\u8bf4\u660e \u00b6 \u6709\u5173Knative\u53d1\u884c\u7248\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4ee5\u4e0b\u9875\u9762: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"\u53d1\u5e03\u8bf4\u660e"},{"location":"reference/relnotes/#knative","text":"\u6709\u5173Knative\u53d1\u884c\u7248\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4ee5\u4e0b\u9875\u9762: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"Knative \u53d1\u5e03\u8bf4\u660e"},{"location":"reference/security/","text":"Knative\u5b89\u5168\u548c\u62ab\u9732\u4fe1\u606f \u00b6 \u6b64\u9875\u9762\u63cf\u8ff0Knative\u5b89\u5168\u6027\u548c\u62ab\u9732\u4fe1\u606f\u3002 Knative \u5a01\u80c1\u6a21\u578b \u00b6 \u5a01\u80c1\u6a21\u578b \u62a5\u544a\u4e00\u4e2a\u6f0f\u6d1e \u00b6 \u6211\u4eec\u975e\u5e38\u611f\u8c22\u5411Knative\u5f00\u6e90\u793e\u533a\u62a5\u544a\u6f0f\u6d1e\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u7528\u6237\u3002\u6240\u6709\u7684\u62a5\u544a\u90fd\u7ecf\u8fc7\u4e00\u7ec4\u793e\u533a\u5fd7\u613f\u8005\u7684\u5f7b\u5e95\u8c03\u67e5\u3002 \u8981\u5236\u4f5c\u62a5\u544a\uff0c\u8bf7\u5c06\u5b89\u5168\u7ec6\u8282\u548c\u6240\u6709Knative\u9519\u8bef\u62a5\u544a\u7684\u9884\u671f\u7ec6\u8282\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6\u53d1\u9001\u5230\u79c1\u6709\u7684security@knative.team\u5217\u8868\u3002 \u4f55\u65f6\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e? \u00b6 \u4f60\u8ba4\u4e3a\u4f60\u53d1\u73b0\u4e86Knative\u7684\u4e00\u4e2a\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e \u60a8\u4e0d\u786e\u5b9a\u6f0f\u6d1e\u5982\u4f55\u5f71\u54cdKnative \u4f60\u8ba4\u4e3a\u4f60\u5728Knative\u6240\u4f9d\u8d56\u7684\u53e6\u4e00\u4e2a\u9879\u76ee\u4e2d\u53d1\u73b0\u4e86\u6f0f\u6d1e \u5bf9\u4e8e\u6709\u81ea\u5df1\u6f0f\u6d1e\u62a5\u544a\u548c\u62ab\u9732\u6d41\u7a0b\u7684\u9879\u76ee\uff0c\u8bf7\u76f4\u63a5\u5728\u90a3\u91cc\u62a5\u544a \u4ec0\u4e48\u65f6\u5019\u4e0d\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e? \u00b6 \u60a8\u9700\u8981\u5e2e\u52a9\u8c03\u4f18Knative\u7ec4\u4ef6\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027 \u60a8\u9700\u8981\u5e2e\u52a9\u5e94\u7528\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u66f4\u65b0 \u4f60\u7684\u95ee\u9898\u4e0e\u5b89\u5168\u65e0\u5173 \u6f0f\u6d1e\u54cd\u5e94 \u00b6 \u53ca\u65e9\u62ab\u9732\u5b89\u5168\u6f0f\u6d1e \u6f0f\u6d1e\u62ab\u9732\u54cd\u5e94\u7b56\u7565 \u5b89\u5168\u5de5\u4f5c\u5c0f\u7ec4 \u00b6 \u4e00\u822c\u4fe1\u606f \u4fdd\u5b89\u5de5\u4f5c\u5c0f\u7ec4\u7ae0\u7a0b","title":"\u5b89\u5168"},{"location":"reference/security/#knative","text":"\u6b64\u9875\u9762\u63cf\u8ff0Knative\u5b89\u5168\u6027\u548c\u62ab\u9732\u4fe1\u606f\u3002","title":"Knative\u5b89\u5168\u548c\u62ab\u9732\u4fe1\u606f"},{"location":"reference/security/#knative_1","text":"\u5a01\u80c1\u6a21\u578b","title":"Knative \u5a01\u80c1\u6a21\u578b"},{"location":"reference/security/#_1","text":"\u6211\u4eec\u975e\u5e38\u611f\u8c22\u5411Knative\u5f00\u6e90\u793e\u533a\u62a5\u544a\u6f0f\u6d1e\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u548c\u7528\u6237\u3002\u6240\u6709\u7684\u62a5\u544a\u90fd\u7ecf\u8fc7\u4e00\u7ec4\u793e\u533a\u5fd7\u613f\u8005\u7684\u5f7b\u5e95\u8c03\u67e5\u3002 \u8981\u5236\u4f5c\u62a5\u544a\uff0c\u8bf7\u5c06\u5b89\u5168\u7ec6\u8282\u548c\u6240\u6709Knative\u9519\u8bef\u62a5\u544a\u7684\u9884\u671f\u7ec6\u8282\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6\u53d1\u9001\u5230\u79c1\u6709\u7684security@knative.team\u5217\u8868\u3002","title":"\u62a5\u544a\u4e00\u4e2a\u6f0f\u6d1e"},{"location":"reference/security/#_2","text":"\u4f60\u8ba4\u4e3a\u4f60\u53d1\u73b0\u4e86Knative\u7684\u4e00\u4e2a\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e \u60a8\u4e0d\u786e\u5b9a\u6f0f\u6d1e\u5982\u4f55\u5f71\u54cdKnative \u4f60\u8ba4\u4e3a\u4f60\u5728Knative\u6240\u4f9d\u8d56\u7684\u53e6\u4e00\u4e2a\u9879\u76ee\u4e2d\u53d1\u73b0\u4e86\u6f0f\u6d1e \u5bf9\u4e8e\u6709\u81ea\u5df1\u6f0f\u6d1e\u62a5\u544a\u548c\u62ab\u9732\u6d41\u7a0b\u7684\u9879\u76ee\uff0c\u8bf7\u76f4\u63a5\u5728\u90a3\u91cc\u62a5\u544a","title":"\u4f55\u65f6\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e?"},{"location":"reference/security/#_3","text":"\u60a8\u9700\u8981\u5e2e\u52a9\u8c03\u4f18Knative\u7ec4\u4ef6\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027 \u60a8\u9700\u8981\u5e2e\u52a9\u5e94\u7528\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u66f4\u65b0 \u4f60\u7684\u95ee\u9898\u4e0e\u5b89\u5168\u65e0\u5173","title":"\u4ec0\u4e48\u65f6\u5019\u4e0d\u5e94\u8be5\u62a5\u544a\u6f0f\u6d1e?"},{"location":"reference/security/#_4","text":"\u53ca\u65e9\u62ab\u9732\u5b89\u5168\u6f0f\u6d1e \u6f0f\u6d1e\u62ab\u9732\u54cd\u5e94\u7b56\u7565","title":"\u6f0f\u6d1e\u54cd\u5e94"},{"location":"reference/security/#_5","text":"\u4e00\u822c\u4fe1\u606f \u4fdd\u5b89\u5de5\u4f5c\u5c0f\u7ec4\u7ae0\u7a0b","title":"\u5b89\u5168\u5de5\u4f5c\u5c0f\u7ec4"},{"location":"samples/","text":"Knative \u4ee3\u7801\u793a\u4f8b \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528Knative\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u5efa\u7acb\u548c\u8fd0\u884c\u5e38\u7528\u7528\u4f8b\u3002 Knative\u6240\u6709\u6837\u54c1 \u00b6 \u7531Knative\u5de5\u4f5c\u7ec4\u79ef\u6781\u6d4b\u8bd5\u548c\u7ef4\u62a4\u7684Knative\u4ee3\u7801\u793a\u4f8b: \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u670d\u52a1\u4ee3\u7801\u793a\u4f8b \u793e\u533a\u62e5\u6709\u7684\u6837\u54c1 \u00b6 \u4f7f\u7528\u4e00\u4e2a\u793e\u533a\u4ee3\u7801\u793a\u4f8b\u542f\u52a8\u5e76\u8fd0\u884c\u3002 \u8fd9\u4e9b\u6837\u672c\u662f\u7531Knative\u793e\u533a\u7684\u6210\u5458\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u3002 \u67e5\u770b\u7531\u793e\u533a\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u4ee3\u7801\u793a\u4f8b . Note \u8fd9\u4e9b\u6837\u672c\u53ef\u80fd\u4f1a\u8fc7\u65f6\uff0c\u6216\u8005\u539f\u59cb\u4f5c\u8005\u53ef\u80fd\u65e0\u6cd5\u7ef4\u6301\u4ed6\u4eec\u7684\u8d21\u732e\u3002\u5982\u679c\u4f60\u53d1\u73b0\u6709\u4ec0\u4e48\u4e1c\u897f\u4e0d\u80fd\u5de5\u4f5c\uff0c\u4f38\u51fa\u63f4\u52a9\u4e4b\u624b\uff0c\u5728\u516c\u5171\u5173\u7cfb\u4e2d\u89e3\u51b3\u5b83\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u6837\u672c\u5bff\u547d\u7684\u4fe1\u606f Sample Name Description Language(s) Hello World \u5feb\u901f\u4ecb\u7ecdKnative\u670d\u52a1\uff0c\u91cd\u70b9\u4ecb\u7ecd\u5982\u4f55\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Clojure , Dart , Elixir , Haskell , Java - Micronaut , Java - Quarkus , R - Go Server , Rust , Swift , Vertx Machine Learning \u5feb\u901f\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u670d\u52a1 Python - BentoML \u5916\u90e8\u4ee3\u7801\u793a\u4f8b \u00b6 \u4f4d\u4e8eKnative repos\u4e4b\u5916\u7684Knative\u4ee3\u7801\u793a\u4f8b\u94fe\u63a5\u5217\u8868: Image processing using Knative Eventing, Cloud Run on GKE and Google Cloud Vision API Knative Eventing Examples knfun Getting Started with Knative 2020 Image Processing Pipeline BigQuery Processing Pipeline Simple Performance Testing with SLOs Tip \u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u94fe\u63a5\u5230\u60a8\u7684\u5916\u90e8\u6258\u7ba1Knative\u4ee3\u7801\u793a\u4f8b\u3002","title":"\u4ee3\u7801\u793a\u4f8b\u6982\u8ff0"},{"location":"samples/#knative","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528Knative\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u5efa\u7acb\u548c\u8fd0\u884c\u5e38\u7528\u7528\u4f8b\u3002","title":"Knative \u4ee3\u7801\u793a\u4f8b"},{"location":"samples/#knative_1","text":"\u7531Knative\u5de5\u4f5c\u7ec4\u79ef\u6781\u6d4b\u8bd5\u548c\u7ef4\u62a4\u7684Knative\u4ee3\u7801\u793a\u4f8b: \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u7684\u6e90\u4ee3\u7801\u793a\u4f8b \u670d\u52a1\u4ee3\u7801\u793a\u4f8b","title":"Knative\u6240\u6709\u6837\u54c1"},{"location":"samples/#_1","text":"\u4f7f\u7528\u4e00\u4e2a\u793e\u533a\u4ee3\u7801\u793a\u4f8b\u542f\u52a8\u5e76\u8fd0\u884c\u3002 \u8fd9\u4e9b\u6837\u672c\u662f\u7531Knative\u793e\u533a\u7684\u6210\u5458\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u3002 \u67e5\u770b\u7531\u793e\u533a\u8d21\u732e\u548c\u7ef4\u62a4\u7684\u4ee3\u7801\u793a\u4f8b . Note \u8fd9\u4e9b\u6837\u672c\u53ef\u80fd\u4f1a\u8fc7\u65f6\uff0c\u6216\u8005\u539f\u59cb\u4f5c\u8005\u53ef\u80fd\u65e0\u6cd5\u7ef4\u6301\u4ed6\u4eec\u7684\u8d21\u732e\u3002\u5982\u679c\u4f60\u53d1\u73b0\u6709\u4ec0\u4e48\u4e1c\u897f\u4e0d\u80fd\u5de5\u4f5c\uff0c\u4f38\u51fa\u63f4\u52a9\u4e4b\u624b\uff0c\u5728\u516c\u5171\u5173\u7cfb\u4e2d\u89e3\u51b3\u5b83\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u6837\u672c\u5bff\u547d\u7684\u4fe1\u606f Sample Name Description Language(s) Hello World \u5feb\u901f\u4ecb\u7ecdKnative\u670d\u52a1\uff0c\u91cd\u70b9\u4ecb\u7ecd\u5982\u4f55\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Clojure , Dart , Elixir , Haskell , Java - Micronaut , Java - Quarkus , R - Go Server , Rust , Swift , Vertx Machine Learning \u5feb\u901f\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u670d\u52a1 Python - BentoML","title":"\u793e\u533a\u62e5\u6709\u7684\u6837\u54c1"},{"location":"samples/#_2","text":"\u4f4d\u4e8eKnative repos\u4e4b\u5916\u7684Knative\u4ee3\u7801\u793a\u4f8b\u94fe\u63a5\u5217\u8868: Image processing using Knative Eventing, Cloud Run on GKE and Google Cloud Vision API Knative Eventing Examples knfun Getting Started with Knative 2020 Image Processing Pipeline BigQuery Processing Pipeline Simple Performance Testing with SLOs Tip \u5728\u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a\u94fe\u63a5\u5230\u60a8\u7684\u5916\u90e8\u6258\u7ba1Knative\u4ee3\u7801\u793a\u4f8b\u3002","title":"\u5916\u90e8\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/eventing/","text":"Knative \u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3 Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6e90\u7684\u5404\u79cd\u7528\u4f8b\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6765\u6e90 . \u53c2\u89c1 GitHub \u4e2d\u7684 \u6240\u6709 Knative \u4ee3\u7801\u793a\u4f8b \u3002 Name Description Languages Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go and Python CloudAuditLogsSource \u914d\u7f6e\u4e00\u4e2a CloudAuditLogsSource \u8d44\u6e90\uff0c\u4ece\u4e91\u5ba1\u8ba1\u65e5\u5fd7\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u4ee5 CloudEvents \u683c\u5f0f\u76f4\u63a5\u53d1\u5e03\u5230\u5e95\u5c42\u4f20\u8f93(Pub/Sub)\u3002 YAML CloudPubSubSource \u914d\u7f6e\u4e00\u4e2a CloudPubSubSource\uff0c\u4f7f\u5176\u6bcf\u6b21\u5728\u4e91\u53d1\u5e03/\u8ba2\u9605\u4e3b\u9898\u4e0a\u53d1\u5e03\u6d88\u606f\u65f6\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u6b64\u6e90\u4f7f\u7528\u4e0e push \u517c\u5bb9\u7684\u683c\u5f0f\u53d1\u9001\u4e8b\u4ef6\u3002 YAML CloudSchedulerSource \u914d\u7f6e CloudSchedulerSource \u8d44\u6e90\uff0c\u7528\u4e8e\u4ece\u8c37\u6b4c CloudScheduler \u63a5\u6536\u9884\u5b9a\u4e8b\u4ef6\u3002 YAML CloudStorageSource \u914d\u7f6e CloudStorageSource \u8d44\u6e90\uff0c\u5f53\u4e00\u4e2a\u65b0\u5bf9\u8c61\u88ab\u6dfb\u52a0\u5230\u8c37\u6b4c\u4e91\u5b58\u50a8(GCS)\u65f6\uff0c\u8be5\u8d44\u6e90\u5c06\u4e0b\u53d1\u5bf9\u8c61\u901a\u77e5\u3002 YAML GitHub source \u5c55\u793a\u5982\u4f55\u8fde\u63a5 GitHub \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML GitLab source \u6f14\u793a\u5982\u4f55\u8fde\u63a5 GitLab \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML Apache Kafka Binding KafkaBinding \u8d1f\u8d23\u5c06 Kafka \u5f15\u5bfc\u8fde\u63a5\u4fe1\u606f\u6ce8\u5165\u5230\u5d4c\u5165 PodSpec(\u5982 spec.template.spec )\u7684 Kubernetes \u8d44\u6e90\u4e2d\u3002\u8fd9\u4f7f\u5f97 Kafka \u5ba2\u6237\u7aef\u53ef\u4ee5\u8f7b\u677e\u5f15\u5bfc\u3002 YAML Apache Kafka Channel \u5b89\u88c5\u5e76\u914d\u7f6e Apache Kafka \u901a\u9053\u4f5c\u4e3a Knative event \u7684\u9ed8\u8ba4\u901a\u9053\u914d\u7f6e\u3002 YAML Writing an event source using JavaScript \u672c\u6559\u7a0b\u63d0\u4f9b\u4e86\u5982\u4f55\u7528 JavaScript \u6784\u5efa\u4e8b\u4ef6\u6e90\u5e76\u4f7f\u7528 ContainerSource \u6216 SinkBinding \u5b9e\u73b0\u5b83\u7684\u6307\u5bfc\u3002 JavaScript Parallel with multiple cases \u7528\u4e24\u4e2a\u5206\u652f\u521b\u5efa\u4e00\u4e2a Parallel\u3002 YAML Parallel with mutually exclusive cases \u521b\u5efa\u5177\u6709\u4e92\u65a5\u5206\u652f\u7684 Parallel\u3002 YAML","title":"\u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/eventing/#knative","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u6765\u5e2e\u52a9\u60a8\u7406\u89e3 Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6e90\u7684\u5404\u79cd\u7528\u4f8b\u3002 \u4e86\u89e3\u66f4\u591a\u5173\u4e8e Knative \u4e8b\u4ef6\u548c\u4e8b\u4ef6\u6765\u6e90 . \u53c2\u89c1 GitHub \u4e2d\u7684 \u6240\u6709 Knative \u4ee3\u7801\u793a\u4f8b \u3002 Name Description Languages Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative \u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go and Python CloudAuditLogsSource \u914d\u7f6e\u4e00\u4e2a CloudAuditLogsSource \u8d44\u6e90\uff0c\u4ece\u4e91\u5ba1\u8ba1\u65e5\u5fd7\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u4ee5 CloudEvents \u683c\u5f0f\u76f4\u63a5\u53d1\u5e03\u5230\u5e95\u5c42\u4f20\u8f93(Pub/Sub)\u3002 YAML CloudPubSubSource \u914d\u7f6e\u4e00\u4e2a CloudPubSubSource\uff0c\u4f7f\u5176\u6bcf\u6b21\u5728\u4e91\u53d1\u5e03/\u8ba2\u9605\u4e3b\u9898\u4e0a\u53d1\u5e03\u6d88\u606f\u65f6\u89e6\u53d1\u4e00\u4e2a\u65b0\u4e8b\u4ef6\u3002\u6b64\u6e90\u4f7f\u7528\u4e0e push \u517c\u5bb9\u7684\u683c\u5f0f\u53d1\u9001\u4e8b\u4ef6\u3002 YAML CloudSchedulerSource \u914d\u7f6e CloudSchedulerSource \u8d44\u6e90\uff0c\u7528\u4e8e\u4ece\u8c37\u6b4c CloudScheduler \u63a5\u6536\u9884\u5b9a\u4e8b\u4ef6\u3002 YAML CloudStorageSource \u914d\u7f6e CloudStorageSource \u8d44\u6e90\uff0c\u5f53\u4e00\u4e2a\u65b0\u5bf9\u8c61\u88ab\u6dfb\u52a0\u5230\u8c37\u6b4c\u4e91\u5b58\u50a8(GCS)\u65f6\uff0c\u8be5\u8d44\u6e90\u5c06\u4e0b\u53d1\u5bf9\u8c61\u901a\u77e5\u3002 YAML GitHub source \u5c55\u793a\u5982\u4f55\u8fde\u63a5 GitHub \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML GitLab source \u6f14\u793a\u5982\u4f55\u8fde\u63a5 GitLab \u4e8b\u4ef6\uff0c\u4ee5\u4f9b Knative \u670d\u52a1\u4f7f\u7528\u3002 YAML Apache Kafka Binding KafkaBinding \u8d1f\u8d23\u5c06 Kafka \u5f15\u5bfc\u8fde\u63a5\u4fe1\u606f\u6ce8\u5165\u5230\u5d4c\u5165 PodSpec(\u5982 spec.template.spec )\u7684 Kubernetes \u8d44\u6e90\u4e2d\u3002\u8fd9\u4f7f\u5f97 Kafka \u5ba2\u6237\u7aef\u53ef\u4ee5\u8f7b\u677e\u5f15\u5bfc\u3002 YAML Apache Kafka Channel \u5b89\u88c5\u5e76\u914d\u7f6e Apache Kafka \u901a\u9053\u4f5c\u4e3a Knative event \u7684\u9ed8\u8ba4\u901a\u9053\u914d\u7f6e\u3002 YAML Writing an event source using JavaScript \u672c\u6559\u7a0b\u63d0\u4f9b\u4e86\u5982\u4f55\u7528 JavaScript \u6784\u5efa\u4e8b\u4ef6\u6e90\u5e76\u4f7f\u7528 ContainerSource \u6216 SinkBinding \u5b9e\u73b0\u5b83\u7684\u6307\u5bfc\u3002 JavaScript Parallel with multiple cases \u7528\u4e24\u4e2a\u5206\u652f\u521b\u5efa\u4e00\u4e2a Parallel\u3002 YAML Parallel with mutually exclusive cases \u521b\u5efa\u5177\u6709\u4e92\u65a5\u5206\u652f\u7684 Parallel\u3002 YAML","title":"Knative \u4e8b\u4ef6\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/serving/","text":"Knative Serving code samples \u00b6 Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving . See all Knative code samples in GitHub. Name Description Languages Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u53d1\u9001\u548c\u63a5\u6536\u4e91\u4e8b\u4ef6\u3002 C# , Go , Node.js , Rust , Java (Vert.x) Traffic Splitting \u4e00\u4e2a\u624b\u5de5\u6d41\u91cf\u5206\u5272\u7684\u4f8b\u5b50\u3002 YAML Advanced Deployment \u7b80\u5355\u7684\u84dd\u8272/\u7eff\u8272\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u6a21\u5f0f\u6f14\u793a\u4e86\u5728\u4e0d\u51cf\u5c11\u4efb\u4f55\u6d41\u91cf\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u52a8\u6001\u5e94\u7528\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u3002 YAML Autoscale Knative\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 Go Github Webhook \u4e00\u4e2a\u7b80\u5355\u7684webhook\u5904\u7406\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u4e0eGithub\u7684\u4ea4\u4e92\u3002 Go gRPC \u4e00\u4e2a\u7b80\u5355\u7684gRPC\u670d\u52a1\u5668\u3002 Go Knative Routing \u4f7f\u7528Istio VirtualService\u6982\u5ff5\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e2a\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Kong Routing \u4f7f\u7528Kong API\u7f51\u5173\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e00\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Knative Secrets \u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Kubernetes\u7684\u79d8\u5bc6\u4f5c\u4e3aKnative\u4e2d\u7684\u5377\u3002 Go Multi Container \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u591a\u4e2a\u5bb9\u5668\u6784\u5efa\u548c\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go","title":"\u670d\u52a1\u4ee3\u7801\u793a\u4f8b"},{"location":"samples/serving/#knative-serving-code-samples","text":"Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving . See all Knative code samples in GitHub. Name Description Languages Hello World \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u53d1\u9001\u548c\u63a5\u6536\u4e91\u4e8b\u4ef6\u3002 C# , Go , Node.js , Rust , Java (Vert.x) Traffic Splitting \u4e00\u4e2a\u624b\u5de5\u6d41\u91cf\u5206\u5272\u7684\u4f8b\u5b50\u3002 YAML Advanced Deployment \u7b80\u5355\u7684\u84dd\u8272/\u7eff\u8272\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u6a21\u5f0f\u6f14\u793a\u4e86\u5728\u4e0d\u51cf\u5c11\u4efb\u4f55\u6d41\u91cf\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u52a8\u6001\u5e94\u7528\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u3002 YAML Autoscale Knative\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 Go Github Webhook \u4e00\u4e2a\u7b80\u5355\u7684webhook\u5904\u7406\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u4e0eGithub\u7684\u4ea4\u4e92\u3002 Go gRPC \u4e00\u4e2a\u7b80\u5355\u7684gRPC\u670d\u52a1\u5668\u3002 Go Knative Routing \u4f7f\u7528Istio VirtualService\u6982\u5ff5\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e2a\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Kong Routing \u4f7f\u7528Kong API\u7f51\u5173\u5c06\u591a\u4e2aKnative\u670d\u52a1\u6620\u5c04\u5230\u5355\u4e00\u57df\u540d\u4e0b\u7684\u4e0d\u540c\u8def\u5f84\u7684\u793a\u4f8b\u3002 Go Knative Secrets \u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Kubernetes\u7684\u79d8\u5bc6\u4f5c\u4e3aKnative\u4e2d\u7684\u5377\u3002 Go Multi Container \u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528Knative Serving\u4e3a\u591a\u4e2a\u5bb9\u5668\u6784\u5efa\u548c\u90e8\u7f72\u5e94\u7528\u7a0b\u5e8f\u3002 Go","title":"Knative Serving code samples"},{"location":"serving/","text":"Knative \u670d\u52a1 \u00b6 Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(crd)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002 \u5e38\u89c1\u7528\u4f8b \u00b6 \u652f\u6301\u7684Knative\u670d\u52a1\u7528\u4f8b\u793a\u4f8b: \u5feb\u901f\u90e8\u7f72\u65e0\u670d\u52a1\u5668\u5bb9\u5668\u3002 \u81ea\u52a8\u7f29\u653e\uff0c\u5305\u62ec\u5c06\u8c46\u835a\u7f29\u653e\u5230\u96f6\u3002 \u652f\u6301\u591a\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Contour\u3001Kourier\u548cIstio\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 Knative\u670d\u52a1\u540c\u65f6\u652f\u6301HTTP\u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002 \u5b89\u88c5 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u670d\u52a1. \u5f00\u59cb \u00b6 \u8981\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00\u3002 \u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u5b83\u4e3a\u4f60\u7ba1\u7406\u6240\u6709\u7684\u7ec6\u8282\u3002 \u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002 \u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a \u00b6 Knative\u670d\u52a1\u4ee3\u7801\u793a\u4f8b \u8c03\u8bd5Knative\u670d\u52a1\u95ee\u9898 \u00b6 \u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898 \u914d\u7f6e\u548c\u7f51\u7edc \u00b6 \u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df \u4ea4\u901a\u7ba1\u7406 \u53ef\u89c2\u5bdf\u6027 \u00b6 \u670d\u52a1\u6807\u51c6API \u5df2\u77e5\u7684\u95ee\u9898 \u00b6 \u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u89c1 Knative\u670d\u52a1\u95ee\u9898 \u9875\u9762\u3002","title":"\u670d\u52a1\u6982\u8ff0"},{"location":"serving/#knative","text":"Knative Serving\u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3aKubernetes\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(crd)\u3002 \u8fd9\u4e9b\u8d44\u6e90\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u3002 \u4e3b\u8981\u7684Knative\u670d\u52a1\u8d44\u6e90\u662f\u670d\u52a1\u3001\u8def\u7531\u3001\u914d\u7f6e\u548c\u4fee\u8ba2: \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u60a8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002 \u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u6bcf\u6b21\u670d\u52a1\u66f4\u65b0\u65f6\u90fd\u6709\u8def\u7531\u3001\u914d\u7f6e\u548c\u65b0\u7684\u4fee\u8ba2\u3002 \u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u901a\u4fe1\u8def\u7531\u5230\u6700\u65b0\u4fee\u8ba2\u6216\u56fa\u5b9a\u4fee\u8ba2\u3002 \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u4e00\u4e2a\u7f51\u7edc\u7aef\u70b9\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u4fee\u8ba2\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c\u5305\u62ec\u90e8\u5206\u6d41\u91cf\u548c\u547d\u540d\u8def\u7531\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4e3a\u60a8\u7684\u90e8\u7f72\u7ef4\u62a4\u6240\u9700\u7684\u72b6\u6001\u3002 \u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c\u5e76\u9075\u5faa\u4e86\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u7a0b\u5e8f\u65b9\u6cd5\u3002 \u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002 \u4fee\u8ba2 : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u8ba2\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002 \u4fee\u8ba2\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c\u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\u3002 Knative\u670d\u52a1\u4fee\u8ba2\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u7f29\u653e\u3002 \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 serving Github\u5b58\u50a8\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002","title":"Knative \u670d\u52a1"},{"location":"serving/#_1","text":"\u652f\u6301\u7684Knative\u670d\u52a1\u7528\u4f8b\u793a\u4f8b: \u5feb\u901f\u90e8\u7f72\u65e0\u670d\u52a1\u5668\u5bb9\u5668\u3002 \u81ea\u52a8\u7f29\u653e\uff0c\u5305\u62ec\u5c06\u8c46\u835a\u7f29\u653e\u5230\u96f6\u3002 \u652f\u6301\u591a\u4e2a\u7f51\u7edc\u5c42\uff0c\u5982Contour\u3001Kourier\u548cIstio\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 Knative\u670d\u52a1\u540c\u65f6\u652f\u6301HTTP\u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002","title":"\u5e38\u89c1\u7528\u4f8b"},{"location":"serving/#_2","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e2d\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5Knative\u670d\u52a1.","title":"\u5b89\u88c5"},{"location":"serving/#_3","text":"\u8981\u5f00\u59cb\u4f7f\u7528\u670d\u52a1\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00\u3002 \u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u5b83\u4e3a\u4f60\u7ba1\u7406\u6240\u6709\u7684\u7ec6\u8282\u3002 \u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002 \u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\u3002","title":"\u5f00\u59cb"},{"location":"serving/#_4","text":"Knative\u670d\u52a1\u4ee3\u7801\u793a\u4f8b","title":"\u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a"},{"location":"serving/#knative_1","text":"\u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898","title":"\u8c03\u8bd5Knative\u670d\u52a1\u95ee\u9898"},{"location":"serving/#_5","text":"\u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df \u4ea4\u901a\u7ba1\u7406","title":"\u914d\u7f6e\u548c\u7f51\u7edc"},{"location":"serving/#_6","text":"\u670d\u52a1\u6807\u51c6API","title":"\u53ef\u89c2\u5bdf\u6027"},{"location":"serving/#_7","text":"\u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u89c1 Knative\u670d\u52a1\u95ee\u9898 \u9875\u9762\u3002","title":"\u5df2\u77e5\u7684\u95ee\u9898"},{"location":"serving/accessing-traces/","text":"Accessing request traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests. Configuring Traces \u00b6 You can update the configuration file for tracing in tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or none). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing Zipkin \u00b6 In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call. Jaeger \u00b6 In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"\u8bbf\u95ee\u8bf7\u6c42\u7684\u75d5\u8ff9"},{"location":"serving/accessing-traces/#accessing-request-traces","text":"Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#configuring-traces","text":"You can update the configuration file for tracing in tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or none). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing","title":"Configuring Traces"},{"location":"serving/accessing-traces/#zipkin","text":"In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.","title":"Zipkin"},{"location":"serving/accessing-traces/#jaeger","text":"In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Jaeger"},{"location":"serving/config-ha/","text":"Configuring high-availability components \u00b6 Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica. Disabling leader election \u00b6 For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\". Scaling the control plane \u00b6 With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions. Scaling the data plane \u00b6 The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"\u9ad8\u53ef\u7528\u6027\u914d\u7f6e\u7ec4\u4ef6"},{"location":"serving/config-ha/#configuring-high-availability-components","text":"Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#disabling-leader-election","text":"For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\".","title":"Disabling leader election"},{"location":"serving/config-ha/#scaling-the-control-plane","text":"With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions.","title":"Scaling the control plane"},{"location":"serving/config-ha/#scaling-the-data-plane","text":"The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Scaling the data plane"},{"location":"serving/convert-deployment-to-knative-service/","text":"Converting a Kubernetes Deployment to a Knative Service \u00b6 This topic shows how to convert a Kubernetes Deployment to a Knative Service. Benefits \u00b6 Converting to a Knative Service has the following benefits: Reduces the footprint of the service instance because the instance scales to 0 when it becomes idle. Improves performance due to built-in autoscaling for the Knative Service. Determine if your workload is a good fit for Knative \u00b6 In general, if your Kubernetes workload is a good fit for Knative, you can remove a lot of your manifest to create a Knative Service. There are three aspects you need to consider: All work done is triggered by HTTP. The container is stateless. All state is stored elsewhere or can be re-created. Your workload uses only Secret and ConfigMap volumes. Example conversion \u00b6 The following example shows a Kubernetes Nginx Deployment and Service , and shows how it converts to a Knative Service. Kubernetes Nginx Deployment and Service \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx Knative Service \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"\u5c06K8S\u90e8\u7f72\u8f6c\u4e3aKnative\u670d\u52a1"},{"location":"serving/convert-deployment-to-knative-service/#converting-a-kubernetes-deployment-to-a-knative-service","text":"This topic shows how to convert a Kubernetes Deployment to a Knative Service.","title":"Converting a Kubernetes Deployment to a Knative Service"},{"location":"serving/convert-deployment-to-knative-service/#benefits","text":"Converting to a Knative Service has the following benefits: Reduces the footprint of the service instance because the instance scales to 0 when it becomes idle. Improves performance due to built-in autoscaling for the Knative Service.","title":"Benefits"},{"location":"serving/convert-deployment-to-knative-service/#determine-if-your-workload-is-a-good-fit-for-knative","text":"In general, if your Kubernetes workload is a good fit for Knative, you can remove a lot of your manifest to create a Knative Service. There are three aspects you need to consider: All work done is triggered by HTTP. The container is stateless. All state is stored elsewhere or can be re-created. Your workload uses only Secret and ConfigMap volumes.","title":"Determine if your workload is a good fit for Knative"},{"location":"serving/convert-deployment-to-knative-service/#example-conversion","text":"The following example shows a Kubernetes Nginx Deployment and Service , and shows how it converts to a Knative Service.","title":"Example conversion"},{"location":"serving/convert-deployment-to-knative-service/#kubernetes-nginx-deployment-and-service","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx","title":"Kubernetes Nginx Deployment and Service"},{"location":"serving/convert-deployment-to-knative-service/#knative-service","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"Knative Service"},{"location":"serving/deploying-from-private-registry/","text":"Deploying images from a private container registry \u00b6 You can configure your Knative cluster to deploy images from a private registry across multiple Services and Revisions. To do this, you must create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. You must then add those secrets to the default service account for all Services, or the Revision template for a single Service. Prerequisites \u00b6 You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored. Procedure \u00b6 Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Optional. After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Optional. Add the imagePullSecrets object to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry. Optional. Add the imagePullSecrets object to a Service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : imagePullSecrets : - name : <secret-name> containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\"","title":"\u4ece\u79c1\u6709\u6ce8\u518c\u4e2d\u5fc3\u90e8\u7f72"},{"location":"serving/deploying-from-private-registry/#deploying-images-from-a-private-container-registry","text":"You can configure your Knative cluster to deploy images from a private registry across multiple Services and Revisions. To do this, you must create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. You must then add those secrets to the default service account for all Services, or the Revision template for a single Service.","title":"Deploying images from a private container registry"},{"location":"serving/deploying-from-private-registry/#prerequisites","text":"You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored.","title":"Prerequisites"},{"location":"serving/deploying-from-private-registry/#procedure","text":"Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Optional. After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Optional. Add the imagePullSecrets object to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry. Optional. Add the imagePullSecrets object to a Service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : imagePullSecrets : - name : <secret-name> containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\"","title":"Procedure"},{"location":"serving/istio-authorization/","text":"Enabling requests to Knative services when additional authorization policies are enabled \u00b6 Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods. Before you begin \u00b6 You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation . Mutual TLS in Knative \u00b6 Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ] Health checking and metrics collection \u00b6 In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths. Allowing access from system pods by paths \u00b6 Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u6388\u6743\u7b56\u7565"},{"location":"serving/istio-authorization/#enabling-requests-to-knative-services-when-additional-authorization-policies-are-enabled","text":"Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#before-you-begin","text":"You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation .","title":"Before you begin"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","text":"Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ]","title":"Mutual TLS in Knative"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","text":"In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths.","title":"Health checking and metrics collection"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","text":"Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Allowing access from system pods by paths"},{"location":"serving/knative-kubernetes-services/","text":"Kubernetes services \u00b6 This guide describes the Kubernetes Services that are active when running Knative Serving. Before You Begin \u00b6 This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function. Components \u00b6 Service: activator \u00b6 The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics. Service: autoscaler \u00b6 The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic. Service: controller \u00b6 The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs). Service: webhook \u00b6 The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsistent and invalid objects, and validates and mutates Kubernetes API calls. Deployment: net-certmanager-controller \u00b6 The certmanager reconciles cluster ingresses into cert manager objects. Deployment: net-istio-controller \u00b6 The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service . What's Next \u00b6 For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"Kubernetes \u670d\u52a1"},{"location":"serving/knative-kubernetes-services/#kubernetes-services","text":"This guide describes the Kubernetes Services that are active when running Knative Serving.","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#before-you-begin","text":"This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function.","title":"Before You Begin"},{"location":"serving/knative-kubernetes-services/#components","text":"","title":"Components"},{"location":"serving/knative-kubernetes-services/#service-activator","text":"The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.","title":"Service: activator"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","text":"The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.","title":"Service: autoscaler"},{"location":"serving/knative-kubernetes-services/#service-controller","text":"The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).","title":"Service: controller"},{"location":"serving/knative-kubernetes-services/#service-webhook","text":"The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsistent and invalid objects, and validates and mutates Kubernetes API calls.","title":"Service: webhook"},{"location":"serving/knative-kubernetes-services/#deployment-net-certmanager-controller","text":"The certmanager reconciles cluster ingresses into cert manager objects.","title":"Deployment: net-certmanager-controller"},{"location":"serving/knative-kubernetes-services/#deployment-net-istio-controller","text":"The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service .","title":"Deployment: net-istio-controller"},{"location":"serving/knative-kubernetes-services/#whats-next","text":"For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"What's Next"},{"location":"serving/queue-extensions/","text":"Extending Queue Proxy image with QPOptions \u00b6 Knative service pods include two containers: The user main service container, which is named user-container The Queue Proxy - a sidecar named queue-proxy that serves as a reverse proxy in front of the user-container You can extend Queue Proxy to offer additional features. The QPOptions feature of Queue Proxy allows additional runtime packages to extend Queue Proxy capabilities. For example, the security-guard repository provides an extension that uses the QPOptions feature. The QPOption package enables users to add additional security features to Queue Proxy. The runtime features available are determined when the Queue Proxy image is built. Queue Proxy defines an orderly manner to activate and to configure extensions. Additional information \u00b6 Enabling Queue Proxy Pod Info - discussing a necessary step to enable the use of extensions. Using extensions enabled by QPOptions - discussing how to configure a service to use features implemented in extensions. Adding extensions \u00b6 You can add extensions by replacing the cmd/queue/main.go file before the Queue Proxy image is built. The following example shows a cmd/queue/main.go file that adds the test-gate extension: package main import \"os\" import \"knative.dev/serving/pkg/queue/sharedmain\" import \"knative.dev/security-guard/pkg/qpoption\" import _ \"knative.dev/security-guard/pkg/test-gate\" func main () { qOpt := qpoption . NewQPSecurityPlugs () defer qOpt . Shutdown () if sharedmain . Main ( qOpt . Setup ) != nil { os . Exit ( 1 ) } }","title":"\u7528QPOptions\u6269\u5c55\u961f\u5217\u4ee3\u7406\u6620\u50cf"},{"location":"serving/queue-extensions/#extending-queue-proxy-image-with-qpoptions","text":"Knative service pods include two containers: The user main service container, which is named user-container The Queue Proxy - a sidecar named queue-proxy that serves as a reverse proxy in front of the user-container You can extend Queue Proxy to offer additional features. The QPOptions feature of Queue Proxy allows additional runtime packages to extend Queue Proxy capabilities. For example, the security-guard repository provides an extension that uses the QPOptions feature. The QPOption package enables users to add additional security features to Queue Proxy. The runtime features available are determined when the Queue Proxy image is built. Queue Proxy defines an orderly manner to activate and to configure extensions.","title":"Extending Queue Proxy image with QPOptions"},{"location":"serving/queue-extensions/#additional-information","text":"Enabling Queue Proxy Pod Info - discussing a necessary step to enable the use of extensions. Using extensions enabled by QPOptions - discussing how to configure a service to use features implemented in extensions.","title":"Additional information"},{"location":"serving/queue-extensions/#adding-extensions","text":"You can add extensions by replacing the cmd/queue/main.go file before the Queue Proxy image is built. The following example shows a cmd/queue/main.go file that adds the test-gate extension: package main import \"os\" import \"knative.dev/serving/pkg/queue/sharedmain\" import \"knative.dev/security-guard/pkg/qpoption\" import _ \"knative.dev/security-guard/pkg/test-gate\" func main () { qOpt := qpoption . NewQPSecurityPlugs () defer qOpt . Shutdown () if sharedmain . Main ( qOpt . Setup ) != nil { os . Exit ( 1 ) } }","title":"Adding extensions"},{"location":"serving/rolling-out-latest-revision/","text":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\" \u8def\u7531\u66f4\u65b0\u72b6\u6001 \u00b6 \u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready \u591a\u4e2a\u5ef6\u5c55 \u00b6 \u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/rolling-out-latest-revision/#_1","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/rolling-out-latest-revision/#procedure","text":"You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/rolling-out-latest-revision/#_2","text":"\u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"\u8def\u7531\u66f4\u65b0\u72b6\u6001"},{"location":"serving/rolling-out-latest-revision/#_3","text":"\u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u591a\u4e2a\u5ef6\u5c55"},{"location":"serving/setting-up-custom-ingress-gateway/","text":"Configuring the ingress gateway \u00b6 Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows. Replace the default istio-ingressgateway service \u00b6 Step 1: Create the gateway service and deployment instance \u00b6 You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway Step 2: Update the Knative gateway \u00b6 Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly. Step 3: Update the gateway ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ] Replace the knative-ingress-gateway gateway \u00b6 We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps. Step 1: Create the gateway \u00b6 Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Step 2: Update the gateway ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"\u914d\u7f6e\u5165\u53e3\u7f51\u5173"},{"location":"serving/setting-up-custom-ingress-gateway/#configuring-the-ingress-gateway","text":"Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows.","title":"Configuring the ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-default-istio-ingressgateway-service","text":"","title":"Replace the default istio-ingressgateway service"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway-service-and-deployment-instance","text":"You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway","title":"Step 1: Create the gateway service and deployment instance"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-knative-gateway","text":"Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly.","title":"Step 2: Update the Knative gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-3-update-the-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ]","title":"Step 3: Update the gateway ConfigMap"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-knative-ingress-gateway-gateway","text":"We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps.","title":"Replace the knative-ingress-gateway gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway","text":"Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Step 1: Create the gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"Step 2: Update the gateway ConfigMap"},{"location":"serving/tag-resolution/","text":"Tag resolution \u00b6 Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature. Custom certificates \u00b6 If you are using a registry that has a self-signed certificate, you must configure the default Knative Serving controller Deployment to trust that certificate. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately. Procedure \u00b6 If you are using a custom-certs secret that contains your CA certificates, add the following spec to the default Knative Serving controller Deployment: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. Create a secret in the knative-serving namespace that points to your root CA certificate, and then save the current Knative Serving controller Deployment: kubectl -n knative-serving create secret generic customca --from-file = ca.crt = /root/ca.crt kubectl -n knative-serving get deploy/controller -o yaml > knative-serving-controller.yaml Corporate proxy \u00b6 If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"\u6807\u7b7e\u7684\u51b3\u8bae"},{"location":"serving/tag-resolution/#tag-resolution","text":"Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature.","title":"Tag resolution"},{"location":"serving/tag-resolution/#custom-certificates","text":"If you are using a registry that has a self-signed certificate, you must configure the default Knative Serving controller Deployment to trust that certificate. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately.","title":"Custom certificates"},{"location":"serving/tag-resolution/#procedure","text":"If you are using a custom-certs secret that contains your CA certificates, add the following spec to the default Knative Serving controller Deployment: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. Create a secret in the knative-serving namespace that points to your root CA certificate, and then save the current Knative Serving controller Deployment: kubectl -n knative-serving create secret generic customca --from-file = ca.crt = /root/ca.crt kubectl -n knative-serving get deploy/controller -o yaml > knative-serving-controller.yaml","title":"Procedure"},{"location":"serving/tag-resolution/#corporate-proxy","text":"If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Corporate proxy"},{"location":"serving/traffic-management/","text":"Traffic management \u00b6 You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service. Using tags to create target URLs \u00b6 In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes. Traffic routing examples \u00b6 The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0 Routing and managing traffic by using the Knative CLI \u00b6 You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag revision-0001 = green --tag @latest = blue The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20 Routing and managing traffic with blue/green deployment \u00b6 You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy . Procedure \u00b6 Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"\u4ea4\u901a\u7ba1\u7406"},{"location":"serving/traffic-management/#traffic-management","text":"You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service.","title":"Traffic management"},{"location":"serving/traffic-management/#using-tags-to-create-target-urls","text":"In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes.","title":"Using tags to create target URLs"},{"location":"serving/traffic-management/#traffic-routing-examples","text":"The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0","title":"Traffic routing examples"},{"location":"serving/traffic-management/#routing-and-managing-traffic-by-using-the-knative-cli","text":"You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag revision-0001 = green --tag @latest = blue The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20","title":"Routing and managing traffic by using the Knative CLI"},{"location":"serving/traffic-management/#routing-and-managing-traffic-with-bluegreen-deployment","text":"You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy .","title":"Routing and managing traffic with blue/green deployment"},{"location":"serving/traffic-management/#procedure","text":"Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"Procedure"},{"location":"serving/using-a-custom-domain/","text":"\u914d\u7f6e\u57df\u540d \u00b6 \u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5355\u4e2aKnative\u670d\u52a1\u7684\u57df\uff0c\u6216\u8005\u4e3a\u96c6\u7fa4\u4e0a\u521b\u5efa\u7684\u6240\u6709\u670d\u52a1\u8bbe\u7f6e\u5168\u5c40\u9ed8\u8ba4\u57df\u3002 \u7f3a\u7701\u60c5\u51b5\u4e0b\uff0c\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u4e3a {route}.{namespace}.example.com \u3002 \u4e3a\u5355\u4e2aKnative\u670d\u52a1\u914d\u7f6e\u57df \u00b6 \u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49\u5355\u4e2a\u670d\u52a1\u7684\u57df\uff0c\u8bf7\u53c2\u89c1\u6709\u5173 DomainMapping \u7684\u6587\u6863\u3002 \u4e3a\u96c6\u7fa4\u4e2d\u7684\u6240\u6709Knative\u670d\u52a1\u914d\u7f6e\u9ed8\u8ba4\u57df \u00b6 \u901a\u8fc7\u4fee\u6539 config-domain ConfigMap \uff0c\u53ef\u4ee5\u66f4\u6539\u96c6\u7fa4\u4e2d\u6240\u6709Knative\u670d\u52a1\u7684\u9ed8\u8ba4\u57df\u3002 \u8fc7\u7a0b \u00b6 \u5728\u9ed8\u8ba4\u6587\u672c\u7f16\u8f91\u5668\u4e2d\u6253\u5f00 config-domain ConfigMap kubectl edit configmap config-domain -n knative-serving \u7f16\u8f91\u6587\u4ef6\uff0c\u5c06 example.com \u66ff\u6362\u4e3a\u60a8\u60f3\u8981\u4f7f\u7528\u7684\u57df\uff0c\u7136\u540e\u5220\u9664 _example \u952e\u5e76\u4fdd\u5b58\u66f4\u6539\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c knative.dev \u88ab\u914d\u7f6e\u4e3a\u6240\u6709\u8def\u7531\u7684\u57df: apiVersion : v1 data : knative.dev : \"\" kind : ConfigMap [ ... ] \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u90e8\u7f72\uff0cKnative\u4f1a\u534f\u8c03\u5bf9ConfigMap\u6240\u505a\u7684\u66f4\u6539\uff0c\u5e76\u81ea\u52a8\u66f4\u65b0\u6240\u6709\u90e8\u7f72\u7684\u670d\u52a1\u548c\u8def\u7531\u7684\u4e3b\u673a\u540d\u3002 \u9a8c\u8bc1\u6b65\u9aa4 \u00b6 \u5c06\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa4\u3002 \u68c0\u7d22\u8def\u7531\u7684URL: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" \u5176\u4e2d <route-name> \u662f\u8def\u7531\u7684\u540d\u79f0\u3002 \u89c2\u5bdf\u5df2\u914d\u7f6e\u7684\u81ea\u5b9a\u4e49\u57df\u3002 \u53d1\u5e03\u60a8\u7684\u57df \u00b6 \u8981\u4f7f\u60a8\u7684\u57df\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u60a8\u5fc5\u987b\u66f4\u65b0\u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u4ee5\u6307\u5411\u60a8\u7684\u670d\u52a1\u5165\u53e3\u7684IP\u5730\u5740\u3002 \u4e3a\u547d\u540d\u7a7a\u95f4\u521b\u5efa \u901a\u914d\u7b26\u8bb0\u5f55 \uff0c\u5e76\u81ea\u5b9a\u4e49\u5165\u63a5\u53e3IP\u5730\u5740\u7684\u57df\uff0c\u8fd9\u5c06\u4f7f\u540c\u4e00\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u591a\u4e2a\u670d\u52a1\u7684\u4e3b\u673a\u540d\u80fd\u591f\u5de5\u4f5c\uff0c\u800c\u65e0\u9700\u521b\u5efa\u989d\u5916\u7684DNS\u6761\u76ee\u3002 *.default.knative.dev 59 IN A 35.237.28.44 \u521b\u5efa\u4e00\u4e2aA\u8bb0\u5f55\uff0c\u4ece\u5b8c\u5168\u9650\u5b9a\u7684\u57df\u540d\u6307\u5411\u60a8\u7684Knative\u7f51\u5173\u7684IP\u5730\u5740\u3002 \u9700\u8981\u4e3a\u521b\u5efa\u7684\u6bcf\u4e2aKnative\u670d\u52a1\u6216\u8def\u7531\u6267\u884c\u6b64\u6b65\u9aa4\u3002 helloworld-go.default.knative.dev 59 IN A 35.237.28.44 \u5728\u57df\u66f4\u65b0\u4f20\u64ad\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u90e8\u7f72\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u8bbf\u95ee\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u914d\u7f6e\u57df\u540d"},{"location":"serving/using-a-custom-domain/#_1","text":"\u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u5355\u4e2aKnative\u670d\u52a1\u7684\u57df\uff0c\u6216\u8005\u4e3a\u96c6\u7fa4\u4e0a\u521b\u5efa\u7684\u6240\u6709\u670d\u52a1\u8bbe\u7f6e\u5168\u5c40\u9ed8\u8ba4\u57df\u3002 \u7f3a\u7701\u60c5\u51b5\u4e0b\uff0c\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u4e3a {route}.{namespace}.example.com \u3002","title":"\u914d\u7f6e\u57df\u540d"},{"location":"serving/using-a-custom-domain/#knative","text":"\u5982\u679c\u60a8\u60f3\u81ea\u5b9a\u4e49\u5355\u4e2a\u670d\u52a1\u7684\u57df\uff0c\u8bf7\u53c2\u89c1\u6709\u5173 DomainMapping \u7684\u6587\u6863\u3002","title":"\u4e3a\u5355\u4e2aKnative\u670d\u52a1\u914d\u7f6e\u57df"},{"location":"serving/using-a-custom-domain/#knative_1","text":"\u901a\u8fc7\u4fee\u6539 config-domain ConfigMap \uff0c\u53ef\u4ee5\u66f4\u6539\u96c6\u7fa4\u4e2d\u6240\u6709Knative\u670d\u52a1\u7684\u9ed8\u8ba4\u57df\u3002","title":"\u4e3a\u96c6\u7fa4\u4e2d\u7684\u6240\u6709Knative\u670d\u52a1\u914d\u7f6e\u9ed8\u8ba4\u57df"},{"location":"serving/using-a-custom-domain/#_2","text":"\u5728\u9ed8\u8ba4\u6587\u672c\u7f16\u8f91\u5668\u4e2d\u6253\u5f00 config-domain ConfigMap kubectl edit configmap config-domain -n knative-serving \u7f16\u8f91\u6587\u4ef6\uff0c\u5c06 example.com \u66ff\u6362\u4e3a\u60a8\u60f3\u8981\u4f7f\u7528\u7684\u57df\uff0c\u7136\u540e\u5220\u9664 _example \u952e\u5e76\u4fdd\u5b58\u66f4\u6539\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c knative.dev \u88ab\u914d\u7f6e\u4e3a\u6240\u6709\u8def\u7531\u7684\u57df: apiVersion : v1 data : knative.dev : \"\" kind : ConfigMap [ ... ] \u5982\u679c\u60a8\u6709\u4e00\u4e2a\u73b0\u6709\u7684\u90e8\u7f72\uff0cKnative\u4f1a\u534f\u8c03\u5bf9ConfigMap\u6240\u505a\u7684\u66f4\u6539\uff0c\u5e76\u81ea\u52a8\u66f4\u65b0\u6240\u6709\u90e8\u7f72\u7684\u670d\u52a1\u548c\u8def\u7531\u7684\u4e3b\u673a\u540d\u3002","title":"\u8fc7\u7a0b"},{"location":"serving/using-a-custom-domain/#_3","text":"\u5c06\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa4\u3002 \u68c0\u7d22\u8def\u7531\u7684URL: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" \u5176\u4e2d <route-name> \u662f\u8def\u7531\u7684\u540d\u79f0\u3002 \u89c2\u5bdf\u5df2\u914d\u7f6e\u7684\u81ea\u5b9a\u4e49\u57df\u3002","title":"\u9a8c\u8bc1\u6b65\u9aa4"},{"location":"serving/using-a-custom-domain/#_4","text":"\u8981\u4f7f\u60a8\u7684\u57df\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u60a8\u5fc5\u987b\u66f4\u65b0\u60a8\u7684DNS\u63d0\u4f9b\u7a0b\u5e8f\u4ee5\u6307\u5411\u60a8\u7684\u670d\u52a1\u5165\u53e3\u7684IP\u5730\u5740\u3002 \u4e3a\u547d\u540d\u7a7a\u95f4\u521b\u5efa \u901a\u914d\u7b26\u8bb0\u5f55 \uff0c\u5e76\u81ea\u5b9a\u4e49\u5165\u63a5\u53e3IP\u5730\u5740\u7684\u57df\uff0c\u8fd9\u5c06\u4f7f\u540c\u4e00\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u591a\u4e2a\u670d\u52a1\u7684\u4e3b\u673a\u540d\u80fd\u591f\u5de5\u4f5c\uff0c\u800c\u65e0\u9700\u521b\u5efa\u989d\u5916\u7684DNS\u6761\u76ee\u3002 *.default.knative.dev 59 IN A 35.237.28.44 \u521b\u5efa\u4e00\u4e2aA\u8bb0\u5f55\uff0c\u4ece\u5b8c\u5168\u9650\u5b9a\u7684\u57df\u540d\u6307\u5411\u60a8\u7684Knative\u7f51\u5173\u7684IP\u5730\u5740\u3002 \u9700\u8981\u4e3a\u521b\u5efa\u7684\u6bcf\u4e2aKnative\u670d\u52a1\u6216\u8def\u7531\u6267\u884c\u6b64\u6b65\u9aa4\u3002 helloworld-go.default.knative.dev 59 IN A 35.237.28.44 \u5728\u57df\u66f4\u65b0\u4f20\u64ad\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u90e8\u7f72\u8def\u7531\u7684\u5b8c\u5168\u9650\u5b9a\u57df\u540d\u8bbf\u95ee\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u3002","title":"\u53d1\u5e03\u60a8\u7684\u57df"},{"location":"serving/using-a-tls-cert/","text":"Configuring HTTPS with TLS certificates \u00b6 Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the procedures later in this topic for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Warning Certificates issued by Let's Encrypt are valid for only 90days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires. Before you begin \u00b6 You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Warning Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve. Obtaining a TLS certificate \u00b6 If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers Using Certbot to manually obtain Let\u2019s Encrypt certificates \u00b6 Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret . Using cert-manager to obtain Let's Encrypt certificates \u00b6 You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic. Manually adding a TLS certificate \u00b6 If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, follow the steps in the relevant tab to manually add a certificate: Contour Istio To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin. Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by running the command: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note Take note of the namespace and secret name. You will need these in future steps. To use this certificate and private key in different namespaces, you must create a delegation. To do so, create a YAML file using the following template: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Update the Knative Contour plugin to use the certificate as a fallback when autoTLS is disabled by running the command: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In this example, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation What's next: \u00b6 After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"\u914d\u7f6eHTTPS\u8fde\u63a5"},{"location":"serving/using-a-tls-cert/#configuring-https-with-tls-certificates","text":"Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the procedures later in this topic for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Warning Certificates issued by Let's Encrypt are valid for only 90days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires.","title":"Configuring HTTPS with TLS certificates"},{"location":"serving/using-a-tls-cert/#before-you-begin","text":"You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Warning Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve.","title":"Before you begin"},{"location":"serving/using-a-tls-cert/#obtaining-a-tls-certificate","text":"If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers","title":"Obtaining a TLS certificate"},{"location":"serving/using-a-tls-cert/#using-certbot-to-manually-obtain-lets-encrypt-certificates","text":"Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret .","title":"Using Certbot to manually obtain Let\u2019s Encrypt certificates"},{"location":"serving/using-a-tls-cert/#using-cert-manager-to-obtain-lets-encrypt-certificates","text":"You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic.","title":"Using cert-manager to obtain Let's Encrypt certificates"},{"location":"serving/using-a-tls-cert/#manually-adding-a-tls-certificate","text":"If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, follow the steps in the relevant tab to manually add a certificate: Contour Istio To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin. Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by running the command: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note Take note of the namespace and secret name. You will need these in future steps. To use this certificate and private key in different namespaces, you must create a delegation. To do so, create a YAML file using the following template: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Update the Knative Contour plugin to use the certificate as a fallback when autoTLS is disabled by running the command: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In this example, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation","title":"Manually adding a TLS certificate"},{"location":"serving/using-a-tls-cert/#whats-next","text":"After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"What's next:"},{"location":"serving/using-auto-tls/","text":"Enabling auto-TLS certs \u00b6 If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates . Before you begin \u00b6 The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, or Contour v1.1 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . cert-manager version 1.0.0 or higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider. Automatic TLS provision mode \u00b6 Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. Provision Certificate per namespace is supported when using DNS-01 challenge mode. This is the recommended mode for faster certificate provision. In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluster ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace. Enabling Auto TLS \u00b6 Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : test-email@knative.dev privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True . DNS-01 challenge only: Configure your DNS provider \u00b6 If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com . Install net-certmanager-controller deployment \u00b6 Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Provisioning certificates per namespace (wildcard certificates) \u00b6 Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" . Configure config-certmanager ConfigMap \u00b6 Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager -n knative-serving Add the issuerRef within the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-certmanager namespace : knative-serving labels : networking.knative.dev/certificate-provider : cert-manager data : issuerRef : | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer is used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager -n knative-serving -o yaml Turn on Auto TLS \u00b6 Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network -n knative-serving Add the auto-tls: Enabled attribute under the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data : http-protocol : Redirected Example: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled http-protocol : Redirected ... Ensure that the file was updated successfully: kubectl get configmap config-network -n knative-serving -o yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic. Verify Auto TLS \u00b6 Run the following command to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default. { custom-domain } autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case. Disable Auto TLS per service or route \u00b6 If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"\u542f\u7528auto-TLS"},{"location":"serving/using-auto-tls/#enabling-auto-tls-certs","text":"If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates .","title":"Enabling auto-TLS certs"},{"location":"serving/using-auto-tls/#before-you-begin","text":"The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, or Contour v1.1 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . cert-manager version 1.0.0 or higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider.","title":"Before you begin"},{"location":"serving/using-auto-tls/#automatic-tls-provision-mode","text":"Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. Provision Certificate per namespace is supported when using DNS-01 challenge mode. This is the recommended mode for faster certificate provision. In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluster ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace.","title":"Automatic TLS provision mode"},{"location":"serving/using-auto-tls/#enabling-auto-tls","text":"Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : test-email@knative.dev privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True .","title":"Enabling Auto TLS"},{"location":"serving/using-auto-tls/#dns-01-challenge-only-configure-your-dns-provider","text":"If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com .","title":"DNS-01 challenge only: Configure your DNS provider"},{"location":"serving/using-auto-tls/#install-net-certmanager-controller-deployment","text":"Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml","title":"Install net-certmanager-controller deployment"},{"location":"serving/using-auto-tls/#provisioning-certificates-per-namespace-wildcard-certificates","text":"Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" .","title":"Provisioning certificates per namespace (wildcard certificates)"},{"location":"serving/using-auto-tls/#configure-config-certmanager-configmap","text":"Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager -n knative-serving Add the issuerRef within the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-certmanager namespace : knative-serving labels : networking.knative.dev/certificate-provider : cert-manager data : issuerRef : | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer is used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager -n knative-serving -o yaml","title":"Configure config-certmanager ConfigMap"},{"location":"serving/using-auto-tls/#turn-on-auto-tls","text":"Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network -n knative-serving Add the auto-tls: Enabled attribute under the data section: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data : http-protocol : Redirected Example: apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : ... auto-tls : Enabled http-protocol : Redirected ... Ensure that the file was updated successfully: kubectl get configmap config-network -n knative-serving -o yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic.","title":"Turn on Auto TLS"},{"location":"serving/using-auto-tls/#verify-auto-tls","text":"Run the following command to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default. { custom-domain } autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case.","title":"Verify Auto TLS"},{"location":"serving/using-auto-tls/#disable-auto-tls-per-service-or-route","text":"If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Disable Auto TLS per service or route"},{"location":"serving/webhook-customizations/","text":"Exclude namespaces from the Knative webhook \u00b6 The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"\u6392\u9664\u540d\u79f0\u7a7a\u95f4"},{"location":"serving/webhook-customizations/#exclude-namespaces-from-the-knative-webhook","text":"The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/app-security/security-guard-about/","text":"About Security-Guard \u00b6 Security-Guard provides visibility into the security status of deployed Knative Services, by monitoring the behaviors of user containers and events. Security-Guard profile and criteria \u00b6 Security-Guard creates a profile of the user container behavior and of event behavior. The behaviors are then compared to a pre-defined criteria. If the profile does not meet the criteria, Security-Guard can log alerts, block events, or stop a Service instance, depending on user configurations. The criteria that a profile is compared to is composed of a set of micro-rules. These rules describe expected behaviors for events and user containers, including expected responses. You can choose to set micro-rules manually, or use Security-Guard's machine learning feature to automate the creation of micro-rules. Guardians \u00b6 A per-Service set of micro-rules is stored in the Kubernetes system as a Guardian object. Under Knative, Security-Guard store Guardians ny default using the guardians.guard.security.knative.dev CRDs. To list all CRD Guardians use: kubectl get guardians.guard.security.knative.dev Example Output: NAME AGE helloworld-go 10h Using Security-Guard \u00b6 Security-Guard offers situational awareness by writing its alerts to the Service queue proxy log. You may observe the queue-proxy to see alerts. Security alerts appear in the queue proxy log file and start with the string SECURITY ALERT! . The default setup of Security-Guard is to allow any request or response and learn any new pattern after reporting it. When the Service is actively serving requests, it typically takes about 30 min for Security-Guard to learn the patterns of the Service requests and responses and build corresponding micro-rules. After the initial learning period, Security-Guard updates the micro-rules in the Service Guardian, following which, it sends alerts only when a change in behavior is detected. Note that in the default setup, Security-Guard continues to learn any new behavior and therefore avoids reporting alerts repeatedly when the new behavior reoccurs. Correct security procedures should include reviewing any new behavior detected by Security-Guard. Security-Guard can also be configured to operate in other modes of operation, such as: Move from auto learning to manual micro-rules management after the initial learning period Block requests/responses when they do not conform to the micro-rules For more information or for troubleshooting help, see the #security channel in Knative Slack. Security-Guard Use Cases \u00b6 Security-Guard support four different stages in the life of a knative service from a security standpoint. Zero-Day Vulnerable Exploitable Misused We next detail each stage and how Security-Guard is used to manage the security of the service in that stage. Zero-Day \u00b6 Under normal conditions, the Knative user who owns the service is not aware of any known vulnerabilities in the service. Yet, it is reasonable to assume that the service has weaknesses. Security-Guard offers Knative users the ability to detect/block patterns sent as part of incoming events that may be used to exploit unknown, zero-day, service vulnerabilities. Vulnerable \u00b6 Once a CVE that describes a vulnerability in the service is published, the Knative user who owns the service is required to start a process to eliminate the vulnerability by introducing a new revision of the service. This process of removing a known vulnerability may take many weeks to accomplish. Security-Guard enables Knative users to set micro-rules to detect/block incoming events that include patterns that may be used as part of some future exploit targeting the discovered vulnerability. In this way, users are able to continue offering services, although the service has a known vulnerability. Exploitable \u00b6 When a known exploit is found effective in compromising a service, the Knative user who owns the Service needs a way to filter incoming events that contain the specific exploit. This is normally the case during a successful attack, where a working exploit is able to compromise the user-container. Security-Guard enables Knative users a way to set micro-rules to detect/block incoming events that include specific exploits while allowing other events to be served. Misused \u00b6 When an offender has established an attack pattern that is able to take over a service instance, by first exploiting one or more vulnerabilities and then starting to misuse the service instance, stopping the service instance requires the offender to repeat the attack pattern. At any given time, some service instances may be compromised and misused while others behave as designed. Security-Guard enables Knative users a way to detect/remove misused Service instances while allowing other instances to continue serve events. Additional resources \u00b6 See Readme files in the Security-Guard Github Repository .","title":"\u5173\u4e8e\u4fdd\u5b89"},{"location":"serving/app-security/security-guard-about/#about-security-guard","text":"Security-Guard provides visibility into the security status of deployed Knative Services, by monitoring the behaviors of user containers and events.","title":"About Security-Guard"},{"location":"serving/app-security/security-guard-about/#security-guard-profile-and-criteria","text":"Security-Guard creates a profile of the user container behavior and of event behavior. The behaviors are then compared to a pre-defined criteria. If the profile does not meet the criteria, Security-Guard can log alerts, block events, or stop a Service instance, depending on user configurations. The criteria that a profile is compared to is composed of a set of micro-rules. These rules describe expected behaviors for events and user containers, including expected responses. You can choose to set micro-rules manually, or use Security-Guard's machine learning feature to automate the creation of micro-rules.","title":"Security-Guard profile and criteria"},{"location":"serving/app-security/security-guard-about/#guardians","text":"A per-Service set of micro-rules is stored in the Kubernetes system as a Guardian object. Under Knative, Security-Guard store Guardians ny default using the guardians.guard.security.knative.dev CRDs. To list all CRD Guardians use: kubectl get guardians.guard.security.knative.dev Example Output: NAME AGE helloworld-go 10h","title":"Guardians"},{"location":"serving/app-security/security-guard-about/#using-security-guard","text":"Security-Guard offers situational awareness by writing its alerts to the Service queue proxy log. You may observe the queue-proxy to see alerts. Security alerts appear in the queue proxy log file and start with the string SECURITY ALERT! . The default setup of Security-Guard is to allow any request or response and learn any new pattern after reporting it. When the Service is actively serving requests, it typically takes about 30 min for Security-Guard to learn the patterns of the Service requests and responses and build corresponding micro-rules. After the initial learning period, Security-Guard updates the micro-rules in the Service Guardian, following which, it sends alerts only when a change in behavior is detected. Note that in the default setup, Security-Guard continues to learn any new behavior and therefore avoids reporting alerts repeatedly when the new behavior reoccurs. Correct security procedures should include reviewing any new behavior detected by Security-Guard. Security-Guard can also be configured to operate in other modes of operation, such as: Move from auto learning to manual micro-rules management after the initial learning period Block requests/responses when they do not conform to the micro-rules For more information or for troubleshooting help, see the #security channel in Knative Slack.","title":"Using Security-Guard"},{"location":"serving/app-security/security-guard-about/#security-guard-use-cases","text":"Security-Guard support four different stages in the life of a knative service from a security standpoint. Zero-Day Vulnerable Exploitable Misused We next detail each stage and how Security-Guard is used to manage the security of the service in that stage.","title":"Security-Guard Use Cases"},{"location":"serving/app-security/security-guard-about/#zero-day","text":"Under normal conditions, the Knative user who owns the service is not aware of any known vulnerabilities in the service. Yet, it is reasonable to assume that the service has weaknesses. Security-Guard offers Knative users the ability to detect/block patterns sent as part of incoming events that may be used to exploit unknown, zero-day, service vulnerabilities.","title":"Zero-Day"},{"location":"serving/app-security/security-guard-about/#vulnerable","text":"Once a CVE that describes a vulnerability in the service is published, the Knative user who owns the service is required to start a process to eliminate the vulnerability by introducing a new revision of the service. This process of removing a known vulnerability may take many weeks to accomplish. Security-Guard enables Knative users to set micro-rules to detect/block incoming events that include patterns that may be used as part of some future exploit targeting the discovered vulnerability. In this way, users are able to continue offering services, although the service has a known vulnerability.","title":"Vulnerable"},{"location":"serving/app-security/security-guard-about/#exploitable","text":"When a known exploit is found effective in compromising a service, the Knative user who owns the Service needs a way to filter incoming events that contain the specific exploit. This is normally the case during a successful attack, where a working exploit is able to compromise the user-container. Security-Guard enables Knative users a way to set micro-rules to detect/block incoming events that include specific exploits while allowing other events to be served.","title":"Exploitable"},{"location":"serving/app-security/security-guard-about/#misused","text":"When an offender has established an attack pattern that is able to take over a service instance, by first exploiting one or more vulnerabilities and then starting to misuse the service instance, stopping the service instance requires the offender to repeat the attack pattern. At any given time, some service instances may be compromised and misused while others behave as designed. Security-Guard enables Knative users a way to detect/remove misused Service instances while allowing other instances to continue serve events.","title":"Misused"},{"location":"serving/app-security/security-guard-about/#additional-resources","text":"See Readme files in the Security-Guard Github Repository .","title":"Additional resources"},{"location":"serving/app-security/security-guard-example-alerts/","text":"Security-Guard example alerts \u00b6 Send an event with unexpected query string, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io?a=3\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: QueryString: KeyVal: Key a is not known... Send an event with unexpected long url, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io/AAAAAAAAAAAAAAAA\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: Url: KeyVal: Letters: Counter out of Range: 16 ...","title":"\u4fdd\u5b89\u8b66\u62a5\u4f8b\u5b50"},{"location":"serving/app-security/security-guard-example-alerts/#security-guard-example-alerts","text":"Send an event with unexpected query string, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io?a=3\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: QueryString: KeyVal: Key a is not known... Send an event with unexpected long url, for example: curl \"http://helloworld-go.default.52.118.14.2.sslip.io/AAAAAAAAAAAAAAAA\" This returns an output similar to the following: Hello Secured World! Check alerts: kubectl logs deployment/helloworld-go-00001-deployment queue-proxy | grep \"SECURITY ALERT!\" This returns an output similar to the following: ...SECURITY ALERT! HttpRequest: Url: KeyVal: Letters: Counter out of Range: 16 ...","title":"Security-Guard example alerts"},{"location":"serving/app-security/security-guard-install/","text":"Installing Security-Guard \u00b6 Here we show how to install Security-Guard in Knative. Security-Guard is an enhancement to knative-Serving and needs to be installed after the Knative-Serving is successfully installed. Using Security-Guard requires that your cluster will use an enhanced queue-proxy image. In addition, Security-Guard includes automation for auto-learning a per service Guardian. Auto-learning requires you to deploy a guard-service on your kubernetes cluster. guard-service should be installed in any namespace where you deploy knative services that require Security-Guard protection. Before you begin \u00b6 Before installing Security-Guard, learn about Security-Guard Install steps \u00b6 To start this tutorial, after installing Knative Serving, run the following procedure to replace your queue-proxy image and deploy a guard-service in the current namespace. Install from source Install from released images and yamls Clone the Security-Guard repository using git clone git@github.com:knative-sandbox/security-guard.git Do cd security-guard Run ko apply -Rf ./config Use released images to update your system to enable Security-Guard: Set the feature named queueproxy.mount-podinfo to allowed in the config-features ConfigMap. An easy way to do that is using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/deploy/config-features.yaml Set the deployment parameter queue-sidecar-image to gcr.io/knative-releases/knative.dev/security-guard/cmd/queue in the config-deployment ConfigMap. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/queue-proxy.yaml Add the necessary Security-Guard resources to your cluster using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/gateAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/serviceAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/guardiansCrd.yaml Deploy guard-service on your system to enable automated learning of micro-rules. In the current version, it is recommended to deploy guard-service in any namespace where knative services are deployed. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/guard-service.yaml","title":"\u5b89\u88c5\u7684\u4fdd\u5b89"},{"location":"serving/app-security/security-guard-install/#installing-security-guard","text":"Here we show how to install Security-Guard in Knative. Security-Guard is an enhancement to knative-Serving and needs to be installed after the Knative-Serving is successfully installed. Using Security-Guard requires that your cluster will use an enhanced queue-proxy image. In addition, Security-Guard includes automation for auto-learning a per service Guardian. Auto-learning requires you to deploy a guard-service on your kubernetes cluster. guard-service should be installed in any namespace where you deploy knative services that require Security-Guard protection.","title":"Installing Security-Guard"},{"location":"serving/app-security/security-guard-install/#before-you-begin","text":"Before installing Security-Guard, learn about Security-Guard","title":"Before you begin"},{"location":"serving/app-security/security-guard-install/#install-steps","text":"To start this tutorial, after installing Knative Serving, run the following procedure to replace your queue-proxy image and deploy a guard-service in the current namespace. Install from source Install from released images and yamls Clone the Security-Guard repository using git clone git@github.com:knative-sandbox/security-guard.git Do cd security-guard Run ko apply -Rf ./config Use released images to update your system to enable Security-Guard: Set the feature named queueproxy.mount-podinfo to allowed in the config-features ConfigMap. An easy way to do that is using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/deploy/config-features.yaml Set the deployment parameter queue-sidecar-image to gcr.io/knative-releases/knative.dev/security-guard/cmd/queue in the config-deployment ConfigMap. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/queue-proxy.yaml Add the necessary Security-Guard resources to your cluster using: kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/gateAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/serviceAccount.yaml kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/security-guard/release-0.1/config/resources/guardiansCrd.yaml Deploy guard-service on your system to enable automated learning of micro-rules. In the current version, it is recommended to deploy guard-service in any namespace where knative services are deployed. An easy way to do that is using: kubectl apply -f https://github.com/knative-sandbox/security-guard/releases/download/v0.1.0/guard-service.yaml","title":"Install steps"},{"location":"serving/app-security/security-guard-quickstart/","text":"Security-Guard monitoring quickstart \u00b6 This tutorial shows how you can use Security-Guard to protect a deployed Knative Service. Before you begin \u00b6 Before starting the tutorial, make sure to install Security-Guard Creating and deploying a service \u00b6 Tip The following commands create a helloworld-go sample Service while activating and configuring the Security-Guard extension for this Service. You can modify these commands, including changing the Security-Guard configuration for your service using either the kn CLI or changing the service yaml based on this example. Create a sample securedService: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/guard-activate : enable spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Secured World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env \"TARGET=Secured World\" \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/guard-activate=enable After the Service has been created, Guard starts monitoring the Service Pods and all Events sent to the Service. Continue to Security-Guard alert example to test your installation See the Using Security-Guard section to learn about managing the security of the service Cleanup \u00b6 To remove the deployed service use: Apply YAML kn CLI Delete using the YAML file used to create the service by running the command: kubectl delete -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service delete helloworld-go To remove the Guardian of the deployed service use: ```bash kubectl delete guardians.guard.security.knative.dev helloworld-go ```","title":"\u4fdd\u5b89\u5feb\u901f\u5165\u95e8"},{"location":"serving/app-security/security-guard-quickstart/#security-guard-monitoring-quickstart","text":"This tutorial shows how you can use Security-Guard to protect a deployed Knative Service.","title":"Security-Guard monitoring quickstart"},{"location":"serving/app-security/security-guard-quickstart/#before-you-begin","text":"Before starting the tutorial, make sure to install Security-Guard","title":"Before you begin"},{"location":"serving/app-security/security-guard-quickstart/#creating-and-deploying-a-service","text":"Tip The following commands create a helloworld-go sample Service while activating and configuring the Security-Guard extension for this Service. You can modify these commands, including changing the Security-Guard configuration for your service using either the kn CLI or changing the service yaml based on this example. Create a sample securedService: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/guard-activate : enable spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Secured World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env \"TARGET=Secured World\" \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/guard-activate=enable After the Service has been created, Guard starts monitoring the Service Pods and all Events sent to the Service. Continue to Security-Guard alert example to test your installation See the Using Security-Guard section to learn about managing the security of the service","title":"Creating and deploying a service"},{"location":"serving/app-security/security-guard-quickstart/#cleanup","text":"To remove the deployed service use: Apply YAML kn CLI Delete using the YAML file used to create the service by running the command: kubectl delete -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service delete helloworld-go To remove the Guardian of the deployed service use: ```bash kubectl delete guardians.guard.security.knative.dev helloworld-go ```","title":"Cleanup"},{"location":"serving/autoscaling/","text":"\u81ea\u52a8\u7f29\u653e \u00b6 Knative Serving\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u81ea\u52a8\u4f38\u7f29\uff0c\u6216 autoscaling \uff0c\u4ee5\u5339\u914d\u4f20\u5165\u7684\u9700\u6c42\u3002 \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528Knative Pod Autoscaler (KPA)\u9ed8\u8ba4\u63d0\u4f9b\u7684\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u6ca1\u6709\u63a5\u6536\u6d41\u91cf\uff0c\u5e76\u4e14\u542f\u7528\u4e86\u5411\u96f6\u6269\u5c55\uff0cKnative Serving\u5c06\u5e94\u7528\u7a0b\u5e8f\u6269\u5c55\u5230\u96f6\u526f\u672c\u3002 \u5982\u679c\u7981\u7528\u4e86\u5411\u96f6\u4f38\u7f29\uff0c\u5219\u5e94\u7528\u7a0b\u5e8f\u5c06\u88ab\u7f29\u5c0f\u5230\u96c6\u7fa4\u4e0a\u4e3a\u5e94\u7528\u7a0b\u5e8f\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u91cf\u3002 \u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u7684\u6d41\u91cf\u589e\u52a0\uff0c\u526f\u672c\u5c06\u88ab\u6269\u5927\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u4f38\u7f29\u81f3\u96f6\u529f\u80fd\u3002 \u53c2\u89c1 \u914d\u7f6e\u7f29\u653e\u5230\u96f6 \u3002 \u5982\u679c\u5728\u60a8\u7684\u96c6\u7fa4\u4e0a\u542f\u7528\u4e86\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u8981\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u60a8\u5fc5\u987b\u914d\u7f6e \u5e76\u53d1 \u548c \u4f38\u7f29\u8fb9\u754c \u3002 \u989d\u5916\u7684\u8d44\u6e90 \u00b6 \u8bd5\u8bd5 Go Autoscale Sample App . \u914d\u7f6e\u60a8\u7684Knative\u90e8\u7f72\u4ee5\u4f7f\u7528Kubernetes Horizontal Pod Autoscaler (HPA)\u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA\u3002\u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29\u5668\u4f7f\u7528\u7684 \u5ea6\u91cf\u7c7b\u578b . \u914d\u7f6e\u60a8\u7684Knative\u670d\u52a1\u4f7f\u7528 container-freeze \uff0c\u5b83\u4f1a\u5728Pod\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u6700\u6709\u4ef7\u503c\u7684\u597d\u5904\u662f\u51cf\u5c11\u4e86\u8fd9\u79cd\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u5173\u4e8e\u81ea\u52a8\u7f29\u653e"},{"location":"serving/autoscaling/#_1","text":"Knative Serving\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u81ea\u52a8\u4f38\u7f29\uff0c\u6216 autoscaling \uff0c\u4ee5\u5339\u914d\u4f20\u5165\u7684\u9700\u6c42\u3002 \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528Knative Pod Autoscaler (KPA)\u9ed8\u8ba4\u63d0\u4f9b\u7684\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u6ca1\u6709\u63a5\u6536\u6d41\u91cf\uff0c\u5e76\u4e14\u542f\u7528\u4e86\u5411\u96f6\u6269\u5c55\uff0cKnative Serving\u5c06\u5e94\u7528\u7a0b\u5e8f\u6269\u5c55\u5230\u96f6\u526f\u672c\u3002 \u5982\u679c\u7981\u7528\u4e86\u5411\u96f6\u4f38\u7f29\uff0c\u5219\u5e94\u7528\u7a0b\u5e8f\u5c06\u88ab\u7f29\u5c0f\u5230\u96c6\u7fa4\u4e0a\u4e3a\u5e94\u7528\u7a0b\u5e8f\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u91cf\u3002 \u5982\u679c\u5e94\u7528\u7a0b\u5e8f\u7684\u6d41\u91cf\u589e\u52a0\uff0c\u526f\u672c\u5c06\u88ab\u6269\u5927\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u4f38\u7f29\u81f3\u96f6\u529f\u80fd\u3002 \u53c2\u89c1 \u914d\u7f6e\u7f29\u653e\u5230\u96f6 \u3002 \u5982\u679c\u5728\u60a8\u7684\u96c6\u7fa4\u4e0a\u542f\u7528\u4e86\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u8981\u4e3a\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29\u529f\u80fd\uff0c\u60a8\u5fc5\u987b\u914d\u7f6e \u5e76\u53d1 \u548c \u4f38\u7f29\u8fb9\u754c \u3002","title":"\u81ea\u52a8\u7f29\u653e"},{"location":"serving/autoscaling/#_2","text":"\u8bd5\u8bd5 Go Autoscale Sample App . \u914d\u7f6e\u60a8\u7684Knative\u90e8\u7f72\u4ee5\u4f7f\u7528Kubernetes Horizontal Pod Autoscaler (HPA)\u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA\u3002\u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29\u5668\u4f7f\u7528\u7684 \u5ea6\u91cf\u7c7b\u578b . \u914d\u7f6e\u60a8\u7684Knative\u670d\u52a1\u4f7f\u7528 container-freeze \uff0c\u5b83\u4f1a\u5728Pod\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u6700\u6709\u4ef7\u503c\u7684\u597d\u5904\u662f\u51cf\u5c11\u4e86\u8fd9\u79cd\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u989d\u5916\u7684\u8d44\u6e90"},{"location":"serving/autoscaling/autoscaler-types/","text":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u7c7b\u578b \u00b6 Knative\u670d\u52a1\u652f\u6301Knative Pod Autoscaler (KPA)\u548cKubernetes\u7684 Horizontal Pod Autoscaler (HPA)\u7684\u5b9e\u73b0\u3002 \u672c\u4e3b\u9898\u5217\u51fa\u6bcf\u79cd\u81ea\u52a8\u7f29\u653e\u5668\u7684\u7279\u6027\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981\u7684 \u5982\u679c\u60a8\u60f3\u4f7f\u7528Kubernetes\u6c34\u5e73Pod\u81ea\u52a8\u7f29\u653e\u5668(HPA)\uff0c\u60a8\u5fc5\u987b\u5728\u5b89\u88c5Knative\u670d\u52a1\u4e4b\u540e\u5b89\u88c5\u5b83\u3002 \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 . Knative Pod Autoscaler (KPA) \u00b6 Knative\u670d\u52a1\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5Knative\u670d\u52a1\u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8ecpu\u7684\u81ea\u52a8\u4f38\u7f29\u3002 Horizontal Pod Autoscaler (HPA) \u00b6 \u4e0d\u662fKnative Serving\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u5fc5\u987b\u5148\u5b89\u88c5Knative Serving\u3002 \u4e0d\u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301CPU-based\u81ea\u52a8\u5b9a\u91cf\u3002 \u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0 \u00b6 \u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0\u7684\u7c7b\u578b(KPA\u6216HPA)\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 class \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u952e: pod-autoscaler-class \u6bcf\u4e2a\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/class \u53ef\u80fd\u7684\u503c: \"kpa.autoscaling.knative.dev\" \u6216 \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" \u5168\u5c40\u548c\u6bcf\u4e2a\u4fee\u8ba2\u8bbe\u7f6e \u00b6 Knative\u4e2d\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u6216\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u8fdb\u884c\u8bbe\u7f6e\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u6bcf\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u8bbe\u7f6e\uff0c\u5219\u5c06\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u6bcf\u4fee\u8ba2\u7684\u8bbe\u7f6e\uff0c\u5f53\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u8bbe\u7f6e\u90fd\u5b58\u5728\u65f6\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u5c06\u8986\u76d6\u5168\u5c40\u8bbe\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e \u00b6 \u4f7f\u7528 config-autoscaler ConfigMap\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u7684\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u8fd0\u8425\u5546\u5b89\u88c5\u4e86Knative\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5728 spec.config.autoscaler ConfigMap \u4e2d\u8bbe\u7f6e\u5168\u5c40\u914d\u7f6e\u8bbe\u7f6e\uff0c\u4f4d\u4e8e KnativeServing \u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4e2d\u3002 \u9ed8\u8ba4\u81ea\u52a8\u4f38\u7f29 ConfigMap \u793a\u4f8b \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\" \u6bcf\u4fee\u8ba2\u8bbe\u7f6e \u00b6 \u81ea\u52a8\u4f38\u7f29\u7684\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u662f\u901a\u8fc7\u5411\u7248\u672c\u6dfb\u52a0 annotations \u6765\u914d\u7f6e\u7684\u3002 Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u670d\u52a1\u6216\u914d\u7f6e\u521b\u5efa\u4fee\u8ba2\uff0c\u5219\u5fc5\u987b\u5728 revision template \u4e2d\u8bbe\u7f6e\u6ce8\u91ca\uff0c\u4ee5\u4fbf\u5728\u521b\u5efa\u6bcf\u4e2a\u4fee\u8ba2\u65f6\u5c06\u4efb\u4f55\u4fee\u6539\u5e94\u7528\u4e8e\u5b83\u4eec\u3002 \u5728\u5355\u4e2a\u4fee\u8ba2\u7684\u9876\u5c42\u5143\u6570\u636e\u4e2d\u8bbe\u7f6e\u6ce8\u91ca\u4e0d\u4f1a\u5c06\u66f4\u6539\u4f20\u64ad\u5230\u5176\u4ed6\u4fee\u8ba2\uff0c\u4e5f\u4e0d\u4f1a\u5c06\u66f4\u6539\u5e94\u7528\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u4e2d\u3002","title":"\u652f\u6301\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#_1","text":"Knative\u670d\u52a1\u652f\u6301Knative Pod Autoscaler (KPA)\u548cKubernetes\u7684 Horizontal Pod Autoscaler (HPA)\u7684\u5b9e\u73b0\u3002 \u672c\u4e3b\u9898\u5217\u51fa\u6bcf\u79cd\u81ea\u52a8\u7f29\u653e\u5668\u7684\u7279\u6027\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981\u7684 \u5982\u679c\u60a8\u60f3\u4f7f\u7528Kubernetes\u6c34\u5e73Pod\u81ea\u52a8\u7f29\u653e\u5668(HPA)\uff0c\u60a8\u5fc5\u987b\u5728\u5b89\u88c5Knative\u670d\u52a1\u4e4b\u540e\u5b89\u88c5\u5b83\u3002 \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA\uff0c\u8bf7\u53c2\u89c1 \u5b89\u88c5\u53ef\u9009\u670d\u52a1\u6269\u5c55 .","title":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#knative-pod-autoscaler-kpa","text":"Knative\u670d\u52a1\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5Knative\u670d\u52a1\u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8ecpu\u7684\u81ea\u52a8\u4f38\u7f29\u3002","title":"Knative Pod Autoscaler (KPA)"},{"location":"serving/autoscaling/autoscaler-types/#horizontal-pod-autoscaler-hpa","text":"\u4e0d\u662fKnative Serving\u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u4f60\u5fc5\u987b\u5148\u5b89\u88c5Knative Serving\u3002 \u4e0d\u652f\u6301\u4f38\u7f29\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301CPU-based\u81ea\u52a8\u5b9a\u91cf\u3002","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"serving/autoscaling/autoscaler-types/#_2","text":"\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0\u7684\u7c7b\u578b(KPA\u6216HPA)\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 class \u6ce8\u91ca\u6765\u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u952e: pod-autoscaler-class \u6bcf\u4e2a\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/class \u53ef\u80fd\u7684\u503c: \"kpa.autoscaling.knative.dev\" \u6216 \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\"","title":"\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u5668\u5b9e\u73b0"},{"location":"serving/autoscaling/autoscaler-types/#_3","text":"Knative\u4e2d\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u6216\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u8fdb\u884c\u8bbe\u7f6e\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u6bcf\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u8bbe\u7f6e\uff0c\u5219\u5c06\u4f7f\u7528\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u6307\u5b9a\u4e86\u6bcf\u4fee\u8ba2\u7684\u8bbe\u7f6e\uff0c\u5f53\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u8bbe\u7f6e\u90fd\u5b58\u5728\u65f6\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u5c06\u8986\u76d6\u5168\u5c40\u8bbe\u7f6e\u3002","title":"\u5168\u5c40\u548c\u6bcf\u4e2a\u4fee\u8ba2\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaler-types/#_4","text":"\u4f7f\u7528 config-autoscaler ConfigMap\u914d\u7f6e\u81ea\u52a8\u7f29\u653e\u7684\u5168\u5c40\u8bbe\u7f6e\u3002 \u5982\u679c\u60a8\u4f7f\u7528\u8fd0\u8425\u5546\u5b89\u88c5\u4e86Knative\u670d\u52a1\uff0c\u60a8\u53ef\u4ee5\u5728 spec.config.autoscaler ConfigMap \u4e2d\u8bbe\u7f6e\u5168\u5c40\u914d\u7f6e\u8bbe\u7f6e\uff0c\u4f4d\u4e8e KnativeServing \u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u4e2d\u3002","title":"\u5168\u5c40\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaler-types/#configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\"","title":"\u9ed8\u8ba4\u81ea\u52a8\u4f38\u7f29 ConfigMap \u793a\u4f8b"},{"location":"serving/autoscaling/autoscaler-types/#_5","text":"\u81ea\u52a8\u4f38\u7f29\u7684\u6bcf\u4fee\u8ba2\u8bbe\u7f6e\u662f\u901a\u8fc7\u5411\u7248\u672c\u6dfb\u52a0 annotations \u6765\u914d\u7f6e\u7684\u3002 Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important \u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528\u670d\u52a1\u6216\u914d\u7f6e\u521b\u5efa\u4fee\u8ba2\uff0c\u5219\u5fc5\u987b\u5728 revision template \u4e2d\u8bbe\u7f6e\u6ce8\u91ca\uff0c\u4ee5\u4fbf\u5728\u521b\u5efa\u6bcf\u4e2a\u4fee\u8ba2\u65f6\u5c06\u4efb\u4f55\u4fee\u6539\u5e94\u7528\u4e8e\u5b83\u4eec\u3002 \u5728\u5355\u4e2a\u4fee\u8ba2\u7684\u9876\u5c42\u5143\u6570\u636e\u4e2d\u8bbe\u7f6e\u6ce8\u91ca\u4e0d\u4f1a\u5c06\u66f4\u6539\u4f20\u64ad\u5230\u5176\u4ed6\u4fee\u8ba2\uff0c\u4e5f\u4e0d\u4f1a\u5c06\u66f4\u6539\u5e94\u7528\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u81ea\u52a8\u4f38\u7f29\u914d\u7f6e\u4e2d\u3002","title":"\u6bcf\u4fee\u8ba2\u8bbe\u7f6e"},{"location":"serving/autoscaling/autoscaling-metrics/","text":"\u6307\u6807 \u00b6 \u6307\u6807\u914d\u7f6e\u5b9a\u4e49\u7531Autoscaler\u76d1\u89c6\u7684\u5ea6\u91cf\u7c7b\u578b\u3002 \u8bbe\u7f6e\u6bcf\u4e2a\u4fee\u8ba2\u7684\u6307\u6807 \u00b6 \u5bf9\u4e8e \u6bcf\u4fee\u8ba2 \u914d\u7f6e\uff0c\u8fd9\u662f\u4f7f\u7528 autoscaling.knative.dev/metric \u6ce8\u91ca\u786e\u5b9a\u7684\u3002 \u53ef\u4ee5\u5728\u6bcf\u4e2a\u7248\u672c\u4e2d\u914d\u7f6e\u7684\u53ef\u80fd\u7684\u5ea6\u91cf\u7c7b\u578b\u53d6\u51b3\u4e8e\u60a8\u6b63\u5728\u4f7f\u7528\u7684Autoscaler\u5b9e\u73b0\u7684\u7c7b\u578b: \u9ed8\u8ba4\u7684KPA Autoscaler\u652f\u6301 concurrency \u548c rps \u6307\u6807\u3002 HPA Autoscaler \u652f\u6301 cpu \u6307\u6807\u3002 \u6709\u5173KPA\u548cHPA\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u578b \u7684\u6587\u6863\u3002 \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/metric \u53ef\u80fd\u503c: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" \u6216\u4efb\u4f55\u81ea\u5b9a\u4e49\u6307\u6807\u540d\u79f0\uff0c\u8fd9\u53d6\u51b3\u4e8e\u60a8\u7684Autoscaler\u7c7b\u578b. \"cpu\" , \"memory\" \u548c \"custom\" \u6307\u6807\u4ec5\u5728\u4f7f\u7528HPA\u7c7b\u7684\u7248\u672c\u4e2d\u652f\u6301\u3002 \u9ed8\u8ba4: \"concurrency\" \u6bcf\u4fee\u8ba2\u5e76\u53d1\u914d\u7f6e \u6bcf\u4fee\u8ba2rps\u914d\u7f6e \u6bcf\u4fee\u8ba2cpu\u914d\u7f6e \u6bcf\u4fee\u8ba2\u5185\u5b58\u914d\u7f6e \u6bcf\u4fee\u8ba2\u81ea\u5b9a\u4e49\u6307\u6807\u914d\u7f6e apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" \u60a8\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aHPA\uff0c\u4ee5\u6839\u636e\u60a8\u6307\u5b9a\u7684\u6307\u6807\u6765\u6269\u5c55\u4fee\u8ba2\u3002 HPA\u5c06\u88ab\u914d\u7f6e\u4e3a\u5728\u4fee\u8ba2\u7684\u6240\u6709Pods\u4e2d\u4f7f\u7528\u5ea6\u91cf\u7684 \u5e73\u5747\u503c \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric. \u4e0b\u4e00\u4e2a\u6b65\u9aa4 \u00b6 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e \u5e76\u53d1\u76ee\u6807 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u526f\u672c\u914d\u7f6e \u6bcf\u79d2\u8bf7\u6c42\u76ee\u6807","title":"\u914d\u7f6e\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_1","text":"\u6307\u6807\u914d\u7f6e\u5b9a\u4e49\u7531Autoscaler\u76d1\u89c6\u7684\u5ea6\u91cf\u7c7b\u578b\u3002","title":"\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_2","text":"\u5bf9\u4e8e \u6bcf\u4fee\u8ba2 \u914d\u7f6e\uff0c\u8fd9\u662f\u4f7f\u7528 autoscaling.knative.dev/metric \u6ce8\u91ca\u786e\u5b9a\u7684\u3002 \u53ef\u4ee5\u5728\u6bcf\u4e2a\u7248\u672c\u4e2d\u914d\u7f6e\u7684\u53ef\u80fd\u7684\u5ea6\u91cf\u7c7b\u578b\u53d6\u51b3\u4e8e\u60a8\u6b63\u5728\u4f7f\u7528\u7684Autoscaler\u5b9e\u73b0\u7684\u7c7b\u578b: \u9ed8\u8ba4\u7684KPA Autoscaler\u652f\u6301 concurrency \u548c rps \u6307\u6807\u3002 HPA Autoscaler \u652f\u6301 cpu \u6307\u6807\u3002 \u6709\u5173KPA\u548cHPA\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u578b \u7684\u6587\u6863\u3002 \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/metric \u53ef\u80fd\u503c: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" \u6216\u4efb\u4f55\u81ea\u5b9a\u4e49\u6307\u6807\u540d\u79f0\uff0c\u8fd9\u53d6\u51b3\u4e8e\u60a8\u7684Autoscaler\u7c7b\u578b. \"cpu\" , \"memory\" \u548c \"custom\" \u6307\u6807\u4ec5\u5728\u4f7f\u7528HPA\u7c7b\u7684\u7248\u672c\u4e2d\u652f\u6301\u3002 \u9ed8\u8ba4: \"concurrency\" \u6bcf\u4fee\u8ba2\u5e76\u53d1\u914d\u7f6e \u6bcf\u4fee\u8ba2rps\u914d\u7f6e \u6bcf\u4fee\u8ba2cpu\u914d\u7f6e \u6bcf\u4fee\u8ba2\u5185\u5b58\u914d\u7f6e \u6bcf\u4fee\u8ba2\u81ea\u5b9a\u4e49\u6307\u6807\u914d\u7f6e apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" \u60a8\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aHPA\uff0c\u4ee5\u6839\u636e\u60a8\u6307\u5b9a\u7684\u6307\u6807\u6765\u6269\u5c55\u4fee\u8ba2\u3002 HPA\u5c06\u88ab\u914d\u7f6e\u4e3a\u5728\u4fee\u8ba2\u7684\u6240\u6709Pods\u4e2d\u4f7f\u7528\u5ea6\u91cf\u7684 \u5e73\u5747\u503c \u3002 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric.","title":"\u8bbe\u7f6e\u6bcf\u4e2a\u4fee\u8ba2\u7684\u6307\u6807"},{"location":"serving/autoscaling/autoscaling-metrics/#_3","text":"\u4e3a\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e \u5e76\u53d1\u76ee\u6807 \u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u526f\u672c\u914d\u7f6e \u6bcf\u79d2\u8bf7\u6c42\u76ee\u6807","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4"},{"location":"serving/autoscaling/autoscaling-targets/","text":"\u76ee\u6807 \u00b6 \u914d\u7f6e\u76ee\u6807\u4e3aAutoscaler\u63d0\u4f9b\u4e00\u4e2a\u503c\uff0c\u5b83\u8bd5\u56fe\u4e3a\u4fee\u8ba2\u7684\u914d\u7f6e\u6307\u6807\u7ef4\u62a4\u8be5\u503c\u3002 \u6709\u5173\u53ef\u914d\u7f6e\u5ea6\u91cf\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u6307\u6807 \u6587\u6863\u3002 \u7528\u4e8e\u914d\u7f6e\u6bcf\u4e2a\u7248\u672c\u76ee\u6807\u7684 target \u6ce8\u91ca\u662f metric agnostic \u3002 \u8fd9\u610f\u5473\u7740\u76ee\u6807\u53ea\u662f\u4e00\u4e2a\u6574\u6570\u503c\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u5ea6\u91cf\u7c7b\u578b\u3002 \u914d\u7f6e\u76ee\u6807 \u00b6 \u5168\u5c40\u8bbe\u7f6e\u952e: container-concurrency-target-default . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u6307\u6807 \u7684\u6587\u6863\u3002. \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u80fd\u503c: An integer (metric agnostic). \u9ed8\u8ba4\u503c: \"100\" for container-concurrency-target-default . \u6ca1\u6709\u4e3a target \u6ce8\u91ca\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u76ee\u6807\u6ce8\u91ca-\u6bcf\u4fee\u8ba2 \u5e76\u53d1\u76ee\u6807-\u5168\u5c40(ConfigMap) \u5e76\u53d1\u76ee\u6807-\u5168\u5c40\u5bb9\u5668(Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u914d\u7f6e\u76ee\u6807"},{"location":"serving/autoscaling/autoscaling-targets/#_1","text":"\u914d\u7f6e\u76ee\u6807\u4e3aAutoscaler\u63d0\u4f9b\u4e00\u4e2a\u503c\uff0c\u5b83\u8bd5\u56fe\u4e3a\u4fee\u8ba2\u7684\u914d\u7f6e\u6307\u6807\u7ef4\u62a4\u8be5\u503c\u3002 \u6709\u5173\u53ef\u914d\u7f6e\u5ea6\u91cf\u7c7b\u578b\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u6307\u6807 \u6587\u6863\u3002 \u7528\u4e8e\u914d\u7f6e\u6bcf\u4e2a\u7248\u672c\u76ee\u6807\u7684 target \u6ce8\u91ca\u662f metric agnostic \u3002 \u8fd9\u610f\u5473\u7740\u76ee\u6807\u53ea\u662f\u4e00\u4e2a\u6574\u6570\u503c\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u5ea6\u91cf\u7c7b\u578b\u3002","title":"\u76ee\u6807"},{"location":"serving/autoscaling/autoscaling-targets/#_2","text":"\u5168\u5c40\u8bbe\u7f6e\u952e: container-concurrency-target-default . \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u6307\u6807 \u7684\u6587\u6863\u3002. \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u80fd\u503c: An integer (metric agnostic). \u9ed8\u8ba4\u503c: \"100\" for container-concurrency-target-default . \u6ca1\u6709\u4e3a target \u6ce8\u91ca\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u76ee\u6807\u6ce8\u91ca-\u6bcf\u4fee\u8ba2 \u5e76\u53d1\u76ee\u6807-\u5168\u5c40(ConfigMap) \u5e76\u53d1\u76ee\u6807-\u5168\u5c40\u5bb9\u5668(Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u914d\u7f6e\u76ee\u6807"},{"location":"serving/autoscaling/concurrency/","text":"\u914d\u7f6e\u5e76\u53d1 \u00b6 \u5e76\u53d1\u6027\u51b3\u5b9a\u4e86\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u5728\u4efb\u4f55\u7ed9\u5b9a\u65f6\u95f4\u5185\u53ef\u4ee5\u5904\u7406\u7684\u5e76\u53d1\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u7248\u672c\u5e76\u53d1\uff0c\u60a8\u5fc5\u987b\u540c\u65f6\u914d\u7f6e autoscaling.knative.dev/metric \u548c autoscaling.knative.dev/target \u4f5c\u4e3a \u8f6f\u9650\u5236 \uff0c\u6216 containerConcurrency \u4f5c\u4e3a \u786c\u9650\u5236 \u3002 \u5bf9\u4e8e\u5168\u5c40\u5e76\u53d1\u6027\uff0c\u60a8\u53ef\u4ee5\u8bbe\u7f6e container-concurrency-target-default \u503c\u3002 \u8f6f\u4e0e\u786c\u5e76\u53d1\u9650\u5236 \u00b6 \u53ef\u4ee5\u8bbe\u7f6e \u8f6f / \u786c \u5e76\u53d1\u9650\u5236\u3002 Note \u5982\u679c\u540c\u65f6\u6307\u5b9a\u4e86\u8f6f\u9650\u5236\u548c\u786c\u9650\u5236\uff0c\u5219\u5c06\u4f7f\u7528\u4e24\u4e2a\u503c\u4e2d\u8f83\u5c0f\u7684\u503c\u3002 \u8fd9\u53ef\u4ee5\u9632\u6b62Autoscaler\u62e5\u6709\u786c\u9650\u5236\u503c\u4e0d\u5141\u8bb8\u7684\u76ee\u6807\u503c\u3002 \u8f6f\u9650\u989d\u662f\u4e00\u4e2a\u6709\u9488\u5bf9\u6027\u7684\u9650\u989d\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u4e25\u683c\u6267\u884c\u7684\u9650\u989d\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8bf7\u6c42\u7a81\u7136\u7206\u53d1\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u786c\u6027\u9650\u5236\u662f\u5f3a\u5236\u7684\u4e0a\u9650\u3002 \u5982\u679c\u5e76\u53d1\u6027\u8fbe\u5230\u786c\u9650\u5236\uff0c\u591a\u4f59\u7684\u8bf7\u6c42\u5c06\u88ab\u7f13\u51b2\uff0c\u5fc5\u987b\u7b49\u5f85\uff0c\u76f4\u5230\u6709\u8db3\u591f\u7684\u7a7a\u95f2\u5bb9\u91cf\u6765\u6267\u884c\u8bf7\u6c42\u3002 Warning \u53ea\u6709\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6709\u660e\u786e\u7684\u7528\u4f8b\u65f6\uff0c\u624d\u5efa\u8bae\u4f7f\u7528\u786c\u9650\u5236\u914d\u7f6e\u3002 \u6307\u5b9a\u8f83\u4f4e\u7684\u786c\u9650\u5236\u53ef\u80fd\u4f1a\u5bf9\u5e94\u7528\u7a0b\u5e8f\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u989d\u5916\u7684\u51b7\u542f\u52a8\u3002 \u8f6f\u9650\u5236 \u00b6 \u5168\u5c40\u952e: container-concurrency-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"100\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" \u786c\u9650\u5236 \u00b6 \u786c\u9650\u5236\u662f\u5728\u6bcf\u4e2a\u4fee\u8ba2\u4e2d\u4f7f\u7528\u4fee\u8ba2\u89c4\u8303\u4e0a\u7684 containerConcurrency \u5b57\u6bb5\u6307\u5b9a\u7684\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u662f\u6ce8\u91ca\u3002 \u5728\u81ea\u52a8\u4f38\u7f29 ConfigMap \u4e2d\u6ca1\u6709\u786c\u9650\u5236\u7684\u5168\u5c40\u8bbe\u7f6e\uff0c\u56e0\u4e3a containerConcurrency \u5728\u81ea\u52a8\u4f38\u7f29\u4e4b\u5916\u4e5f\u6709\u5f71\u54cd\uff0c\u6bd4\u5982\u5bf9\u8bf7\u6c42\u7684\u7f13\u51b2\u548c\u6392\u961f\u3002 \u4f46\u662f\uff0c\u53ef\u4ee5\u5728 config-defaults.yaml \u4e2d\u4e3a\u4fee\u8ba2\u7248\u7684 containerConcurrency \u5b57\u6bb5\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u9ed8\u8ba4\u503c\u662f 0 \uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9650\u5236\u5141\u8bb8\u6d41\u5165\u4fee\u8ba2\u7248\u7684\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5927\u4e8e 0 \u7684\u503c\u6307\u5b9a\u5728\u4efb\u4f55\u65f6\u5019\u5141\u8bb8\u6d41\u5411\u526f\u672c\u7684\u786e\u5207\u8bf7\u6c42\u6570\u3002 \u5168\u5c40\u952e: container-concurrency (in config-defaults.yaml ) \u6bcf\u4fee\u8ba2\u89c4\u8303\u952e: containerConcurrency \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 , \u610f\u601d\u662f\u6ca1\u6709\u9650\u5236 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (\u9ed8\u8ba4 ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\" \u76ee\u6807\u5229\u7528\u7387 \u00b6 \u9664\u4e86\u524d\u9762\u89e3\u91ca\u7684\u6587\u5b57\u8bbe\u7f6e\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u76ee\u6807\u5229\u7528\u7387\u503c \u8fdb\u4e00\u6b65\u8c03\u6574\u5e76\u53d1\u503c\u3002 \u8be5\u503c\u6307\u5b9aAutoscaler\u5b9e\u9645\u9488\u5bf9\u524d\u9762\u6307\u5b9a\u7684\u76ee\u6807\u7684\u767e\u5206\u6bd4\u3002 \u8fd9\u4e5f\u88ab\u79f0\u4e3a\u6307\u5b9a\u526f\u672c\u8fd0\u884c\u65f6\u7684 hotness \uff0c\u8fd9\u5c06\u5bfc\u81f4Autoscaler\u5728\u8fbe\u5230\u5b9a\u4e49\u7684\u786c\u9650\u5236\u4e4b\u524d\u6269\u5927\u3002 \u4f8b\u5982\uff0c\u5982\u679c containerConcurrency \u8bbe\u7f6e\u4e3a10\uff0c\u76ee\u6807\u5229\u7528\u7387\u8bbe\u7f6e\u4e3a70%(\u767e\u5206\u4e4b\u4e03\u5341)\uff0c\u5f53\u6240\u6709\u73b0\u6709\u526f\u672c\u7684\u5e73\u5747\u5e76\u53d1\u8bf7\u6c42\u6570\u8fbe\u52307\u65f6\uff0cAutoscaler\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u526f\u672c\u3002 \u7f16\u53f7\u4e3a7\u523010\u7684\u8bf7\u6c42\u4ecd\u7136\u4f1a\u88ab\u53d1\u9001\u5230\u73b0\u6709\u7684\u526f\u672c\uff0c\u4f46\u8fd9\u5141\u8bb8\u5728\u8fbe\u5230 containerConcurrency \u9650\u5236\u65f6\u542f\u52a8\u989d\u5916\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: container-concurrency-target-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target-utilization-percentage \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 70 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"\u914d\u7f6e\u5e76\u53d1"},{"location":"serving/autoscaling/concurrency/#_1","text":"\u5e76\u53d1\u6027\u51b3\u5b9a\u4e86\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u5728\u4efb\u4f55\u7ed9\u5b9a\u65f6\u95f4\u5185\u53ef\u4ee5\u5904\u7406\u7684\u5e76\u53d1\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u7248\u672c\u5e76\u53d1\uff0c\u60a8\u5fc5\u987b\u540c\u65f6\u914d\u7f6e autoscaling.knative.dev/metric \u548c autoscaling.knative.dev/target \u4f5c\u4e3a \u8f6f\u9650\u5236 \uff0c\u6216 containerConcurrency \u4f5c\u4e3a \u786c\u9650\u5236 \u3002 \u5bf9\u4e8e\u5168\u5c40\u5e76\u53d1\u6027\uff0c\u60a8\u53ef\u4ee5\u8bbe\u7f6e container-concurrency-target-default \u503c\u3002","title":"\u914d\u7f6e\u5e76\u53d1"},{"location":"serving/autoscaling/concurrency/#_2","text":"\u53ef\u4ee5\u8bbe\u7f6e \u8f6f / \u786c \u5e76\u53d1\u9650\u5236\u3002 Note \u5982\u679c\u540c\u65f6\u6307\u5b9a\u4e86\u8f6f\u9650\u5236\u548c\u786c\u9650\u5236\uff0c\u5219\u5c06\u4f7f\u7528\u4e24\u4e2a\u503c\u4e2d\u8f83\u5c0f\u7684\u503c\u3002 \u8fd9\u53ef\u4ee5\u9632\u6b62Autoscaler\u62e5\u6709\u786c\u9650\u5236\u503c\u4e0d\u5141\u8bb8\u7684\u76ee\u6807\u503c\u3002 \u8f6f\u9650\u989d\u662f\u4e00\u4e2a\u6709\u9488\u5bf9\u6027\u7684\u9650\u989d\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u4e25\u683c\u6267\u884c\u7684\u9650\u989d\u3002 \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8bf7\u6c42\u7a81\u7136\u7206\u53d1\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u786c\u6027\u9650\u5236\u662f\u5f3a\u5236\u7684\u4e0a\u9650\u3002 \u5982\u679c\u5e76\u53d1\u6027\u8fbe\u5230\u786c\u9650\u5236\uff0c\u591a\u4f59\u7684\u8bf7\u6c42\u5c06\u88ab\u7f13\u51b2\uff0c\u5fc5\u987b\u7b49\u5f85\uff0c\u76f4\u5230\u6709\u8db3\u591f\u7684\u7a7a\u95f2\u5bb9\u91cf\u6765\u6267\u884c\u8bf7\u6c42\u3002 Warning \u53ea\u6709\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6709\u660e\u786e\u7684\u7528\u4f8b\u65f6\uff0c\u624d\u5efa\u8bae\u4f7f\u7528\u786c\u9650\u5236\u914d\u7f6e\u3002 \u6307\u5b9a\u8f83\u4f4e\u7684\u786c\u9650\u5236\u53ef\u80fd\u4f1a\u5bf9\u5e94\u7528\u7a0b\u5e8f\u7684\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u989d\u5916\u7684\u51b7\u542f\u52a8\u3002","title":"\u8f6f\u4e0e\u786c\u5e76\u53d1\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_3","text":"\u5168\u5c40\u952e: container-concurrency-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"100\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u8f6f\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_4","text":"\u786c\u9650\u5236\u662f\u5728\u6bcf\u4e2a\u4fee\u8ba2\u4e2d\u4f7f\u7528\u4fee\u8ba2\u89c4\u8303\u4e0a\u7684 containerConcurrency \u5b57\u6bb5\u6307\u5b9a\u7684\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u662f\u6ce8\u91ca\u3002 \u5728\u81ea\u52a8\u4f38\u7f29 ConfigMap \u4e2d\u6ca1\u6709\u786c\u9650\u5236\u7684\u5168\u5c40\u8bbe\u7f6e\uff0c\u56e0\u4e3a containerConcurrency \u5728\u81ea\u52a8\u4f38\u7f29\u4e4b\u5916\u4e5f\u6709\u5f71\u54cd\uff0c\u6bd4\u5982\u5bf9\u8bf7\u6c42\u7684\u7f13\u51b2\u548c\u6392\u961f\u3002 \u4f46\u662f\uff0c\u53ef\u4ee5\u5728 config-defaults.yaml \u4e2d\u4e3a\u4fee\u8ba2\u7248\u7684 containerConcurrency \u5b57\u6bb5\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u3002 \u9ed8\u8ba4\u503c\u662f 0 \uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9650\u5236\u5141\u8bb8\u6d41\u5165\u4fee\u8ba2\u7248\u7684\u8bf7\u6c42\u7684\u6570\u91cf\u3002 \u5927\u4e8e 0 \u7684\u503c\u6307\u5b9a\u5728\u4efb\u4f55\u65f6\u5019\u5141\u8bb8\u6d41\u5411\u526f\u672c\u7684\u786e\u5207\u8bf7\u6c42\u6570\u3002 \u5168\u5c40\u952e: container-concurrency (in config-defaults.yaml ) \u6bcf\u4fee\u8ba2\u89c4\u8303\u952e: containerConcurrency \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 , \u610f\u601d\u662f\u6ca1\u6709\u9650\u5236 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (\u9ed8\u8ba4 ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\"","title":"\u786c\u9650\u5236"},{"location":"serving/autoscaling/concurrency/#_5","text":"\u9664\u4e86\u524d\u9762\u89e3\u91ca\u7684\u6587\u5b57\u8bbe\u7f6e\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 \u76ee\u6807\u5229\u7528\u7387\u503c \u8fdb\u4e00\u6b65\u8c03\u6574\u5e76\u53d1\u503c\u3002 \u8be5\u503c\u6307\u5b9aAutoscaler\u5b9e\u9645\u9488\u5bf9\u524d\u9762\u6307\u5b9a\u7684\u76ee\u6807\u7684\u767e\u5206\u6bd4\u3002 \u8fd9\u4e5f\u88ab\u79f0\u4e3a\u6307\u5b9a\u526f\u672c\u8fd0\u884c\u65f6\u7684 hotness \uff0c\u8fd9\u5c06\u5bfc\u81f4Autoscaler\u5728\u8fbe\u5230\u5b9a\u4e49\u7684\u786c\u9650\u5236\u4e4b\u524d\u6269\u5927\u3002 \u4f8b\u5982\uff0c\u5982\u679c containerConcurrency \u8bbe\u7f6e\u4e3a10\uff0c\u76ee\u6807\u5229\u7528\u7387\u8bbe\u7f6e\u4e3a70%(\u767e\u5206\u4e4b\u4e03\u5341)\uff0c\u5f53\u6240\u6709\u73b0\u6709\u526f\u672c\u7684\u5e73\u5747\u5e76\u53d1\u8bf7\u6c42\u6570\u8fbe\u52307\u65f6\uff0cAutoscaler\u5c06\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u526f\u672c\u3002 \u7f16\u53f7\u4e3a7\u523010\u7684\u8bf7\u6c42\u4ecd\u7136\u4f1a\u88ab\u53d1\u9001\u5230\u73b0\u6709\u7684\u526f\u672c\uff0c\u4f46\u8fd9\u5141\u8bb8\u5728\u8fbe\u5230 containerConcurrency \u9650\u5236\u65f6\u542f\u52a8\u989d\u5916\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: container-concurrency-target-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/target-utilization-percentage \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 70 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"\u76ee\u6807\u5229\u7528\u7387"},{"location":"serving/autoscaling/container-freezer/","text":"\u914d\u7f6e\u5bb9\u5668\u51b0\u7bb1 \u00b6 \u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u5728\u5176\u6d41\u91cf\u964d\u81f3\u96f6\u6216\u4ece\u96f6\u6269\u5c55\u65f6\u8c03\u7528\u7aef\u70b9API\u3002 \u5728\u793e\u533a\u7ef4\u62a4\u7684\u7aef\u70b9API\u5b9e\u73b0\u5bb9\u5668-\u51b7\u51bb\u4e2d\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u51bb\u7ed3\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u4ece\u96f6\u4e0a\u5347\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u6062\u590d\u3002 \u4f46\u662f\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u8fd0\u884c\u81ea\u5df1\u7684\u5b9e\u73b0(\u4f8b\u5982\uff0c\u4f5c\u4e3a\u8ba1\u8d39\u7ec4\u4ef6\uff0c\u5728\u5904\u7406\u8bf7\u6c42\u65f6\u8bb0\u5f55\u65e5\u5fd7)\u3002 \u914d\u7f6emin-scale \u00b6 \u8981\u4f7f\u7528\u5bb9\u5668\u51b7\u51bb\uff0c\u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e autoscaling.knative.dev/min-scale \u7684\u503c\u5fc5\u987b\u5927\u4e8e\u96f6\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go \u914d\u7f6e\u7aef\u70b9API\u5730\u5740 \u00b6 \u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u8c03\u7528\u7aef\u70b9API\u5730\u5740\uff0c\u56e0\u6b64\u9700\u8981\u914d\u7f6eAPI\u5730\u5740\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6253\u5f00 config-deployment ConfigMap: kubectl edit configmap config-deployment -n knative-serving \u4f8b\u5982\uff0c\u7f16\u8f91\u8be5\u6587\u4ef6\u4ee5\u914d\u7f6e\u7aef\u70b9API\u5730\u5740: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note \u5982\u679c\u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0, \u4f7f\u7528 http://$HOST_IP:9696 \u4f5c\u4e3a concurrency-state-endpoint \u7684\u503c, \u7531\u4e8e\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\uff0c\u76f8\u5e94\u7684\u503c\u5c06\u5728\u8fd0\u884c\u65f6\u7531\u961f\u5217\u4ee3\u7406\u63d2\u5165. \u5982\u679c\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\u5b9e\u73b0\u90e8\u7f72\u5728\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u4e2d\uff0c\u5219\u4f7f\u7528\u7279\u5b9a\u7684\u670d\u52a1\u5730\u5740\uff0c\u4f8b\u5982 http://billing.default.svc:9696 . \u4e0b\u4e00\u6b65 \u00b6 \u5b9e\u73b0\u60a8\u81ea\u5df1\u7684\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684 container-freeze \u5b9e\u73b0\u3002","title":"container-freezer"},{"location":"serving/autoscaling/container-freezer/#_1","text":"\u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u5728\u5176\u6d41\u91cf\u964d\u81f3\u96f6\u6216\u4ece\u96f6\u6269\u5c55\u65f6\u8c03\u7528\u7aef\u70b9API\u3002 \u5728\u793e\u533a\u7ef4\u62a4\u7684\u7aef\u70b9API\u5b9e\u73b0\u5bb9\u5668-\u51b7\u51bb\u4e2d\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u964d\u4e3a\u96f6\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u51bb\u7ed3\uff0c\u5f53\u5bb9\u5668\u7684\u6d41\u91cf\u4ece\u96f6\u4e0a\u5347\u65f6\uff0c\u8fd0\u884c\u7684\u8fdb\u7a0b\u88ab\u6062\u590d\u3002 \u4f46\u662f\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u8fd0\u884c\u81ea\u5df1\u7684\u5b9e\u73b0(\u4f8b\u5982\uff0c\u4f5c\u4e3a\u8ba1\u8d39\u7ec4\u4ef6\uff0c\u5728\u5904\u7406\u8bf7\u6c42\u65f6\u8bb0\u5f55\u65e5\u5fd7)\u3002","title":"\u914d\u7f6e\u5bb9\u5668\u51b0\u7bb1"},{"location":"serving/autoscaling/container-freezer/#min-scale","text":"\u8981\u4f7f\u7528\u5bb9\u5668\u51b7\u51bb\uff0c\u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e autoscaling.knative.dev/min-scale \u7684\u503c\u5fc5\u987b\u5927\u4e8e\u96f6\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u914d\u7f6emin-scale"},{"location":"serving/autoscaling/container-freezer/#api","text":"\u5f53\u542f\u7528\u5bb9\u5668\u51b7\u51bb\u65f6\uff0c\u961f\u5217\u4ee3\u7406\u8c03\u7528\u7aef\u70b9API\u5730\u5740\uff0c\u56e0\u6b64\u9700\u8981\u914d\u7f6eAPI\u5730\u5740\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6253\u5f00 config-deployment ConfigMap: kubectl edit configmap config-deployment -n knative-serving \u4f8b\u5982\uff0c\u7f16\u8f91\u8be5\u6587\u4ef6\u4ee5\u914d\u7f6e\u7aef\u70b9API\u5730\u5740: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note \u5982\u679c\u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0, \u4f7f\u7528 http://$HOST_IP:9696 \u4f5c\u4e3a concurrency-state-endpoint \u7684\u503c, \u7531\u4e8e\u793e\u533a\u7ef4\u62a4\u7684\u5b9e\u73b0\u662f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\uff0c\u76f8\u5e94\u7684\u503c\u5c06\u5728\u8fd0\u884c\u65f6\u7531\u961f\u5217\u4ee3\u7406\u63d2\u5165. \u5982\u679c\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\u5b9e\u73b0\u90e8\u7f72\u5728\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u4e2d\uff0c\u5219\u4f7f\u7528\u7279\u5b9a\u7684\u670d\u52a1\u5730\u5740\uff0c\u4f8b\u5982 http://billing.default.svc:9696 .","title":"\u914d\u7f6e\u7aef\u70b9API\u5730\u5740"},{"location":"serving/autoscaling/container-freezer/#_2","text":"\u5b9e\u73b0\u60a8\u81ea\u5df1\u7684\u7279\u5b9a\u4e8e\u7528\u6237\u7684\u7aef\u70b9API\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u96c6\u7fa4\u4e2d\u3002 \u4f7f\u7528\u793e\u533a\u7ef4\u62a4\u7684 container-freeze \u5b9e\u73b0\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"serving/autoscaling/kpa-specific/","text":"Knative Pod Autoscaler\u7684\u989d\u5916\u81ea\u52a8\u7f29\u653e\u914d\u7f6e \u00b6 \u4ee5\u4e0b\u8bbe\u7f6e\u662f\u9488\u5bf9Knative Pod Autoscaler (KPA)\u7684\u3002 \u6a21\u5f0f \u00b6 KPA\u5bf9\u57fa\u4e8e\u65f6\u95f4\u7684\u7a97\u53e3\u4e2d\u805a\u5408\u7684 \u6307\u6807 \u8d77\u4f5c\u7528\u3002 \u8fd9\u4e9b\u7a97\u53e3\u5b9a\u4e49Autoscaler\u8003\u8651\u7684\u5386\u53f2\u6570\u636e\u91cf\uff0c\u5e76\u7528\u4e8e\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u5e73\u6ed1\u6570\u636e\u3002 \u8fd9\u4e9b\u7a97\u53e3\u8d8a\u77ed\uff0cAutoscaler\u7684\u53cd\u5e94\u5c31\u8d8a\u5feb\u3002 KPA\u7684\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f: stable and panic \u3002 \u6bcf\u4e2a\u6a21\u5f0f\u90fd\u6709\u5355\u72ec\u7684\u805a\u5408\u7a97\u53e3: stable-window and panic-window \u3002 \u7a33\u5b9a\u6a21\u5f0f\u7528\u4e8e\u4e00\u822c\u64cd\u4f5c\uff0c\u800c\u6050\u614c\u6a21\u5f0f\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6709\u4e00\u4e2a\u66f4\u77ed\u7684\u7a97\u53e3\uff0c\u5982\u679c\u6d41\u91cf\u6fc0\u589e\uff0c\u5c06\u88ab\u7528\u4e8e\u5feb\u901f\u6269\u5c55\u4fee\u8ba2\u3002 \u6ce8\u91ca \u5f53\u4f7f\u7528\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u7248\u672c\u5c06\u4e0d\u4f1a\u7f29\u5c0f\u4ee5\u907f\u514d\u6d41\u5931\u3002 \u5982\u679c\u5728\u7a33\u5b9a\u7a97\u53e3\u65f6\u95f4\u5185\u6ca1\u6709\u5feb\u901f\u53cd\u5e94\u7684\u7406\u7531\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u79bb\u5f00\u6050\u614c\u6a21\u5f0f\u3002 \u7a33\u5b9a\u7a97\u53e3 \u00b6 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5f53\u526f\u672c\u5f52\u96f6\u65f6\uff0c\u53ea\u6709\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u4efb\u4f55\u8bbf\u95ee\u4fee\u8ba2\u7248\u672c\u7684\u6d41\u91cf\u65f6\uff0c\u624d\u4f1a\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\" \u6050\u614c\u7a97\u53e3 \u00b6 \u6050\u614c\u7a97\u53e3\u88ab\u5b9a\u4e49\u4e3a\u7a33\u5b9a\u7a97\u53e3\u7684\u767e\u5206\u6bd4\uff0c\u786e\u4fdd\u4e24\u8005\u5728\u5de5\u4f5c\u65b9\u5f0f\u4e0a\u5f7c\u6b64\u76f8\u5bf9\u3002 \u6b64\u503c\u6307\u793a\u5728\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u5bf9\u5386\u53f2\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u7684\u7a97\u53e3\u5c06\u5982\u4f55\u6536\u7f29\u3002 \u4f8b\u5982\uff0c\u503c 10.0 \u610f\u5473\u7740\u5728\u7d27\u6025\u6a21\u5f0f\u4e0b\uff0c\u7a97\u53e3\u5c06\u662f\u7a33\u5b9a\u7a97\u53e3\u5927\u5c0f\u768410%\u3002 \u5168\u5c40\u952e: panic-window-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-window-percentage \u53ef\u7528\u503c: float, 1.0 <= value <= 100.0 \u9ed8\u8ba4\u503c: 10.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\" \u6050\u614c\u7684\u9608\u503c \u00b6 \u8fd9\u4e2a\u9608\u503c\u5b9a\u4e49Autoscaler\u4f55\u65f6\u4ece\u7a33\u5b9a\u6a21\u5f0f\u8f6c\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u8be5\u503c\u662f\u5f53\u524d\u526f\u672c\u6570\u91cf\u6240\u80fd\u5904\u7406\u7684\u6d41\u91cf\u7684\u767e\u5206\u6bd4\u3002 Note \u503c 100.0 (100%)\u610f\u5473\u7740Autoscaler\u603b\u662f\u5904\u4e8e\u7d27\u6025\u6a21\u5f0f\uff0c\u56e0\u6b64\u6700\u5c0f\u503c\u5e94\u8be5\u9ad8\u4e8e 100.0 \u3002 \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a 200.0 \u610f\u5473\u7740\u5982\u679c\u6d41\u91cf\u662f\u5f53\u524d\u526f\u672c\u603b\u4f53\u53ef\u4ee5\u5904\u7406\u7684\u4e24\u500d\uff0c\u5c06\u542f\u52a8\u6050\u614c\u6a21\u5f0f\u3002 \u5168\u5c40\u952e: panic-threshold-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-threshold-percentage \u53ef\u7528\u503c: float, 110.0 <= value <= 1000.0 \u9ed8\u8ba4\u503c: 200.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\" \u4f38\u7f29\u6bd4\u7387 \u00b6 \u8fd9\u4e9b\u8bbe\u7f6e\u901a\u8fc7\u5728\u5355\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u590d\u5236\u96c6\u7fa4\u53ef\u4ee5\u6269\u5927\u6216\u7f29\u5c0f\u591a\u5c11\u6765\u63a7\u5236\u3002 \u5728\u6bcf\u4e2a\u65b9\u5411\u4e0a\u603b\u662f\u5141\u8bb8\u4e00\u4e2a\u526f\u672c\u7684\u6700\u5c0f\u53d8\u5316\uff0c\u56e0\u6b64Autoscaler\u53ef\u4ee5\u968f\u65f6\u6269\u5c55\u5230+/- 1\u526f\u672c\uff0c\u800c\u4e0d\u7ba1\u8bbe\u7f6e\u7684\u7f29\u653e\u7387\u5982\u4f55\u3002 \u6269\u5927\u7387 \u00b6 \u6b64\u8bbe\u7f6e\u786e\u5b9a\u6240\u9700\u4e0e\u73b0\u6709Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u503c\u4e3a 2.0 \uff0c\u5219\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 2*N \u4e2aPods\u3002 \u5168\u5c40\u952e: max-scale-up-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 1000.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\" \u964d\u4f4e\u7387 \u00b6 \u6b64\u8bbe\u7f6e\u786e\u5b9a\u73b0\u6709Pod\u4e0e\u6240\u9700Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5f53\u503c\u4e3a 2.0 \u65f6\uff0c\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u5185\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 N/2 \u4e2apod\u3002 \u5168\u5c40\u952e: max-scale-down-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 2.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"\u989d\u5916\u914d\u7f6e"},{"location":"serving/autoscaling/kpa-specific/#knative-pod-autoscaler","text":"\u4ee5\u4e0b\u8bbe\u7f6e\u662f\u9488\u5bf9Knative Pod Autoscaler (KPA)\u7684\u3002","title":"Knative Pod Autoscaler\u7684\u989d\u5916\u81ea\u52a8\u7f29\u653e\u914d\u7f6e"},{"location":"serving/autoscaling/kpa-specific/#_1","text":"KPA\u5bf9\u57fa\u4e8e\u65f6\u95f4\u7684\u7a97\u53e3\u4e2d\u805a\u5408\u7684 \u6307\u6807 \u8d77\u4f5c\u7528\u3002 \u8fd9\u4e9b\u7a97\u53e3\u5b9a\u4e49Autoscaler\u8003\u8651\u7684\u5386\u53f2\u6570\u636e\u91cf\uff0c\u5e76\u7528\u4e8e\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u5e73\u6ed1\u6570\u636e\u3002 \u8fd9\u4e9b\u7a97\u53e3\u8d8a\u77ed\uff0cAutoscaler\u7684\u53cd\u5e94\u5c31\u8d8a\u5feb\u3002 KPA\u7684\u5b9e\u73b0\u6709\u4e24\u79cd\u6a21\u5f0f: stable and panic \u3002 \u6bcf\u4e2a\u6a21\u5f0f\u90fd\u6709\u5355\u72ec\u7684\u805a\u5408\u7a97\u53e3: stable-window and panic-window \u3002 \u7a33\u5b9a\u6a21\u5f0f\u7528\u4e8e\u4e00\u822c\u64cd\u4f5c\uff0c\u800c\u6050\u614c\u6a21\u5f0f\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6709\u4e00\u4e2a\u66f4\u77ed\u7684\u7a97\u53e3\uff0c\u5982\u679c\u6d41\u91cf\u6fc0\u589e\uff0c\u5c06\u88ab\u7528\u4e8e\u5feb\u901f\u6269\u5c55\u4fee\u8ba2\u3002 \u6ce8\u91ca \u5f53\u4f7f\u7528\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u7248\u672c\u5c06\u4e0d\u4f1a\u7f29\u5c0f\u4ee5\u907f\u514d\u6d41\u5931\u3002 \u5982\u679c\u5728\u7a33\u5b9a\u7a97\u53e3\u65f6\u95f4\u5185\u6ca1\u6709\u5feb\u901f\u53cd\u5e94\u7684\u7406\u7531\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u79bb\u5f00\u6050\u614c\u6a21\u5f0f\u3002","title":"\u6a21\u5f0f"},{"location":"serving/autoscaling/kpa-specific/#_2","text":"\u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5f53\u526f\u672c\u5f52\u96f6\u65f6\uff0c\u53ea\u6709\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u4efb\u4f55\u8bbf\u95ee\u4fee\u8ba2\u7248\u672c\u7684\u6d41\u91cf\u65f6\uff0c\u624d\u4f1a\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u7a33\u5b9a\u7a97\u53e3"},{"location":"serving/autoscaling/kpa-specific/#_3","text":"\u6050\u614c\u7a97\u53e3\u88ab\u5b9a\u4e49\u4e3a\u7a33\u5b9a\u7a97\u53e3\u7684\u767e\u5206\u6bd4\uff0c\u786e\u4fdd\u4e24\u8005\u5728\u5de5\u4f5c\u65b9\u5f0f\u4e0a\u5f7c\u6b64\u76f8\u5bf9\u3002 \u6b64\u503c\u6307\u793a\u5728\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u65f6\uff0c\u5bf9\u5386\u53f2\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u7684\u7a97\u53e3\u5c06\u5982\u4f55\u6536\u7f29\u3002 \u4f8b\u5982\uff0c\u503c 10.0 \u610f\u5473\u7740\u5728\u7d27\u6025\u6a21\u5f0f\u4e0b\uff0c\u7a97\u53e3\u5c06\u662f\u7a33\u5b9a\u7a97\u53e3\u5927\u5c0f\u768410%\u3002 \u5168\u5c40\u952e: panic-window-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-window-percentage \u53ef\u7528\u503c: float, 1.0 <= value <= 100.0 \u9ed8\u8ba4\u503c: 10.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\"","title":"\u6050\u614c\u7a97\u53e3"},{"location":"serving/autoscaling/kpa-specific/#_4","text":"\u8fd9\u4e2a\u9608\u503c\u5b9a\u4e49Autoscaler\u4f55\u65f6\u4ece\u7a33\u5b9a\u6a21\u5f0f\u8f6c\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u8be5\u503c\u662f\u5f53\u524d\u526f\u672c\u6570\u91cf\u6240\u80fd\u5904\u7406\u7684\u6d41\u91cf\u7684\u767e\u5206\u6bd4\u3002 Note \u503c 100.0 (100%)\u610f\u5473\u7740Autoscaler\u603b\u662f\u5904\u4e8e\u7d27\u6025\u6a21\u5f0f\uff0c\u56e0\u6b64\u6700\u5c0f\u503c\u5e94\u8be5\u9ad8\u4e8e 100.0 \u3002 \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a 200.0 \u610f\u5473\u7740\u5982\u679c\u6d41\u91cf\u662f\u5f53\u524d\u526f\u672c\u603b\u4f53\u53ef\u4ee5\u5904\u7406\u7684\u4e24\u500d\uff0c\u5c06\u542f\u52a8\u6050\u614c\u6a21\u5f0f\u3002 \u5168\u5c40\u952e: panic-threshold-percentage \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/panic-threshold-percentage \u53ef\u7528\u503c: float, 110.0 <= value <= 1000.0 \u9ed8\u8ba4\u503c: 200.0 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\"","title":"\u6050\u614c\u7684\u9608\u503c"},{"location":"serving/autoscaling/kpa-specific/#_5","text":"\u8fd9\u4e9b\u8bbe\u7f6e\u901a\u8fc7\u5728\u5355\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u590d\u5236\u96c6\u7fa4\u53ef\u4ee5\u6269\u5927\u6216\u7f29\u5c0f\u591a\u5c11\u6765\u63a7\u5236\u3002 \u5728\u6bcf\u4e2a\u65b9\u5411\u4e0a\u603b\u662f\u5141\u8bb8\u4e00\u4e2a\u526f\u672c\u7684\u6700\u5c0f\u53d8\u5316\uff0c\u56e0\u6b64Autoscaler\u53ef\u4ee5\u968f\u65f6\u6269\u5c55\u5230+/- 1\u526f\u672c\uff0c\u800c\u4e0d\u7ba1\u8bbe\u7f6e\u7684\u7f29\u653e\u7387\u5982\u4f55\u3002","title":"\u4f38\u7f29\u6bd4\u7387"},{"location":"serving/autoscaling/kpa-specific/#_6","text":"\u6b64\u8bbe\u7f6e\u786e\u5b9a\u6240\u9700\u4e0e\u73b0\u6709Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u503c\u4e3a 2.0 \uff0c\u5219\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u4e2d\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 2*N \u4e2aPods\u3002 \u5168\u5c40\u952e: max-scale-up-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 1000.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\"","title":"\u6269\u5927\u7387"},{"location":"serving/autoscaling/kpa-specific/#_7","text":"\u6b64\u8bbe\u7f6e\u786e\u5b9a\u73b0\u6709Pod\u4e0e\u6240\u9700Pod\u7684\u6700\u5927\u6bd4\u4f8b\u3002 \u4f8b\u5982\uff0c\u5f53\u503c\u4e3a 2.0 \u65f6\uff0c\u4fee\u8ba2\u5728\u4e00\u4e2a\u8bc4\u4f30\u5468\u671f\u5185\u53ea\u80fd\u4ece N \u6269\u5c55\u5230 N/2 \u4e2apod\u3002 \u5168\u5c40\u952e: max-scale-down-rate \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u7528\u503c: float \u9ed8\u8ba4\u503c: 2.0 \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"\u964d\u4f4e\u7387"},{"location":"serving/autoscaling/rps-target/","text":"\u914d\u7f6e\u6bcf\u79d2\u8bf7\u6c42\u6570(RPS)\u76ee\u6807 \u00b6 \u6b64\u8bbe\u7f6e\u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u6307\u5b9a\u6bcf\u79d2\u8bf7\u6c42\u6570\u7684\u76ee\u6807\u3002 \u60a8\u7684\u4fee\u8ba2\u8fd8\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 rps \u6307\u6807\u6ce8\u91ca \u3002 \u5168\u5c40\u503c: requests-per-second-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u503c: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"200\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"\u914d\u7f6eRPS"},{"location":"serving/autoscaling/rps-target/#rps","text":"\u6b64\u8bbe\u7f6e\u4e3a\u5e94\u7528\u7a0b\u5e8f\u7684\u6bcf\u4e2a\u526f\u672c\u6307\u5b9a\u6bcf\u79d2\u8bf7\u6c42\u6570\u7684\u76ee\u6807\u3002 \u60a8\u7684\u4fee\u8ba2\u8fd8\u5fc5\u987b\u914d\u7f6e\u4e3a\u4f7f\u7528 rps \u6307\u6807\u6ce8\u91ca \u3002 \u5168\u5c40\u503c: requests-per-second-target-default \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u503c: autoscaling.knative.dev/target \u53ef\u7528\u503c: An integer. \u9ed8\u8ba4\u503c: \"200\" \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"\u914d\u7f6e\u6bcf\u79d2\u8bf7\u6c42\u6570(RPS)\u76ee\u6807"},{"location":"serving/autoscaling/scale-bounds/","text":"\u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u00b6 \u60a8\u53ef\u4ee5\u914d\u7f6e\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u63a7\u5236\u81ea\u52a8\u7f29\u653e\u884c\u4e3a\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5728\u521b\u5efa\u4e4b\u540e\u7acb\u5373\u5c06\u4fee\u8ba2\u6269\u5c55\u5230\u7684\u521d\u59cb\u89c4\u6a21\u3002 \u8fd9\u53ef\u4ee5\u662f\u6240\u6709\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u662f\u4f7f\u7528\u6ce8\u91ca\u7684\u7279\u5b9a\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002 \u4e0b\u754c \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 Knative\u5c06\u5c1d\u8bd5\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u90fd\u4e0d\u5c11\u4e8e\u8fd9\u4e2a\u6570\u91cf\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: min-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/min-scale \u53ef\u80fd\u503c: integer \u9ed8\u8ba4\u503c: 0 \u5982\u679c\u542f\u7528\u4e86scale-to-zero\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86KPA\u7c7b\uff0c\u5219\u4e3a1 Note \u6709\u5173\u7f29\u96f6\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7f29\u96f6\u914d\u7f6e \u6587\u6863\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\" \u4e0a\u5c4a \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5927\u6570\u91cf\u3002 Knative\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u8fd0\u884c\u7684\u526f\u672c\u6216\u6b63\u5728\u521b\u5efa\u7684\u526f\u672c\u7684\u6570\u91cf\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u6570\u76ee\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 max-scale-limit \u5168\u5c40\u952e\uff0cKnative\u5c06\u786e\u4fdd\u5168\u5c40\u6700\u5927\u523b\u5ea6\u548c\u6bcf\u4e2a\u65b0\u4fee\u8ba2\u7248\u672c\u7684\u6700\u5927\u523b\u5ea6\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u5f53 max-scale-limit \u8bbe\u7f6e\u4e3a\u6b63\u503c\u65f6\uff0c\u4e0d\u5141\u8bb8\u6700\u5927\u523b\u5ea6\u9ad8\u4e8e\u8be5\u503c(\u5305\u62ec0\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9650)\u7684\u4fee\u8ba2\u3002 \u5168\u5c40\u952e: max-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/max-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 \u8fd9\u610f\u5473\u7740\u65e0\u9650\u7684 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\" \u521d\u59cb\u7f29\u653e \u00b6 \u8fd9\u4e2a\u503c\u63a7\u5236\u4e86\u4e00\u4e2a\u4fee\u8ba2\u5728\u88ab\u6807\u8bb0\u4e3a Ready \u4e4b\u524d\u5fc5\u987b\u7acb\u5373\u8fbe\u5230\u7684\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u3002 \u5728\u4fee\u8ba2\u4e00\u6b21\u8fbe\u5230\u6b64\u89c4\u6a21\u540e\uff0c\u8be5\u503c\u5c06\u88ab\u5ffd\u7565\u3002 \u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u5b9e\u9645\u63a5\u6536\u7684\u6d41\u91cf\u53ea\u9700\u8981\u8f83\u5c0f\u7684\u89c4\u6a21\uff0c\u5219\u5728\u8fbe\u5230\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u540e\uff0c\u4fee\u8ba2\u5c06\u9010\u6b65\u7f29\u5c0f\u3002 \u5728\u521b\u5efa\u4fee\u8ba2\u65f6\uff0c\u81ea\u52a8\u9009\u62e9\u521d\u59cb\u6807\u5ea6\u548c\u4e0b\u9650\u4e2d\u8f83\u5927\u7684\u4f5c\u4e3a\u521d\u59cb\u76ee\u6807\u6807\u5ea6\u3002 \u5168\u5c40\u952e: initial-scale \u7ed3\u5408 allow-zero-initial-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/initial-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\" \u6269\u5927\u6700\u4f4e \u00b6 \u6b64\u503c\u63a7\u5236\u5f53\u4fee\u8ba2\u4ece\u96f6\u6269\u5c55\u65f6\u5c06\u521b\u5efa\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 \u5168\u5c40\u952e: n/a \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/activation-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/activation-scale : \"5\" spec : containers : - image : gcr.io/knative-samples/helloworld-go \u7f29\u5ef6\u8fdf \u00b6 \u7f29\u5ef6\u8fdf\u6307\u5b9a\u5728\u5e94\u7528\u7f29\u5c0f\u51b3\u7b56\u4e4b\u524d\uff0c\u5728\u5e76\u53d1\u51cf\u5c11\u65f6\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u7a97\u53e3\u3002 \u8fd9\u53ef\u80fd\u5f88\u6709\u7528\uff0c\u4f8b\u5982\uff0c\u5728\u53ef\u914d\u7f6e\u7684\u6301\u7eed\u65f6\u95f4\u5185\u4fdd\u7559\u5bb9\u5668\uff0c\u4ee5\u907f\u514d\u5728\u65b0\u8bf7\u6c42\u8fdb\u5165\u65f6\u51fa\u73b0\u51b7\u542f\u52a8\u60e9\u7f5a\u3002 \u4e0e\u8bbe\u7f6e\u4e0b\u9650\u4e0d\u540c\u7684\u662f\uff0c\u5982\u679c\u5728\u5ef6\u8fdf\u671f\u95f4\u7ef4\u6301\u8f83\u4f4e\u7684\u5e76\u53d1\u6027\uff0c\u5219\u4fee\u8ba2\u6700\u7ec8\u5c06\u88ab\u7f29\u5c0f\u3002 Note \u53ea\u652f\u6301\u9ed8\u8ba4\u7684KPA\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u3002 \u5168\u5c40\u952e: scale-down-delay \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/scale-down-delay \u53ef\u7528\u503c: Duration, 0s <= value <= 1h \u9ed8\u8ba4\u503c: 0s (no delay) \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\" \u7a33\u5b9a\u7a97\u53e3 \u00b6 \u7a33\u5b9a\u7a97\u53e3\u5b9a\u4e49\u6ed1\u52a8\u65f6\u95f4\u7a97\u53e3\uff0c\u5f53\u81ea\u52a8\u7f29\u653e\u5668\u4e0d\u5904\u4e8e Panic mode \u65f6\uff0c\u5728\u8be5\u65f6\u95f4\u7a97\u53e3\u4e0a\u5bf9\u6307\u6807\u8fdb\u884c\u5e73\u5747\uff0c\u4ee5\u63d0\u4f9b\u7f29\u653e\u51b3\u7b56\u7684\u8f93\u5165\u3002 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5728\u7f29\u5c0f\u671f\u95f4\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u8bbf\u95ee\u4fee\u8ba2\u7248\u7684\u6d41\u91cf\u540e\uff0c\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u88ab\u5220\u9664\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u914d\u7f6e\u8fb9\u754c"},{"location":"serving/autoscaling/scale-bounds/#_1","text":"\u60a8\u53ef\u4ee5\u914d\u7f6e\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u63a7\u5236\u81ea\u52a8\u7f29\u653e\u884c\u4e3a\u3002 \u60a8\u8fd8\u53ef\u4ee5\u6307\u5b9a\u5728\u521b\u5efa\u4e4b\u540e\u7acb\u5373\u5c06\u4fee\u8ba2\u6269\u5c55\u5230\u7684\u521d\u59cb\u89c4\u6a21\u3002 \u8fd9\u53ef\u4ee5\u662f\u6240\u6709\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u662f\u4f7f\u7528\u6ce8\u91ca\u7684\u7279\u5b9a\u4fee\u8ba2\u7684\u9ed8\u8ba4\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u7f29\u653e\u8fb9\u754c"},{"location":"serving/autoscaling/scale-bounds/#_2","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 Knative\u5c06\u5c1d\u8bd5\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u90fd\u4e0d\u5c11\u4e8e\u8fd9\u4e2a\u6570\u91cf\u7684\u526f\u672c\u3002 \u5168\u5c40\u952e: min-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/min-scale \u53ef\u80fd\u503c: integer \u9ed8\u8ba4\u503c: 0 \u5982\u679c\u542f\u7528\u4e86scale-to-zero\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86KPA\u7c7b\uff0c\u5219\u4e3a1 Note \u6709\u5173\u7f29\u96f6\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 \u7f29\u96f6\u914d\u7f6e \u6587\u6863\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\"","title":"\u4e0b\u754c"},{"location":"serving/autoscaling/scale-bounds/#_3","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u6bcf\u4e2a\u4fee\u8ba2\u7248\u672c\u5e94\u8be5\u62e5\u6709\u7684\u526f\u672c\u7684\u6700\u5927\u6570\u91cf\u3002 Knative\u5728\u4efb\u4f55\u65f6\u95f4\u70b9\u4e0a\u8fd0\u884c\u7684\u526f\u672c\u6216\u6b63\u5728\u521b\u5efa\u7684\u526f\u672c\u7684\u6570\u91cf\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u6570\u76ee\u3002 \u5982\u679c\u8bbe\u7f6e\u4e86 max-scale-limit \u5168\u5c40\u952e\uff0cKnative\u5c06\u786e\u4fdd\u5168\u5c40\u6700\u5927\u523b\u5ea6\u548c\u6bcf\u4e2a\u65b0\u4fee\u8ba2\u7248\u672c\u7684\u6700\u5927\u523b\u5ea6\u90fd\u4e0d\u4f1a\u8d85\u8fc7\u8fd9\u4e2a\u503c\u3002 \u5f53 max-scale-limit \u8bbe\u7f6e\u4e3a\u6b63\u503c\u65f6\uff0c\u4e0d\u5141\u8bb8\u6700\u5927\u523b\u5ea6\u9ad8\u4e8e\u8be5\u503c(\u5305\u62ec0\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9650)\u7684\u4fee\u8ba2\u3002 \u5168\u5c40\u952e: max-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/max-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 0 \u8fd9\u610f\u5473\u7740\u65e0\u9650\u7684 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\"","title":"\u4e0a\u5c4a"},{"location":"serving/autoscaling/scale-bounds/#_4","text":"\u8fd9\u4e2a\u503c\u63a7\u5236\u4e86\u4e00\u4e2a\u4fee\u8ba2\u5728\u88ab\u6807\u8bb0\u4e3a Ready \u4e4b\u524d\u5fc5\u987b\u7acb\u5373\u8fbe\u5230\u7684\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u3002 \u5728\u4fee\u8ba2\u4e00\u6b21\u8fbe\u5230\u6b64\u89c4\u6a21\u540e\uff0c\u8be5\u503c\u5c06\u88ab\u5ffd\u7565\u3002 \u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u5b9e\u9645\u63a5\u6536\u7684\u6d41\u91cf\u53ea\u9700\u8981\u8f83\u5c0f\u7684\u89c4\u6a21\uff0c\u5219\u5728\u8fbe\u5230\u521d\u59cb\u76ee\u6807\u89c4\u6a21\u540e\uff0c\u4fee\u8ba2\u5c06\u9010\u6b65\u7f29\u5c0f\u3002 \u5728\u521b\u5efa\u4fee\u8ba2\u65f6\uff0c\u81ea\u52a8\u9009\u62e9\u521d\u59cb\u6807\u5ea6\u548c\u4e0b\u9650\u4e2d\u8f83\u5927\u7684\u4f5c\u4e3a\u521d\u59cb\u76ee\u6807\u6807\u5ea6\u3002 \u5168\u5c40\u952e: initial-scale \u7ed3\u5408 allow-zero-initial-scale \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/initial-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\"","title":"\u521d\u59cb\u7f29\u653e"},{"location":"serving/autoscaling/scale-bounds/#_5","text":"\u6b64\u503c\u63a7\u5236\u5f53\u4fee\u8ba2\u4ece\u96f6\u6269\u5c55\u65f6\u5c06\u521b\u5efa\u7684\u526f\u672c\u7684\u6700\u5c0f\u6570\u91cf\u3002 \u5168\u5c40\u952e: n/a \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/activation-scale \u53ef\u7528\u503c: integer \u9ed8\u8ba4\u503c: 1 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/activation-scale : \"5\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u6269\u5927\u6700\u4f4e"},{"location":"serving/autoscaling/scale-bounds/#_6","text":"\u7f29\u5ef6\u8fdf\u6307\u5b9a\u5728\u5e94\u7528\u7f29\u5c0f\u51b3\u7b56\u4e4b\u524d\uff0c\u5728\u5e76\u53d1\u51cf\u5c11\u65f6\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u7a97\u53e3\u3002 \u8fd9\u53ef\u80fd\u5f88\u6709\u7528\uff0c\u4f8b\u5982\uff0c\u5728\u53ef\u914d\u7f6e\u7684\u6301\u7eed\u65f6\u95f4\u5185\u4fdd\u7559\u5bb9\u5668\uff0c\u4ee5\u907f\u514d\u5728\u65b0\u8bf7\u6c42\u8fdb\u5165\u65f6\u51fa\u73b0\u51b7\u542f\u52a8\u60e9\u7f5a\u3002 \u4e0e\u8bbe\u7f6e\u4e0b\u9650\u4e0d\u540c\u7684\u662f\uff0c\u5982\u679c\u5728\u5ef6\u8fdf\u671f\u95f4\u7ef4\u6301\u8f83\u4f4e\u7684\u5e76\u53d1\u6027\uff0c\u5219\u4fee\u8ba2\u6700\u7ec8\u5c06\u88ab\u7f29\u5c0f\u3002 Note \u53ea\u652f\u6301\u9ed8\u8ba4\u7684KPA\u81ea\u52a8\u7f29\u653e\u5668\u7c7b\u3002 \u5168\u5c40\u952e: scale-down-delay \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/scale-down-delay \u53ef\u7528\u503c: Duration, 0s <= value <= 1h \u9ed8\u8ba4\u503c: 0s (no delay) \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"\u7f29\u5ef6\u8fdf"},{"location":"serving/autoscaling/scale-bounds/#_7","text":"\u7a33\u5b9a\u7a97\u53e3\u5b9a\u4e49\u6ed1\u52a8\u65f6\u95f4\u7a97\u53e3\uff0c\u5f53\u81ea\u52a8\u7f29\u653e\u5668\u4e0d\u5904\u4e8e Panic mode \u65f6\uff0c\u5728\u8be5\u65f6\u95f4\u7a97\u53e3\u4e0a\u5bf9\u6307\u6807\u8fdb\u884c\u5e73\u5747\uff0c\u4ee5\u63d0\u4f9b\u7f29\u653e\u51b3\u7b56\u7684\u8f93\u5165\u3002 \u5168\u5c40\u952e: stable-window \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: autoscaling.knative.dev/window \u53ef\u7528\u503c: Duration, 6s <= value <= 1h \u9ed8\u8ba4\u503c: 60s Note \u5728\u7f29\u5c0f\u671f\u95f4\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u5728\u7a33\u5b9a\u7a97\u53e3\u7684\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u6ca1\u6709\u8bbf\u95ee\u4fee\u8ba2\u7248\u7684\u6d41\u91cf\u540e\uff0c\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u88ab\u5220\u9664\u3002 \u4e3e\u4f8b: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u7a33\u5b9a\u7a97\u53e3"},{"location":"serving/autoscaling/scale-to-zero/","text":"\u914d\u7f6e\u7f29\u96f6 \u00b6 Warning \u53ea\u6709\u5728\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\uff0c\u624d\u80fd\u542f\u7528\u7f29\u653e\u5230\u96f6\uff0c\u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002\u6709\u5173\u4f7f\u7528KPA\u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684Autoscaler\u7c7b\u578b \u7684\u6587\u6863\u3002 \u7f29\u653e\u5230\u96f6 \u00b6 \u7f29\u653e\u5230\u96f6\u7684\u503c\u63a7\u5236Knative\u662f\u5426\u5141\u8bb8\u526f\u672c\u7f29\u5c0f\u5230\u96f6(\u5982\u679c\u8bbe\u7f6e\u4e3a true )\uff0c\u6216\u5982\u679c\u8bbe\u7f6e\u4e3a false \uff0c\u57281\u4e2a\u526f\u672c\u65f6\u505c\u6b62\u3002 Note \u6709\u5173\u6bcf\u4fee\u8ba2\u7f29\u653e\u8fb9\u754c\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u7684\u6587\u6863\u3002 \u5168\u5c40\u952e: enable-scale-to-zero \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: No per-revision setting. \u53ef\u80fd\u503c: boolean \u9ed8\u8ba4\u7684: true \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\" \u7f29\u96f6\u5bbd\u9650\u671f \u00b6 \u6b64\u8bbe\u7f6e\u6307\u5b9a\u4e86\u4e00\u4e2a\u4e0a\u9650\u65f6\u95f4\u9650\u5236\uff0c\u5728\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u4e4b\u524d\uff0c\u7cfb\u7edf\u5c06\u5728\u5185\u90e8\u7b49\u5f85\u4ece\u96f6\u5f00\u59cb\u6269\u5c55\u673a\u5236\u5230\u4f4d\u3002 Warning \u8fd9\u662f\u4e00\u4e2a\u63a7\u5236\u5141\u8bb8\u5185\u90e8\u7f51\u7edc\u7f16\u7a0b\u7684\u65f6\u95f4\u7684\u503c\uff0c\u53ea\u6709\u5f53\u60a8\u9047\u5230\u5728\u4fee\u8ba2\u7f29\u5230\u96f6\u526f\u672c\u65f6\u8bf7\u6c42\u88ab\u4e22\u5f03\u7684\u95ee\u9898\u65f6\uff0c\u624d\u5e94\u8be5\u8c03\u6574\u8be5\u503c\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u4f1a\u8c03\u6574\u6d41\u91cf\u7ed3\u675f\u540e\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u4fdd\u7559\u591a\u957f\u65f6\u95f4\uff0c\u4e5f\u4e0d\u4fdd\u8bc1\u5728\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u5b9e\u9645\u4fdd\u7559\u8be5\u526f\u672c\u3002 \u5168\u5c40\u952e: scale-to-zero-grace-period \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u80fd\u503c: Duration \u9ed8\u8ba4\u503c: 30s \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\" \u7f29\u96f6\u4fdd\u7559\u671f \u00b6 scale-to-zero-pod-retention-period \u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u5c06\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u5c0f\u65f6\u95f4\u3002 \u8fd9\u4e0e scale-to-zero-grace-period \u6807\u5fd7\u5f62\u6210\u5bf9\u6bd4\uff0c\u8be5\u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u957f\u65f6\u95f4\u3002 Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: \u975e\u8d1f\u65f6\u95f4\u5b57\u7b26\u4e32 Default: 0s Example: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"\u914d\u7f6e\u7f29\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_1","text":"Warning \u53ea\u6709\u5728\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\uff0c\u624d\u80fd\u542f\u7528\u7f29\u653e\u5230\u96f6\uff0c\u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002\u6709\u5173\u4f7f\u7528KPA\u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u652f\u6301\u7684Autoscaler\u7c7b\u578b \u7684\u6587\u6863\u3002","title":"\u914d\u7f6e\u7f29\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_2","text":"\u7f29\u653e\u5230\u96f6\u7684\u503c\u63a7\u5236Knative\u662f\u5426\u5141\u8bb8\u526f\u672c\u7f29\u5c0f\u5230\u96f6(\u5982\u679c\u8bbe\u7f6e\u4e3a true )\uff0c\u6216\u5982\u679c\u8bbe\u7f6e\u4e3a false \uff0c\u57281\u4e2a\u526f\u672c\u65f6\u505c\u6b62\u3002 Note \u6709\u5173\u6bcf\u4fee\u8ba2\u7f29\u653e\u8fb9\u754c\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5173\u4e8e \u914d\u7f6e\u7f29\u653e\u8fb9\u754c \u7684\u6587\u6863\u3002 \u5168\u5c40\u952e: enable-scale-to-zero \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: No per-revision setting. \u53ef\u80fd\u503c: boolean \u9ed8\u8ba4\u7684: true \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\"","title":"\u7f29\u653e\u5230\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_3","text":"\u6b64\u8bbe\u7f6e\u6307\u5b9a\u4e86\u4e00\u4e2a\u4e0a\u9650\u65f6\u95f4\u9650\u5236\uff0c\u5728\u5220\u9664\u6700\u540e\u4e00\u4e2a\u526f\u672c\u4e4b\u524d\uff0c\u7cfb\u7edf\u5c06\u5728\u5185\u90e8\u7b49\u5f85\u4ece\u96f6\u5f00\u59cb\u6269\u5c55\u673a\u5236\u5230\u4f4d\u3002 Warning \u8fd9\u662f\u4e00\u4e2a\u63a7\u5236\u5141\u8bb8\u5185\u90e8\u7f51\u7edc\u7f16\u7a0b\u7684\u65f6\u95f4\u7684\u503c\uff0c\u53ea\u6709\u5f53\u60a8\u9047\u5230\u5728\u4fee\u8ba2\u7f29\u5230\u96f6\u526f\u672c\u65f6\u8bf7\u6c42\u88ab\u4e22\u5f03\u7684\u95ee\u9898\u65f6\uff0c\u624d\u5e94\u8be5\u8c03\u6574\u8be5\u503c\u3002 \u6b64\u8bbe\u7f6e\u4e0d\u4f1a\u8c03\u6574\u6d41\u91cf\u7ed3\u675f\u540e\u6700\u540e\u4e00\u4e2a\u526f\u672c\u5c06\u4fdd\u7559\u591a\u957f\u65f6\u95f4\uff0c\u4e5f\u4e0d\u4fdd\u8bc1\u5728\u6574\u4e2a\u6301\u7eed\u65f6\u95f4\u5185\u5b9e\u9645\u4fdd\u7559\u8be5\u526f\u672c\u3002 \u5168\u5c40\u952e: scale-to-zero-grace-period \u6bcf\u4fee\u8ba2\u6ce8\u91ca\u952e: n/a \u53ef\u80fd\u503c: Duration \u9ed8\u8ba4\u503c: 30s \u4e3e\u4f8b: \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\"","title":"\u7f29\u96f6\u5bbd\u9650\u671f"},{"location":"serving/autoscaling/scale-to-zero/#_4","text":"scale-to-zero-pod-retention-period \u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u5c06\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u5c0f\u65f6\u95f4\u3002 \u8fd9\u4e0e scale-to-zero-grace-period \u6807\u5fd7\u5f62\u6210\u5bf9\u6bd4\uff0c\u8be5\u6807\u5fd7\u51b3\u5b9a\u4e86\u5728\u81ea\u52a8\u7f29\u653e\u5668\u51b3\u5b9a\u5c06Pod\u7f29\u5230\u96f6\u540e\uff0c\u6700\u540e\u4e00\u4e2aPod\u4fdd\u6301\u6d3b\u52a8\u7684\u6700\u957f\u65f6\u95f4\u3002 Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: \u975e\u8d1f\u65f6\u95f4\u5b57\u7b26\u4e32 Default: 0s Example: \u6bcf\u4fee\u8ba2 \u5168\u5c40 (ConfigMap) \u5168\u5c40 (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"\u7f29\u96f6\u4fdd\u7559\u671f"},{"location":"serving/autoscaling/autoscale-go/","text":"\u81ea\u52a8\u7f29\u653e\u6837\u672c\u5e94\u7528\u7a0b\u5e8f-Go \u00b6 Knative\u670d\u52a1\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u5b89\u88c5\u4e86 Knative\u670d\u52a1 \u7684Kubernetes\u96c6\u7fa4 . \u5b89\u88c5\u4e86 hey \u8d1f\u8f7d\u751f\u6210\u5668 ( go get -u github.com/rakyll/hey ). \u514b\u9686\u8fd9\u4e2a\u5b58\u50a8\u5e93\uff0c\u5e76\u79fb\u52a8\u5230\u793a\u4f8b\u76ee\u5f55: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs \u90e8\u7f72\u670d\u52a1 \u00b6 \u90e8\u7f72 \u6837\u4f8b Knative\u670d\u52a1: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml \u83b7\u53d6\u670d\u52a1\u7684URL(\u4e00\u65e6\u4e3a Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True \u52a0\u8f7d\u670d\u52a1 \u00b6 \u5411\u81ea\u52a8\u4f38\u7f29\u5e94\u7528\u7a0b\u5e8f\u53d1\u51fa\u4e00\u4e2a\u8bf7\u6c42\uff0c\u4ee5\u67e5\u770b\u5b83\u6d88\u8017\u4e00\u4e9b\u8d44\u6e90\u3002 curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. \u53d1\u900130\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u630150\u4e2a\u98de\u884c\u8bf7\u6c42\u3002 hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s \u5206\u6790 \u00b6 \u7b97\u6cd5 \u00b6 Knative\u670d\u52a1\u81ea\u52a8\u4f38\u7f29\u662f\u57fa\u4e8e\u6bcf\u4e2aPod\u7684\u98de\u884c\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf(\u5e76\u53d1\u6027)\u3002 \u7cfb\u7edf\u9ed8\u8ba4 \u76ee\u6807\u5e76\u53d1\u6570\u4e3a100 (\u641c\u7d22 container-concurrency-target-default),\u4f46\u6211\u4eec\u7684\u670d\u52a1 \u7528\u4e8610 \u3002 \u6211\u4eec\u52a0\u8f7d\u4e8650\u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u670d\u52a1\uff0c\u56e0\u6b64\u81ea\u52a8\u7f29\u653e\u5668\u521b\u5efa\u4e865\u4e2aPod( 50 concurrent requests / target of 10 = 5 pods ) \u6050\u614c \u00b6 \u81ea\u52a8\u8ba1\u7b97\u5668\u572860\u79d2\u7684\u7a97\u53e3\u5185\u8ba1\u7b97\u5e73\u5747\u5e76\u53d1\u6027\uff0c\u56e0\u6b64\u7cfb\u7edf\u9700\u8981\u4e00\u5206\u949f\u65f6\u95f4\u624d\u80fd\u7a33\u5b9a\u5728\u6240\u9700\u7684\u5e76\u53d1\u6027\u6c34\u5e73\u4e0a\u3002 \u7136\u800c\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u4e5f\u4f1a\u8ba1\u7b97\u4e00\u4e2a6\u79d2\u7684\u201c\u6050\u614c\u201d\u7a97\u53e3\uff0c\u5982\u679c\u8be5\u7a97\u53e3\u8fbe\u5230\u76ee\u6807\u5e76\u53d1\u6570\u76842\u500d\uff0c\u5c31\u4f1a\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u5728\u6050\u614c\u6a21\u5f0f\u4e0b\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5728\u66f4\u77ed\u3001\u66f4\u654f\u611f\u7684\u6050\u614c\u7a97\u53e3\u4e0a\u64cd\u4f5c\u3002 \u4e00\u65e6\u6050\u614c\u6761\u4ef6\u572860\u79d2\u5185\u4e0d\u518d\u6ee1\u8db3\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u8fd4\u56de\u5230\u6700\u521d\u768460\u79d2\u201c\u7a33\u5b9a\u201d\u7a97\u53e3\u3002 | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME \u5b9a\u5236 \u00b6 \u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u901a\u8fc7\u6ce8\u91ca\u8fdb\u884c\u5b9a\u5236\u3002Knative\u4e2d\u5185\u7f6e\u4e86\u4e24\u4e2a\u81ea\u52a8\u7f29\u653e\u7c7b: kpa.autoscaling.knative.dev \u5b83\u662f\u524d\u9762\u63cf\u8ff0\u7684\u57fa\u4e8e\u5e76\u53d1\u7684\u81ea\u52a8\u7f29\u653e\u5668(\u9ed8\u8ba4\u503c), \u548c hpa.autoscaling.knative.dev \u5b83\u59d4\u6258\u7ed9Kubernetes HPA\uff0c\u5b83\u4f1a\u6839\u636eCPU\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29. \u5728CPU\u4e0a\u6269\u5c55\u670d\u52a1\u7684\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 \u6b64\u5916\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u76ee\u6807\u548c\u7f29\u653e\u8fb9\u754c\u53ef\u4ee5\u5728\u6ce8\u91ca\u4e2d\u6307\u5b9a\u3002\u5177\u6709\u81ea\u5b9a\u4e49\u76ee\u6807\u548c\u89c4\u6a21\u8fb9\u754c\u7684\u670d\u52a1\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note \u5bf9\u4e8e hpa.autoscaling.knative.dev \u7c7b\u670d\u52a1\uff0c autoscaling.knative.dev/target \u6307\u5b9aCPU\u767e\u5206\u6bd4\u76ee\u6807(\u9ed8\u8ba4\u4e3a \"80\" )\u3002 \u6f14\u793a \u00b6 \u67e5\u770bKnative\u81ea\u52a8\u7f29\u653e\u81ea\u5b9a\u4e49\u7684 Kubecon\u6f14\u793a (32\u5206\u949f)\u3002 \u5176\u4ed6\u7684\u5b9e\u9a8c \u00b6 \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u6301100\u4e2a\u5e76\u53d1\u8bf7\u6c42\u3002 hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u77ed\u8bf7\u6c42(10\u6beb\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u957f\u8bf7\u6c42(1\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" \u53d1\u900160\u79d2\u7684\u9ad8CPU\u4f7f\u7528\u7387\u7684\u6d41\u91cf(~1 cpu/sec/request\uff0c\u603b\u5171100\u4e2aCPU)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" \u53d1\u900160\u79d2\u7684\u9ad8\u5185\u5b58\u4f7f\u7528\u6d41\u91cf(\u6bcf\u8bf7\u6c421gb\uff0c\u51715gb)\u3002 hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\" \u6e05\u7406 \u00b6 kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml \u8fdb\u4e00\u6b65\u7684\u9605\u8bfb \u00b6 \u81ea\u52a8\u5b9a\u91cf\u5f00\u53d1\u4eba\u5458\u6587\u6863","title":"\u6837\u672c\u5e94\u7528"},{"location":"serving/autoscaling/autoscale-go/#-go","text":"Knative\u670d\u52a1\u4fee\u8ba2\u7684\u81ea\u52a8\u7f29\u653e\u529f\u80fd\u7684\u6f14\u793a\u3002","title":"\u81ea\u52a8\u7f29\u653e\u6837\u672c\u5e94\u7528\u7a0b\u5e8f-Go"},{"location":"serving/autoscaling/autoscale-go/#_1","text":"\u5b89\u88c5\u4e86 Knative\u670d\u52a1 \u7684Kubernetes\u96c6\u7fa4 . \u5b89\u88c5\u4e86 hey \u8d1f\u8f7d\u751f\u6210\u5668 ( go get -u github.com/rakyll/hey ). \u514b\u9686\u8fd9\u4e2a\u5b58\u50a8\u5e93\uff0c\u5e76\u79fb\u52a8\u5230\u793a\u4f8b\u76ee\u5f55: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"serving/autoscaling/autoscale-go/#_2","text":"\u90e8\u7f72 \u6837\u4f8b Knative\u670d\u52a1: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml \u83b7\u53d6\u670d\u52a1\u7684URL(\u4e00\u65e6\u4e3a Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True","title":"\u90e8\u7f72\u670d\u52a1"},{"location":"serving/autoscaling/autoscale-go/#_3","text":"\u5411\u81ea\u52a8\u4f38\u7f29\u5e94\u7528\u7a0b\u5e8f\u53d1\u51fa\u4e00\u4e2a\u8bf7\u6c42\uff0c\u4ee5\u67e5\u770b\u5b83\u6d88\u8017\u4e00\u4e9b\u8d44\u6e90\u3002 curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. \u53d1\u900130\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u630150\u4e2a\u98de\u884c\u8bf7\u6c42\u3002 hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s","title":"\u52a0\u8f7d\u670d\u52a1"},{"location":"serving/autoscaling/autoscale-go/#_4","text":"","title":"\u5206\u6790"},{"location":"serving/autoscaling/autoscale-go/#_5","text":"Knative\u670d\u52a1\u81ea\u52a8\u4f38\u7f29\u662f\u57fa\u4e8e\u6bcf\u4e2aPod\u7684\u98de\u884c\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf(\u5e76\u53d1\u6027)\u3002 \u7cfb\u7edf\u9ed8\u8ba4 \u76ee\u6807\u5e76\u53d1\u6570\u4e3a100 (\u641c\u7d22 container-concurrency-target-default),\u4f46\u6211\u4eec\u7684\u670d\u52a1 \u7528\u4e8610 \u3002 \u6211\u4eec\u52a0\u8f7d\u4e8650\u4e2a\u5e76\u53d1\u8bf7\u6c42\u7684\u670d\u52a1\uff0c\u56e0\u6b64\u81ea\u52a8\u7f29\u653e\u5668\u521b\u5efa\u4e865\u4e2aPod( 50 concurrent requests / target of 10 = 5 pods )","title":"\u7b97\u6cd5"},{"location":"serving/autoscaling/autoscale-go/#_6","text":"\u81ea\u52a8\u8ba1\u7b97\u5668\u572860\u79d2\u7684\u7a97\u53e3\u5185\u8ba1\u7b97\u5e73\u5747\u5e76\u53d1\u6027\uff0c\u56e0\u6b64\u7cfb\u7edf\u9700\u8981\u4e00\u5206\u949f\u65f6\u95f4\u624d\u80fd\u7a33\u5b9a\u5728\u6240\u9700\u7684\u5e76\u53d1\u6027\u6c34\u5e73\u4e0a\u3002 \u7136\u800c\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u4e5f\u4f1a\u8ba1\u7b97\u4e00\u4e2a6\u79d2\u7684\u201c\u6050\u614c\u201d\u7a97\u53e3\uff0c\u5982\u679c\u8be5\u7a97\u53e3\u8fbe\u5230\u76ee\u6807\u5e76\u53d1\u6570\u76842\u500d\uff0c\u5c31\u4f1a\u8fdb\u5165\u6050\u614c\u6a21\u5f0f\u3002 \u5728\u6050\u614c\u6a21\u5f0f\u4e0b\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5728\u66f4\u77ed\u3001\u66f4\u654f\u611f\u7684\u6050\u614c\u7a97\u53e3\u4e0a\u64cd\u4f5c\u3002 \u4e00\u65e6\u6050\u614c\u6761\u4ef6\u572860\u79d2\u5185\u4e0d\u518d\u6ee1\u8db3\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u5c06\u8fd4\u56de\u5230\u6700\u521d\u768460\u79d2\u201c\u7a33\u5b9a\u201d\u7a97\u53e3\u3002 | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME","title":"\u6050\u614c"},{"location":"serving/autoscaling/autoscale-go/#_7","text":"\u81ea\u52a8\u7f29\u653e\u5668\u652f\u6301\u901a\u8fc7\u6ce8\u91ca\u8fdb\u884c\u5b9a\u5236\u3002Knative\u4e2d\u5185\u7f6e\u4e86\u4e24\u4e2a\u81ea\u52a8\u7f29\u653e\u7c7b: kpa.autoscaling.knative.dev \u5b83\u662f\u524d\u9762\u63cf\u8ff0\u7684\u57fa\u4e8e\u5e76\u53d1\u7684\u81ea\u52a8\u7f29\u653e\u5668(\u9ed8\u8ba4\u503c), \u548c hpa.autoscaling.knative.dev \u5b83\u59d4\u6258\u7ed9Kubernetes HPA\uff0c\u5b83\u4f1a\u6839\u636eCPU\u4f7f\u7528\u81ea\u52a8\u4f38\u7f29. \u5728CPU\u4e0a\u6269\u5c55\u670d\u52a1\u7684\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 \u6b64\u5916\uff0c\u81ea\u52a8\u7f29\u653e\u5668\u76ee\u6807\u548c\u7f29\u653e\u8fb9\u754c\u53ef\u4ee5\u5728\u6ce8\u91ca\u4e2d\u6307\u5b9a\u3002\u5177\u6709\u81ea\u5b9a\u4e49\u76ee\u6807\u548c\u89c4\u6a21\u8fb9\u754c\u7684\u670d\u52a1\u793a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note \u5bf9\u4e8e hpa.autoscaling.knative.dev \u7c7b\u670d\u52a1\uff0c autoscaling.knative.dev/target \u6307\u5b9aCPU\u767e\u5206\u6bd4\u76ee\u6807(\u9ed8\u8ba4\u4e3a \"80\" )\u3002","title":"\u5b9a\u5236"},{"location":"serving/autoscaling/autoscale-go/#_8","text":"\u67e5\u770bKnative\u81ea\u52a8\u7f29\u653e\u81ea\u5b9a\u4e49\u7684 Kubecon\u6f14\u793a (32\u5206\u949f)\u3002","title":"\u6f14\u793a"},{"location":"serving/autoscaling/autoscale-go/#_9","text":"\u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u7ef4\u6301100\u4e2a\u5e76\u53d1\u8bf7\u6c42\u3002 hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u77ed\u8bf7\u6c42(10\u6beb\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" \u53d1\u900160\u79d2\u7684\u6d41\u91cf\uff0c\u4fdd\u6301100 qps\u7684\u957f\u8bf7\u6c42(1\u79d2)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" \u53d1\u900160\u79d2\u7684\u9ad8CPU\u4f7f\u7528\u7387\u7684\u6d41\u91cf(~1 cpu/sec/request\uff0c\u603b\u5171100\u4e2aCPU)\u3002 hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" \u53d1\u900160\u79d2\u7684\u9ad8\u5185\u5b58\u4f7f\u7528\u6d41\u91cf(\u6bcf\u8bf7\u6c421gb\uff0c\u51715gb)\u3002 hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\"","title":"\u5176\u4ed6\u7684\u5b9e\u9a8c"},{"location":"serving/autoscaling/autoscale-go/#_10","text":"kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml","title":"\u6e05\u7406"},{"location":"serving/autoscaling/autoscale-go/#_11","text":"\u81ea\u52a8\u5b9a\u91cf\u5f00\u53d1\u4eba\u5458\u6587\u6863","title":"\u8fdb\u4e00\u6b65\u7684\u9605\u8bfb"},{"location":"serving/configuration/config-defaults/","text":"\u914d\u7f6e\u9ed8\u8ba4ConfigMap \u00b6 The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml \u4f8b\u5b50 config-defaults ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property. \u5c5e\u6027 \u00b6 \u4fee\u8ba2\u7684\u8d85\u65f6\u79d2 \u00b6 The revision timeout value determines the default number of seconds to use for the revision's per-request timeout if none is specified. Global key: revision-timeout-seconds Per-revision spec key: timeoutSeconds Possible values: integer Default: \"300\" (5 minutes) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : timeoutSeconds : 300 containers : - image : gcr.io/knative-samples/helloworld-go \u6700\u5927\u4fee\u8ba2\u8d85\u65f6\u79d2\u6570 \u00b6 The max-revision-timeout-seconds value determines the maximum number of seconds that can be used for revision-timeout-seconds . This value must be greater than or equal to revision-timeout-seconds . If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests from being disrupted. Global key: max-revision-timeout-seconds Per-revision annotation key: N/A Possible values: integer Default: \"600\" (10 minutes) Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\" \u4fee\u8ba2\u7684CPU\u8bf7\u6c42 \u00b6 The revision-cpu-request value determines the CPU allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-request Per-revision annotation key: cpu Possible values: integer Default: \"400m\" (0.4 of a CPU, or 400 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"400m\" \u4fee\u8ba2\u7684\u5185\u5b58\u8bf7\u6c42 \u00b6 The revision-memory-request value determines the memory allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-request Per-revision annotation key: memory Possible values: integer Default: \"100M\" (100 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"100M\" \u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u8bf7\u6c42 \u00b6 The revision-ephemeral-storage-request value determines the ephemeral storage allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-request Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"500M\" (500 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"500M\" \u4fee\u8ba2 CPU \u9650\u5236 \u00b6 The revision-cpu-limit value determines the default CPU allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-limit Per-revision annotation key: cpu Possible values: integer Default: \"1000m\" (1 CPU, or 1000 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"1000m\" \u4fee\u8ba2\u5185\u5b58\u9650\u5236 \u00b6 The revision-memory-limit value determines the default memory allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-limit Per-revision annotation key: memory Possible values: integer Default: \"200M\" (200 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"200M\" \u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u9650\u5236 \u00b6 The revision-ephemeral-storage-limit value determines the default ephemeral storage limit allocated to revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-limit Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"750M\" (750 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"750M\" \u5bb9\u5668\u540d\u79f0\u6a21\u677f \u00b6 The container-name-template value provides a template for the default container name if no container name is specified. This field supports Go templating and is supplied by the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Global key: container-name-template Per-revision annotation key: name Possible values: string Default: \"user-container\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go \u5bb9\u5668\u7684\u5e76\u53d1 \u00b6 The container-concurrency value specifies the maximum number of requests the container can handle at once. Requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Global key: container-concurrency Per-revision spec key: containerConcurrency Possible values: integer Default: \"0\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 0 \u5bb9\u5668\u5e76\u53d1\u6700\u5927\u9650\u5236 \u00b6 The container-concurrency-max-limit setting disables arbitrary large concurrency values, or autoscaling targets, for individual revisions. The container-concurrency default setting must be at or below this value. The value of the container-concurrency-max-limit setting must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of zero (unbounded), unless allow-container-concurrency-zero is set to \"false\" . Global key: container-concurrency-max-limit Per-revision annotation key: N/A Possible values: integer Default: \"1000\" Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\" apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : defaults : container-concurrency-max-limit : \"1000\" \u5141\u8bb8\u5bb9\u5668\u5e76\u53d1\u4e3a\u96f6 \u00b6 The allow-container-concurrency-zero value determines whether users can specify 0 (unbounded) for containerConcurrency . Global key: allow-container-concurrency-zero Per-revision annotation key: N/A Possible values: boolean Default: \"true\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\" \u4f7f\u670d\u52a1\u94fe\u63a5 \u00b6 The enable-service-links value specifies the default value used for the enableServiceLinks field of the PodSpec when it is omitted by the user. See the Kubernetes documentation about the enableServiceLinks feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of Services, it is suggested to set this value to false . See serving#8498 . Global key: enable-service-links Per-revision annotation key: N/A Possible values: true|false|default Default: \"false\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"\u914d\u7f6eConfigMap\u9ed8\u8ba4\u503c"},{"location":"serving/configuration/config-defaults/#configmap","text":"The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml","title":"\u914d\u7f6e\u9ed8\u8ba4ConfigMap"},{"location":"serving/configuration/config-defaults/#config-defaults-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property.","title":"\u4f8b\u5b50 config-defaults ConfigMap"},{"location":"serving/configuration/config-defaults/#_1","text":"","title":"\u5c5e\u6027"},{"location":"serving/configuration/config-defaults/#_2","text":"The revision timeout value determines the default number of seconds to use for the revision's per-request timeout if none is specified. Global key: revision-timeout-seconds Per-revision spec key: timeoutSeconds Possible values: integer Default: \"300\" (5 minutes) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : timeoutSeconds : 300 containers : - image : gcr.io/knative-samples/helloworld-go","title":"\u4fee\u8ba2\u7684\u8d85\u65f6\u79d2"},{"location":"serving/configuration/config-defaults/#_3","text":"The max-revision-timeout-seconds value determines the maximum number of seconds that can be used for revision-timeout-seconds . This value must be greater than or equal to revision-timeout-seconds . If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests from being disrupted. Global key: max-revision-timeout-seconds Per-revision annotation key: N/A Possible values: integer Default: \"600\" (10 minutes) Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\"","title":"\u6700\u5927\u4fee\u8ba2\u8d85\u65f6\u79d2\u6570"},{"location":"serving/configuration/config-defaults/#cpu","text":"The revision-cpu-request value determines the CPU allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-request Per-revision annotation key: cpu Possible values: integer Default: \"400m\" (0.4 of a CPU, or 400 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"400m\"","title":"\u4fee\u8ba2\u7684CPU\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#_4","text":"The revision-memory-request value determines the memory allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-request Per-revision annotation key: memory Possible values: integer Default: \"100M\" (100 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"100M\"","title":"\u4fee\u8ba2\u7684\u5185\u5b58\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#_5","text":"The revision-ephemeral-storage-request value determines the ephemeral storage allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-request Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"500M\" (500 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"500M\"","title":"\u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u8bf7\u6c42"},{"location":"serving/configuration/config-defaults/#cpu_1","text":"The revision-cpu-limit value determines the default CPU allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-cpu-limit Per-revision annotation key: cpu Possible values: integer Default: \"1000m\" (1 CPU, or 1000 milli-CPU) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : cpu : \"1000m\"","title":"\u4fee\u8ba2 CPU \u9650\u5236"},{"location":"serving/configuration/config-defaults/#_6","text":"The revision-memory-limit value determines the default memory allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-memory-limit Per-revision annotation key: memory Possible values: integer Default: \"200M\" (200 megabytes of memory) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : memory : \"200M\"","title":"\u4fee\u8ba2\u5185\u5b58\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_7","text":"The revision-ephemeral-storage-limit value determines the default ephemeral storage limit allocated to revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative. Global key: revision-ephemeral-storage-limit Per-revision annotation key: ephemeral-storage Possible values: integer Default: \"750M\" (750 megabytes of storage) Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go resources : requests : ephemeral-storage : \"750M\"","title":"\u4fee\u8ba2\u4e34\u65f6\u5b58\u50a8\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_8","text":"The container-name-template value provides a template for the default container name if no container name is specified. This field supports Go templating and is supplied by the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Global key: container-name-template Per-revision annotation key: name Possible values: string Default: \"user-container\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go","title":"\u5bb9\u5668\u540d\u79f0\u6a21\u677f"},{"location":"serving/configuration/config-defaults/#_9","text":"The container-concurrency value specifies the maximum number of requests the container can handle at once. Requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Global key: container-concurrency Per-revision spec key: containerConcurrency Possible values: integer Default: \"0\" Example: Global (ConfigMap) Per Revision apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 0","title":"\u5bb9\u5668\u7684\u5e76\u53d1"},{"location":"serving/configuration/config-defaults/#_10","text":"The container-concurrency-max-limit setting disables arbitrary large concurrency values, or autoscaling targets, for individual revisions. The container-concurrency default setting must be at or below this value. The value of the container-concurrency-max-limit setting must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of zero (unbounded), unless allow-container-concurrency-zero is set to \"false\" . Global key: container-concurrency-max-limit Per-revision annotation key: N/A Possible values: integer Default: \"1000\" Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\" apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : defaults : container-concurrency-max-limit : \"1000\"","title":"\u5bb9\u5668\u5e76\u53d1\u6700\u5927\u9650\u5236"},{"location":"serving/configuration/config-defaults/#_11","text":"The allow-container-concurrency-zero value determines whether users can specify 0 (unbounded) for containerConcurrency . Global key: allow-container-concurrency-zero Per-revision annotation key: N/A Possible values: boolean Default: \"true\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\"","title":"\u5141\u8bb8\u5bb9\u5668\u5e76\u53d1\u4e3a\u96f6"},{"location":"serving/configuration/config-defaults/#_12","text":"The enable-service-links value specifies the default value used for the enableServiceLinks field of the PodSpec when it is omitted by the user. See the Kubernetes documentation about the enableServiceLinks feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of Services, it is suggested to set this value to false . See serving#8498 . Global key: enable-service-links Per-revision annotation key: N/A Possible values: true|false|default Default: \"false\" Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"\u4f7f\u670d\u52a1\u94fe\u63a5"},{"location":"serving/configuration/deployment/","text":"Configure Deployment resources \u00b6 The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml Example config-deployment ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\" Configuring progress deadlines \u00b6 Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\" Skipping tag resolution \u00b6 You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com Enable container-freezer service \u00b6 You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"\u914d\u7f6e\u90e8\u7f72\u8d44\u6e90"},{"location":"serving/configuration/deployment/#configure-deployment-resources","text":"The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml","title":"Configure Deployment resources"},{"location":"serving/configuration/deployment/#example-config-deployment-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\"","title":"Example config-deployment ConfigMap"},{"location":"serving/configuration/deployment/#configuring-progress-deadlines","text":"Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\"","title":"Configuring progress deadlines"},{"location":"serving/configuration/deployment/#skipping-tag-resolution","text":"You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com","title":"Skipping tag resolution"},{"location":"serving/configuration/deployment/#enable-container-freezer-service","text":"You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"Enable container-freezer service"},{"location":"serving/configuration/feature-flags/","text":"Feature and extension flags \u00b6 The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API. Configuring flags \u00b6 Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used. Lifecycle \u00b6 When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases. Feature lifecycle stages \u00b6 Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative. Extension lifecycle stages \u00b6 An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default. Available Flags \u00b6 Multiple containers \u00b6 Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java Kubernetes EmptyDir Volume \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {} Kubernetes PersistentVolumeClaim (PVC) \u00b6 Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true Kubernetes node affinity \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2 Kubernetes host aliases \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" Kubernetes node selector \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue Kubernetes toleration \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Kubernetes Downward API \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName Kubernetes priority class name \u00b6 Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ... Kubernetes dry run \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun: enabled annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun : enabled ... Kubernetes runtime class \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-runtimeclassname This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ... Kubernetes security context \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ... Kubernetes security context capabilities \u00b6 Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE Tag header based routing \u00b6 Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled. Kubernetes init containers \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ... Queue Proxy Pod Info \u00b6 Type : Extension ConfigMap key: queueproxy.mount-podinfo You must set this feature to either \"enabled or \"allowed\" when using QPOptions. The flag controls whether Knative mounts the pod-info volume to the queue-proxy container. Mounting the pod-info volume allows extensions that use QPOptions to access the Service annotations, by reading the /etc/podinfo/annnotations file. See Extending Queue Proxy image with QPOptions for more details. When this feature is enabled , the pod-info volume is always mounted. This is helpful in case where all or most of the cluster Services are required to use extensions that rely on QPOptions. When this feature is allowed , the pod-info volume is not mounted by default. Instead, the volume is mounted only for Services that add the features.knative.dev/queueproxy-podinfo: enabled annotation as shown below: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled ... Kubernetes Topology Spread Constraints \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-topologyspreadconstraints This flag controls whether topology spread constraints can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... topologySpreadConstraints : - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar ... Kubernetes DNS Policy \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-dnspolicy This flag controls whether a DNS policy can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : dnsPolicy : ClusterFirstWithHostNet ... Kubernetes Scheduler Name \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-schedulername This flag controls whether a scheduler name can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... schedulerName : custom-scheduler-example ...","title":"\u7279\u6027\u548c\u6269\u5c55\u6807\u5fd7"},{"location":"serving/configuration/feature-flags/#feature-and-extension-flags","text":"The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API.","title":"Feature and extension flags"},{"location":"serving/configuration/feature-flags/#configuring-flags","text":"Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used.","title":"Configuring flags"},{"location":"serving/configuration/feature-flags/#lifecycle","text":"When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases.","title":"Lifecycle"},{"location":"serving/configuration/feature-flags/#feature-lifecycle-stages","text":"Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative.","title":"Feature lifecycle stages"},{"location":"serving/configuration/feature-flags/#extension-lifecycle-stages","text":"An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default.","title":"Extension lifecycle stages"},{"location":"serving/configuration/feature-flags/#available-flags","text":"","title":"Available Flags"},{"location":"serving/configuration/feature-flags/#multiple-containers","text":"Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java","title":"Multiple containers"},{"location":"serving/configuration/feature-flags/#kubernetes-emptydir-volume","text":"Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {}","title":"Kubernetes EmptyDir Volume"},{"location":"serving/configuration/feature-flags/#kubernetes-persistentvolumeclaim-pvc","text":"Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true","title":"Kubernetes PersistentVolumeClaim (PVC)"},{"location":"serving/configuration/feature-flags/#kubernetes-node-affinity","text":"Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2","title":"Kubernetes node affinity"},{"location":"serving/configuration/feature-flags/#kubernetes-host-aliases","text":"Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\"","title":"Kubernetes host aliases"},{"location":"serving/configuration/feature-flags/#kubernetes-node-selector","text":"Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue","title":"Kubernetes node selector"},{"location":"serving/configuration/feature-flags/#kubernetes-toleration","text":"Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\"","title":"Kubernetes toleration"},{"location":"serving/configuration/feature-flags/#kubernetes-downward-api","text":"Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName","title":"Kubernetes Downward API"},{"location":"serving/configuration/feature-flags/#kubernetes-priority-class-name","text":"Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ...","title":"Kubernetes priority class name"},{"location":"serving/configuration/feature-flags/#kubernetes-dry-run","text":"Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun: enabled annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun : enabled ...","title":"Kubernetes dry run"},{"location":"serving/configuration/feature-flags/#kubernetes-runtime-class","text":"Type : Extension ConfigMap key: kubernetes.podspec-runtimeclassname This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ...","title":"Kubernetes runtime class"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context","text":"Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ...","title":"Kubernetes security context"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context-capabilities","text":"Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE","title":"Kubernetes security context capabilities"},{"location":"serving/configuration/feature-flags/#tag-header-based-routing","text":"Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Tag header based routing"},{"location":"serving/configuration/feature-flags/#kubernetes-init-containers","text":"Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ...","title":"Kubernetes init containers"},{"location":"serving/configuration/feature-flags/#queue-proxy-pod-info","text":"Type : Extension ConfigMap key: queueproxy.mount-podinfo You must set this feature to either \"enabled or \"allowed\" when using QPOptions. The flag controls whether Knative mounts the pod-info volume to the queue-proxy container. Mounting the pod-info volume allows extensions that use QPOptions to access the Service annotations, by reading the /etc/podinfo/annnotations file. See Extending Queue Proxy image with QPOptions for more details. When this feature is enabled , the pod-info volume is always mounted. This is helpful in case where all or most of the cluster Services are required to use extensions that rely on QPOptions. When this feature is allowed , the pod-info volume is not mounted by default. Instead, the volume is mounted only for Services that add the features.knative.dev/queueproxy-podinfo: enabled annotation as shown below: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled ...","title":"Queue Proxy Pod Info"},{"location":"serving/configuration/feature-flags/#kubernetes-topology-spread-constraints","text":"Type : Extension ConfigMap key: kubernetes.podspec-topologyspreadconstraints This flag controls whether topology spread constraints can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... topologySpreadConstraints : - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar ...","title":"Kubernetes Topology Spread Constraints"},{"location":"serving/configuration/feature-flags/#kubernetes-dns-policy","text":"Type : Extension ConfigMap key: kubernetes.podspec-dnspolicy This flag controls whether a DNS policy can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : dnsPolicy : ClusterFirstWithHostNet ...","title":"Kubernetes DNS Policy"},{"location":"serving/configuration/feature-flags/#kubernetes-scheduler-name","text":"Type : Extension ConfigMap key: kubernetes.podspec-schedulername This flag controls whether a scheduler name can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... schedulerName : custom-scheduler-example ...","title":"Kubernetes Scheduler Name"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/","text":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\" \u8def\u7531\u66f4\u65b0\u72b6\u6001 \u00b6 \u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready \u591a\u4e2a\u5ef6\u5c55 \u00b6 \u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_1","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"\u914d\u7f6e\u9010\u6b65\u5411\u4fee\u8ba2\u7248\u63a8\u51fa\u6d41\u91cf"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#procedure","text":"You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_2","text":"\u5728rollout\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u66f4\u65b0\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u3002 traffic and conditions \u72b6\u6001\u53c2\u6570\u90fd\u53d7\u5230\u5f71\u54cd\u3002 \u4ee5\u4ee5\u4e0b\u6d41\u91cf\u914d\u7f6e\u4e3a\u4f8b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u6700\u521d\uff0c1%\u7684\u6d41\u91cf\u88ab\u63a8\u51fa\u5230\u4fee\u8ba2: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. \u7136\u540e\u5176\u4f59\u7684\u6d41\u91cf\u4ee518%\u7684\u589e\u91cf\u63a8\u51fa: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u7ee7\u7eed\u8fdb\u884c\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u6d41\u91cf\u914d\u7f6e: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. rollout\u8fc7\u7a0b\u4e2d\uff0c\u8def\u7531\u548cKnative\u670d\u52a1\u72b6\u6001\u6761\u4ef6\u5982\u4e0b: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"\u8def\u7531\u66f4\u65b0\u72b6\u6001"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#_3","text":"\u5982\u679c\u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fee\u8ba2\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c06\u7acb\u5373\u5f00\u59cb\u5c06\u6d41\u91cf\u8f6c\u79fb\u5230\u6700\u65b0\u7684\u4fee\u8ba2\uff0c\u5e76\u5c06\u4e0d\u5b8c\u6574\u7684\u5b9e\u65bd\u4ece\u6700\u65b0\u7684\u8f6c\u79fb\u5230\u6700\u65e7\u7684\u3002","title":"\u591a\u4e2a\u5ef6\u5c55"},{"location":"serving/load-balancing/","text":"Load balancing \u00b6 You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled. Activator pod selection \u00b6 Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency . Configuring target burst capacity \u00b6 Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"\u5bf9\u8d1f\u8f7d\u5e73\u8861"},{"location":"serving/load-balancing/#load-balancing","text":"You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled.","title":"Load balancing"},{"location":"serving/load-balancing/#activator-pod-selection","text":"Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency .","title":"Activator pod selection"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","text":"Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/activator-capacity/","text":"Configuring Activator capacity \u00b6 If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity. Setting the Activator capacity \u00b6 Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"\u914d\u7f6e\u6fc0\u6d3b\u80fd\u529b"},{"location":"serving/load-balancing/activator-capacity/#configuring-activator-capacity","text":"If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity.","title":"Configuring Activator capacity"},{"location":"serving/load-balancing/activator-capacity/#setting-the-activator-capacity","text":"Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"Setting the Activator capacity"},{"location":"serving/load-balancing/target-burst-capacity/","text":"Configuring target burst capacity \u00b6 Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the target burst capacity \u00b6 Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"\u914d\u7f6e\u76ee\u6807\u7a81\u53d1\u5bb9\u91cf"},{"location":"serving/load-balancing/target-burst-capacity/#configuring-target-burst-capacity","text":"Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","text":"Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Setting the target burst capacity"},{"location":"serving/observability/logging/collecting-logs/","text":"\u65e5\u5fd7 \u00b6 You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders. \u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6 \u00b6 Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath. \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. \u8fc7\u7a0b \u00b6 Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 \u5efa\u7acb\u8d27\u4ee3 \u00b6 See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True \u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668 \u00b6 Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in. Docker \u684c\u9762 \u00b6 Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"\u6536\u96c6\u65e5\u5fd7"},{"location":"serving/observability/logging/collecting-logs/#_1","text":"You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.","title":"\u65e5\u5fd7"},{"location":"serving/observability/logging/collecting-logs/#_2","text":"Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.","title":"\u8bbe\u7f6e\u65e5\u5fd7\u8bb0\u5f55\u7ec4\u4ef6"},{"location":"serving/observability/logging/collecting-logs/#_3","text":"The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"serving/observability/logging/collecting-logs/#_4","text":"Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"\u8fc7\u7a0b"},{"location":"serving/observability/logging/collecting-logs/#_5","text":"See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"\u5efa\u7acb\u8d27\u4ee3"},{"location":"serving/observability/logging/collecting-logs/#_6","text":"Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"\u8bbe\u7f6e\u672c\u5730\u6536\u96c6\u5668"},{"location":"serving/observability/logging/collecting-logs/#kind","text":"When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in.","title":"Kind"},{"location":"serving/observability/logging/collecting-logs/#docker","text":"Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker \u684c\u9762"},{"location":"serving/observability/logging/collecting-logs/#minikube","text":"Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Minikube"},{"location":"serving/observability/logging/config-logging/","text":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e \u00b6 Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bb0\u5f55"},{"location":"serving/observability/logging/config-logging/#_1","text":"Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"\u914d\u7f6e\u65e5\u5fd7\u8bbe\u7f6e"},{"location":"serving/observability/metrics/collecting-metrics/","text":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf \u00b6 Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics. \u5173\u4e8e Prometheus \u00b6 Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used. \u914d\u7f6e Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml \u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . \u75ca\u6108 OpenTelemetry \u00b6 OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries. \u7406\u89e3\u6536\u96c6\u5668 \u00b6 The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector. \u8bbe\u7f6e\u6536\u96c6\u5668 \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' \u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u6536\u96c6\u5ea6\u91cf\u6807\u51c6"},{"location":"serving/observability/metrics/collecting-metrics/#knative","text":"Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics.","title":"\u5728Knative\u4e2d\u6536\u96c6\u5ea6\u91cf"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus","text":"Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.","title":"\u5173\u4e8e Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus_1","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"\u914d\u7f6e Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#prometheus_2","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"\u672c\u5730\u8bbf\u95eePrometheus\u5b9e\u4f8b"},{"location":"serving/observability/metrics/collecting-metrics/#opentelemetry","text":"OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.","title":"\u75ca\u6108 OpenTelemetry"},{"location":"serving/observability/metrics/collecting-metrics/#_1","text":"The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.","title":"\u7406\u89e3\u6536\u96c6\u5668"},{"location":"serving/observability/metrics/collecting-metrics/#_2","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"\u8bbe\u7f6e\u6536\u96c6\u5668"},{"location":"serving/observability/metrics/collecting-metrics/#_3","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"\u9a8c\u8bc1\u6536\u96c6\u5668\u8bbe\u7f6e"},{"location":"serving/observability/metrics/serving-metrics/","text":"Knative Serving metrics \u00b6 Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next. Activator \u00b6 The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable Autoscaler \u00b6 Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable Controller \u00b6 The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable Webhook \u00b6 Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable Go Runtime - memstats \u00b6 Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Knative\u670d\u52a1\u6307\u6807"},{"location":"serving/observability/metrics/serving-metrics/#knative-serving-metrics","text":"Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next.","title":"Knative Serving metrics"},{"location":"serving/observability/metrics/serving-metrics/#activator","text":"The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable","title":"Activator"},{"location":"serving/observability/metrics/serving-metrics/#autoscaler","text":"Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable","title":"Autoscaler"},{"location":"serving/observability/metrics/serving-metrics/#controller","text":"The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable","title":"Controller"},{"location":"serving/observability/metrics/serving-metrics/#webhook","text":"Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable","title":"Webhook"},{"location":"serving/observability/metrics/serving-metrics/#go-runtime-memstats","text":"Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Go Runtime - memstats"},{"location":"serving/reference/serving-api/","text":"This file is updated to the correct version from the serving repo (docs/serving-api.md) during the build.","title":"\u670d\u52a1API"},{"location":"serving/revisions/","text":"\u5173\u4e8e\u4fee\u8ba2 \u00b6 \u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002 \u989d\u5916\u8d44\u6e90 \u00b6 \u4fee\u8ba2\u6982\u5ff5\u6587\u6863","title":"\u5173\u4e8e\u4fee\u8ba2"},{"location":"serving/revisions/#_1","text":"\u4fee\u8ba2\u662f Knative \u670d\u52a1\u8d44\u6e90\uff0c\u5176\u4e2d\u5305\u542b\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff0c\u4ee5\u53ca\u5bf9 Knative \u670d\u52a1\u6240\u505a\u7684\u6bcf\u6b21\u66f4\u6539\u7684\u914d\u7f6e\u3002 \u60a8\u4e0d\u80fd\u76f4\u63a5\u521b\u5efa\u4fee\u8ba2\u6216\u66f4\u65b0\u4fee\u8ba2\u89c4\u8303;\u4fee\u8ba2\u603b\u662f\u6839\u636e\u914d\u7f6e\u89c4\u8303\u7684\u66f4\u65b0\u800c\u521b\u5efa\u7684\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5f3a\u5236\u5220\u9664\u4fee\u8ba2\uff0c\u4ee5\u5904\u7406\u6cc4\u6f0f\u7684\u8d44\u6e90\uff0c\u4ee5\u53ca\u5220\u9664\u5df2\u77e5\u7684\u574f\u4fee\u8ba2\uff0c\u4ee5\u907f\u514d\u5728\u7ba1\u7406 Knative Service \u65f6\u51fa\u73b0\u672a\u6765\u7684\u9519\u8bef\u3002 \u4fee\u8ba2\u901a\u5e38\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u9664\u975e\u5b83\u4eec\u53ef\u80fd\u5f15\u7528\u53ef\u53d8\u7684\u6838\u5fc3 Kubernetes \u8d44\u6e90\uff0c\u5982 ConfigMaps \u548c Secrets\u3002 \u4fee\u6539\u7248\u672c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u7248\u672c\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u800c\u53d1\u751f\u7a81\u53d8\u3002 \u5bf9\u9ed8\u8ba4\u503c\u7684\u66f4\u6539\u4f1a\u4f7f\u4fee\u8ba2\u7248\u672c\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u901a\u5e38\u662f\u8bed\u6cd5\u4e0a\u7684\uff0c\u800c\u4e0d\u662f\u8bed\u4e49\u4e0a\u7684\u3002","title":"\u5173\u4e8e\u4fee\u8ba2"},{"location":"serving/revisions/#_2","text":"\u4fee\u8ba2\u6982\u5ff5\u6587\u6863","title":"\u989d\u5916\u8d44\u6e90"},{"location":"serving/revisions/revision-admin-config-options/","text":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a Knative Services \u7684 revision \u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 config-gc ConfigMap \u6765\u8bbe\u7f6e\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u914d\u7f6e\u3002 \u53ef\u4ee5\u4fee\u6539\u4ee5\u4e0b\u5783\u573e\u6536\u96c6\u8bbe\u7f6e: Name Description retain-since-create-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u521b\u5efa\u4fee\u8ba2\u5f00\u59cb\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 retain-since-last-active-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u4fee\u8ba2\u6700\u540e\u4e00\u6b21\u6d3b\u52a8\u5230\u4fee\u8ba2\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 min-non-active-revisions \u8981\u4fdd\u7559\u7684\u975e\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5c0f\u6570\u91cf\u3002 max-non-active-revisions \u8981\u4fdd\u7559\u7684\u672a\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5927\u6570\u91cf\u3002 \u5982\u679c\u4fee\u8ba2\u7248\u5c5e\u4e8e\u4e0b\u5217\u4efb\u4f55\u4e00\u7c7b\uff0c\u5219\u59cb\u7ec8\u4fdd\u7559: \u4fee\u8ba2\u662f\u6d3b\u52a8\u7684\uff0c\u5e76\u7531\u8def\u7531\u5f15\u7528\u3002 \u5728 retain-since-create-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u521b\u5efa\u4fee\u8ba2\u3002 \u5728 retain-since-last-active-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\uff0c\u8def\u7531\u6700\u540e\u4e00\u6b21\u5f15\u7528\u4fee\u8ba2\u3002 \u73b0\u6709\u4fee\u8ba2\u7684\u6570\u91cf\u5c11\u4e8e min-non-active-revisions \u8bbe\u7f6e\u6240\u6307\u5b9a\u7684\u6570\u91cf\u3002 \u4f8b\u5b50 \u00b6 \u7acb\u5373\u6e05\u7406\u4efb\u4f55\u4e0d\u6d3b\u8dc3\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" \u4fdd\u7559\u6700\u540e 10 \u4e2a\u672a\u6fc0\u6d3b\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\" \u5728\u96c6\u7fa4\u4e0a\u7981\u7528\u5783\u573e\u6536\u96c6: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\"","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-admin-config-options/#_1","text":"\u5982\u679c\u60a8\u5bf9 Knative \u5b89\u88c5\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u5219\u53ef\u4ee5\u4fee\u6539 ConfigMaps \u4ee5\u66f4\u6539\u96c6\u7fa4\u4e0a Knative Services \u7684 revision \u7684\u5168\u5c40\u9ed8\u8ba4\u914d\u7f6e\u9009\u9879\u3002","title":"\u7ba1\u7406\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-admin-config-options/#_2","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 config-gc ConfigMap \u6765\u8bbe\u7f6e\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u914d\u7f6e\u3002 \u53ef\u4ee5\u4fee\u6539\u4ee5\u4e0b\u5783\u573e\u6536\u96c6\u8bbe\u7f6e: Name Description retain-since-create-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u521b\u5efa\u4fee\u8ba2\u5f00\u59cb\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 retain-since-last-active-time \u5728\u8003\u8651\u8fdb\u884c\u5783\u573e\u6536\u96c6\u4e4b\u524d\uff0c\u4ece\u4fee\u8ba2\u6700\u540e\u4e00\u6b21\u6d3b\u52a8\u5230\u4fee\u8ba2\u5fc5\u987b\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002 min-non-active-revisions \u8981\u4fdd\u7559\u7684\u975e\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5c0f\u6570\u91cf\u3002 max-non-active-revisions \u8981\u4fdd\u7559\u7684\u672a\u6fc0\u6d3b\u4fee\u8ba2\u7684\u6700\u5927\u6570\u91cf\u3002 \u5982\u679c\u4fee\u8ba2\u7248\u5c5e\u4e8e\u4e0b\u5217\u4efb\u4f55\u4e00\u7c7b\uff0c\u5219\u59cb\u7ec8\u4fdd\u7559: \u4fee\u8ba2\u662f\u6d3b\u52a8\u7684\uff0c\u5e76\u7531\u8def\u7531\u5f15\u7528\u3002 \u5728 retain-since-create-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u521b\u5efa\u4fee\u8ba2\u3002 \u5728 retain-since-last-active-time \u8bbe\u7f6e\u6307\u5b9a\u7684\u65f6\u95f4\u5185\uff0c\u8def\u7531\u6700\u540e\u4e00\u6b21\u5f15\u7528\u4fee\u8ba2\u3002 \u73b0\u6709\u4fee\u8ba2\u7684\u6570\u91cf\u5c11\u4e8e min-non-active-revisions \u8bbe\u7f6e\u6240\u6307\u5b9a\u7684\u6570\u91cf\u3002","title":"\u5783\u573e\u6536\u96c6"},{"location":"serving/revisions/revision-admin-config-options/#_3","text":"\u7acb\u5373\u6e05\u7406\u4efb\u4f55\u4e0d\u6d3b\u8dc3\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" \u4fdd\u7559\u6700\u540e 10 \u4e2a\u672a\u6fc0\u6d3b\u7684\u4fee\u8ba2: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\" \u5728\u96c6\u7fa4\u4e0a\u7981\u7528\u5783\u573e\u6536\u96c6: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\"","title":"\u4f8b\u5b50"},{"location":"serving/revisions/revision-developer-config-options/","text":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879 \u00b6 \u867d\u7136\u5728\u4e0d\u4fee\u6539 Knative \u670d\u52a1\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u624b\u52a8\u521b\u5efa\u4fee\u8ba2\uff0c\u4f46\u60a8\u53ef\u4ee5\u4fee\u6539\u73b0\u6709\u4fee\u8ba2\u7684\u89c4\u8303\u4ee5\u66f4\u6539\u5176\u884c\u4e3a\u3002 \u5783\u573e\u6536\u96c6 \u00b6 \u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u4e3a\u4fee\u8ba2\u7248\u7981\u7528\u5783\u573e\u6536\u96c6 \u00b6 \u4f60\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0 serving.knative.dev/no-gc: \"true\" \u6ce8\u91ca\u6765\u914d\u7f6e Revision\uff0c\u4f7f\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u5783\u573e\u56de\u6536: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\"","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-developer-config-options/#_1","text":"\u867d\u7136\u5728\u4e0d\u4fee\u6539 Knative \u670d\u52a1\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u65e0\u6cd5\u624b\u52a8\u521b\u5efa\u4fee\u8ba2\uff0c\u4f46\u60a8\u53ef\u4ee5\u4fee\u6539\u73b0\u6709\u4fee\u8ba2\u7684\u89c4\u8303\u4ee5\u66f4\u6539\u5176\u884c\u4e3a\u3002","title":"\u5f00\u53d1\u4eba\u5458\u914d\u7f6e\u9009\u9879"},{"location":"serving/revisions/revision-developer-config-options/#_2","text":"\u5f53 Knative \u670d\u52a1\u7684\u4fee\u8ba2\u7248\u5904\u4e8e\u4e0d\u6d3b\u52a8\u72b6\u6001\u65f6\uff0c\u5b83\u4eec\u5c06\u5728\u8bbe\u5b9a\u7684\u65f6\u95f4\u6bb5\u540e\u81ea\u52a8\u6e05\u7406\u5e76\u56de\u6536\u96c6\u7fa4\u8d44\u6e90\u3002 \u8fd9\u5c31\u662f\u6240\u8c13\u7684 \u5783\u573e\u6536\u96c6 . \u5982\u679c\u60a8\u662f\u5f00\u53d1\u4eba\u5458\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u7248\u672c\u4e2d\u914d\u7f6e\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u4e0a\u6240\u6709\u670d\u52a1\u7684\u6240\u6709\u90e8\u5206\u914d\u7f6e\u9ed8\u8ba4\u7684\u3001\u96c6\u7fa4\u8303\u56f4\u7684\u5783\u573e\u6536\u96c6\u53c2\u6570\u3002","title":"\u5783\u573e\u6536\u96c6"},{"location":"serving/revisions/revision-developer-config-options/#_3","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0 serving.knative.dev/no-gc: \"true\" \u6ce8\u91ca\u6765\u914d\u7f6e Revision\uff0c\u4f7f\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u5783\u573e\u56de\u6536: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\"","title":"\u4e3a\u4fee\u8ba2\u7248\u7981\u7528\u5783\u573e\u6536\u96c6"},{"location":"serving/services/","text":"About Knative Services \u00b6 Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic. Modifying Knative services \u00b6 Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present. Additional resources \u00b6 For more information about the Knative Service object, see the Resource Types documentation.","title":"\u5173\u4e8e\u670d\u52a1"},{"location":"serving/services/#about-knative-services","text":"Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.","title":"About Knative Services"},{"location":"serving/services/#modifying-knative-services","text":"Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present.","title":"Modifying Knative services"},{"location":"serving/services/#additional-resources","text":"For more information about the Knative Service object, see the Resource Types documentation.","title":"Additional resources"},{"location":"serving/services/byo-certificate/","text":"Using a custom TLS certificate for DomainMapping \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation. Prerequisites \u00b6 You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed. Procedure \u00b6 Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"\u4e3aDomainMapping\u4f7f\u7528\u81ea\u5b9a\u4e49TLS\u8bc1\u4e66"},{"location":"serving/services/byo-certificate/#using-a-custom-tls-certificate-for-domainmapping","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation.","title":"Using a custom TLS certificate for DomainMapping"},{"location":"serving/services/byo-certificate/#prerequisites","text":"You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed.","title":"Prerequisites"},{"location":"serving/services/byo-certificate/#procedure","text":"Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"Procedure"},{"location":"serving/services/certificate-class/","text":"Configuring a custom certificate class for a Service \u00b6 When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation. Using the certificate class annotation \u00b6 Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"\u914d\u7f6e\u8bc1\u4e66\u7c7b"},{"location":"serving/services/certificate-class/#configuring-a-custom-certificate-class-for-a-service","text":"When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation.","title":"Configuring a custom certificate class for a Service"},{"location":"serving/services/certificate-class/#using-the-certificate-class-annotation","text":"Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"Using the certificate class annotation"},{"location":"serving/services/configure-requests-limits-services/","text":"Configure resource requests and limits \u00b6 You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1 Additional resources \u00b6 For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"\u914d\u7f6e\u8d44\u6e90\u8bf7\u6c42\u548c\u9650\u5236"},{"location":"serving/services/configure-requests-limits-services/#configure-resource-requests-and-limits","text":"You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1","title":"Configure resource requests and limits"},{"location":"serving/services/configure-requests-limits-services/#additional-resources","text":"For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"Additional resources"},{"location":"serving/services/creating-services/","text":"Creating a Service \u00b6 You can create a Knative service by applying a YAML file or using the kn service create CLI command. Prerequisites \u00b6 To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI . Procedure \u00b6 Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"\u521b\u5efa\u670d\u52a1"},{"location":"serving/services/creating-services/#creating-a-service","text":"You can create a Knative service by applying a YAML file or using the kn service create CLI command.","title":"Creating a Service"},{"location":"serving/services/creating-services/#prerequisites","text":"To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI .","title":"Prerequisites"},{"location":"serving/services/creating-services/#procedure","text":"Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"Procedure"},{"location":"serving/services/custom-domains/","text":"Configuring custom domains \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain . Prerequisites \u00b6 You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar. Procedure \u00b6 To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u7684\u57df"},{"location":"serving/services/custom-domains/#configuring-custom-domains","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain .","title":"Configuring custom domains"},{"location":"serving/services/custom-domains/#prerequisites","text":"You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar.","title":"Prerequisites"},{"location":"serving/services/custom-domains/#procedure","text":"To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Procedure"},{"location":"serving/services/http-protocol/","text":"HTTPS redirection \u00b6 Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details. Overriding the default HTTP behavior \u00b6 You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"HTTPS\u91cd\u5b9a\u5411"},{"location":"serving/services/http-protocol/#https-redirection","text":"Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details.","title":"HTTPS redirection"},{"location":"serving/services/http-protocol/#overriding-the-default-http-behavior","text":"You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"Overriding the default HTTP behavior"},{"location":"serving/services/ingress-class/","text":"Configuring Services custom ingress class \u00b6 When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services Using the ingress class annotation \u00b6 Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"\u914d\u7f6e\u5bfc\u5165\u7c7b"},{"location":"serving/services/ingress-class/#configuring-services-custom-ingress-class","text":"When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services","title":"Configuring Services custom ingress class"},{"location":"serving/services/ingress-class/#using-the-ingress-class-annotation","text":"Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"Using the ingress class annotation"},{"location":"serving/services/private-services/","text":"Configuring private Services \u00b6 By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway. Using the cluster-local label \u00b6 To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local Example \u00b6 You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"\u914d\u7f6e\u79c1\u4eba\u670d\u52a1"},{"location":"serving/services/private-services/#configuring-private-services","text":"By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway.","title":"Configuring private Services"},{"location":"serving/services/private-services/#using-the-cluster-local-label","text":"To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local","title":"Using the cluster-local label"},{"location":"serving/services/private-services/#example","text":"You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"Example"},{"location":"serving/services/service-metrics/","text":"Service metrics \u00b6 Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side. Queue proxy metrics \u00b6 Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"\u670d\u52a1\u6307\u6807"},{"location":"serving/services/service-metrics/#service-metrics","text":"Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side.","title":"Service metrics"},{"location":"serving/services/service-metrics/#queue-proxy-metrics","text":"Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Queue proxy metrics"},{"location":"serving/services/using-queue-extensions/","text":"Using extensions enabled by QPOptions \u00b6 QPOptions is a Queue Proxy feature that enables extending Queue Proxy with additional Go packages. For example, the security-guard repository extends Queue Proxy by adding runtime security features to protect user services. Once your cluster is setup with extensions enabled by QPOptions, a Service can decide which extensions it wish to use and how to configure such extensions. Activating and configuring extensions is described here. Overview \u00b6 A Service can activate and configure extensions by adding qpoption.knative.dev/* annotations under the: spec.template.metadata of the Service Custom Resource Definition (CRD). Setting a value of: qpoption.knative.dev/<ExtensionName>-activate: \"enable\" activates the extension. Setting a value of: qpoption.knative.dev/<extension-name>-config-<key>: \"<value>\" adds a configuration of key: value to the extension. In addition, the Service must ensure that the Pod Info volume is mounted by adding the features.knative.dev/queueproxy-podinfo: enabled annotation under the: spec.template.metadata of the Service CRD. You can create a Knative Service by applying a YAML file or by using the kn service create CLI command. Prerequisites \u00b6 Before you can use extensions enabled by QPOptions, you must: Prepare your cluster: Make sure you are using a Queue Proxy image that was built with the extensions that you wish to use - See Extending Queue Proxy image with QPOptions . Make sure that the cluster config-features is set with queueproxy.mount-podinfo: allowed . See Enabling Queue Proxy Pod Info for more details. Meet the prerequisites in Creating a Service Procedure \u00b6 Tip The following commands create a helloworld-go sample Service while activating and configuring the test-gate extension for this Service. You can modify these commands, including the extension(s) to be activated and the extension configuration. Create a sample Service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/testgate-activate : enable qpoption.knative.dev/testgate-config-response : CU qpoption.knative.dev/testgate-config-sender : Joe spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env TARGET=World \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/testgate-activate=enable \\ --annotation qpoption.knative.dev/testgate-config-response=Goodbye \\ --annotation qpoption.knative.dev/testgate-config-sender=Joe After the Service has been created, Knative propagates the annotations to the podSpec of the Service deployment. When a Service pod is created, the Queue Proxy sidecar will mount a volume that contains the pod annotations and activate the testgate extension. This occurs if the testgate extension is available in the Queue Proxy image. The testgate extension will then be configured with the configuration: { sender: \"Joe\", response: \"CU\"} .","title":"\u4f7f\u7528QPOptions\u542f\u7528\u7684\u6269\u5c55"},{"location":"serving/services/using-queue-extensions/#using-extensions-enabled-by-qpoptions","text":"QPOptions is a Queue Proxy feature that enables extending Queue Proxy with additional Go packages. For example, the security-guard repository extends Queue Proxy by adding runtime security features to protect user services. Once your cluster is setup with extensions enabled by QPOptions, a Service can decide which extensions it wish to use and how to configure such extensions. Activating and configuring extensions is described here.","title":"Using extensions enabled by QPOptions"},{"location":"serving/services/using-queue-extensions/#overview","text":"A Service can activate and configure extensions by adding qpoption.knative.dev/* annotations under the: spec.template.metadata of the Service Custom Resource Definition (CRD). Setting a value of: qpoption.knative.dev/<ExtensionName>-activate: \"enable\" activates the extension. Setting a value of: qpoption.knative.dev/<extension-name>-config-<key>: \"<value>\" adds a configuration of key: value to the extension. In addition, the Service must ensure that the Pod Info volume is mounted by adding the features.knative.dev/queueproxy-podinfo: enabled annotation under the: spec.template.metadata of the Service CRD. You can create a Knative Service by applying a YAML file or by using the kn service create CLI command.","title":"Overview"},{"location":"serving/services/using-queue-extensions/#prerequisites","text":"Before you can use extensions enabled by QPOptions, you must: Prepare your cluster: Make sure you are using a Queue Proxy image that was built with the extensions that you wish to use - See Extending Queue Proxy image with QPOptions . Make sure that the cluster config-features is set with queueproxy.mount-podinfo: allowed . See Enabling Queue Proxy Pod Info for more details. Meet the prerequisites in Creating a Service","title":"Prerequisites"},{"location":"serving/services/using-queue-extensions/#procedure","text":"Tip The following commands create a helloworld-go sample Service while activating and configuring the test-gate extension for this Service. You can modify these commands, including the extension(s) to be activated and the extension configuration. Create a sample Service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : features.knative.dev/queueproxy-podinfo : enabled qpoption.knative.dev/testgate-activate : enable qpoption.knative.dev/testgate-config-response : CU qpoption.knative.dev/testgate-config-sender : Joe spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"World\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go \\ --image gcr.io/knative-samples/helloworld-go \\ --env TARGET=World \\ --annotation features.knative.dev/queueproxy-podinfo=enabled \\ --annotation qpoption.knative.dev/testgate-activate=enable \\ --annotation qpoption.knative.dev/testgate-config-response=Goodbye \\ --annotation qpoption.knative.dev/testgate-config-sender=Joe After the Service has been created, Knative propagates the annotations to the podSpec of the Service deployment. When a Service pod is created, the Queue Proxy sidecar will mount a volume that contains the pod annotations and activate the testgate extension. This occurs if the testgate extension is available in the Queue Proxy image. The testgate extension will then be configured with the configuration: { sender: \"Joe\", response: \"CU\"} .","title":"Procedure"},{"location":"serving/troubleshooting/debugging-application-issues/","text":"Debugging application issues \u00b6 If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application. Check terminal output \u00b6 Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1 Check Route status \u00b6 Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting . Check Ingress/Istio routing \u00b6 To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familiar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide . Check Ingress status \u00b6 Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue. Check Revision status \u00b6 If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status . Check Pod status \u00b6 To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"\u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898"},{"location":"serving/troubleshooting/debugging-application-issues/#debugging-application-issues","text":"If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application.","title":"Debugging application issues"},{"location":"serving/troubleshooting/debugging-application-issues/#check-terminal-output","text":"Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1","title":"Check terminal output"},{"location":"serving/troubleshooting/debugging-application-issues/#check-route-status","text":"Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting .","title":"Check Route status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingressistio-routing","text":"To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familiar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide .","title":"Check Ingress/Istio routing"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingress-status","text":"Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.","title":"Check Ingress status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-revision-status","text":"If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status .","title":"Check Revision status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-pod-status","text":"To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Check Pod status"}]}